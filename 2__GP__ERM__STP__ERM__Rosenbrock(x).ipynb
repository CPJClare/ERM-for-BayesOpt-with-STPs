{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rosenbrock synthetic function:\n",
    "\n",
    "GP ERM versus STP nu = 5 ERM (winner)\n",
    "\n",
    "https://www.sfu.ca/~ssurjano/sumsqu.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import modules:\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "from pyGPGO.logger import EventLogger\n",
    "from pyGPGO.GPGO import GPGO\n",
    "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\n",
    "from pyGPGO.surrogates.tStudentProcess import tStudentProcess, logpdf\n",
    "from pyGPGO.acquisition import Acquisition\n",
    "from pyGPGO.covfunc import squaredExponential\n",
    "\n",
    "from collections import OrderedDict\n",
    "from joblib import Parallel, delayed\n",
    "from numpy.linalg import slogdet, inv, cholesky, solve\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.special import gamma\n",
    "from scipy.stats import norm, t\n",
    "from matplotlib.pyplot import rc\n",
    "\n",
    "rc('text', usetex=False)\n",
    "plt.rcParams['text.latex.preamble']=[r'\\usepackage{amsmath}']\n",
    "plt.rcParams['text.latex.preamble'] = [r'\\boldmath']\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inputs:\n",
    "\n",
    "obj_func = 'Rosenbrock'\n",
    "n_test = 50 # test points\n",
    "df = 5 # nu\n",
    "\n",
    "util_loser = 'RegretMinimized'\n",
    "util_winner = 'tRegretMinimized'\n",
    "n_init = 5 # random initialisations\n",
    "\n",
    "cov_func = squaredExponential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Objective function:\n",
    "\n",
    "if obj_func == 'Rosenbrock':\n",
    "            \n",
    "    # True y bounds:\n",
    "    y_lb = 0\n",
    "    operator = -1 # targets global minimum \n",
    "    y_global_orig = y_lb * operator # targets global minimum\n",
    "            \n",
    "# Constraints:\n",
    "    lb = -2.048 \n",
    "    ub = +2.048 \n",
    "    \n",
    "# Input array dimension(s):\n",
    "    dim = 2\n",
    "\n",
    "# 2-D inputs' parameter bounds:\n",
    "    param = {'x1_training': ('cont', [lb, ub]),\n",
    "             'x2_training': ('cont', [lb, ub])}\n",
    "    \n",
    "    max_iter = (10 * dim)*0 + 100  # iterations of Bayesian optimisation\n",
    "    \n",
    "# Test data:\n",
    "    x1_test = np.linspace(lb, ub, n_test)\n",
    "    x2_test = np.linspace(lb, ub, n_test)\n",
    "    Xstar_d = np.column_stack((x1_test, x2_test))\n",
    "    \n",
    "    def f_syn_polarity(x1_training, x2_training):\n",
    "        return operator * (100 * (x2_training - x1_training ** 2) ** 2 + (x1_training - 1) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cumulative Regret Calculator:\n",
    "\n",
    "def min_max_array(x):\n",
    "    new_list = []\n",
    "    for i, num in enumerate(x):\n",
    "            new_list.append(np.min(x[0:i+1]))\n",
    "    return new_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set-seeds:\n",
    "\n",
    "run_num_1 = 111\n",
    "run_num_2 = 113\n",
    "run_num_3 = 3333\n",
    "run_num_4 = 444\n",
    "run_num_5 = 5555\n",
    "run_num_6 = 6\n",
    "run_num_7 = 777\n",
    "run_num_8 = 887\n",
    "run_num_9 = 99\n",
    "run_num_10 = 1000\n",
    "run_num_11 = 1113\n",
    "run_num_12 = 1234\n",
    "run_num_13 = 2345\n",
    "run_num_14 = 88\n",
    "run_num_15 = 1556\n",
    "run_num_16 = 1666\n",
    "run_num_17 = 717\n",
    "run_num_18 = 8\n",
    "run_num_19 = 1998\n",
    "run_num_20 = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Acquisition function - ERM:\n",
    "\n",
    "class Acquisition_new(Acquisition):    \n",
    "    def __init__(self, mode, eps=1e-06, **params):\n",
    "        \n",
    "        self.params = params\n",
    "        self.eps = eps\n",
    "\n",
    "        mode_dict = {\n",
    "            'RegretMinimized': self.RegretMinimized,\n",
    "            'tRegretMinimized': self.tRegretMinimized\n",
    "        }\n",
    "\n",
    "        self.f = mode_dict[mode]\n",
    "   \n",
    "    def RegretMinimized(self, tau, mean, std):\n",
    "        \n",
    "        z = (mean - y_global_orig - self.eps) / (std + self.eps)\n",
    "        return z * (std + self.eps) * norm.cdf(z) + (std + self.eps) * norm.pdf(z)[0]\n",
    "    \n",
    "    def tRegretMinimized(self, tau, mean, std, nu=3.0):\n",
    "        \n",
    "        gamma = (mean - y_global_orig - self.eps) / (std + self.eps)\n",
    "        return gamma * (std + self.eps) * t.cdf(gamma, df=nu) + (std + self.eps) * (nu + gamma ** 2)/(nu - 1) * t.pdf(gamma, df=nu)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.45944904 -1.35549029]. \t  -245.71064611316496 \t -108.5713485785257\n",
      "init   \t [-0.26190226  1.10289909]. \t  -108.5713485785257 \t -108.5713485785257\n",
      "init   \t [-0.83834755 -1.43702853]. \t  -461.2775269355244 \t -108.5713485785257\n",
      "init   \t [-1.95592878 -0.32676048]. \t  -1732.9949421003257 \t -108.5713485785257\n",
      "init   \t [-1.07035795 -0.66496024]. \t  -332.12317010404104 \t -108.5713485785257\n",
      "1      \t [-0.22246619 -0.23774559]. \t  \u001b[92m-9.744921046208802\u001b[0m \t -9.744921046208802\n",
      "2      \t [-0.44259375 -0.41212289]. \t  -39.0489503873149 \t -9.744921046208802\n",
      "3      \t [ 1.38387448 -0.09997246]. \t  -406.2025216458289 \t -9.744921046208802\n",
      "4      \t [-0.16665514  0.08681738]. \t  \u001b[92m-1.7096970021626978\u001b[0m \t -1.7096970021626978\n",
      "5      \t [ 1.53746021 -2.048     ]. \t  -1946.6725713151652 \t -1.7096970021626978\n",
      "6      \t [0.0419833  0.29493492]. \t  -9.512796927906653 \t -1.7096970021626978\n",
      "7      \t [ 1.78803192 -0.60648493]. \t  -1447.3149830671293 \t -1.7096970021626978\n",
      "8      \t [1.53108318 1.0603856 ]. \t  -165.10401995611414 \t -1.7096970021626978\n",
      "9      \t [0.58686429 0.11291367]. \t  -5.529721991981178 \t -1.7096970021626978\n",
      "10     \t [-1.46718824  0.91875406]. \t  -158.33479736047443 \t -1.7096970021626978\n",
      "11     \t [-1.74048967  1.90210614]. \t  -134.5678515165159 \t -1.7096970021626978\n",
      "12     \t [-0.94792724  1.03619755]. \t  -5.688663462388562 \t -1.7096970021626978\n",
      "13     \t [ 0.24301181 -0.13160576]. \t  -4.208173503027023 \t -1.7096970021626978\n",
      "14     \t [0.9684561 1.7270646]. \t  -62.27793206271763 \t -1.7096970021626978\n",
      "15     \t [-0.13798151  1.2120092 ]. \t  -143.61281532895606 \t -1.7096970021626978\n",
      "16     \t [-0.43805569  0.3064678 ]. \t  -3.380747551456502 \t -1.7096970021626978\n",
      "17     \t [1.02381932 2.048     ]. \t  -99.95937133528344 \t -1.7096970021626978\n",
      "18     \t [ 0.74775298 -0.63960254]. \t  -143.7606812354837 \t -1.7096970021626978\n",
      "19     \t [-1.27748654  1.27771342]. \t  -17.736848720672292 \t -1.7096970021626978\n",
      "20     \t [1.58184171 1.11098514]. \t  -193.89287342355146 \t -1.7096970021626978\n",
      "21     \t [0.84228194 0.70428355]. \t  \u001b[92m-0.027532719372608695\u001b[0m \t -0.027532719372608695\n",
      "22     \t [0.80534723 0.54015208]. \t  -1.2136413443257084 \t -0.027532719372608695\n",
      "23     \t [1.06089113 1.82174536]. \t  -48.480860367576014 \t -0.027532719372608695\n",
      "24     \t [-1.92372447 -0.91331464]. \t  -2137.475875853538 \t -0.027532719372608695\n",
      "25     \t [-0.53298446  0.06339036]. \t  -7.2200991789372235 \t -0.027532719372608695\n",
      "26     \t [-0.08235995  0.39477545]. \t  -16.22530470798019 \t -0.027532719372608695\n",
      "27     \t [-1.13241259  1.7134821 ]. \t  -23.133958372920567 \t -0.027532719372608695\n",
      "28     \t [-1.02080879 -0.13940014]. \t  -143.66625118755718 \t -0.027532719372608695\n",
      "29     \t [1.03733951 1.09386625]. \t  -0.033053331883434386 \t -0.027532719372608695\n",
      "30     \t [0.95956921 0.90708699]. \t  \u001b[92m-0.020365513722093033\u001b[0m \t -0.020365513722093033\n",
      "31     \t [0.9692753  0.93453261]. \t  \u001b[92m-0.003406144889479281\u001b[0m \t -0.003406144889479281\n",
      "32     \t [0.53386496 0.02594546]. \t  -6.928818537696993 \t -0.003406144889479281\n",
      "33     \t [-1.39253331 -0.59085225]. \t  -645.8148494698042 \t -0.003406144889479281\n",
      "34     \t [-0.85840066  1.22147162]. \t  -26.939300528904813 \t -0.003406144889479281\n",
      "35     \t [ 0.81411133 -0.95859836]. \t  -262.92044163522456 \t -0.003406144889479281\n",
      "36     \t [-1.23462122  0.38938055]. \t  -133.79537733532314 \t -0.003406144889479281\n",
      "37     \t [0.68281647 0.45551651]. \t  -0.11210114314710315 \t -0.003406144889479281\n",
      "38     \t [-0.26683513 -0.66227277]. \t  -55.40324609566223 \t -0.003406144889479281\n",
      "39     \t [0.96814601 1.03308968]. \t  -0.9184525702177563 \t -0.003406144889479281\n",
      "40     \t [-1.53740048  1.2231408 ]. \t  -136.50317180697115 \t -0.003406144889479281\n",
      "41     \t [1.9426309 1.9633803]. \t  -328.65586656807943 \t -0.003406144889479281\n",
      "42     \t [0.08183052 1.23046565]. \t  -150.60419279831945 \t -0.003406144889479281\n",
      "43     \t [-1.16223016 -1.42849377]. \t  -777.1109183233424 \t -0.003406144889479281\n",
      "44     \t [-1.04316404  1.15873988]. \t  -4.672230813198803 \t -0.003406144889479281\n",
      "45     \t [0.74327672 1.96714473]. \t  -200.19911334585981 \t -0.003406144889479281\n",
      "46     \t [-2.01047749 -0.74297611]. \t  -2298.681505002316 \t -0.003406144889479281\n",
      "47     \t [-1.21082569  1.68390588]. \t  -9.631740102570523 \t -0.003406144889479281\n",
      "48     \t [-1.33437787  0.68815167]. \t  -124.78585917988336 \t -0.003406144889479281\n",
      "49     \t [-0.23094754 -1.61276938]. \t  -279.10620252253784 \t -0.003406144889479281\n",
      "50     \t [0.66014216 1.64749383]. \t  -146.93868689891644 \t -0.003406144889479281\n",
      "51     \t [-0.43226986  1.33958364]. \t  -134.92921491596502 \t -0.003406144889479281\n",
      "52     \t [1.22158747 1.08415614]. \t  -16.70527922550893 \t -0.003406144889479281\n",
      "53     \t [ 0.16555488 -0.53612718]. \t  -32.4535352858786 \t -0.003406144889479281\n",
      "54     \t [ 0.42173736 -1.26611057]. \t  -208.84018173094518 \t -0.003406144889479281\n",
      "55     \t [-0.5189422  -1.68782909]. \t  -385.34301020541506 \t -0.003406144889479281\n",
      "56     \t [-0.95070309  1.48236624]. \t  -37.27492459488175 \t -0.003406144889479281\n",
      "57     \t [ 0.54384183 -0.89280069]. \t  -141.4766651669655 \t -0.003406144889479281\n",
      "58     \t [0.8051323  0.49947153]. \t  -2.2511201711580875 \t -0.003406144889479281\n",
      "59     \t [0.40937437 1.18707475]. \t  -104.28428965658428 \t -0.003406144889479281\n",
      "60     \t [1.02250523 1.72285278]. \t  -45.878888676329034 \t -0.003406144889479281\n",
      "61     \t [-0.54492688 -0.24007982]. \t  -31.226397664342578 \t -0.003406144889479281\n",
      "62     \t [0.25962153 1.26514291]. \t  -144.0061684773042 \t -0.003406144889479281\n",
      "63     \t [-1.12854391  1.29562523]. \t  -4.57916022324821 \t -0.003406144889479281\n",
      "64     \t [-1.00705448  1.16548517]. \t  -6.318237027567069 \t -0.003406144889479281\n",
      "65     \t [-0.21515472  1.99157176]. \t  -379.8881065908075 \t -0.003406144889479281\n",
      "66     \t [0.97254944 0.91457515]. \t  -0.0985801857821006 \t -0.003406144889479281\n",
      "67     \t [ 1.46348283 -1.48988797]. \t  -1319.1174901038878 \t -0.003406144889479281\n",
      "68     \t [-1.95064235 -0.88388403]. \t  -2207.274882433743 \t -0.003406144889479281\n",
      "69     \t [0.16198391 0.10840563]. \t  -1.3774100518971923 \t -0.003406144889479281\n",
      "70     \t [-1.55333206 -1.38547729]. \t  -1449.24130480916 \t -0.003406144889479281\n",
      "71     \t [1.27468998 0.27710306]. \t  -181.71346766613016 \t -0.003406144889479281\n",
      "72     \t [-0.28009762  1.23351318]. \t  -135.0546635396757 \t -0.003406144889479281\n",
      "73     \t [0.59019269 0.36939939]. \t  -0.21234486038223965 \t -0.003406144889479281\n",
      "74     \t [0.44485672 0.22472667]. \t  -0.38016447122718194 \t -0.003406144889479281\n",
      "75     \t [-0.24921281  0.39865629]. \t  -12.887073322359871 \t -0.003406144889479281\n",
      "76     \t [-0.52180996 -0.0714736 ]. \t  -14.132946809213406 \t -0.003406144889479281\n",
      "77     \t [ 0.215878   -0.57694162]. \t  -39.495675767076335 \t -0.003406144889479281\n",
      "78     \t [1.02999374 2.00653251]. \t  -89.42542473466041 \t -0.003406144889479281\n",
      "79     \t [-0.4332399 -0.1926707]. \t  -16.522120495331716 \t -0.003406144889479281\n",
      "80     \t [1.0448602  1.09298009]. \t  \u001b[92m-0.002168003450219745\u001b[0m \t -0.002168003450219745\n",
      "81     \t [ 0.66521893 -0.23347934]. \t  -45.80907842624932 \t -0.002168003450219745\n",
      "82     \t [1.03815875 1.07881697]. \t  \u001b[92m-0.0015649549045195304\u001b[0m \t -0.0015649549045195304\n",
      "83     \t [0.80777678 0.66365278]. \t  -0.049380795879166925 \t -0.0015649549045195304\n",
      "84     \t [1.18993284 1.65277456]. \t  -5.6451275735506075 \t -0.0015649549045195304\n",
      "85     \t [-1.82086888  0.66929264]. \t  -708.232246310807 \t -0.0015649549045195304\n",
      "86     \t [ 0.74271154 -1.83825245]. \t  -571.2154314512803 \t -0.0015649549045195304\n",
      "87     \t [-0.39927098  1.30131624]. \t  -132.3512741366938 \t -0.0015649549045195304\n",
      "88     \t [0.60703817 0.37768558]. \t  -0.16286504638093893 \t -0.0015649549045195304\n",
      "89     \t [0.88647147 0.72135748]. \t  -0.42858072349503895 \t -0.0015649549045195304\n",
      "90     \t [ 1.21960029 -0.11987661]. \t  -258.3900302193461 \t -0.0015649549045195304\n",
      "91     \t [1.10148711 1.21372887]. \t  -0.010320337327623346 \t -0.0015649549045195304\n",
      "92     \t [ 1.46806628 -0.6833924 ]. \t  -805.9903261107626 \t -0.0015649549045195304\n",
      "93     \t [1.10093847 1.23767273]. \t  -0.07576147281820803 \t -0.0015649549045195304\n",
      "94     \t [-1.17118342  0.76949043]. \t  -40.976133562082566 \t -0.0015649549045195304\n",
      "95     \t [ 1.65361628 -1.69356049]. \t  -1961.1520765663745 \t -0.0015649549045195304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [-1.65372101 -1.45526157]. \t  -1762.698113551249 \t -0.0015649549045195304\n",
      "97     \t [1.00007027 0.99805295]. \t  \u001b[92m-0.00043581245498646804\u001b[0m \t -0.00043581245498646804\n",
      "98     \t [ 1.72114382 -0.85529876]. \t  -1457.9536047382583 \t -0.00043581245498646804\n",
      "99     \t [0.34268958 0.38226986]. \t  -7.445746616971624 \t -0.00043581245498646804\n",
      "100    \t [-1.0954881   0.33887054]. \t  -78.56168510444935 \t -0.00043581245498646804\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\n",
    "\n",
    "np.random.seed(run_num_1)\n",
    "surrogate_loser_1 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_1 = GPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_1.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.44173258 -1.74529086]. \t  -1462.4037722675753 \t -1.3013277264983028\n",
      "init   \t [ 1.6176405  -0.26012243]. \t  -828.0271838688934 \t -1.3013277264983028\n",
      "init   \t [-1.525032    0.31071385]. \t  -412.40181251788414 \t -1.3013277264983028\n",
      "init   \t [ 1.39456889 -0.26574622]. \t  -488.8170397507174 \t -1.3013277264983028\n",
      "init   \t [0.80244966 0.75627765]. \t  -1.3013277264983028 \t -1.3013277264983028\n",
      "1      \t [ 0.38382722 -0.20002744]. \t  -12.444925217581003 \t -1.3013277264983028\n",
      "2      \t [0.74575449 0.06127072]. \t  -24.555167089118154 \t -1.3013277264983028\n",
      "3      \t [0.86803262 1.82156653]. \t  -114.09816275934874 \t -1.3013277264983028\n",
      "4      \t [ 0.46185146 -0.65999866]. \t  -76.5558415660385 \t -1.3013277264983028\n",
      "5      \t [0.5105136  1.17310968]. \t  -83.5025837806539 \t -1.3013277264983028\n",
      "6      \t [1.18411797 1.28058677]. \t  -1.5113054183090753 \t -1.3013277264983028\n",
      "7      \t [0.96695624 1.09506056]. \t  -2.5628902934529054 \t -1.3013277264983028\n",
      "8      \t [1.8774407 2.048    ]. \t  -218.85888120416146 \t -1.3013277264983028\n",
      "9      \t [-0.35320399 -0.14491695]. \t  -9.103352584475711 \t -1.3013277264983028\n",
      "10     \t [-0.10103105  0.24295934]. \t  -6.629622007414247 \t -1.3013277264983028\n",
      "11     \t [-1.84652963 -2.03028032]. \t  -2967.410518416418 \t -1.3013277264983028\n",
      "12     \t [1.05930778 1.01952393]. \t  \u001b[92m-1.0563789641769186\u001b[0m \t -1.0563789641769186\n",
      "13     \t [0.56365564 1.79548012]. \t  -218.5715331592075 \t -1.0563789641769186\n",
      "14     \t [ 0.30627648 -0.14910958]. \t  -6.382015315679451 \t -1.0563789641769186\n",
      "15     \t [-0.08960228 -0.17361579]. \t  -4.4867005802384305 \t -1.0563789641769186\n",
      "16     \t [0.21334486 0.17524857]. \t  -2.3018794648950673 \t -1.0563789641769186\n",
      "17     \t [-0.61156446  0.86155167]. \t  -26.366721503073624 \t -1.0563789641769186\n",
      "18     \t [-1.19063296  1.74954686]. \t  -15.81729039775714 \t -1.0563789641769186\n",
      "19     \t [-0.93346442  1.42219356]. \t  -34.080506807049005 \t -1.0563789641769186\n",
      "20     \t [-0.3413371  -0.26077155]. \t  -16.033398685166116 \t -1.0563789641769186\n",
      "21     \t [-1.7213462  2.048    ]. \t  -91.13421482691442 \t -1.0563789641769186\n",
      "22     \t [-1.8893095  -0.46960809]. \t  -1639.7797752822478 \t -1.0563789641769186\n",
      "23     \t [1.25096243 0.99435341]. \t  -32.616121404735196 \t -1.0563789641769186\n",
      "24     \t [0.93081782 0.10040177]. \t  -58.68345579354755 \t -1.0563789641769186\n",
      "25     \t [1.1025212  1.23366077]. \t  \u001b[92m-0.043299717652430215\u001b[0m \t -0.043299717652430215\n",
      "26     \t [-0.32829337 -1.80462139]. \t  -367.49094444375294 \t -0.043299717652430215\n",
      "27     \t [0.46466716 1.54737497]. \t  -177.56499598933985 \t -0.043299717652430215\n",
      "28     \t [1.08781736 1.18314713]. \t  \u001b[92m-0.00771586898309462\u001b[0m \t -0.00771586898309462\n",
      "29     \t [-1.20169539  2.048     ]. \t  -41.3203893046589 \t -0.00771586898309462\n",
      "30     \t [-0.71700127  0.34279055]. \t  -5.882471910940302 \t -0.00771586898309462\n",
      "31     \t [ 0.99179266 -0.2276715 ]. \t  -146.7306964172385 \t -0.00771586898309462\n",
      "32     \t [1.08855921 1.18143373]. \t  -0.009087008296729085 \t -0.00771586898309462\n",
      "33     \t [-0.24541195 -1.78732495]. \t  -342.8958826553927 \t -0.00771586898309462\n",
      "34     \t [0.66841581 0.44191857]. \t  -0.11231112905277234 \t -0.00771586898309462\n",
      "35     \t [0.73005674 0.51754951]. \t  -0.09668813696464773 \t -0.00771586898309462\n",
      "36     \t [ 1.74293882 -0.80536341]. \t  -1477.5699245440094 \t -0.00771586898309462\n",
      "37     \t [1.19934537 0.68577621]. \t  -56.6884084518539 \t -0.00771586898309462\n",
      "38     \t [0.82481015 1.10117661]. \t  -17.743411896257598 \t -0.00771586898309462\n",
      "39     \t [-0.98073994 -2.00640186]. \t  -884.9757352175648 \t -0.00771586898309462\n",
      "40     \t [-0.80967392 -0.64047878]. \t  -171.24964427357978 \t -0.00771586898309462\n",
      "41     \t [1.03165866 0.66417548]. \t  -16.01253240286429 \t -0.00771586898309462\n",
      "42     \t [-0.34286574  1.94133637]. \t  -334.4204397773866 \t -0.00771586898309462\n",
      "43     \t [-0.75738889  0.35083364]. \t  -8.05259127383539 \t -0.00771586898309462\n",
      "44     \t [-1.22092697  1.33908676]. \t  -7.230042157302588 \t -0.00771586898309462\n",
      "45     \t [ 1.99501367 -1.7679637 ]. \t  -3304.990185125095 \t -0.00771586898309462\n",
      "46     \t [0.79806246 0.64724868]. \t  -0.051480636842797745 \t -0.00771586898309462\n",
      "47     \t [-1.92446685  2.01946897]. \t  -292.1730256583349 \t -0.00771586898309462\n",
      "48     \t [-0.79094099 -1.53717212]. \t  -470.96045277147226 \t -0.00771586898309462\n",
      "49     \t [-0.57036757 -1.33478685]. \t  -278.0612514402123 \t -0.00771586898309462\n",
      "50     \t [0.21598433 0.09697089]. \t  -0.8679074804196298 \t -0.00771586898309462\n",
      "51     \t [1.05914411 1.12345495]. \t  \u001b[92m-0.0037764840180860245\u001b[0m \t -0.0037764840180860245\n",
      "52     \t [-1.51848669  1.61216493]. \t  -54.45599160830821 \t -0.0037764840180860245\n",
      "53     \t [2.00029358 1.68210447]. \t  -538.8091297035712 \t -0.0037764840180860245\n",
      "54     \t [-0.57362638  0.37193407]. \t  -2.6602281661743064 \t -0.0037764840180860245\n",
      "55     \t [1.06225137 1.12584387]. \t  -0.004517398255667639 \t -0.0037764840180860245\n",
      "56     \t [1.31403595 0.46630098]. \t  -158.95678900240353 \t -0.0037764840180860245\n",
      "57     \t [1.96646943 0.26738163]. \t  -1296.6607706834602 \t -0.0037764840180860245\n",
      "58     \t [1.33818709 1.95798627]. \t  -2.9113447674351685 \t -0.0037764840180860245\n",
      "59     \t [-1.32427512  0.34521736]. \t  -203.78588019736992 \t -0.0037764840180860245\n",
      "60     \t [0.28483277 1.61744199]. \t  -236.53700601606423 \t -0.0037764840180860245\n",
      "61     \t [1.57259458 0.00595459]. \t  -608.9856696190132 \t -0.0037764840180860245\n",
      "62     \t [1.85709005 1.58285552]. \t  -348.9033138942158 \t -0.0037764840180860245\n",
      "63     \t [-0.58054926 -0.49970011]. \t  -72.51110784024942 \t -0.0037764840180860245\n",
      "64     \t [0.84829766 0.72116891]. \t  -0.02325695956156507 \t -0.0037764840180860245\n",
      "65     \t [-0.11136909  1.32217248]. \t  -172.78473047588355 \t -0.0037764840180860245\n",
      "66     \t [ 0.08406173 -2.02016692]. \t  -411.80642514590784 \t -0.0037764840180860245\n",
      "67     \t [ 0.60840368 -1.00349936]. \t  -188.84598804941893 \t -0.0037764840180860245\n",
      "68     \t [-0.04636727 -0.07615456]. \t  -1.7080436883233414 \t -0.0037764840180860245\n",
      "69     \t [-1.14585149 -1.04130742]. \t  -558.8695561563712 \t -0.0037764840180860245\n",
      "70     \t [ 0.27362341 -0.21059029]. \t  -8.676367778288038 \t -0.0037764840180860245\n",
      "71     \t [-0.96363475  0.86302257]. \t  -4.2857956363608976 \t -0.0037764840180860245\n",
      "72     \t [1.31178016 1.69705082]. \t  -0.15345341524354045 \t -0.0037764840180860245\n",
      "73     \t [0.95314975 0.73193607]. \t  -3.1194807181052044 \t -0.0037764840180860245\n",
      "74     \t [0.88155552 0.77067489]. \t  -0.018209025228376006 \t -0.0037764840180860245\n",
      "75     \t [0.84361066 0.69447202]. \t  -0.05406546866753627 \t -0.0037764840180860245\n",
      "76     \t [1.38178252 1.96641555]. \t  -0.4717144597903842 \t -0.0037764840180860245\n",
      "77     \t [-1.18605588  1.00865149]. \t  -20.62537567388703 \t -0.0037764840180860245\n",
      "78     \t [ 0.59733697 -0.66741213]. \t  -105.06553138860436 \t -0.0037764840180860245\n",
      "79     \t [-1.03351079  1.23317937]. \t  -6.858815070908965 \t -0.0037764840180860245\n",
      "80     \t [-1.03425542  0.62109077]. \t  -24.261808152218876 \t -0.0037764840180860245\n",
      "81     \t [0.54678719 1.70335149]. \t  -197.43238918791414 \t -0.0037764840180860245\n",
      "82     \t [1.4277567 2.048    ]. \t  -0.19202133139501618 \t -0.0037764840180860245\n",
      "83     \t [2.010771   1.52827972]. \t  -633.504066675221 \t -0.0037764840180860245\n",
      "84     \t [ 1.63095484 -0.77756562]. \t  -1182.0932557353174 \t -0.0037764840180860245\n",
      "85     \t [-0.08660451 -0.70069306]. \t  -51.33449912311258 \t -0.0037764840180860245\n",
      "86     \t [1.46932108 1.16443248]. \t  -99.11770622554333 \t -0.0037764840180860245\n",
      "87     \t [1.889115   0.21074836]. \t  -1128.411706159457 \t -0.0037764840180860245\n",
      "88     \t [-0.34497038 -1.43593559]. \t  -243.5928354559929 \t -0.0037764840180860245\n",
      "89     \t [0.74091969 0.55119627]. \t  -0.06762180313179203 \t -0.0037764840180860245\n",
      "90     \t [0.82631999 0.68359394]. \t  -0.030227031754108093 \t -0.0037764840180860245\n",
      "91     \t [-0.98007518  1.8084631 ]. \t  -75.81680806136724 \t -0.0037764840180860245\n",
      "92     \t [-0.28495107 -1.45635923]. \t  -238.05904995480884 \t -0.0037764840180860245\n",
      "93     \t [-0.35843956  0.40250036]. \t  -9.35413291051952 \t -0.0037764840180860245\n",
      "94     \t [-1.60692968  1.75495421]. \t  -75.2334457526766 \t -0.0037764840180860245\n",
      "95     \t [1.24586252 1.53227738]. \t  -0.10003360765156546 \t -0.0037764840180860245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [1.24293068 1.52543964]. \t  -0.09679515991743427 \t -0.0037764840180860245\n",
      "97     \t [1.2432263  1.52651501]. \t  -0.09562714401257486 \t -0.0037764840180860245\n",
      "98     \t [-1.20891657  0.77681768]. \t  -51.75546165606691 \t -0.0037764840180860245\n",
      "99     \t [ 1.04183685 -0.83097723]. \t  -367.26112762398185 \t -0.0037764840180860245\n",
      "100    \t [-0.67481668  0.98829967]. \t  -31.20560964966402 \t -0.0037764840180860245\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\n",
    "\n",
    "np.random.seed(run_num_2)\n",
    "surrogate_loser_2 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_2 = GPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_2.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.03045369 -1.60050678]. \t  -708.8071948417274 \t -1.118465165857483\n",
      "init   \t [-0.05684724 -0.0006915 ]. \t  -1.118465165857483 \t -1.118465165857483\n",
      "init   \t [ 0.64352256 -1.08181622]. \t  -223.90997740226942 \t -1.118465165857483\n",
      "init   \t [ 0.46200787 -1.55790376]. \t  -314.05929962166397 \t -1.118465165857483\n",
      "init   \t [ 0.86918061 -0.52199202]. \t  -163.20929387450548 \t -1.118465165857483\n",
      "1      \t [-0.41275174 -0.97286658]. \t  -132.69348247966587 \t -1.118465165857483\n",
      "2      \t [ 0.20874147 -0.31693142]. \t  -13.622434004259702 \t -1.118465165857483\n",
      "3      \t [-0.49114702  1.07775462]. \t  -72.20163389946413 \t -1.118465165857483\n",
      "4      \t [-0.16863905  0.31002998]. \t  -9.2950576707444 \t -1.118465165857483\n",
      "5      \t [-0.50104472  0.20894064]. \t  -2.4304197416885485 \t -1.118465165857483\n",
      "6      \t [-2.02343475  1.58463911]. \t  -638.9749977990072 \t -1.118465165857483\n",
      "7      \t [-0.27229474  0.03287976]. \t  -1.789011151972603 \t -1.118465165857483\n",
      "8      \t [1.97427299 1.65111495]. \t  -505.6878315179781 \t -1.118465165857483\n",
      "9      \t [ 1.7223881  -0.75279135]. \t  -1383.9245050803652 \t -1.118465165857483\n",
      "10     \t [-1.50727822 -0.74652847]. \t  -917.3700151977407 \t -1.118465165857483\n",
      "11     \t [ 0.45756182 -0.02415879]. \t  -5.747473493975391 \t -1.118465165857483\n",
      "12     \t [-0.37864797  0.25280462]. \t  -3.0981699608693862 \t -1.118465165857483\n",
      "13     \t [-0.09116721  2.048     ]. \t  -417.22357969734804 \t -1.118465165857483\n",
      "14     \t [ 0.23888875 -0.01007463]. \t  \u001b[92m-1.030101366771608\u001b[0m \t -1.030101366771608\n",
      "15     \t [0.17293491 1.45901441]. \t  -204.91898369264595 \t -1.030101366771608\n",
      "16     \t [ 0.8520427 -0.3307208]. \t  -111.6828633500786 \t -1.030101366771608\n",
      "17     \t [0.8575366  1.97026682]. \t  -152.51755456713286 \t -1.030101366771608\n",
      "18     \t [-0.81755149 -1.84721376]. \t  -636.1299433662065 \t -1.030101366771608\n",
      "19     \t [-0.95370942  0.69716305]. \t  -8.328297141371449 \t -1.030101366771608\n",
      "20     \t [-0.38415023 -0.51128198]. \t  -45.324649641757965 \t -1.030101366771608\n",
      "21     \t [-1.93971307  0.59809255]. \t  -1009.9810140546972 \t -1.030101366771608\n",
      "22     \t [ 0.04936119 -1.5908052 ]. \t  -254.74563352649488 \t -1.030101366771608\n",
      "23     \t [-0.83722349  0.80377519]. \t  -4.432832532058066 \t -1.030101366771608\n",
      "24     \t [-0.86774288  0.63332426]. \t  -4.920158121310172 \t -1.030101366771608\n",
      "25     \t [1.560757   1.33396179]. \t  -121.75498398698987 \t -1.030101366771608\n",
      "26     \t [ 0.86474998 -1.06134866]. \t  -327.31747625263705 \t -1.030101366771608\n",
      "27     \t [-0.17477802  1.48873115]. \t  -214.0101020827603 \t -1.030101366771608\n",
      "28     \t [1.38753524 1.99555813]. \t  \u001b[92m-0.6444500539681968\u001b[0m \t -0.6444500539681968\n",
      "29     \t [1.11792007 1.30692342]. \t  \u001b[92m-0.340839128169687\u001b[0m \t -0.340839128169687\n",
      "30     \t [-1.88548814  0.78108077]. \t  -777.8251909301468 \t -0.340839128169687\n",
      "31     \t [ 1.13060795 -1.47916085]. \t  -760.3619397240727 \t -0.340839128169687\n",
      "32     \t [-0.98261309  0.04318056]. \t  -89.00332324256519 \t -0.340839128169687\n",
      "33     \t [-0.67499788  1.75886749]. \t  -172.6504632335947 \t -0.340839128169687\n",
      "34     \t [-1.28509462  0.25780746]. \t  -199.45068044631728 \t -0.340839128169687\n",
      "35     \t [1.32419917 1.54971625]. \t  -4.258027089273375 \t -0.340839128169687\n",
      "36     \t [1.20598647 1.56484595]. \t  -1.2621869560688832 \t -0.340839128169687\n",
      "37     \t [-1.1211341   1.26739421]. \t  -4.510135427706172 \t -0.340839128169687\n",
      "38     \t [-1.94580847 -0.71166838]. \t  -2031.7333252118815 \t -0.340839128169687\n",
      "39     \t [-1.03861437 -1.56074171]. \t  -700.8316600995099 \t -0.340839128169687\n",
      "40     \t [1.63843289 1.36021904]. \t  -175.76962581466148 \t -0.340839128169687\n",
      "41     \t [ 0.78733669 -2.04448324]. \t  -709.9385288558885 \t -0.340839128169687\n",
      "42     \t [1.1996008  1.42033209]. \t  \u001b[92m-0.07484681006073628\u001b[0m \t -0.07484681006073628\n",
      "43     \t [-0.3711829 -0.3645665]. \t  -27.115015768712222 \t -0.07484681006073628\n",
      "44     \t [-0.54203671 -1.29930436]. \t  -256.1772363256532 \t -0.07484681006073628\n",
      "45     \t [-1.83494523 -0.90244545]. \t  -1830.873844658998 \t -0.07484681006073628\n",
      "46     \t [ 0.34057347 -0.16994831]. \t  -8.610931318744916 \t -0.07484681006073628\n",
      "47     \t [0.89277581 0.8005285 ]. \t  \u001b[92m-0.012707961825170618\u001b[0m \t -0.012707961825170618\n",
      "48     \t [1.62675981 0.68831337]. \t  -383.7825862242523 \t -0.012707961825170618\n",
      "49     \t [0.98434196 0.97976621]. \t  \u001b[92m-0.011989492271941809\u001b[0m \t -0.011989492271941809\n",
      "50     \t [-1.43196859  1.68221133]. \t  -19.48063398134607 \t -0.011989492271941809\n",
      "51     \t [-1.45985298  2.03008325]. \t  -7.0727447641879655 \t -0.011989492271941809\n",
      "52     \t [-1.32246083  2.00388105]. \t  -11.89522283234405 \t -0.011989492271941809\n",
      "53     \t [1.2380622  1.52779857]. \t  -0.059173061193383475 \t -0.011989492271941809\n",
      "54     \t [1.38281945 0.84828167]. \t  -113.33656656331196 \t -0.011989492271941809\n",
      "55     \t [1.2891623  1.69196965]. \t  -0.1737962096053322 \t -0.011989492271941809\n",
      "56     \t [-0.93135105 -0.28698016]. \t  -136.99288509973206 \t -0.011989492271941809\n",
      "57     \t [-1.39071243  0.62388476]. \t  -177.3769387689481 \t -0.011989492271941809\n",
      "58     \t [1.11547996 1.21235946]. \t  -0.11532694725225504 \t -0.011989492271941809\n",
      "59     \t [-1.1281658   0.19057678]. \t  -121.6407261095112 \t -0.011989492271941809\n",
      "60     \t [-0.94243415  0.15022679]. \t  -58.23085882279816 \t -0.011989492271941809\n",
      "61     \t [0.85438349 0.72719617]. \t  -0.021974218416746995 \t -0.011989492271941809\n",
      "62     \t [-0.40346979 -0.06644562]. \t  -7.22452655935794 \t -0.011989492271941809\n",
      "63     \t [-1.72920729  0.0166434 ]. \t  -891.627389019488 \t -0.011989492271941809\n",
      "64     \t [0.85532453 0.75972732]. \t  -0.10015786780147526 \t -0.011989492271941809\n",
      "65     \t [-1.21261842 -1.9313326 ]. \t  -1162.1036949545671 \t -0.011989492271941809\n",
      "66     \t [1.66604392 0.39622405]. \t  -566.6353090913572 \t -0.011989492271941809\n",
      "67     \t [1.196341   1.40582986]. \t  -0.1030756069623869 \t -0.011989492271941809\n",
      "68     \t [ 0.19994297 -1.39459727]. \t  -206.44047973584387 \t -0.011989492271941809\n",
      "69     \t [0.97054361 0.76680745]. \t  -3.0685303410528384 \t -0.011989492271941809\n",
      "70     \t [1.76609451 0.29053984]. \t  -800.6563950055495 \t -0.011989492271941809\n",
      "71     \t [-0.33250964 -0.39777081]. \t  -27.615873630731052 \t -0.011989492271941809\n",
      "72     \t [ 1.73459313 -1.33251473]. \t  -1885.252561455859 \t -0.011989492271941809\n",
      "73     \t [0.68488567 0.48532686]. \t  -0.12573086419628893 \t -0.011989492271941809\n",
      "74     \t [ 0.05185248 -0.61927705]. \t  -39.58312041762906 \t -0.011989492271941809\n",
      "75     \t [ 1.39075807 -1.89625902]. \t  -1467.4004652258593 \t -0.011989492271941809\n",
      "76     \t [ 0.65582879 -0.27149012]. \t  -49.342923630423144 \t -0.011989492271941809\n",
      "77     \t [-1.13297187 -0.62810777]. \t  -370.021887695414 \t -0.011989492271941809\n",
      "78     \t [-0.16329517 -0.12542853]. \t  -3.6665093913833675 \t -0.011989492271941809\n",
      "79     \t [0.79676663 0.71758119]. \t  -0.7259629876842714 \t -0.011989492271941809\n",
      "80     \t [ 1.59495137 -0.17072792]. \t  -737.2580887986887 \t -0.011989492271941809\n",
      "81     \t [-0.28444708 -0.11423451]. \t  -5.457947591478899 \t -0.011989492271941809\n",
      "82     \t [0.68399128 0.74996957]. \t  -8.059340982336861 \t -0.011989492271941809\n",
      "83     \t [-1.37415569  1.40566405]. \t  -28.9307325057905 \t -0.011989492271941809\n",
      "84     \t [-0.71861415  0.91425582]. \t  -18.78205873832965 \t -0.011989492271941809\n",
      "85     \t [-1.7717308   2.02961981]. \t  -130.761591605953 \t -0.011989492271941809\n",
      "86     \t [0.37447845 0.96941186]. \t  -69.14485198697025 \t -0.011989492271941809\n",
      "87     \t [ 0.13406096 -0.30988621]. \t  -11.49897327350787 \t -0.011989492271941809\n",
      "88     \t [-1.97757266  0.61735958]. \t  -1093.5367051180658 \t -0.011989492271941809\n",
      "89     \t [-0.04372883 -0.81460824]. \t  -67.75993521587327 \t -0.011989492271941809\n",
      "90     \t [-0.9563154  -0.45374215]. \t  -191.04654219354157 \t -0.011989492271941809\n",
      "91     \t [0.51069402 0.31521316]. \t  -0.5354083256716863 \t -0.011989492271941809\n",
      "92     \t [ 0.56971754 -0.26063979]. \t  -34.43313749931704 \t -0.011989492271941809\n",
      "93     \t [ 0.71186019 -0.97858099]. \t  -220.70233264226943 \t -0.011989492271941809\n",
      "94     \t [-0.68390632 -0.72930392]. \t  -146.12404581908285 \t -0.011989492271941809\n",
      "95     \t [0.76579775 1.19033201]. \t  -36.5226583551558 \t -0.011989492271941809\n",
      "96     \t [ 1.7242307  -0.10450261]. \t  -947.6092010408778 \t -0.011989492271941809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [-1.25274348  1.20739645]. \t  -18.17706518631196 \t -0.011989492271941809\n",
      "98     \t [-1.8188858  -2.01500327]. \t  -2841.750389796985 \t -0.011989492271941809\n",
      "99     \t [0.28598232 1.17276022]. \t  -119.53232007701105 \t -0.011989492271941809\n",
      "100    \t [ 1.94508247 -0.03896131]. \t  -1461.8963614633797 \t -0.011989492271941809\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\n",
    "\n",
    "np.random.seed(run_num_3)\n",
    "surrogate_loser_3 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_3 = GPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_3.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [1.39002285 1.31602418]. \t  -38.11488737154776 \t -12.122423820878506\n",
      "init   \t [0.58248055 0.68494384]. \t  -12.122423820878506 \t -12.122423820878506\n",
      "init   \t [-1.88768295 -1.7790059 ]. \t  -2862.4120613667637 \t -12.122423820878506\n",
      "init   \t [-0.91545338 -0.75218738]. \t  -256.5560079635026 \t -12.122423820878506\n",
      "init   \t [-1.25504548 -1.24070756]. \t  -797.98450090518 \t -12.122423820878506\n",
      "1      \t [-0.00933519 -1.72218378]. \t  -297.64047024942215 \t -12.122423820878506\n",
      "2      \t [0.55438246 1.26879645]. \t  -92.63844192618087 \t -12.122423820878506\n",
      "3      \t [1.4102362 0.3572151]. \t  -266.3641717959405 \t -12.122423820878506\n",
      "4      \t [2.048 2.048]. \t  -461.7603900415999 \t -12.122423820878506\n",
      "5      \t [0.9509412  0.96688936]. \t  \u001b[92m-0.3942852631873592\u001b[0m \t -0.3942852631873592\n",
      "6      \t [0.74756226 0.81859824]. \t  -6.810674116298312 \t -0.3942852631873592\n",
      "7      \t [ 1.73111495 -0.26146777]. \t  -1062.138680699351 \t -0.3942852631873592\n",
      "8      \t [-1.256087  2.048   ]. \t  -27.203005810605532 \t -0.3942852631873592\n",
      "9      \t [ 1.56120513 -1.00487715]. \t  -1185.2156248763604 \t -0.3942852631873592\n",
      "10     \t [1.35128497 0.97742276]. \t  -72.12682233348824 \t -0.3942852631873592\n",
      "11     \t [0.92697914 2.048     ]. \t  -141.3083993154159 \t -0.3942852631873592\n",
      "12     \t [0.64998767 0.17858348]. \t  -6.071253778808317 \t -0.3942852631873592\n",
      "13     \t [-2.03015267  2.048     ]. \t  -439.1302929844424 \t -0.3942852631873592\n",
      "14     \t [-0.9025929  1.8022134]. \t  -101.14327863611892 \t -0.3942852631873592\n",
      "15     \t [0.72473896 0.44654373]. \t  -0.6951823581833387 \t -0.3942852631873592\n",
      "16     \t [1.13234936 1.4403697 ]. \t  -2.5188053345230363 \t -0.3942852631873592\n",
      "17     \t [ 1.73164586 -0.32150232]. \t  -1102.8415029977223 \t -0.3942852631873592\n",
      "18     \t [1.11463274 1.25423886]. \t  \u001b[92m-0.027141986001183165\u001b[0m \t -0.027141986001183165\n",
      "19     \t [ 1.12210002 -1.56110415]. \t  -795.3748184470774 \t -0.027141986001183165\n",
      "20     \t [-0.61916606 -0.43957734]. \t  -70.34537379874358 \t -0.027141986001183165\n",
      "21     \t [-0.31508739  0.34382049]. \t  -7.709456709289073 \t -0.027141986001183165\n",
      "22     \t [-0.11945395 -0.38997863]. \t  -17.59481156165661 \t -0.027141986001183165\n",
      "23     \t [-0.17127353  0.00822367]. \t  -1.4164489203904496 \t -0.027141986001183165\n",
      "24     \t [1.83420092 1.92805264]. \t  -206.97453394502486 \t -0.027141986001183165\n",
      "25     \t [1.41537596 2.048     ]. \t  -0.3724435264845297 \t -0.027141986001183165\n",
      "26     \t [-1.47557584  0.22933391]. \t  -385.5950403975736 \t -0.027141986001183165\n",
      "27     \t [1.35807506 1.81672227]. \t  -0.2046456703022251 \t -0.027141986001183165\n",
      "28     \t [-0.55290055  1.30781461]. \t  -102.83506472840799 \t -0.027141986001183165\n",
      "29     \t [-1.07206331  1.34781616]. \t  -8.233529643120526 \t -0.027141986001183165\n",
      "30     \t [-0.97184682  1.01589148]. \t  -4.398050767233996 \t -0.027141986001183165\n",
      "31     \t [ 1.48633283 -0.77964418]. \t  -893.5466803786309 \t -0.027141986001183165\n",
      "32     \t [-0.55885019 -1.36792651]. \t  -284.7506778298577 \t -0.027141986001183165\n",
      "33     \t [0.24363327 1.74337258]. \t  -284.16287928828496 \t -0.027141986001183165\n",
      "34     \t [-0.01587209 -0.58220972]. \t  -34.958152717907716 \t -0.027141986001183165\n",
      "35     \t [-1.70714828 -1.19774021]. \t  -1698.2615538946 \t -0.027141986001183165\n",
      "36     \t [0.85401762 0.70594319]. \t  -0.07608043211474838 \t -0.027141986001183165\n",
      "37     \t [1.34122372 1.78401813]. \t  -0.13852429536418476 \t -0.027141986001183165\n",
      "38     \t [0.8472206  0.70417018]. \t  -0.04187173874758621 \t -0.027141986001183165\n",
      "39     \t [-1.12292196  1.80136345]. \t  -33.71106413908456 \t -0.027141986001183165\n",
      "40     \t [ 0.82992569 -0.41506505]. \t  -121.87557525235378 \t -0.027141986001183165\n",
      "41     \t [-0.25533525 -0.95263609]. \t  -105.17410174748937 \t -0.027141986001183165\n",
      "42     \t [1.19079437 0.37061068]. \t  -109.7370036372121 \t -0.027141986001183165\n",
      "43     \t [0.67382959 0.44672827]. \t  -0.1117425145908162 \t -0.027141986001183165\n",
      "44     \t [0.68886396 0.4655227 ]. \t  -0.10492518129202977 \t -0.027141986001183165\n",
      "45     \t [-1.37429839  1.73310574]. \t  -8.058127826750235 \t -0.027141986001183165\n",
      "46     \t [-0.23860944 -1.69672867]. \t  -309.06759363010303 \t -0.027141986001183165\n",
      "47     \t [0.1361602  0.91973458]. \t  -81.9614576900326 \t -0.027141986001183165\n",
      "48     \t [0.26502029 1.06660612]. \t  -99.81558628550876 \t -0.027141986001183165\n",
      "49     \t [-0.85724841 -1.36641014]. \t  -444.9892310789567 \t -0.027141986001183165\n",
      "50     \t [-1.91655642 -0.80527284]. \t  -2014.1679084374393 \t -0.027141986001183165\n",
      "51     \t [-0.69945851  0.39396279]. \t  -3.795975973719777 \t -0.027141986001183165\n",
      "52     \t [1.96827608 0.51385253]. \t  -1130.0710754655745 \t -0.027141986001183165\n",
      "53     \t [1.88164587 0.89465385]. \t  -700.875734542644 \t -0.027141986001183165\n",
      "54     \t [-1.2761108   1.32629013]. \t  -14.311269225975838 \t -0.027141986001183165\n",
      "55     \t [0.30291503 0.11430787]. \t  -0.5367793351067386 \t -0.027141986001183165\n",
      "56     \t [0.52351276 0.29023773]. \t  -0.2531938719557695 \t -0.027141986001183165\n",
      "57     \t [0.52180079 0.33005145]. \t  -0.5624739375058474 \t -0.027141986001183165\n",
      "58     \t [-1.13628881 -0.58089985]. \t  -355.02164384040896 \t -0.027141986001183165\n",
      "59     \t [-1.90671123  0.45571025]. \t  -1019.5855922763678 \t -0.027141986001183165\n",
      "60     \t [ 1.69350482 -0.70147587]. \t  -1274.5671653469997 \t -0.027141986001183165\n",
      "61     \t [1.38831163 1.92221886]. \t  -0.15347985766694422 \t -0.027141986001183165\n",
      "62     \t [ 1.0450378  -1.71499994]. \t  -787.9852810986221 \t -0.027141986001183165\n",
      "63     \t [-0.68900583 -0.93015724]. \t  -200.2232850113495 \t -0.027141986001183165\n",
      "64     \t [-1.9511115   1.59976625]. \t  -495.82478171662814 \t -0.027141986001183165\n",
      "65     \t [0.1433744  0.57671401]. \t  -31.66495620399632 \t -0.027141986001183165\n",
      "66     \t [-0.75099975 -0.36212237]. \t  -88.83637904884068 \t -0.027141986001183165\n",
      "67     \t [1.04061146 1.08656081]. \t  \u001b[92m-0.0030098714087796576\u001b[0m \t -0.0030098714087796576\n",
      "68     \t [0.1155818 0.9244811]. \t  -83.79651624949977 \t -0.0030098714087796576\n",
      "69     \t [0.9302396  0.89922877]. \t  -0.11967264793471892 \t -0.0030098714087796576\n",
      "70     \t [-1.47252835  0.50797933]. \t  -281.7930650445806 \t -0.0030098714087796576\n",
      "71     \t [-0.60496585  0.88407464]. \t  -29.41773966484951 \t -0.0030098714087796576\n",
      "72     \t [0.13545758 1.00238521]. \t  -97.58020718432583 \t -0.0030098714087796576\n",
      "73     \t [-0.29352304 -0.09093165]. \t  -4.8091973629883595 \t -0.0030098714087796576\n",
      "74     \t [-0.84245666 -1.64991733]. \t  -560.1897177737782 \t -0.0030098714087796576\n",
      "75     \t [ 1.60969256 -1.00479884]. \t  -1293.4278517665257 \t -0.0030098714087796576\n",
      "76     \t [0.33941695 0.41635123]. \t  -9.50534310900239 \t -0.0030098714087796576\n",
      "77     \t [1.43984719 1.69352848]. \t  -14.605468984402297 \t -0.0030098714087796576\n",
      "78     \t [1.74876814 1.68137527]. \t  -190.12253463601016 \t -0.0030098714087796576\n",
      "79     \t [0.59607989 0.36541864]. \t  -0.17336742519789472 \t -0.0030098714087796576\n",
      "80     \t [-1.6419876  -1.70374082]. \t  -1942.8604978327307 \t -0.0030098714087796576\n",
      "81     \t [0.77102632 0.77151141]. \t  -3.1863844894260773 \t -0.0030098714087796576\n",
      "82     \t [1.08616483 1.14274229]. \t  -0.14441133113668123 \t -0.0030098714087796576\n",
      "83     \t [0.15927452 0.02955016]. \t  -0.7085680687742651 \t -0.0030098714087796576\n",
      "84     \t [1.01586111 1.01434984]. \t  -0.03131192845511335 \t -0.0030098714087796576\n",
      "85     \t [1.78889777 0.96399803]. \t  -500.6622668173832 \t -0.0030098714087796576\n",
      "86     \t [ 0.62086675 -1.99945441]. \t  -568.9328188065471 \t -0.0030098714087796576\n",
      "87     \t [-1.99508571  1.16711341]. \t  -800.410112024754 \t -0.0030098714087796576\n",
      "88     \t [ 2.00197735 -0.98261503]. \t  -2491.5412798331467 \t -0.0030098714087796576\n",
      "89     \t [-0.75829718 -1.78029594]. \t  -557.8403897443384 \t -0.0030098714087796576\n",
      "90     \t [ 1.26053213 -0.50401093]. \t  -438.1127623760935 \t -0.0030098714087796576\n",
      "91     \t [2.04206484 1.39503674]. \t  -771.1439943589678 \t -0.0030098714087796576\n",
      "92     \t [ 1.1015313  -0.52844185]. \t  -303.40158362456754 \t -0.0030098714087796576\n",
      "93     \t [1.11703424 1.1815487 ]. \t  -0.4521633044595778 \t -0.0030098714087796576\n",
      "94     \t [0.97817863 0.96346351]. \t  -0.0048719716429606475 \t -0.0030098714087796576\n",
      "95     \t [-0.26471549 -0.51119417]. \t  -35.38680766315234 \t -0.0030098714087796576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [ 1.35705638 -0.0890798 ]. \t  -372.880721250297 \t -0.0030098714087796576\n",
      "97     \t [0.79248782 0.00551502]. \t  -38.79641589678887 \t -0.0030098714087796576\n",
      "98     \t [-1.22520577  1.49539825]. \t  -4.954825042362605 \t -0.0030098714087796576\n",
      "99     \t [-0.26276442  1.13854958]. \t  -115.97854910800217 \t -0.0030098714087796576\n",
      "100    \t [ 1.28981407 -0.97803313]. \t  -697.9172874987738 \t -0.0030098714087796576\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\n",
    "\n",
    "np.random.seed(run_num_4)\n",
    "surrogate_loser_4 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_4 = GPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_4.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.23491901 0.17105153]. \t  -1.9278091788796494 \t -1.9278091788796494\n",
      "init   \t [ 0.24749582 -0.48313101]. \t  -30.201785940713883 \t -1.9278091788796494\n",
      "init   \t [-1.56019024  1.41721238]. \t  -109.97965348801837 \t -1.9278091788796494\n",
      "init   \t [0.98198164 1.77093658]. \t  -65.06852657321838 \t -1.9278091788796494\n",
      "init   \t [1.47180752 0.85239655]. \t  -172.83511538815276 \t -1.9278091788796494\n",
      "1      \t [-0.20787038  0.38221562]. \t  -12.951425052058712 \t -1.9278091788796494\n",
      "2      \t [-2.0143131  -1.31251332]. \t  -2892.7444960501225 \t -1.9278091788796494\n",
      "3      \t [ 1.01950798 -1.24720923]. \t  -522.85696842208 \t -1.9278091788796494\n",
      "4      \t [ 0.07100164 -0.05570017]. \t  \u001b[92m-1.2319897893378482\u001b[0m \t -1.2319897893378482\n",
      "5      \t [-0.31009759  1.36182696]. \t  -161.9075115269378 \t -1.2319897893378482\n",
      "6      \t [1.84256052 1.9255108 ]. \t  -216.6583564474458 \t -1.2319897893378482\n",
      "7      \t [-0.36629948 -0.2342708 ]. \t  -15.44202785243899 \t -1.2319897893378482\n",
      "8      \t [0.57953198 0.81222119]. \t  -22.869047410351204 \t -1.2319897893378482\n",
      "9      \t [0.65694789 0.32591047]. \t  -1.2343010204164098 \t -1.2319897893378482\n",
      "10     \t [-1.45782604 -0.16358394]. \t  -529.920081399145 \t -1.2319897893378482\n",
      "11     \t [-1.08106709  0.51892388]. \t  -46.552527354468715 \t -1.2319897893378482\n",
      "12     \t [-2.048  2.048]. \t  -469.9523900415999 \t -1.2319897893378482\n",
      "13     \t [-1.87846585  0.86816008]. \t  -716.0976929688445 \t -1.2319897893378482\n",
      "14     \t [-1.1702916   1.79222005]. \t  -22.572421994235256 \t -1.2319897893378482\n",
      "15     \t [-1.10742968  1.36657059]. \t  -6.406025499802244 \t -1.2319897893378482\n",
      "16     \t [-0.68365498  0.19131149]. \t  -10.456304765846308 \t -1.2319897893378482\n",
      "17     \t [-1.27730525  1.58969233]. \t  -5.36098004471088 \t -1.2319897893378482\n",
      "18     \t [-1.56321837  0.0803017 ]. \t  -565.1123929380714 \t -1.2319897893378482\n",
      "19     \t [-0.82359378  0.69999699]. \t  -3.372541060140701 \t -1.2319897893378482\n",
      "20     \t [-0.73274214 -0.48359735]. \t  -107.14613451292593 \t -1.2319897893378482\n",
      "21     \t [ 0.43346255 -0.75667882]. \t  -89.54194982904822 \t -1.2319897893378482\n",
      "22     \t [-0.00355248 -1.6214444 ]. \t  -263.91940341438124 \t -1.2319897893378482\n",
      "23     \t [-0.34012153  0.38623137]. \t  -9.115586071104392 \t -1.2319897893378482\n",
      "24     \t [ 0.81500145 -0.12707556]. \t  -62.650256265456946 \t -1.2319897893378482\n",
      "25     \t [-1.33177649 -1.0812734 ]. \t  -820.4837430337147 \t -1.2319897893378482\n",
      "26     \t [-0.15807809  0.05456156]. \t  -1.4286003721129885 \t -1.2319897893378482\n",
      "27     \t [0.8013301  0.17053804]. \t  -22.279359836832548 \t -1.2319897893378482\n",
      "28     \t [ 0.87329433 -0.30051235]. \t  -113.04598083597291 \t -1.2319897893378482\n",
      "29     \t [-0.64761208 -0.63941521]. \t  -114.82388800391946 \t -1.2319897893378482\n",
      "30     \t [-1.47691244 -1.57602495]. \t  -1417.861890443338 \t -1.2319897893378482\n",
      "31     \t [ 0.4327339  -1.35970691]. \t  -239.6320259130426 \t -1.2319897893378482\n",
      "32     \t [-1.18032486 -1.07964242]. \t  -616.2323504683599 \t -1.2319897893378482\n",
      "33     \t [2.04191633 1.94083269]. \t  -497.7467544769756 \t -1.2319897893378482\n",
      "34     \t [1.42561837 2.048     ]. \t  \u001b[92m-0.20552528304956458\u001b[0m \t -0.20552528304956458\n",
      "35     \t [1.32113337 1.60272788]. \t  -2.1384712079062433 \t -0.20552528304956458\n",
      "36     \t [-0.5033492  -1.35122097]. \t  -259.7282014742476 \t -0.20552528304956458\n",
      "37     \t [1.39165245 1.85940615]. \t  -0.7507722241897082 \t -0.20552528304956458\n",
      "38     \t [-0.75365745 -1.90976445]. \t  -617.0067622668932 \t -0.20552528304956458\n",
      "39     \t [-1.12527059  1.17804048]. \t  -5.29458284839629 \t -0.20552528304956458\n",
      "40     \t [0.51026574 0.26985254]. \t  -0.24882936367099734 \t -0.20552528304956458\n",
      "41     \t [-1.90053733 -1.91620189]. \t  -3064.5613349294117 \t -0.20552528304956458\n",
      "42     \t [1.08338442 1.21664741]. \t  \u001b[92m-0.1912136837522159\u001b[0m \t -0.1912136837522159\n",
      "43     \t [-1.06064946  0.22065185]. \t  -86.02672492343893 \t -0.1912136837522159\n",
      "44     \t [-1.4610998   1.76453335]. \t  -19.767685852850942 \t -0.1912136837522159\n",
      "45     \t [-0.25640987  0.21172911]. \t  -3.7096718553017576 \t -0.1912136837522159\n",
      "46     \t [ 0.15915991 -0.78903105]. \t  -67.02570961865136 \t -0.1912136837522159\n",
      "47     \t [0.92467799 0.87198466]. \t  \u001b[92m-0.034421572121576025\u001b[0m \t -0.034421572121576025\n",
      "48     \t [0.4302708  0.17511064]. \t  -0.3346360517196895 \t -0.034421572121576025\n",
      "49     \t [-0.12274723  1.13054738]. \t  -125.69023654816054 \t -0.034421572121576025\n",
      "50     \t [1.12805075 1.41851326]. \t  -2.1484280874112036 \t -0.034421572121576025\n",
      "51     \t [-0.15643341 -0.63785765]. \t  -45.20531701437525 \t -0.034421572121576025\n",
      "52     \t [1.04133418 1.11839936]. \t  -0.1174615126135806 \t -0.034421572121576025\n",
      "53     \t [-0.57842583 -1.50280606]. \t  -340.0888766860873 \t -0.034421572121576025\n",
      "54     \t [0.49963123 0.23086254]. \t  -0.2855958144241081 \t -0.034421572121576025\n",
      "55     \t [ 0.82248618 -1.04625839]. \t  -296.8154796675672 \t -0.034421572121576025\n",
      "56     \t [-0.64689927  1.63209789]. \t  -149.9994387123553 \t -0.034421572121576025\n",
      "57     \t [0.93196297 0.90382006]. \t  -0.1289916174950372 \t -0.034421572121576025\n",
      "58     \t [-1.45316867 -0.38311089]. \t  -628.4257640568197 \t -0.034421572121576025\n",
      "59     \t [1.58170114 0.41906204]. \t  -434.1091590560216 \t -0.034421572121576025\n",
      "60     \t [ 1.54664472 -0.85143791]. \t  -1052.3590489943044 \t -0.034421572121576025\n",
      "61     \t [-0.81535936  0.58043406]. \t  -4.007474413946899 \t -0.034421572121576025\n",
      "62     \t [-0.97376053 -1.4571699 ]. \t  -582.480767503922 \t -0.034421572121576025\n",
      "63     \t [-0.61116527 -1.07885359]. \t  -213.53562625344438 \t -0.034421572121576025\n",
      "64     \t [ 0.53230149 -1.91970845]. \t  -485.56313408956726 \t -0.034421572121576025\n",
      "65     \t [1.39449448 0.02403067]. \t  -369.01998822142326 \t -0.034421572121576025\n",
      "66     \t [0.99036749 0.33968709]. \t  -41.10622914377682 \t -0.034421572121576025\n",
      "67     \t [-0.41301572  0.94702602]. \t  -62.28314763695713 \t -0.034421572121576025\n",
      "68     \t [1.18565431 0.24203744]. \t  -135.46324582697912 \t -0.034421572121576025\n",
      "69     \t [1.08108308 1.15926859]. \t  \u001b[92m-0.015546421776397694\u001b[0m \t -0.015546421776397694\n",
      "70     \t [-1.63445622 -1.84554485]. \t  -2047.2620122828723 \t -0.015546421776397694\n",
      "71     \t [ 1.47354817 -0.45096802]. \t  -687.8763878924361 \t -0.015546421776397694\n",
      "72     \t [ 0.51006207 -0.81553677]. \t  -115.9531049663765 \t -0.015546421776397694\n",
      "73     \t [-1.70649306  0.19121674]. \t  -747.6557790250947 \t -0.015546421776397694\n",
      "74     \t [ 1.44126017 -1.05174425]. \t  -979.2432473247527 \t -0.015546421776397694\n",
      "75     \t [-1.52326452 -0.02977024]. \t  -558.6662390049898 \t -0.015546421776397694\n",
      "76     \t [ 0.25685491 -1.73171831]. \t  -323.72218821205206 \t -0.015546421776397694\n",
      "77     \t [1.95511732 1.8879273 ]. \t  -375.16310905375974 \t -0.015546421776397694\n",
      "78     \t [-0.80366534  0.34169506]. \t  -12.505933083898942 \t -0.015546421776397694\n",
      "79     \t [-1.16481664  1.87316636]. \t  -31.35007917044794 \t -0.015546421776397694\n",
      "80     \t [ 0.20382741 -0.87757499]. \t  -85.11215914353261 \t -0.015546421776397694\n",
      "81     \t [ 0.92531161 -0.60711948]. \t  -214.1364263200481 \t -0.015546421776397694\n",
      "82     \t [-1.48490288  1.04258879]. \t  -141.27997684480601 \t -0.015546421776397694\n",
      "83     \t [0.39968827 1.86533819]. \t  -291.263238083348 \t -0.015546421776397694\n",
      "84     \t [-0.14592709  1.77647586]. \t  -309.37923450719995 \t -0.015546421776397694\n",
      "85     \t [-0.93470523  0.42585779]. \t  -23.79700799426778 \t -0.015546421776397694\n",
      "86     \t [0.525904   0.88949121]. \t  -37.79139283682128 \t -0.015546421776397694\n",
      "87     \t [-1.57565981  0.39355628]. \t  -443.08777565153105 \t -0.015546421776397694\n",
      "88     \t [ 0.32296706 -1.50958795]. \t  -260.9242971549489 \t -0.015546421776397694\n",
      "89     \t [ 1.48541484 -0.86606207]. \t  -944.273132645884 \t -0.015546421776397694\n",
      "90     \t [-1.46338831 -1.42839435]. \t  -1280.4866590917438 \t -0.015546421776397694\n",
      "91     \t [ 1.87252561 -0.1611803 ]. \t  -1345.8407490325253 \t -0.015546421776397694\n",
      "92     \t [ 1.77148003 -1.40361085]. \t  -2063.34662094588 \t -0.015546421776397694\n",
      "93     \t [-1.54232992  0.34864832]. \t  -418.6075536786673 \t -0.015546421776397694\n",
      "94     \t [-1.09556612  1.43234486]. \t  -9.777497992412677 \t -0.015546421776397694\n",
      "95     \t [ 1.7290815  -1.47455581]. \t  -1993.509957799454 \t -0.015546421776397694\n",
      "96     \t [1.62807101 0.40241991]. \t  -505.8326830375496 \t -0.015546421776397694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [ 1.8800602  -0.87843687]. \t  -1948.2872100578213 \t -0.015546421776397694\n",
      "98     \t [-1.81468614  1.5420402 ]. \t  -314.5385193068393 \t -0.015546421776397694\n",
      "99     \t [-0.74573766  0.83727171]. \t  -10.951966768460675 \t -0.015546421776397694\n",
      "100    \t [-0.72720043 -1.80539374]. \t  -547.8388160624896 \t -0.015546421776397694\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\n",
    "\n",
    "np.random.seed(run_num_5)\n",
    "surrogate_loser_5 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_5 = GPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_5.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.60915518 -0.68821072]. \t  -1074.6314195925434 \t -3.0269049669752817\n",
      "init   \t [ 1.31575449 -1.87721062]. \t  -1302.169546932896 \t -3.0269049669752817\n",
      "init   \t [-1.60703824  0.38933325]. \t  -487.8262244570432 \t -3.0269049669752817\n",
      "init   \t [ 0.12213192 -0.33256477]. \t  -12.844955340992902 \t -3.0269049669752817\n",
      "init   \t [-0.67416945  0.50183959]. \t  -3.0269049669752817 \t -3.0269049669752817\n",
      "1      \t [-0.00884268  0.57735456]. \t  -34.3425644814277 \t -3.0269049669752817\n",
      "2      \t [-0.41056426 -0.32434024]. \t  -26.285052479462937 \t -3.0269049669752817\n",
      "3      \t [-0.18259276  0.05642632]. \t  \u001b[92m-1.451822910191488\u001b[0m \t -1.451822910191488\n",
      "4      \t [-0.69918937  1.52001968]. \t  -109.21508306962447 \t -1.451822910191488\n",
      "5      \t [-0.49491752  0.74319853]. \t  -27.06060131172997 \t -1.451822910191488\n",
      "6      \t [1.25253705 1.38194386]. \t  -3.5571302781407854 \t -1.451822910191488\n",
      "7      \t [0.94735483 0.96480224]. \t  \u001b[92m-0.4559842442259708\u001b[0m \t -0.4559842442259708\n",
      "8      \t [-0.52290982  0.33972368]. \t  -2.7586773885486493 \t -0.4559842442259708\n",
      "9      \t [1.63300715 2.048     ]. \t  -38.681197246392834 \t -0.4559842442259708\n",
      "10     \t [1.01522948 1.32032244]. \t  -8.388875513891564 \t -0.4559842442259708\n",
      "11     \t [0.0982027  0.04786953]. \t  -0.9593592644593685 \t -0.4559842442259708\n",
      "12     \t [-0.03614372 -0.06604734]. \t  -1.5272460498346703 \t -0.4559842442259708\n",
      "13     \t [-0.51042121 -1.54998546]. \t  -330.07792889119304 \t -0.4559842442259708\n",
      "14     \t [-1.4958887  2.048    ]. \t  -9.827424105787463 \t -0.4559842442259708\n",
      "15     \t [0.70156348 0.6219094 ]. \t  -1.7717425513018328 \t -0.4559842442259708\n",
      "16     \t [1.17722931 0.04356452]. \t  -180.20950318768865 \t -0.4559842442259708\n",
      "17     \t [-1.27021696  1.63855742]. \t  -5.216917632880278 \t -0.4559842442259708\n",
      "18     \t [1.13837128 0.90777876]. \t  -15.082115815769942 \t -0.4559842442259708\n",
      "19     \t [-0.23267667 -1.00058447]. \t  -112.76353295205944 \t -0.4559842442259708\n",
      "20     \t [2.01114585 1.80204342]. \t  -503.9766974406107 \t -0.4559842442259708\n",
      "21     \t [1.32272775 2.048     ]. \t  -9.00789058785884 \t -0.4559842442259708\n",
      "22     \t [-1.2777204  2.048    ]. \t  -22.446267520276574 \t -0.4559842442259708\n",
      "23     \t [0.01844167 0.44839901]. \t  -21.039136270192785 \t -0.4559842442259708\n",
      "24     \t [1.13585219 1.29946512]. \t  \u001b[92m-0.02711395550337144\u001b[0m \t -0.02711395550337144\n",
      "25     \t [1.44161565 2.048     ]. \t  -0.28656499806897306 \t -0.02711395550337144\n",
      "26     \t [0.11354968 1.01192361]. \t  -100.59190414532765 \t -0.02711395550337144\n",
      "27     \t [1.12747574 1.2727106 ]. \t  \u001b[92m-0.01647778719065169\u001b[0m \t -0.01647778719065169\n",
      "28     \t [0.10568514 0.11910562]. \t  -1.9648230673134135 \t -0.01647778719065169\n",
      "29     \t [-1.16042126 -0.48243   ]. \t  -339.1942622308875 \t -0.01647778719065169\n",
      "30     \t [ 0.70648111 -1.04731814]. \t  -239.23187114732363 \t -0.01647778719065169\n",
      "31     \t [ 0.48119433 -0.05490514]. \t  -8.47469889811514 \t -0.01647778719065169\n",
      "32     \t [-1.1389719   1.10184857]. \t  -8.393645889324755 \t -0.01647778719065169\n",
      "33     \t [1.7123502  0.67173204]. \t  -511.4533086722718 \t -0.01647778719065169\n",
      "34     \t [ 1.01819213 -0.91463989]. \t  -380.7790056557882 \t -0.01647778719065169\n",
      "35     \t [ 1.72965811 -0.3611323 ]. \t  -1124.692361971671 \t -0.01647778719065169\n",
      "36     \t [0.7715961  0.54976729]. \t  -0.26004276824002376 \t -0.01647778719065169\n",
      "37     \t [1.30363716 1.71805819]. \t  -0.126748190186758 \t -0.01647778719065169\n",
      "38     \t [0.71253773 0.50293746]. \t  -0.08491228159486391 \t -0.01647778719065169\n",
      "39     \t [ 1.21764807 -1.67881643]. \t  -999.5450123676623 \t -0.01647778719065169\n",
      "40     \t [ 0.32029479 -1.97746247]. \t  -433.1233074488358 \t -0.01647778719065169\n",
      "41     \t [0.24322351 0.69817329]. \t  -41.40680608386912 \t -0.01647778719065169\n",
      "42     \t [2.01611511 0.40900302]. \t  -1337.4592511891349 \t -0.01647778719065169\n",
      "43     \t [-1.54621797  1.68497034]. \t  -56.30136737596896 \t -0.01647778719065169\n",
      "44     \t [ 1.58689748 -1.98885517]. \t  -2031.7383830266047 \t -0.01647778719065169\n",
      "45     \t [-0.06960103  0.39329284]. \t  -16.233272792355127 \t -0.01647778719065169\n",
      "46     \t [0.61907672 0.37675687]. \t  -0.1493263881047734 \t -0.01647778719065169\n",
      "47     \t [1.1860111  0.69200666]. \t  -51.10215552529505 \t -0.01647778719065169\n",
      "48     \t [ 1.09019811 -0.85902057]. \t  -419.2552535068078 \t -0.01647778719065169\n",
      "49     \t [0.07221105 0.65215451]. \t  -42.71393786596149 \t -0.01647778719065169\n",
      "50     \t [0.58884561 0.34929577]. \t  -0.1697015622203558 \t -0.01647778719065169\n",
      "51     \t [-0.77698697  1.56001318]. \t  -94.60949982977071 \t -0.01647778719065169\n",
      "52     \t [-0.93002234 -0.66266491]. \t  -237.0831376175313 \t -0.01647778719065169\n",
      "53     \t [-0.14395639  0.83083395]. \t  -66.93653951463152 \t -0.01647778719065169\n",
      "54     \t [ 1.64283333 -0.54736037]. \t  -1054.234750181094 \t -0.01647778719065169\n",
      "55     \t [ 1.26456752 -1.16271586]. \t  -762.8498168433456 \t -0.01647778719065169\n",
      "56     \t [-1.31227773  2.03619975]. \t  -15.214200315819033 \t -0.01647778719065169\n",
      "57     \t [-0.80713641 -0.74818458]. \t  -199.16880468917404 \t -0.01647778719065169\n",
      "58     \t [0.65318575 1.34301448]. \t  -84.09236747648043 \t -0.01647778719065169\n",
      "59     \t [ 1.18645898 -0.35980089]. \t  -312.43536793777577 \t -0.01647778719065169\n",
      "60     \t [-1.34258878 -1.30591657]. \t  -971.7408229441094 \t -0.01647778719065169\n",
      "61     \t [0.42215344 0.18138096]. \t  -0.33490990235066975 \t -0.01647778719065169\n",
      "62     \t [1.17688397 1.38796941]. \t  -0.032136803507514675 \t -0.01647778719065169\n",
      "63     \t [ 0.89735443 -0.11846241]. \t  -85.33406945143894 \t -0.01647778719065169\n",
      "64     \t [1.16666026 1.36111467]. \t  -0.027775677164163383 \t -0.01647778719065169\n",
      "65     \t [-1.91902188 -1.03299139]. \t  -2232.243308095427 \t -0.01647778719065169\n",
      "66     \t [1.47166778 0.25534753]. \t  -365.20765281558084 \t -0.01647778719065169\n",
      "67     \t [0.48556553 0.58101052]. \t  -12.183476168587521 \t -0.01647778719065169\n",
      "68     \t [-0.55155062  0.27051137]. \t  -2.520856175085368 \t -0.01647778719065169\n",
      "69     \t [-1.28137897 -1.19756858]. \t  -811.4810884152644 \t -0.01647778719065169\n",
      "70     \t [1.78871846 1.50721248]. \t  -287.01042744204386 \t -0.01647778719065169\n",
      "71     \t [-1.52580274 -1.09844367]. \t  -1180.4820266939364 \t -0.01647778719065169\n",
      "72     \t [1.74924623 1.41644118]. \t  -270.64469116755384 \t -0.01647778719065169\n",
      "73     \t [ 0.26996934 -0.00881265]. \t  -1.2003699574098554 \t -0.01647778719065169\n",
      "74     \t [ 0.39624454 -1.5080447 ]. \t  -277.60514967002473 \t -0.01647778719065169\n",
      "75     \t [-1.13532691  1.12907122]. \t  -7.11629317883051 \t -0.01647778719065169\n",
      "76     \t [1.59467551 0.79854923]. \t  -304.6609988132639 \t -0.01647778719065169\n",
      "77     \t [1.43432384 2.048     ]. \t  -0.19725809382739182 \t -0.01647778719065169\n",
      "78     \t [ 0.45875963 -1.83421845]. \t  -418.36409948972675 \t -0.01647778719065169\n",
      "79     \t [-0.54559481 -2.02115196]. \t  -540.0841043685225 \t -0.01647778719065169\n",
      "80     \t [-0.14442587  1.30848992]. \t  -167.10909260384756 \t -0.01647778719065169\n",
      "81     \t [1.16813263 0.97525035]. \t  -15.182431900590537 \t -0.01647778719065169\n",
      "82     \t [-1.66291297  0.60205752]. \t  -475.0440655718367 \t -0.01647778719065169\n",
      "83     \t [-1.72488578  0.24382631]. \t  -753.4821381841246 \t -0.01647778719065169\n",
      "84     \t [0.85061931 0.72472308]. \t  -0.022451450323285087 \t -0.01647778719065169\n",
      "85     \t [-0.40389518 -1.25094207]. \t  -201.93127424599834 \t -0.01647778719065169\n",
      "86     \t [0.99042542 0.98749623]. \t  \u001b[92m-0.004386775852060192\u001b[0m \t -0.004386775852060192\n",
      "87     \t [1.82363278 1.78019387]. \t  -239.51767098091653 \t -0.004386775852060192\n",
      "88     \t [1.00944389 1.02531674]. \t  \u001b[92m-0.004108453769549359\u001b[0m \t -0.004108453769549359\n",
      "89     \t [1.37944416 1.87282225]. \t  -0.23424169414195317 \t -0.004108453769549359\n",
      "90     \t [-0.80526177 -0.21591912]. \t  -77.97176485445083 \t -0.004108453769549359\n",
      "91     \t [-1.68156932 -1.93210104]. \t  -2272.737961281629 \t -0.004108453769549359\n",
      "92     \t [ 1.55930984 -1.22208842]. \t  -1335.1450532415909 \t -0.004108453769549359\n",
      "93     \t [-0.73903477  0.13573331]. \t  -19.87026636959404 \t -0.004108453769549359\n",
      "94     \t [-0.0111504  1.0522277]. \t  -111.71457518049557 \t -0.004108453769549359\n",
      "95     \t [1.35707418 1.8092718 ]. \t  -0.23233890330060425 \t -0.004108453769549359\n",
      "96     \t [-1.80615082 -1.30792707]. \t  -2096.4630705644536 \t -0.004108453769549359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [-1.51158698 -1.05923967]. \t  -1124.6318739826365 \t -0.004108453769549359\n",
      "98     \t [-1.21562782 -1.17840815]. \t  -710.4271513891267 \t -0.004108453769549359\n",
      "99     \t [0.95872571 0.92954807]. \t  -0.01250519048231594 \t -0.004108453769549359\n",
      "100    \t [-1.45895949 -1.65195926]. \t  -1435.2811721917542 \t -0.004108453769549359\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\n",
    "\n",
    "np.random.seed(run_num_6)\n",
    "surrogate_loser_6 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_6 = GPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_6.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-1.42268934 -0.80954733]. \t  -808.7939502616841 \t -2.0077595729598063\n",
      "init   \t [-1.79389885 -0.16441204]. \t  -1151.9264213711547 \t -2.0077595729598063\n",
      "init   \t [1.37319786 1.74897991]. \t  -2.0077595729598063 \t -2.0077595729598063\n",
      "init   \t [0.92974688 1.09976052]. \t  -5.543015908052858 \t -2.0077595729598063\n",
      "init   \t [-0.94533605  0.58994398]. \t  -13.008689168416776 \t -2.0077595729598063\n",
      "1      \t [-0.47273281  1.16874249]. \t  -91.52175707418381 \t -2.0077595729598063\n",
      "2      \t [-0.02450474  0.04538531]. \t  \u001b[92m-1.2501780197199097\u001b[0m \t -1.2501780197199097\n",
      "3      \t [1.68585264 0.78240463]. \t  -424.70453514476327 \t -1.2501780197199097\n",
      "4      \t [-0.42007355  0.486865  ]. \t  -11.651624457822306 \t -1.2501780197199097\n",
      "5      \t [-1.44885198  1.39505365]. \t  -55.57515125944981 \t -1.2501780197199097\n",
      "6      \t [ 0.24875508 -1.16743118]. \t  -151.68474322135503 \t -1.2501780197199097\n",
      "7      \t [0.93084749 2.03834578]. \t  -137.33241456947934 \t -1.2501780197199097\n",
      "8      \t [1.06444123 1.42879822]. \t  -8.75173381079799 \t -1.2501780197199097\n",
      "9      \t [-0.98099823  0.95588429]. \t  -3.928544263348288 \t -1.2501780197199097\n",
      "10     \t [2.048 2.048]. \t  -461.7603900415999 \t -1.2501780197199097\n",
      "11     \t [0.62288294 0.18463974]. \t  -4.277071860372212 \t -1.2501780197199097\n",
      "12     \t [-0.69809527  0.50777279]. \t  -2.925289656046603 \t -1.2501780197199097\n",
      "13     \t [1.47313155 1.22820186]. \t  -88.94418548055519 \t -1.2501780197199097\n",
      "14     \t [0.47519087 0.42362014]. \t  -4.188453817223986 \t -1.2501780197199097\n",
      "15     \t [ 0.26461578 -0.15711781]. \t  -5.700016984471431 \t -1.2501780197199097\n",
      "16     \t [-0.06599621 -0.01552053]. \t  \u001b[92m-1.1758535835385266\u001b[0m \t -1.1758535835385266\n",
      "17     \t [1.24180127 1.48658269]. \t  \u001b[92m-0.3663562110243889\u001b[0m \t -0.3663562110243889\n",
      "18     \t [0.47256492 1.92336581]. \t  -289.29457645503635 \t -0.3663562110243889\n",
      "19     \t [-0.92606685  0.78687317]. \t  -4.209959456551836 \t -0.3663562110243889\n",
      "20     \t [ 1.118485   -1.11392497]. \t  -559.3051604113011 \t -0.3663562110243889\n",
      "21     \t [-0.61359581 -2.048     ]. \t  -590.4236287757818 \t -0.3663562110243889\n",
      "22     \t [0.44493233 0.14422343]. \t  -0.5969133387990369 \t -0.3663562110243889\n",
      "23     \t [ 0.41310572 -1.76305071]. \t  -374.26673998424303 \t -0.3663562110243889\n",
      "24     \t [-0.40424352  0.18651119]. \t  -2.0252533407865503 \t -0.3663562110243889\n",
      "25     \t [-0.23527315  1.89308588]. \t  -339.25194620375197 \t -0.3663562110243889\n",
      "26     \t [1.28518478 1.62368908]. \t  \u001b[92m-0.15979097945501483\u001b[0m \t -0.15979097945501483\n",
      "27     \t [-1.51502555 -0.57082521]. \t  -827.7941113195253 \t -0.15979097945501483\n",
      "28     \t [-1.87469876 -0.93454369]. \t  -1987.6588142819153 \t -0.15979097945501483\n",
      "29     \t [-0.46501394 -0.4439421 ]. \t  -45.730037758046485 \t -0.15979097945501483\n",
      "30     \t [-1.93949334  1.69699813]. \t  -434.9129180923196 \t -0.15979097945501483\n",
      "31     \t [-1.2491973   1.36576703]. \t  -8.850743886580085 \t -0.15979097945501483\n",
      "32     \t [ 1.54002649 -1.83787344]. \t  -1772.3269971951645 \t -0.15979097945501483\n",
      "33     \t [-1.8431759  -1.77951983]. \t  -2688.0272945478355 \t -0.15979097945501483\n",
      "34     \t [-1.280005   1.5420202]. \t  -6.127575986568125 \t -0.15979097945501483\n",
      "35     \t [ 0.2736799  -0.01522965]. \t  -1.3398885876565299 \t -0.15979097945501483\n",
      "36     \t [-0.63864392  1.52983507]. \t  -128.5665997916627 \t -0.15979097945501483\n",
      "37     \t [-0.73283867  0.87632897]. \t  -14.513580664318035 \t -0.15979097945501483\n",
      "38     \t [-1.26947568  2.048     ]. \t  -24.19776588850033 \t -0.15979097945501483\n",
      "39     \t [-1.72517692 -0.55026939]. \t  -1251.0501918294053 \t -0.15979097945501483\n",
      "40     \t [0.05564954 0.60649055]. \t  -37.300190512549484 \t -0.15979097945501483\n",
      "41     \t [-1.82254402  1.35506658]. \t  -394.71835903777065 \t -0.15979097945501483\n",
      "42     \t [1.4493702  1.88796319]. \t  -4.72652160481193 \t -0.15979097945501483\n",
      "43     \t [0.8199031  1.16600161]. \t  -24.41237951303199 \t -0.15979097945501483\n",
      "44     \t [-0.86486021  0.97471617]. \t  -8.61848851357872 \t -0.15979097945501483\n",
      "45     \t [1.34803695 0.03901215]. \t  -316.3176151563217 \t -0.15979097945501483\n",
      "46     \t [1.4062769 2.048    ]. \t  -0.6604695406562099 \t -0.15979097945501483\n",
      "47     \t [ 0.83852599 -0.34052251]. \t  -108.9462605973516 \t -0.15979097945501483\n",
      "48     \t [ 1.28403975 -1.80099872]. \t  -1190.162876097512 \t -0.15979097945501483\n",
      "49     \t [1.34714981 1.81726515]. \t  \u001b[92m-0.1211144876496195\u001b[0m \t -0.1211144876496195\n",
      "50     \t [0.7791266  0.59108038]. \t  \u001b[92m-0.07425045027656486\u001b[0m \t -0.07425045027656486\n",
      "51     \t [-1.01553322 -0.07176961]. \t  -125.74033204142796 \t -0.07425045027656486\n",
      "52     \t [ 0.58352362 -0.23479548]. \t  -33.2699195494349 \t -0.07425045027656486\n",
      "53     \t [1.2308309  1.49906477]. \t  -0.07850010578585372 \t -0.07425045027656486\n",
      "54     \t [ 1.95242307 -1.48669605]. \t  -2808.478305233238 \t -0.07425045027656486\n",
      "55     \t [ 0.40544191 -1.72069599]. \t  -355.7058359961106 \t -0.07425045027656486\n",
      "56     \t [0.24163853 0.08129877]. \t  -0.6275970524778705 \t -0.07425045027656486\n",
      "57     \t [-0.46836524  0.05583377]. \t  -4.830375481641471 \t -0.07425045027656486\n",
      "58     \t [0.70000036 0.47705868]. \t  -0.10674884830409836 \t -0.07425045027656486\n",
      "59     \t [-1.23998148 -0.56124367]. \t  -445.5127150577988 \t -0.07425045027656486\n",
      "60     \t [-0.48624993  1.60285367]. \t  -188.91784469060016 \t -0.07425045027656486\n",
      "61     \t [ 1.7078212  -0.23497913]. \t  -993.779676556309 \t -0.07425045027656486\n",
      "62     \t [1.93673741 0.97984521]. \t  -768.7806515093562 \t -0.07425045027656486\n",
      "63     \t [ 1.97666469 -0.70501524]. \t  -2128.209855573222 \t -0.07425045027656486\n",
      "64     \t [1.07958268 1.14061527]. \t  \u001b[92m-0.06825221475838486\u001b[0m \t -0.06825221475838486\n",
      "65     \t [-1.58125633  0.36595552]. \t  -462.2360771499493 \t -0.06825221475838486\n",
      "66     \t [1.11681299 1.23270021]. \t  \u001b[92m-0.034876787138852644\u001b[0m \t -0.034876787138852644\n",
      "67     \t [ 1.29866214 -0.78551704]. \t  -611.1875774241657 \t -0.034876787138852644\n",
      "68     \t [1.06543941 1.11951553]. \t  \u001b[92m-0.028760793969299633\u001b[0m \t -0.028760793969299633\n",
      "69     \t [1.35648952 1.84591095]. \t  -0.13050367421886694 \t -0.028760793969299633\n",
      "70     \t [-1.453335    0.70777001]. \t  -203.2563341481985 \t -0.028760793969299633\n",
      "71     \t [1.55375794 0.04422647]. \t  -561.9669176444024 \t -0.028760793969299633\n",
      "72     \t [-0.12450811 -1.3949929 ]. \t  -200.21418176685694 \t -0.028760793969299633\n",
      "73     \t [0.59921347 0.36182552]. \t  -0.16139643161135042 \t -0.028760793969299633\n",
      "74     \t [1.36052246 1.85931937]. \t  -0.13686212611877535 \t -0.028760793969299633\n",
      "75     \t [-0.62409191 -0.74202643]. \t  -130.6707793379426 \t -0.028760793969299633\n",
      "76     \t [ 1.84142169 -0.68915833]. \t  -1665.3415997891302 \t -0.028760793969299633\n",
      "77     \t [-1.11398796 -0.24648974]. \t  -225.72234792939003 \t -0.028760793969299633\n",
      "78     \t [-0.09797941  0.996038  ]. \t  -98.51155747480169 \t -0.028760793969299633\n",
      "79     \t [1.03613652 1.03388204]. \t  -0.15888987957400896 \t -0.028760793969299633\n",
      "80     \t [ 1.05092279 -1.43042672]. \t  -642.5568651619753 \t -0.028760793969299633\n",
      "81     \t [0.20377698 0.76973986]. \t  -53.66365112401365 \t -0.028760793969299633\n",
      "82     \t [1.0043492  1.01149819]. \t  \u001b[92m-0.0007922403871347229\u001b[0m \t -0.0007922403871347229\n",
      "83     \t [1.84719157 0.07493504]. \t  -1114.3958721029478 \t -0.0007922403871347229\n",
      "84     \t [ 1.10464633 -1.45254347]. \t  -714.3899783273297 \t -0.0007922403871347229\n",
      "85     \t [1.10113693 1.20180191]. \t  -0.021679006356357104 \t -0.0007922403871347229\n",
      "86     \t [-1.15538548 -0.32404392]. \t  -279.8603611900302 \t -0.0007922403871347229\n",
      "87     \t [-0.43085748 -1.0387392 ]. \t  -151.95734865395278 \t -0.0007922403871347229\n",
      "88     \t [-0.33939423  0.24049692]. \t  -3.364198389531944 \t -0.0007922403871347229\n",
      "89     \t [1.35936773 0.81886257]. \t  -106.01696195155297 \t -0.0007922403871347229\n",
      "90     \t [-1.9380262   0.13610953]. \t  -1318.9532876497697 \t -0.0007922403871347229\n",
      "91     \t [ 0.9721382  -0.41205174]. \t  -184.17401640929336 \t -0.0007922403871347229\n",
      "92     \t [ 0.14583905 -0.10642548]. \t  -2.3601797106240374 \t -0.0007922403871347229\n",
      "93     \t [0.01802314 2.0015417 ]. \t  -401.4511721215828 \t -0.0007922403871347229\n",
      "94     \t [ 0.15498551 -0.88449689]. \t  -83.25443505500101 \t -0.0007922403871347229\n",
      "95     \t [0.63363781 0.41420535]. \t  -0.15037179969733092 \t -0.0007922403871347229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [-1.20940169 -1.15883084]. \t  -692.0989167941194 \t -0.0007922403871347229\n",
      "97     \t [1.36083287 1.87749507]. \t  -0.1958847502149283 \t -0.0007922403871347229\n",
      "98     \t [0.63313443 0.41442596]. \t  -0.15299603144374702 \t -0.0007922403871347229\n",
      "99     \t [1.14033305 1.29017386]. \t  -0.03006803698557653 \t -0.0007922403871347229\n",
      "100    \t [-0.54071186  1.67082572]. \t  -192.38799757088222 \t -0.0007922403871347229\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\n",
    "\n",
    "np.random.seed(run_num_7)\n",
    "surrogate_loser_7 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_7 = GPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_7.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-1.59948851 -1.8750873 ]. \t  -1972.3059423071145 \t -7.238275380208551\n",
      "init   \t [ 0.40834083 -0.81972959]. \t  -97.66272647293047 \t -7.238275380208551\n",
      "init   \t [ 1.21492186 -0.54806067]. \t  -409.7425700197761 \t -7.238275380208551\n",
      "init   \t [ 1.55897178 -0.82804067]. \t  -1062.051446884513 \t -7.238275380208551\n",
      "init   \t [-0.69999303  0.28146449]. \t  -7.238275380208551 \t -7.238275380208551\n",
      "1      \t [0.6164745  0.32466281]. \t  \u001b[92m-0.45376407586037554\u001b[0m \t -0.45376407586037554\n",
      "2      \t [ 0.50908955 -0.18347256]. \t  -19.834429029373293 \t -0.45376407586037554\n",
      "3      \t [-1.32333068  0.93303667]. \t  -72.33765762594129 \t -0.45376407586037554\n",
      "4      \t [1.44508292 0.8206238 ]. \t  -160.8894336881283 \t -0.45376407586037554\n",
      "5      \t [-0.62676042  0.76344834]. \t  -16.382247344473406 \t -0.45376407586037554\n",
      "6      \t [-1.032139    0.43005276]. \t  -44.48488220039099 \t -0.45376407586037554\n",
      "7      \t [-2.048  2.048]. \t  -469.9523900415999 \t -0.45376407586037554\n",
      "8      \t [-0.56185309  0.40368017]. \t  -3.213807568923504 \t -0.45376407586037554\n",
      "9      \t [0.88835827 0.23244623]. \t  -31.00775999418488 \t -0.45376407586037554\n",
      "10     \t [0.43882728 1.55440029]. \t  -185.7732570546789 \t -0.45376407586037554\n",
      "11     \t [ 0.64467069 -0.79401276]. \t  -146.44263235022598 \t -0.45376407586037554\n",
      "12     \t [0.2852873  0.61793147]. \t  -29.298613771896225 \t -0.45376407586037554\n",
      "13     \t [2.048 2.048]. \t  -461.7603900415999 \t -0.45376407586037554\n",
      "14     \t [-1.07680911 -0.66245332]. \t  -336.27103308075306 \t -0.45376407586037554\n",
      "15     \t [-1.39359154  0.69519013]. \t  -161.20705136277712 \t -0.45376407586037554\n",
      "16     \t [ 1.9323183 -1.5303425]. \t  -2772.0457234584587 \t -0.45376407586037554\n",
      "17     \t [-1.09598532  1.29103451]. \t  -5.200469214996014 \t -0.45376407586037554\n",
      "18     \t [-1.54848356 -1.82686842]. \t  -1791.2782240225968 \t -0.45376407586037554\n",
      "19     \t [-0.79976726  0.61285556]. \t  -3.310836720784585 \t -0.45376407586037554\n",
      "20     \t [-0.17881907 -1.96887286]. \t  -401.7293350850919 \t -0.45376407586037554\n",
      "21     \t [-0.09416479 -2.01791579]. \t  -411.98204758939727 \t -0.45376407586037554\n",
      "22     \t [-0.25896759  1.05540738]. \t  -99.26722111254946 \t -0.45376407586037554\n",
      "23     \t [-0.24884643  1.86338313]. \t  -326.08491946804156 \t -0.45376407586037554\n",
      "24     \t [-0.96524174 -1.36581502]. \t  -531.715849495629 \t -0.45376407586037554\n",
      "25     \t [0.48291786 0.24483966]. \t  \u001b[92m-0.28089963048330086\u001b[0m \t -0.28089963048330086\n",
      "26     \t [-1.13686556  0.57091361]. \t  -56.629591495046235 \t -0.28089963048330086\n",
      "27     \t [0.04232686 0.05822716]. \t  -1.2356354817148862 \t -0.28089963048330086\n",
      "28     \t [0.2767406  0.46772662]. \t  -15.82225228302519 \t -0.28089963048330086\n",
      "29     \t [-1.00237446  1.05328022]. \t  -4.244977486898595 \t -0.28089963048330086\n",
      "30     \t [1.99728149 1.79335453]. \t  -483.1390356175105 \t -0.28089963048330086\n",
      "31     \t [-0.45283819  1.95181393]. \t  -307.22482120864316 \t -0.28089963048330086\n",
      "32     \t [ 1.98022044 -0.46929244]. \t  -1928.6673011391758 \t -0.28089963048330086\n",
      "33     \t [-1.25330541  1.95501977]. \t  -19.841831903727883 \t -0.28089963048330086\n",
      "34     \t [ 0.66061502 -0.95589505]. \t  -193.9671294202154 \t -0.28089963048330086\n",
      "35     \t [-1.65658863  0.95732344]. \t  -326.3809421815248 \t -0.28089963048330086\n",
      "36     \t [-1.80825474  0.3658329 ]. \t  -851.1801983666131 \t -0.28089963048330086\n",
      "37     \t [-2.04007732  1.73210541]. \t  -599.6397734779823 \t -0.28089963048330086\n",
      "38     \t [-0.69666159  2.01108644]. \t  -235.66968390762443 \t -0.28089963048330086\n",
      "39     \t [-0.11215858  1.13031619]. \t  -126.17041680603957 \t -0.28089963048330086\n",
      "40     \t [0.91482339 0.79423007]. \t  \u001b[92m-0.18934306041291585\u001b[0m \t -0.18934306041291585\n",
      "41     \t [2.02060323 1.4606836 ]. \t  -688.6106859946037 \t -0.18934306041291585\n",
      "42     \t [1.32620756 1.89312966]. \t  -1.9101456898767162 \t -0.18934306041291585\n",
      "43     \t [1.24394179 1.51526971]. \t  \u001b[92m-0.16268653948898826\u001b[0m \t -0.16268653948898826\n",
      "44     \t [0.31095434 0.0873135 ]. \t  -0.48358067086360496 \t -0.16268653948898826\n",
      "45     \t [1.25805159 0.19123451]. \t  -193.68248995741166 \t -0.16268653948898826\n",
      "46     \t [-1.07582552  1.90269457]. \t  -59.855369374280805 \t -0.16268653948898826\n",
      "47     \t [-0.51424044 -0.06635342]. \t  -13.235566372013212 \t -0.16268653948898826\n",
      "48     \t [0.82402371 0.6225959 ]. \t  -0.34928005198783274 \t -0.16268653948898826\n",
      "49     \t [ 0.18357043 -0.01507331]. \t  -0.9044222849638495 \t -0.16268653948898826\n",
      "50     \t [-0.39428439  1.1020433 ]. \t  -91.54598937155194 \t -0.16268653948898826\n",
      "51     \t [1.03799728 1.22229839]. \t  -2.0998868809733406 \t -0.16268653948898826\n",
      "52     \t [-0.56482597 -1.12583464]. \t  -211.21159278875356 \t -0.16268653948898826\n",
      "53     \t [1.24601934 1.69185633]. \t  -2.000755285254035 \t -0.16268653948898826\n",
      "54     \t [-1.18082414  1.75523799]. \t  -17.78032166588062 \t -0.16268653948898826\n",
      "55     \t [ 1.25588263 -1.705961  ]. \t  -1078.0071343372058 \t -0.16268653948898826\n",
      "56     \t [-1.25725437 -0.32600639]. \t  -368.64375370499255 \t -0.16268653948898826\n",
      "57     \t [1.05624264 1.06021813]. \t  -0.3104159927602636 \t -0.16268653948898826\n",
      "58     \t [0.88169297 1.8141501 ]. \t  -107.50270556616614 \t -0.16268653948898826\n",
      "59     \t [-1.26352511  0.09110064]. \t  -231.7449777164809 \t -0.16268653948898826\n",
      "60     \t [-1.07729279  0.11680071]. \t  -113.25844116075896 \t -0.16268653948898826\n",
      "61     \t [-0.26935638 -0.24773242]. \t  -11.86953142243642 \t -0.16268653948898826\n",
      "62     \t [-1.43699318  0.90086473]. \t  -141.448244037027 \t -0.16268653948898826\n",
      "63     \t [0.99005936 0.96303684]. \t  \u001b[92m-0.029616457175806413\u001b[0m \t -0.029616457175806413\n",
      "64     \t [ 1.58178491 -1.94189443]. \t  -1975.1968996050994 \t -0.029616457175806413\n",
      "65     \t [1.85635975 1.58021445]. \t  -348.87561045171907 \t -0.029616457175806413\n",
      "66     \t [-2.02223264 -0.51849968]. \t  -2132.4307428489506 \t -0.029616457175806413\n",
      "67     \t [-1.37121146  0.53976074]. \t  -185.30597907635834 \t -0.029616457175806413\n",
      "68     \t [-1.10591708 -1.40193983]. \t  -693.4934082066238 \t -0.029616457175806413\n",
      "69     \t [0.88375916 0.93238652]. \t  -2.3043837843615584 \t -0.029616457175806413\n",
      "70     \t [ 0.20673218 -0.15698146]. \t  -4.618067879582819 \t -0.029616457175806413\n",
      "71     \t [ 1.92973463 -0.0818242 ]. \t  -1449.1996114104088 \t -0.029616457175806413\n",
      "72     \t [1.32948596 1.71639491]. \t  -0.37007060608890185 \t -0.029616457175806413\n",
      "73     \t [ 0.57074712 -0.03149577]. \t  -12.946874619150488 \t -0.029616457175806413\n",
      "74     \t [1.28473937 1.60313041]. \t  -0.30598814683547887 \t -0.029616457175806413\n",
      "75     \t [-0.83864115 -0.48317136]. \t  -144.15653311972133 \t -0.029616457175806413\n",
      "76     \t [ 1.81540232 -0.88185794]. \t  -1745.8518633501558 \t -0.029616457175806413\n",
      "77     \t [-1.21131546e+00 -4.31045776e-04]. \t  -220.30899682730103 \t -0.029616457175806413\n",
      "78     \t [ 2.02210299 -1.73191346]. \t  -3389.2322236048617 \t -0.029616457175806413\n",
      "79     \t [-0.12581057 -1.97756492]. \t  -398.6291010451451 \t -0.029616457175806413\n",
      "80     \t [1.40959923 2.048     ]. \t  -0.5402378130625347 \t -0.029616457175806413\n",
      "81     \t [1.30629721 1.66037838]. \t  -0.305730977917589 \t -0.029616457175806413\n",
      "82     \t [1.30180202 1.64962359]. \t  -0.29416903650652076 \t -0.029616457175806413\n",
      "83     \t [ 1.45351392 -0.62312236]. \t  -748.6795639055398 \t -0.029616457175806413\n",
      "84     \t [-1.10985455  1.86904196]. \t  -45.0621325530082 \t -0.029616457175806413\n",
      "85     \t [-0.31437303 -1.03790435]. \t  -130.94416630874125 \t -0.029616457175806413\n",
      "86     \t [ 0.190941   -0.04878897]. \t  -1.3812889957609409 \t -0.029616457175806413\n",
      "87     \t [0.26139067 0.5017304 ]. \t  -19.329560711259465 \t -0.029616457175806413\n",
      "88     \t [-0.93060338  0.17375074]. \t  -51.651269276420294 \t -0.029616457175806413\n",
      "89     \t [0.69718911 0.49831639]. \t  -0.10668536903475861 \t -0.029616457175806413\n",
      "90     \t [ 0.13619448 -0.65511577]. \t  -46.12857340817518 \t -0.029616457175806413\n",
      "91     \t [0.66891714 0.45693919]. \t  -0.11862008224737726 \t -0.029616457175806413\n",
      "92     \t [-0.08108414  1.84567641]. \t  -339.39827472078963 \t -0.029616457175806413\n",
      "93     \t [0.68434031 0.4798637 ]. \t  -0.11296290571470906 \t -0.029616457175806413\n",
      "94     \t [0.65952851 1.43503437]. \t  -100.12722412902917 \t -0.029616457175806413\n",
      "95     \t [-0.8577131   1.03479741]. \t  -12.398713195129323 \t -0.029616457175806413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [-2.01567355 -0.66988734]. \t  -2249.0596095799283 \t -0.029616457175806413\n",
      "97     \t [ 2.02019527 -1.57878686]. \t  -3204.5733881370375 \t -0.029616457175806413\n",
      "98     \t [-0.03460129  1.98045929]. \t  -392.81822179015063 \t -0.029616457175806413\n",
      "99     \t [-0.05600739 -1.1276132 ]. \t  -128.97471474266496 \t -0.029616457175806413\n",
      "100    \t [1.18931045 1.35118693]. \t  -0.43617834160686086 \t -0.029616457175806413\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\n",
    "\n",
    "np.random.seed(run_num_8)\n",
    "surrogate_loser_8 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_8 = GPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_8.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.70565298 -0.04883088]. \t  -29.9831488845538 \t -29.9831488845538\n",
      "init   \t [ 1.33322823 -1.9191956 ]. \t  -1366.6650412963722 \t -29.9831488845538\n",
      "init   \t [1.26177265 0.26876895]. \t  -175.18114988457984 \t -29.9831488845538\n",
      "init   \t [-0.82893825 -1.85673433]. \t  -650.4739702903224 \t -29.9831488845538\n",
      "init   \t [ 2.00960983 -2.0200418 ]. \t  -3671.650548034952 \t -29.9831488845538\n",
      "1      \t [ 0.43606844 -1.67369118]. \t  -347.71053213071485 \t -29.9831488845538\n",
      "2      \t [-0.13839446  0.7147904 ]. \t  -49.68707676672544 \t -29.9831488845538\n",
      "3      \t [-1.8510361   2.03742245]. \t  -201.03611268756 \t -29.9831488845538\n",
      "4      \t [ 1.36280034 -0.74164998]. \t  -675.5466126021265 \t -29.9831488845538\n",
      "5      \t [0.42078817 0.31547559]. \t  \u001b[92m-2.2512996056908405\u001b[0m \t -2.2512996056908405\n",
      "6      \t [-1.19704929  0.13798041]. \t  -172.5156939449146 \t -2.2512996056908405\n",
      "7      \t [-1.33704579  1.04920576]. \t  -59.99789396651467 \t -2.2512996056908405\n",
      "8      \t [-0.15828987  1.24297839]. \t  -149.67520799598637 \t -2.2512996056908405\n",
      "9      \t [-0.68251278  1.25763828]. \t  -65.527882489692 \t -2.2512996056908405\n",
      "10     \t [-2.01346326 -1.00148458]. \t  -2564.9080532207845 \t -2.2512996056908405\n",
      "11     \t [-0.61670841  0.32014372]. \t  -2.9759760760009555 \t -2.2512996056908405\n",
      "12     \t [-0.92944254  0.60020216]. \t  -10.674475932592117 \t -2.2512996056908405\n",
      "13     \t [0.26659492 0.21386467]. \t  -2.576833295854134 \t -2.2512996056908405\n",
      "14     \t [-1.84498644 -1.0743048 ]. \t  -2013.5929248868647 \t -2.2512996056908405\n",
      "15     \t [ 0.3754054  -1.02590489]. \t  -136.54030096647267 \t -2.2512996056908405\n",
      "16     \t [-0.63587684 -0.38921785]. \t  -65.64939686735737 \t -2.2512996056908405\n",
      "17     \t [ 1.63480771 -0.56566643]. \t  -1049.0374887651844 \t -2.2512996056908405\n",
      "18     \t [2.048      1.43539746]. \t  -762.2548335919013 \t -2.2512996056908405\n",
      "19     \t [1.83310149 1.59724977]. \t  -311.51493996050334 \t -2.2512996056908405\n",
      "20     \t [1.24346466 2.048     ]. \t  -25.239161082041527 \t -2.2512996056908405\n",
      "21     \t [1.16346116 1.29482541]. \t  \u001b[92m-0.372657204635996\u001b[0m \t -0.372657204635996\n",
      "22     \t [1.5556845 2.048    ]. \t  -14.158664625390305 \t -0.372657204635996\n",
      "23     \t [1.35470952 1.74065179]. \t  -1.020471893124139 \t -0.372657204635996\n",
      "24     \t [1.05690445 0.86672317]. \t  -6.269440678154241 \t -0.372657204635996\n",
      "25     \t [-1.41959775 -1.09305594]. \t  -972.0158623952941 \t -0.372657204635996\n",
      "26     \t [-1.88587156  0.91344747]. \t  -706.9070146986345 \t -0.372657204635996\n",
      "27     \t [-0.64415618  0.12182517]. \t  -11.294714704146086 \t -0.372657204635996\n",
      "28     \t [-1.11146794  2.048     ]. \t  -70.49651406754407 \t -0.372657204635996\n",
      "29     \t [-1.40918017  1.68587243]. \t  -14.79912853253838 \t -0.372657204635996\n",
      "30     \t [-1.19989204  1.47928733]. \t  -4.995917044029757 \t -0.372657204635996\n",
      "31     \t [0.58181308 1.3615467 ]. \t  -104.83601437806144 \t -0.372657204635996\n",
      "32     \t [1.19426837 1.56898936]. \t  -2.0744237819649807 \t -0.372657204635996\n",
      "33     \t [0.85032359 0.68430792]. \t  \u001b[92m-0.1724995416864819\u001b[0m \t -0.1724995416864819\n",
      "34     \t [1.82219672 1.80090893]. \t  -231.56158444838957 \t -0.1724995416864819\n",
      "35     \t [0.72676339 0.43461712]. \t  -0.9501534583661697 \t -0.1724995416864819\n",
      "36     \t [ 1.63076731 -1.45423047]. \t  -1692.5951079213537 \t -0.1724995416864819\n",
      "37     \t [-0.33797863  0.41658592]. \t  -10.932124152684748 \t -0.1724995416864819\n",
      "38     \t [-0.36329081  1.32777155]. \t  -144.85025260403364 \t -0.1724995416864819\n",
      "39     \t [0.35214645 0.08158721]. \t  -0.5996591297057909 \t -0.1724995416864819\n",
      "40     \t [-0.98511057  1.00135647]. \t  -4.036229232607207 \t -0.1724995416864819\n",
      "41     \t [-0.59836164 -1.03523576]. \t  -196.67556195811156 \t -0.1724995416864819\n",
      "42     \t [-0.15119024 -1.71306133]. \t  -302.6670012948018 \t -0.1724995416864819\n",
      "43     \t [1.25165206 1.51651917]. \t  -0.31446702373142194 \t -0.1724995416864819\n",
      "44     \t [0.99127909 1.00634457]. \t  \u001b[92m-0.05629401970937036\u001b[0m \t -0.05629401970937036\n",
      "45     \t [-1.2051055   1.08983678]. \t  -17.998945242101303 \t -0.05629401970937036\n",
      "46     \t [-1.87945054 -0.53187245]. \t  -1660.0689019354684 \t -0.05629401970937036\n",
      "47     \t [-0.0868701  -0.07454394]. \t  -1.8551691418678062 \t -0.05629401970937036\n",
      "48     \t [0.64058855 1.94804941]. \t  -236.57998974621958 \t -0.05629401970937036\n",
      "49     \t [-1.72312315 -0.3832916 ]. \t  -1131.3041455848916 \t -0.05629401970937036\n",
      "50     \t [ 1.98436426 -0.06430682]. \t  -1602.5760280513484 \t -0.05629401970937036\n",
      "51     \t [ 0.88625641 -1.55731416]. \t  -548.8675271403135 \t -0.05629401970937036\n",
      "52     \t [-0.25987559 -0.36331888]. \t  -20.150820827709044 \t -0.05629401970937036\n",
      "53     \t [-1.54536301  0.97857225]. \t  -205.16892502915746 \t -0.05629401970937036\n",
      "54     \t [-1.03742352  0.77025954]. \t  -13.513961478754117 \t -0.05629401970937036\n",
      "55     \t [1.22428949 1.46928835]. \t  -0.13790057367087188 \t -0.05629401970937036\n",
      "56     \t [ 1.63554429 -0.09981155]. \t  -770.3646643128745 \t -0.05629401970937036\n",
      "57     \t [-0.54231303  0.00288144]. \t  -10.859753918600669 \t -0.05629401970937036\n",
      "58     \t [-1.49595357 -0.91197093]. \t  -998.3840314171645 \t -0.05629401970937036\n",
      "59     \t [0.65487724 0.95429416]. \t  -27.726773753029832 \t -0.05629401970937036\n",
      "60     \t [0.94958602 0.91021023]. \t  \u001b[92m-0.00976080774599615\u001b[0m \t -0.00976080774599615\n",
      "61     \t [ 1.846403  -1.6901432]. \t  -2601.050623124933 \t -0.00976080774599615\n",
      "62     \t [0.91207372 0.84021571]. \t  -0.014681990172699682 \t -0.00976080774599615\n",
      "63     \t [0.46873837 1.96743036]. \t  -305.7329054062828 \t -0.00976080774599615\n",
      "64     \t [ 0.27674062 -1.32846936]. \t  -197.9409839920002 \t -0.00976080774599615\n",
      "65     \t [1.60269915 1.7340784 ]. \t  -70.01331296577641 \t -0.00976080774599615\n",
      "66     \t [0.00140331 1.36362061]. \t  -186.94277558703737 \t -0.00976080774599615\n",
      "67     \t [0.47213099 1.05764644]. \t  -69.95752629871546 \t -0.00976080774599615\n",
      "68     \t [-1.79204133 -1.16368841]. \t  -1921.9459548040177 \t -0.00976080774599615\n",
      "69     \t [-0.55761127 -1.33865854]. \t  -274.54049687577555 \t -0.00976080774599615\n",
      "70     \t [ 1.2980682  -1.25427281]. \t  -864.0101684671865 \t -0.00976080774599615\n",
      "71     \t [-1.84667548 -0.8660456 ]. \t  -1836.7400488056433 \t -0.00976080774599615\n",
      "72     \t [0.35377083 0.55626988]. \t  -19.00371957437456 \t -0.00976080774599615\n",
      "73     \t [ 0.23754772 -0.61027856]. \t  -45.03121945417927 \t -0.00976080774599615\n",
      "74     \t [-1.66305092 -0.65620777]. \t  -1178.0633699721684 \t -0.00976080774599615\n",
      "75     \t [ 0.13847559 -0.18332592]. \t  -4.842906597165351 \t -0.00976080774599615\n",
      "76     \t [-0.88307779  0.82963019]. \t  -3.7940238790094662 \t -0.00976080774599615\n",
      "77     \t [0.81461897 0.64916221]. \t  -0.05522282730712497 \t -0.00976080774599615\n",
      "78     \t [-0.69654673 -1.22160209]. \t  -294.18787606883797 \t -0.00976080774599615\n",
      "79     \t [-0.46708971 -1.31363253]. \t  -236.7951087892674 \t -0.00976080774599615\n",
      "80     \t [ 0.05482189 -1.28791705]. \t  -167.54144919181485 \t -0.00976080774599615\n",
      "81     \t [0.79924822 0.21251998]. \t  -18.211571521285908 \t -0.00976080774599615\n",
      "82     \t [-1.85316016  0.25770396]. \t  -1017.154878475344 \t -0.00976080774599615\n",
      "83     \t [ 1.69336897 -0.61239054]. \t  -1211.4435039774137 \t -0.00976080774599615\n",
      "84     \t [0.84865451 0.72737381]. \t  -0.028031062039438847 \t -0.00976080774599615\n",
      "85     \t [-0.5255028   0.31883678]. \t  -2.509347690056986 \t -0.00976080774599615\n",
      "86     \t [1.78829037 0.77377538]. \t  -588.2993861148012 \t -0.00976080774599615\n",
      "87     \t [-0.2330249  -0.80892747]. \t  -76.03662216236455 \t -0.00976080774599615\n",
      "88     \t [-1.43296824  0.75859945]. \t  -173.569656950802 \t -0.00976080774599615\n",
      "89     \t [-0.93161299 -0.72401088]. \t  -257.1500312220242 \t -0.00976080774599615\n",
      "90     \t [1.14179758 1.48818296]. \t  -3.423439666435232 \t -0.00976080774599615\n",
      "91     \t [0.59657037 0.36192193]. \t  -0.16638641108880176 \t -0.00976080774599615\n",
      "92     \t [1.28316479 1.59543828]. \t  -0.3410335449016928 \t -0.00976080774599615\n",
      "93     \t [0.59182538 0.35597083]. \t  -0.16987097899495665 \t -0.00976080774599615\n",
      "94     \t [-1.25086189 -0.9100251 ]. \t  -617.4707731834394 \t -0.00976080774599615\n",
      "95     \t [-1.19440196 -1.04886507]. \t  -617.6061689884486 \t -0.00976080774599615\n",
      "96     \t [ 1.98571538 -1.49840793]. \t  -2961.9350309063184 \t -0.00976080774599615\n",
      "97     \t [-1.03840028 -0.95345358]. \t  -416.9472377976473 \t -0.00976080774599615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98     \t [0.64266637 0.42010077]. \t  -0.13270096373467086 \t -0.00976080774599615\n",
      "99     \t [-0.45261262  0.90300121]. \t  -50.85045173697143 \t -0.00976080774599615\n",
      "100    \t [ 1.19856704 -0.13210183]. \t  -246.11034989199715 \t -0.00976080774599615\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\n",
    "\n",
    "np.random.seed(run_num_9)\n",
    "surrogate_loser_9 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_9 = GPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_9.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.62910294 -1.57693156]. \t  -389.29291138113445 \t -8.580376531587937\n",
      "init   \t [ 1.84435861 -0.07294402]. \t  -1207.999341247548 \t -8.580376531587937\n",
      "init   \t [ 1.5256557  -1.17828534]. \t  -1229.4172568797705 \t -8.580376531587937\n",
      "init   \t [-1.88125338 -0.42109149]. \t  -1576.6245828972787 \t -8.580376531587937\n",
      "init   \t [-1.09309052  1.39977001]. \t  -8.580376531587937 \t -8.580376531587937\n",
      "1      \t [-0.85767015  2.048     ]. \t  -175.69081547649077 \t -8.580376531587937\n",
      "2      \t [-0.5337917   1.15217037]. \t  -77.5624817749036 \t -8.580376531587937\n",
      "3      \t [-1.87496607  1.74203438]. \t  -322.7826713148287 \t -8.580376531587937\n",
      "4      \t [-0.02273896 -1.98197652]. \t  -394.07407427702145 \t -8.580376531587937\n",
      "5      \t [-0.45198894  1.79927878]. \t  -256.50591736736254 \t -8.580376531587937\n",
      "6      \t [1.82395491 1.22407741]. \t  -442.82797197898645 \t -8.580376531587937\n",
      "7      \t [0.76191163 0.20393692]. \t  -14.237364089085258 \t -8.580376531587937\n",
      "8      \t [0.27906648 0.14523119]. \t  \u001b[92m-0.9733889581797339\u001b[0m \t -0.9733889581797339\n",
      "9      \t [0.53651394 0.48570281]. \t  -4.129503229441299 \t -0.9733889581797339\n",
      "10     \t [0.5075319  0.16639316]. \t  -1.0741861547286604 \t -0.9733889581797339\n",
      "11     \t [0.82681696 0.51407135]. \t  -2.9048798393068798 \t -0.9733889581797339\n",
      "12     \t [-0.41232715  0.43404141]. \t  -8.965731880345292 \t -0.9733889581797339\n",
      "13     \t [0.09685213 1.14732323]. \t  -130.30708024312105 \t -0.9733889581797339\n",
      "14     \t [-0.05292324 -0.03230472]. \t  -1.2318875908055162 \t -0.9733889581797339\n",
      "15     \t [0.67024592 0.41040153]. \t  \u001b[92m-0.25949962289603734\u001b[0m \t -0.25949962289603734\n",
      "16     \t [ 0.25042996 -0.16555912]. \t  -5.7727700106054725 \t -0.25949962289603734\n",
      "17     \t [0.90326441 1.1188775 ]. \t  -9.189706511440672 \t -0.25949962289603734\n",
      "18     \t [0.80699632 0.783624  ]. \t  -1.7897214840504985 \t -0.25949962289603734\n",
      "19     \t [-0.08358053  0.1756553 ]. \t  -4.019090099792217 \t -0.25949962289603734\n",
      "20     \t [-1.0616581   1.04308552]. \t  -4.956578467866227 \t -0.25949962289603734\n",
      "21     \t [ 0.10767256 -0.01557173]. \t  -0.8700425774512041 \t -0.25949962289603734\n",
      "22     \t [-0.54406621  0.55178079]. \t  -8.926110786722774 \t -0.25949962289603734\n",
      "23     \t [1.40780934 1.88021852]. \t  -1.2007728814974612 \t -0.25949962289603734\n",
      "24     \t [-0.08262942  1.92456551]. \t  -368.94394876270246 \t -0.25949962289603734\n",
      "25     \t [1.36484112 2.048     ]. \t  -3.563336267724233 \t -0.25949962289603734\n",
      "26     \t [1.45733524 0.6290517 ]. \t  -223.64417807412963 \t -0.25949962289603734\n",
      "27     \t [-0.24544502  1.24586148]. \t  -142.12018927681916 \t -0.25949962289603734\n",
      "28     \t [1.24267316 1.59589791]. \t  -0.32577948061250184 \t -0.25949962289603734\n",
      "29     \t [1.82728722 0.31366993]. \t  -915.9336552102571 \t -0.25949962289603734\n",
      "30     \t [1.13205766 1.17048564]. \t  -1.2510692927073752 \t -0.25949962289603734\n",
      "31     \t [0.98187324 0.92603428]. \t  \u001b[92m-0.14503875645377873\u001b[0m \t -0.14503875645377873\n",
      "32     \t [-0.31793576  0.82467337]. \t  -54.09523653176086 \t -0.14503875645377873\n",
      "33     \t [0.83897536 0.68296945]. \t  \u001b[92m-0.06965261921905326\u001b[0m \t -0.06965261921905326\n",
      "34     \t [-0.18459878 -0.99157824]. \t  -106.60008293430585 \t -0.06965261921905326\n",
      "35     \t [ 2.04545002 -1.72379955]. \t  -3491.1439590465025 \t -0.06965261921905326\n",
      "36     \t [0.98717698 0.86455261]. \t  -1.2094115911311565 \t -0.06965261921905326\n",
      "37     \t [-0.49143264  0.98704416]. \t  -57.80707919856127 \t -0.06965261921905326\n",
      "38     \t [-1.9731415   1.30257183]. \t  -680.0202743820246 \t -0.06965261921905326\n",
      "39     \t [ 0.55732215 -1.45954397]. \t  -313.5397560590302 \t -0.06965261921905326\n",
      "40     \t [-0.71931561  1.74910406]. \t  -154.66185238171147 \t -0.06965261921905326\n",
      "41     \t [1.01964144 1.02597538]. \t  \u001b[92m-0.019136384783796413\u001b[0m \t -0.019136384783796413\n",
      "42     \t [-1.17017422 -0.53387238]. \t  -366.9190964188499 \t -0.019136384783796413\n",
      "43     \t [ 1.09423273 -1.03254202]. \t  -497.2486072284271 \t -0.019136384783796413\n",
      "44     \t [-1.87395402  0.49278705]. \t  -919.6453670223667 \t -0.019136384783796413\n",
      "45     \t [-0.39397142  0.06160881]. \t  -2.8193397143106482 \t -0.019136384783796413\n",
      "46     \t [ 1.89366666 -1.78216789]. \t  -2882.4927331825857 \t -0.019136384783796413\n",
      "47     \t [1.4471799 2.048    ]. \t  -0.41461353561270375 \t -0.019136384783796413\n",
      "48     \t [1.0304227  1.05837174]. \t  \u001b[92m-0.002080999024356565\u001b[0m \t -0.002080999024356565\n",
      "49     \t [1.02664626 1.0496866 ]. \t  -0.002572769217248201 \t -0.002080999024356565\n",
      "50     \t [1.36812328 1.89993209]. \t  -0.21487408127456034 \t -0.002080999024356565\n",
      "51     \t [ 0.75641574 -1.40332656]. \t  -390.315934888768 \t -0.002080999024356565\n",
      "52     \t [-0.84502623 -1.83974622]. \t  -655.6015088704874 \t -0.002080999024356565\n",
      "53     \t [0.84464057 1.90600766]. \t  -142.25122111186738 \t -0.002080999024356565\n",
      "54     \t [1.90861306 1.11266196]. \t  -640.9873500056517 \t -0.002080999024356565\n",
      "55     \t [-1.49630139  1.46255007]. \t  -66.50621539864875 \t -0.002080999024356565\n",
      "56     \t [-0.77159888  0.08083053]. \t  -29.613117616473627 \t -0.002080999024356565\n",
      "57     \t [-1.44741732  2.048     ]. \t  -6.21091046371543 \t -0.002080999024356565\n",
      "58     \t [-1.34492302  1.79667011]. \t  -5.513420942184953 \t -0.002080999024356565\n",
      "59     \t [1.31417954 1.98169538]. \t  -6.582225985731442 \t -0.002080999024356565\n",
      "60     \t [1.22557584 1.48303182]. \t  -0.08700091094830492 \t -0.002080999024356565\n",
      "61     \t [-1.66270093 -1.55400635]. \t  -1872.1039474825202 \t -0.002080999024356565\n",
      "62     \t [ 1.8351678  -0.07827489]. \t  -1188.26887670644 \t -0.002080999024356565\n",
      "63     \t [-1.81823939  1.2990678 ]. \t  -410.71794193666307 \t -0.002080999024356565\n",
      "64     \t [0.47241056 1.13269949]. \t  -83.00242487243595 \t -0.002080999024356565\n",
      "65     \t [ 1.2739886  -1.98423576]. \t  -1301.3239220270361 \t -0.002080999024356565\n",
      "66     \t [-1.4064444  -0.22099504]. \t  -489.3866516394009 \t -0.002080999024356565\n",
      "67     \t [-1.0497881   1.62391789]. \t  -31.43571377729516 \t -0.002080999024356565\n",
      "68     \t [-0.34965343 -1.95956872]. \t  -435.221610964645 \t -0.002080999024356565\n",
      "69     \t [ 1.51152057 -1.89587699]. \t  -1747.9794032094064 \t -0.002080999024356565\n",
      "70     \t [-1.40558332 -1.91251359]. \t  -1517.579699438475 \t -0.002080999024356565\n",
      "71     \t [ 1.40517301 -0.85657528]. \t  -801.669225988052 \t -0.002080999024356565\n",
      "72     \t [ 0.30886344 -0.26784294]. \t  -13.67196772240941 \t -0.002080999024356565\n",
      "73     \t [ 1.63415265 -1.63483115]. \t  -1853.9509454567055 \t -0.002080999024356565\n",
      "74     \t [2.03618554 0.66993058]. \t  -1209.415371467293 \t -0.002080999024356565\n",
      "75     \t [-1.73400181  1.72905703]. \t  -170.72783899706164 \t -0.002080999024356565\n",
      "76     \t [ 0.66620021 -1.06959045]. \t  -229.15336766252585 \t -0.002080999024356565\n",
      "77     \t [1.13639142 1.29070187]. \t  -0.018649345714716378 \t -0.002080999024356565\n",
      "78     \t [ 0.29383177 -1.78799774]. \t  -351.8117852221128 \t -0.002080999024356565\n",
      "79     \t [0.35188336 2.00297952]. \t  -353.54339364049775 \t -0.002080999024356565\n",
      "80     \t [-1.91804923  0.41488613]. \t  -1073.902053974256 \t -0.002080999024356565\n",
      "81     \t [ 2.02131551 -1.66183259]. \t  -3304.4750197844164 \t -0.002080999024356565\n",
      "82     \t [0.93199488 0.87309423]. \t  -0.006631539674353539 \t -0.002080999024356565\n",
      "83     \t [-1.16956642  1.40716474]. \t  -4.861303510122351 \t -0.002080999024356565\n",
      "84     \t [0.93293501 0.90391066]. \t  -0.11701051986194459 \t -0.002080999024356565\n",
      "85     \t [ 1.66561392 -1.2937774 ]. \t  -1655.3437923316073 \t -0.002080999024356565\n",
      "86     \t [ 0.22527611 -1.7089182 ]. \t  -310.24317612133376 \t -0.002080999024356565\n",
      "87     \t [0.91340862 0.83880666]. \t  -0.009515283152860002 \t -0.002080999024356565\n",
      "88     \t [ 1.45792617 -0.2476256 ]. \t  -563.4053281955346 \t -0.002080999024356565\n",
      "89     \t [-1.67514889 -0.19090665]. \t  -905.3755728724991 \t -0.002080999024356565\n",
      "90     \t [0.36724721 0.11381873]. \t  -0.444693862019727 \t -0.002080999024356565\n",
      "91     \t [1.25236481 1.57561538]. \t  -0.06886878243164578 \t -0.002080999024356565\n",
      "92     \t [0.78705217 0.06078357]. \t  -31.25628888559174 \t -0.002080999024356565\n",
      "93     \t [1.19064195 1.63726879]. \t  -4.860540991330615 \t -0.002080999024356565\n",
      "94     \t [ 0.45046954 -1.93188098]. \t  -456.04070283784597 \t -0.002080999024356565\n",
      "95     \t [-0.29073196 -0.0322581 ]. \t  -3.0298199791996803 \t -0.002080999024356565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [-0.81057464  1.07467568]. \t  -20.720868440719848 \t -0.002080999024356565\n",
      "97     \t [-0.79374399  1.80956559]. \t  -142.348050764057 \t -0.002080999024356565\n",
      "98     \t [-0.65314311 -0.35198627]. \t  -63.351906424072624 \t -0.002080999024356565\n",
      "99     \t [ 1.97285981 -1.2448696 ]. \t  -2639.870017628281 \t -0.002080999024356565\n",
      "100    \t [2.01579377 0.71686644]. \t  -1120.9769394817645 \t -0.002080999024356565\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\n",
    "\n",
    "np.random.seed(run_num_10)\n",
    "surrogate_loser_10 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_10 = GPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_10.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [1.97032207 1.31583119]. \t  -659.5505379977991 \t -45.00854175284071\n",
      "init   \t [ 0.59758024 -0.3125739 ]. \t  -45.00854175284071 \t -45.00854175284071\n",
      "init   \t [-1.21933423 -0.03314987]. \t  -235.94289844157058 \t -45.00854175284071\n",
      "init   \t [-1.48046507 -0.19447384]. \t  -575.5719351651762 \t -45.00854175284071\n",
      "init   \t [-1.58214297 -2.0360213 ]. \t  -2067.0989998073865 \t -45.00854175284071\n",
      "1      \t [-0.59060318  0.68440247]. \t  \u001b[92m-13.792107070246617\u001b[0m \t -13.792107070246617\n",
      "2      \t [-0.43261271  0.05287406]. \t  \u001b[92m-3.8554828095625155\u001b[0m \t -3.8554828095625155\n",
      "3      \t [-1.09092491  0.68223928]. \t  -30.165961943368167 \t -3.8554828095625155\n",
      "4      \t [ 1.36856297 -1.03958351]. \t  -848.4294834321007 \t -3.8554828095625155\n",
      "5      \t [0.52611459 0.12330121]. \t  \u001b[92m-2.580649747779165\u001b[0m \t -2.580649747779165\n",
      "6      \t [ 0.39211637 -1.33428971]. \t  -221.7973022262781 \t -2.580649747779165\n",
      "7      \t [-0.76397583  0.37859956]. \t  -7.316550643665399 \t -2.580649747779165\n",
      "8      \t [-0.21806941  1.95448521]. \t  -365.1222538947518 \t -2.580649747779165\n",
      "9      \t [ 0.03858641 -0.43829987]. \t  -20.265732970797348 \t -2.580649747779165\n",
      "10     \t [ 0.32150197 -0.02041078]. \t  \u001b[92m-1.992367327735597\u001b[0m \t -1.992367327735597\n",
      "11     \t [ 0.13972863 -0.02640609]. \t  \u001b[92m-0.9510250140105745\u001b[0m \t -0.9510250140105745\n",
      "12     \t [0.16392502 0.69941313]. \t  -45.93025761343199 \t -0.9510250140105745\n",
      "13     \t [ 1.18952872 -1.53357098]. \t  -869.4303719620717 \t -0.9510250140105745\n",
      "14     \t [-1.55073664 -0.69812067]. \t  -969.3080665040666 \t -0.9510250140105745\n",
      "15     \t [-1.2386734  -1.13925684]. \t  -719.8085800301743 \t -0.9510250140105745\n",
      "16     \t [1.12681131 0.78354499]. \t  -23.651112251327692 \t -0.9510250140105745\n",
      "17     \t [ 0.99977521 -0.76676678]. \t  -311.9876671060406 \t -0.9510250140105745\n",
      "18     \t [0.46131648 0.31414458]. \t  -1.316991041470905 \t -0.9510250140105745\n",
      "19     \t [ 1.98042259 -0.98573901]. \t  -2409.6237150913776 \t -0.9510250140105745\n",
      "20     \t [-0.28283639 -1.32666863]. \t  -199.5163276605285 \t -0.9510250140105745\n",
      "21     \t [0.8374439 0.655215 ]. \t  \u001b[92m-0.23892052057330462\u001b[0m \t -0.23892052057330462\n",
      "22     \t [ 0.2436819  -0.27884161]. \t  -12.011461671266463 \t -0.23892052057330462\n",
      "23     \t [0.81434629 0.55060573]. \t  -1.3013111020120771 \t -0.23892052057330462\n",
      "24     \t [0.17726688 1.06658703]. \t  -107.8332336585574 \t -0.23892052057330462\n",
      "25     \t [0.346047   0.12361642]. \t  -0.4291505806098731 \t -0.23892052057330462\n",
      "26     \t [ 1.29236218 -0.97473783]. \t  -699.6550875729368 \t -0.23892052057330462\n",
      "27     \t [-1.70805982 -0.79601716]. \t  -1386.3310485804243 \t -0.23892052057330462\n",
      "28     \t [1.27910866 0.64469621]. \t  -98.36981129039162 \t -0.23892052057330462\n",
      "29     \t [-1.47304488 -0.75853993]. \t  -863.669276188972 \t -0.23892052057330462\n",
      "30     \t [-1.65700941  1.77131254]. \t  -101.99893166096496 \t -0.23892052057330462\n",
      "31     \t [-1.09782657 -0.40581759]. \t  -263.94610895647355 \t -0.23892052057330462\n",
      "32     \t [-1.18098345  1.27770353]. \t  -6.126018932441506 \t -0.23892052057330462\n",
      "33     \t [-1.94199151 -1.42106607]. \t  -2704.754071546131 \t -0.23892052057330462\n",
      "34     \t [-1.92237694  1.06979801]. \t  -697.9887655770525 \t -0.23892052057330462\n",
      "35     \t [-1.41815585  2.048     ]. \t  -5.983151996989663 \t -0.23892052057330462\n",
      "36     \t [-0.88702882 -0.79713069]. \t  -254.45089311807683 \t -0.23892052057330462\n",
      "37     \t [-1.0154422   1.16980679]. \t  -5.985330813903839 \t -0.23892052057330462\n",
      "38     \t [-0.27563309  1.9377205 ]. \t  -348.23739126600947 \t -0.23892052057330462\n",
      "39     \t [ 1.93024108 -0.94086061]. \t  -2178.6660545401273 \t -0.23892052057330462\n",
      "40     \t [1.09113823 1.40072801]. \t  -4.4244134725728665 \t -0.23892052057330462\n",
      "41     \t [ 1.22778986 -1.53194722]. \t  -923.8563456372658 \t -0.23892052057330462\n",
      "42     \t [1.01888929 1.1315487 ]. \t  -0.8729615407864829 \t -0.23892052057330462\n",
      "43     \t [-0.3219115  0.186892 ]. \t  -2.4407558565841576 \t -0.23892052057330462\n",
      "44     \t [0.93920879 0.30816011]. \t  -32.94590397346381 \t -0.23892052057330462\n",
      "45     \t [-0.08912908  0.32218779]. \t  -11.061118648132027 \t -0.23892052057330462\n",
      "46     \t [-0.0069477   0.10223989]. \t  -2.058256476849774 \t -0.23892052057330462\n",
      "47     \t [1.92883444 0.33470046]. \t  -1147.160415538827 \t -0.23892052057330462\n",
      "48     \t [-1.30210954  1.78000982]. \t  -6.014080835379149 \t -0.23892052057330462\n",
      "49     \t [-0.5681301   0.61439423]. \t  -10.963395698427673 \t -0.23892052057330462\n",
      "50     \t [-0.69152467  1.25049012]. \t  -62.50347520244296 \t -0.23892052057330462\n",
      "51     \t [0.61794618 1.65841838]. \t  -163.10673822296363 \t -0.23892052057330462\n",
      "52     \t [-1.88092602 -1.71952514]. \t  -2772.333433014948 \t -0.23892052057330462\n",
      "53     \t [ 1.77670957 -0.01402205]. \t  -1005.9491539234891 \t -0.23892052057330462\n",
      "54     \t [ 0.9735238  -1.26120847]. \t  -487.9498313854103 \t -0.23892052057330462\n",
      "55     \t [ 1.13312633 -1.69041578]. \t  -884.7179340438174 \t -0.23892052057330462\n",
      "56     \t [-0.04710502 -1.72545809]. \t  -299.5832021814439 \t -0.23892052057330462\n",
      "57     \t [-0.18157075  0.43033067]. \t  -17.18582347230089 \t -0.23892052057330462\n",
      "58     \t [ 0.47994735 -1.40300951]. \t  -267.056607350391 \t -0.23892052057330462\n",
      "59     \t [1.71424921 1.48059655]. \t  -213.10223813541185 \t -0.23892052057330462\n",
      "60     \t [ 1.86779441 -1.8241196 ]. \t  -2823.3114968565624 \t -0.23892052057330462\n",
      "61     \t [-1.61216036 -0.94126898]. \t  -1260.2170238403203 \t -0.23892052057330462\n",
      "62     \t [-1.64117344 -1.89599003]. \t  -2113.2720115420593 \t -0.23892052057330462\n",
      "63     \t [ 1.31580711 -1.11887593]. \t  -812.477579666318 \t -0.23892052057330462\n",
      "64     \t [1.59027407 2.048     ]. \t  -23.48179444233881 \t -0.23892052057330462\n",
      "65     \t [-0.03637786  1.38300939]. \t  -191.97971095134145 \t -0.23892052057330462\n",
      "66     \t [0.40434128 0.5194397 ]. \t  -13.024695036665513 \t -0.23892052057330462\n",
      "67     \t [-0.59792941 -0.37098712]. \t  -55.62557857024103 \t -0.23892052057330462\n",
      "68     \t [-1.22447442 -1.35309648]. \t  -818.5863114414681 \t -0.23892052057330462\n",
      "69     \t [-0.04818242  0.19123297]. \t  -4.667439202146498 \t -0.23892052057330462\n",
      "70     \t [ 1.96677042 -1.23243807]. \t  -2602.5710986536355 \t -0.23892052057330462\n",
      "71     \t [-1.67254638  0.24145778]. \t  -660.4323974788158 \t -0.23892052057330462\n",
      "72     \t [1.28228069 1.61054021]. \t  \u001b[92m-0.19327533756548054\u001b[0m \t -0.19327533756548054\n",
      "73     \t [ 0.47673951 -1.2253542 ]. \t  -211.28857552365994 \t -0.19327533756548054\n",
      "74     \t [0.94928522 0.07194122]. \t  -68.76003629130035 \t -0.19327533756548054\n",
      "75     \t [1.21962555 1.47605679]. \t  \u001b[92m-0.06129920361782816\u001b[0m \t -0.06129920361782816\n",
      "76     \t [-1.30501148  0.16778885]. \t  -241.01728153385264 \t -0.06129920361782816\n",
      "77     \t [ 1.98216118 -0.56707881]. \t  -2022.4037849155825 \t -0.06129920361782816\n",
      "78     \t [ 0.6256004  -1.77182998]. \t  -468.0861277393353 \t -0.06129920361782816\n",
      "79     \t [-0.14297143 -1.50855436]. \t  -235.0890121843255 \t -0.06129920361782816\n",
      "80     \t [1.56533633 0.93464884]. \t  -230.0327265035141 \t -0.06129920361782816\n",
      "81     \t [1.1813951  1.37179861]. \t  -0.09000498434428916 \t -0.06129920361782816\n",
      "82     \t [0.56217192 1.41420373]. \t  -120.78865068275417 \t -0.06129920361782816\n",
      "83     \t [1.79282834 0.64597849]. \t  -660.2219442582725 \t -0.06129920361782816\n",
      "84     \t [ 1.70460458 -1.09192798]. \t  -1598.5808533328623 \t -0.06129920361782816\n",
      "85     \t [0.81909119 0.68004149]. \t  \u001b[92m-0.04106571157579786\u001b[0m \t -0.04106571157579786\n",
      "86     \t [0.81343408 0.67005565]. \t  -0.041830366349827765 \t -0.04106571157579786\n",
      "87     \t [-2.00994707  0.85300458]. \t  -1024.6818743389156 \t -0.04106571157579786\n",
      "88     \t [-0.88361966 -0.74654485]. \t  -236.82127108162658 \t -0.04106571157579786\n",
      "89     \t [-1.78620936  1.42392272]. \t  -319.8579928979332 \t -0.04106571157579786\n",
      "90     \t [-0.42532623  1.76570948]. \t  -253.19290341592327 \t -0.04106571157579786\n",
      "91     \t [-0.23251562 -0.87987128]. \t  -88.74251487636296 \t -0.04106571157579786\n",
      "92     \t [1.21418723 1.45050123]. \t  -0.10227956434504437 \t -0.04106571157579786\n",
      "93     \t [ 0.63138666 -1.65560771]. \t  -422.13298671981994 \t -0.04106571157579786\n",
      "94     \t [1.55955816 0.71295826]. \t  -295.89976870463903 \t -0.04106571157579786\n",
      "95     \t [1.18209282 0.49641175]. \t  -81.20094879602708 \t -0.04106571157579786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [0.11221832 0.17690602]. \t  -3.488034753685005 \t -0.04106571157579786\n",
      "97     \t [ 1.53115004 -0.15748759]. \t  -626.2364977558283 \t -0.04106571157579786\n",
      "98     \t [-0.04377054  1.07377422]. \t  -115.97749058569023 \t -0.04106571157579786\n",
      "99     \t [-1.07275224  1.30988671]. \t  -6.8272437195548115 \t -0.04106571157579786\n",
      "100    \t [ 1.99729274 -0.68967908]. \t  -2190.1652376019097 \t -0.04106571157579786\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\n",
    "\n",
    "np.random.seed(run_num_11)\n",
    "surrogate_loser_11 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_11 = GPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_11.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-1.26353633  0.50015753]. \t  -125.32555445527122 \t -19.52113145175031\n",
      "init   \t [-0.25506718  1.16882876]. \t  -123.40590278793995 \t -19.52113145175031\n",
      "init   \t [ 1.14678091 -0.93146069]. \t  -504.72793807789463 \t -19.52113145175031\n",
      "init   \t [-0.91560241  1.23646844]. \t  -19.52113145175031 \t -19.52113145175031\n",
      "init   \t [1.87653879 1.53982007]. \t  -393.43336554061835 \t -19.52113145175031\n",
      "1      \t [-1.3787018  2.048    ]. \t  \u001b[92m-7.824456843181751\u001b[0m \t -7.824456843181751\n",
      "2      \t [-1.16736243  1.64664033]. \t  -12.75768117752235 \t -7.824456843181751\n",
      "3      \t [-0.82476779  1.48115298]. \t  -67.47563363670153 \t -7.824456843181751\n",
      "4      \t [-2.048       1.85692053]. \t  -555.6264525326193 \t -7.824456843181751\n",
      "5      \t [-0.12642315  0.45531769]. \t  -20.57034283559652 \t -7.824456843181751\n",
      "6      \t [-1.19370865 -1.3207978 ]. \t  -758.720153086171 \t -7.824456843181751\n",
      "7      \t [0.49818184 1.06984657]. \t  -67.76457057592293 \t -7.824456843181751\n",
      "8      \t [0.5651675  0.63849688]. \t  -10.370448682858372 \t -7.824456843181751\n",
      "9      \t [ 1.76108119 -1.81331805]. \t  -2416.0314336997494 \t -7.824456843181751\n",
      "10     \t [ 0.63661893 -0.25202025]. \t  -43.33688910841324 \t -7.824456843181751\n",
      "11     \t [ 1.05910733 -0.24097216]. \t  -185.69330959029529 \t -7.824456843181751\n",
      "12     \t [-1.18036067  2.048     ]. \t  -47.62355568324965 \t -7.824456843181751\n",
      "13     \t [0.26009262 0.6052131 ]. \t  -29.44506856087163 \t -7.824456843181751\n",
      "14     \t [ 0.29740787 -0.78677559]. \t  -77.0958711997293 \t -7.824456843181751\n",
      "15     \t [ 0.51522622 -0.87850331]. \t  -131.09976624440213 \t -7.824456843181751\n",
      "16     \t [ 0.0009418  -0.14297283]. \t  \u001b[92m-3.0422656212001553\u001b[0m \t -3.0422656212001553\n",
      "17     \t [ 0.53875338 -0.81057991]. \t  -121.39654403776179 \t -3.0422656212001553\n",
      "18     \t [0.28821005 0.07242824]. \t  \u001b[92m-0.5179590750516427\u001b[0m \t -0.5179590750516427\n",
      "19     \t [-0.53669937  0.25529521]. \t  -2.468707731627618 \t -0.5179590750516427\n",
      "20     \t [-0.16822463  0.21355451]. \t  -4.7966896499975835 \t -0.5179590750516427\n",
      "21     \t [-1.8898996  -1.52770608]. \t  -2608.766667606483 \t -0.5179590750516427\n",
      "22     \t [-0.32995286 -1.77276659]. \t  -355.8239814493661 \t -0.5179590750516427\n",
      "23     \t [-0.57465484 -0.70543142]. \t  -109.73861986473887 \t -0.5179590750516427\n",
      "24     \t [-0.58847674  0.39507782]. \t  -2.7611384287272727 \t -0.5179590750516427\n",
      "25     \t [-1.90380966  1.58542849]. \t  -424.20978872368084 \t -0.5179590750516427\n",
      "26     \t [-1.62219888 -0.9583691 ]. \t  -1295.6129145410832 \t -0.5179590750516427\n",
      "27     \t [0.7011432  0.21686683]. \t  -7.637245058596269 \t -0.5179590750516427\n",
      "28     \t [0.53067473 0.26930859]. \t  \u001b[92m-0.2354126426440179\u001b[0m \t -0.2354126426440179\n",
      "29     \t [-0.8380545  -1.56912158]. \t  -519.3301011515124 \t -0.2354126426440179\n",
      "30     \t [ 0.53610384 -1.32590034]. \t  -260.4913625993307 \t -0.2354126426440179\n",
      "31     \t [-0.25784332 -1.26042859]. \t  -177.651652670226 \t -0.2354126426440179\n",
      "32     \t [1.28092455 1.27436466]. \t  -13.504038134636817 \t -0.2354126426440179\n",
      "33     \t [1.11805115 1.34213172]. \t  -0.862054686914709 \t -0.2354126426440179\n",
      "34     \t [1.05307778 1.06445676]. \t  \u001b[92m-0.20098509202956166\u001b[0m \t -0.20098509202956166\n",
      "35     \t [-1.25457954 -0.90390996]. \t  -619.0719516289206 \t -0.20098509202956166\n",
      "36     \t [-1.53137776 -1.39676552]. \t  -1406.5769775535598 \t -0.20098509202956166\n",
      "37     \t [1.16298671 0.3173189 ]. \t  -107.1944404400957 \t -0.20098509202956166\n",
      "38     \t [-0.87070096  1.76451424]. \t  -104.78242689426114 \t -0.20098509202956166\n",
      "39     \t [1.12772407 2.048     ]. \t  -60.27092156868604 \t -0.20098509202956166\n",
      "40     \t [0.51853573 0.27057188]. \t  -0.2320943206474129 \t -0.20098509202956166\n",
      "41     \t [-1.60659348  0.49208595]. \t  -443.21010900556564 \t -0.20098509202956166\n",
      "42     \t [0.52804319 0.28339341]. \t  -0.22482606252578782 \t -0.20098509202956166\n",
      "43     \t [ 0.1722737  -1.09339274]. \t  -126.81397133518486 \t -0.20098509202956166\n",
      "44     \t [ 1.97086788 -1.38085597]. \t  -2773.1505780850493 \t -0.20098509202956166\n",
      "45     \t [-1.1721999   1.78049686]. \t  -21.238145501831017 \t -0.20098509202956166\n",
      "46     \t [ 0.03298517 -0.06661061]. \t  -1.3934281580334764 \t -0.20098509202956166\n",
      "47     \t [ 0.74708299 -1.63196796]. \t  -479.71818826441717 \t -0.20098509202956166\n",
      "48     \t [-1.59011409 -1.17205592]. \t  -1376.0925958070266 \t -0.20098509202956166\n",
      "49     \t [2.0291388  0.88476863]. \t  -1046.0524466450936 \t -0.20098509202956166\n",
      "50     \t [1.48328002 2.048     ]. \t  -2.547597290106557 \t -0.20098509202956166\n",
      "51     \t [0.88069297 0.74146317]. \t  \u001b[92m-0.13090375402891008\u001b[0m \t -0.13090375402891008\n",
      "52     \t [-0.79672646  0.54017111]. \t  -4.1231787832939615 \t -0.13090375402891008\n",
      "53     \t [1.34664239 1.76742198]. \t  -0.33197943541040853 \t -0.13090375402891008\n",
      "54     \t [-1.97029281 -0.58851854]. \t  -2007.4243074353817 \t -0.13090375402891008\n",
      "55     \t [-1.19694562  1.28218297]. \t  -7.0914699026767956 \t -0.13090375402891008\n",
      "56     \t [ 1.81666571 -1.39798178]. \t  -2208.027975628221 \t -0.13090375402891008\n",
      "57     \t [-0.38189943 -0.10655757]. \t  -8.280461489813375 \t -0.13090375402891008\n",
      "58     \t [-0.6690598   0.62857839]. \t  -6.059594112791341 \t -0.13090375402891008\n",
      "59     \t [-1.05187429  0.9227578 ]. \t  -7.584085831378985 \t -0.13090375402891008\n",
      "60     \t [ 0.34967597 -0.49509189]. \t  -38.53689694008928 \t -0.13090375402891008\n",
      "61     \t [1.12547775 1.25767639]. \t  \u001b[92m-0.023887506892937962\u001b[0m \t -0.023887506892937962\n",
      "62     \t [1.11703736 1.24008273]. \t  \u001b[92m-0.019610944445885056\u001b[0m \t -0.019610944445885056\n",
      "63     \t [ 1.38770185 -0.97071159]. \t  -839.0798349707751 \t -0.019610944445885056\n",
      "64     \t [1.1914228  1.38583014]. \t  -0.1499298584385004 \t -0.019610944445885056\n",
      "65     \t [ 1.40147933 -0.0424511 ]. \t  -402.80370032728825 \t -0.019610944445885056\n",
      "66     \t [ 0.62192199 -1.03710214]. \t  -202.88896043583838 \t -0.019610944445885056\n",
      "67     \t [-0.99635422  1.42286402]. \t  -22.48766865522903 \t -0.019610944445885056\n",
      "68     \t [0.94819772 0.93037814]. \t  -0.10064755503383545 \t -0.019610944445885056\n",
      "69     \t [1.16429039 1.33762416]. \t  -0.05920427022767664 \t -0.019610944445885056\n",
      "70     \t [0.67177029 0.74464743]. \t  -8.714454155806434 \t -0.019610944445885056\n",
      "71     \t [-1.47018171  0.40299074]. \t  -315.3141561232518 \t -0.019610944445885056\n",
      "72     \t [1.27200548 0.21893352]. \t  -195.81210967711115 \t -0.019610944445885056\n",
      "73     \t [-0.73846938  1.78295618]. \t  -156.1923914126747 \t -0.019610944445885056\n",
      "74     \t [ 0.1920062  -1.81452282]. \t  -343.41704935735197 \t -0.019610944445885056\n",
      "75     \t [0.7026122  1.41582394]. \t  -85.12635272420111 \t -0.019610944445885056\n",
      "76     \t [1.35840484 0.76388173]. \t  -117.06715361980729 \t -0.019610944445885056\n",
      "77     \t [ 0.41220436 -1.23097664]. \t  -196.59452422588546 \t -0.019610944445885056\n",
      "78     \t [ 0.94679312 -1.0564852 ]. \t  -381.3856099237539 \t -0.019610944445885056\n",
      "79     \t [1.02366783 1.02484337]. \t  -0.05370174572629335 \t -0.019610944445885056\n",
      "80     \t [1.28286425 0.77135349]. \t  -76.53531097115662 \t -0.019610944445885056\n",
      "81     \t [-0.05827082 -1.51731853]. \t  -232.3770506578901 \t -0.019610944445885056\n",
      "82     \t [-0.0245695   0.50057882]. \t  -26.047258765573183 \t -0.019610944445885056\n",
      "83     \t [1.52288195 0.32830641]. \t  -396.62696437497647 \t -0.019610944445885056\n",
      "84     \t [ 0.24020323 -1.74935592]. \t  -327.12153009436537 \t -0.019610944445885056\n",
      "85     \t [-0.37725487  1.1371482 ]. \t  -100.86489912110035 \t -0.019610944445885056\n",
      "86     \t [1.89917358 1.78193519]. \t  -333.84367620804505 \t -0.019610944445885056\n",
      "87     \t [0.95115313 0.90638088]. \t  \u001b[92m-0.0026711546260692227\u001b[0m \t -0.0026711546260692227\n",
      "88     \t [-1.95437521 -0.72886382]. \t  -2077.5646770921367 \t -0.0026711546260692227\n",
      "89     \t [ 1.22901738 -1.40143556]. \t  -847.9798365211404 \t -0.0026711546260692227\n",
      "90     \t [-1.56558783  0.93830891]. \t  -235.42541967516422 \t -0.0026711546260692227\n",
      "91     \t [-0.65059558 -0.51476499]. \t  -90.71629548434643 \t -0.0026711546260692227\n",
      "92     \t [ 1.23184003 -0.68497411]. \t  -485.1120682204871 \t -0.0026711546260692227\n",
      "93     \t [-0.5821073  -1.55784162]. \t  -362.24656034397196 \t -0.0026711546260692227\n",
      "94     \t [-1.93926247  0.49980756]. \t  -1072.006603036883 \t -0.0026711546260692227\n",
      "95     \t [1.41382446 2.048     ]. \t  -0.4123356273755938 \t -0.0026711546260692227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [ 0.21458814 -1.62712584]. \t  -280.5679665819224 \t -0.0026711546260692227\n",
      "97     \t [1.00421244 1.90356606]. \t  -80.12461242201168 \t -0.0026711546260692227\n",
      "98     \t [1.29858931 1.68016183]. \t  -0.09296537477530241 \t -0.0026711546260692227\n",
      "99     \t [-1.00773153 -1.93180522]. \t  -872.7052511651193 \t -0.0026711546260692227\n",
      "100    \t [-0.43163006 -0.55778265]. \t  -57.41613508491781 \t -0.0026711546260692227\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\n",
    "\n",
    "np.random.seed(run_num_12)\n",
    "surrogate_loser_12 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_12 = GPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_12.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.65799909 -0.35389173]. \t  -62.0309701572776 \t -62.0309701572776\n",
      "init   \t [ 0.68854808 -1.10615174]. \t  -249.81607110978882 \t -62.0309701572776\n",
      "init   \t [1.26025046 0.56040843]. \t  -105.7097019264073 \t -62.0309701572776\n",
      "init   \t [-1.34269398 -0.98145948]. \t  -780.713409392333 \t -62.0309701572776\n",
      "init   \t [ 1.70115068 -0.15230767]. \t  -928.4380405275942 \t -62.0309701572776\n",
      "1      \t [0.37995778 0.83552072]. \t  \u001b[92m-48.153672694682946\u001b[0m \t -48.153672694682946\n",
      "2      \t [1.55860765 1.62153229]. \t  -65.55409148154004 \t -48.153672694682946\n",
      "3      \t [0.46023823 0.17397552]. \t  \u001b[92m-0.43455737399098016\u001b[0m \t -0.43455737399098016\n",
      "4      \t [0.95160054 1.01537238]. \t  -1.2085789839255758 \t -0.43455737399098016\n",
      "5      \t [-1.08607807  2.048     ]. \t  -79.76955836944353 \t -0.43455737399098016\n",
      "6      \t [0.76140714 0.61041897]. \t  \u001b[92m-0.15104134638093003\u001b[0m \t -0.15104134638093003\n",
      "7      \t [1.00337308 0.90654961]. \t  -1.004174379817866 \t -0.15104134638093003\n",
      "8      \t [0.07709175 2.048     ]. \t  -417.85138275712023 \t -0.15104134638093003\n",
      "9      \t [-2.01971806  2.00733586]. \t  -438.40608785864225 \t -0.15104134638093003\n",
      "10     \t [0.50166341 0.03248154]. \t  -5.052529742057543 \t -0.15104134638093003\n",
      "11     \t [0.20173424 0.10176067]. \t  -1.0101090063818985 \t -0.15104134638093003\n",
      "12     \t [-0.8193735   0.64318483]. \t  -3.3895768510701876 \t -0.15104134638093003\n",
      "13     \t [-0.53350943  0.4830196 ]. \t  -6.2874026566035255 \t -0.15104134638093003\n",
      "14     \t [0.87696467 0.81183928]. \t  -0.19808426552069966 \t -0.15104134638093003\n",
      "15     \t [-0.90946616  0.89026662]. \t  -4.044700648927882 \t -0.15104134638093003\n",
      "16     \t [ 0.85956863 -0.72063444]. \t  -213.0316067957737 \t -0.15104134638093003\n",
      "17     \t [-1.79116532  0.39368635]. \t  -799.9805187663303 \t -0.15104134638093003\n",
      "18     \t [0.27147229 0.11980829]. \t  -0.743375801871843 \t -0.15104134638093003\n",
      "19     \t [ 1.39317566 -0.10390501]. \t  -418.29305163403814 \t -0.15104134638093003\n",
      "20     \t [0.94413113 0.89997394]. \t  \u001b[92m-0.0105007464795961\u001b[0m \t -0.0105007464795961\n",
      "21     \t [-0.52856026 -1.00599816]. \t  -167.55515641751728 \t -0.0105007464795961\n",
      "22     \t [0.64779313 0.36590906]. \t  -0.41270749627960684 \t -0.0105007464795961\n",
      "23     \t [1.68411474 0.68570093]. \t  -462.9508923919746 \t -0.0105007464795961\n",
      "24     \t [-0.75113232  0.73522321]. \t  -5.991366067000058 \t -0.0105007464795961\n",
      "25     \t [-0.36435495 -2.048     ]. \t  -477.4304949063629 \t -0.0105007464795961\n",
      "26     \t [ 1.74525021 -2.03394314]. \t  -2581.034292136097 \t -0.0105007464795961\n",
      "27     \t [-1.21394464  0.86163408]. \t  -42.359319193002385 \t -0.0105007464795961\n",
      "28     \t [-1.41188748  0.3343338 ]. \t  -281.07598340222137 \t -0.0105007464795961\n",
      "29     \t [ 1.97957486 -1.68024005]. \t  -3135.791165885233 \t -0.0105007464795961\n",
      "30     \t [0.72586704 0.37811578]. \t  -2.2883160295173526 \t -0.0105007464795961\n",
      "31     \t [1.45952525 2.048     ]. \t  -0.887076764915745 \t -0.0105007464795961\n",
      "32     \t [-1.63774371  0.69143832]. \t  -403.2726747555389 \t -0.0105007464795961\n",
      "33     \t [-0.50864684  0.05615614]. \t  -6.379292253906455 \t -0.0105007464795961\n",
      "34     \t [-1.30842607 -1.26558062]. \t  -891.9148312871883 \t -0.0105007464795961\n",
      "35     \t [-0.68082076 -0.94634829]. \t  -201.59714479346763 \t -0.0105007464795961\n",
      "36     \t [-0.14916624 -1.03404296]. \t  -112.89618517418847 \t -0.0105007464795961\n",
      "37     \t [-1.54445384 -0.55743512]. \t  -872.4654088084312 \t -0.0105007464795961\n",
      "38     \t [-1.8437376   0.34724672]. \t  -939.6314711821524 \t -0.0105007464795961\n",
      "39     \t [-0.16795991  0.1217925 ]. \t  -2.2398888638748584 \t -0.0105007464795961\n",
      "40     \t [ 1.19278592 -1.8498044 ]. \t  -1070.9907086052215 \t -0.0105007464795961\n",
      "41     \t [-1.28994606 -1.18169978]. \t  -815.0222927572457 \t -0.0105007464795961\n",
      "42     \t [ 0.4781391  -1.74497403]. \t  -389.7784949090759 \t -0.0105007464795961\n",
      "43     \t [-1.65106047  1.61048312]. \t  -131.46606497698292 \t -0.0105007464795961\n",
      "44     \t [-1.27142181  1.62577656]. \t  -5.167937620004863 \t -0.0105007464795961\n",
      "45     \t [0.05342914 0.77434006]. \t  -60.41496633178125 \t -0.0105007464795961\n",
      "46     \t [-1.54062542  0.92282972]. \t  -216.90694503923066 \t -0.0105007464795961\n",
      "47     \t [-1.92765507 -0.28267287]. \t  -1607.3929445715162 \t -0.0105007464795961\n",
      "48     \t [ 0.88818791 -1.32839832]. \t  -448.2983037969398 \t -0.0105007464795961\n",
      "49     \t [-0.65987705 -0.13774205]. \t  -35.608697485650005 \t -0.0105007464795961\n",
      "50     \t [1.9362214  1.29276233]. \t  -604.1639204282061 \t -0.0105007464795961\n",
      "51     \t [-0.62056197 -1.76656905]. \t  -465.5929716936383 \t -0.0105007464795961\n",
      "52     \t [1.32952403 1.73909692]. \t  -0.19002333551530137 \t -0.0105007464795961\n",
      "53     \t [-1.70191383  1.46957766]. \t  -210.91412597964643 \t -0.0105007464795961\n",
      "54     \t [-1.15543014 -0.80314546]. \t  -461.8205277732385 \t -0.0105007464795961\n",
      "55     \t [-1.41795336  2.048     ]. \t  -5.98643638227141 \t -0.0105007464795961\n",
      "56     \t [0.8318862  1.35426881]. \t  -43.88367000284221 \t -0.0105007464795961\n",
      "57     \t [1.52907419 2.048     ]. \t  -8.693857138901325 \t -0.0105007464795961\n",
      "58     \t [0.74548014 1.42810719]. \t  -76.16712000504089 \t -0.0105007464795961\n",
      "59     \t [ 0.12827911 -0.10851908]. \t  -2.3217626101374083 \t -0.0105007464795961\n",
      "60     \t [1.13506205 1.28160793]. \t  -0.022808723654615434 \t -0.0105007464795961\n",
      "61     \t [-2.04602583 -0.00310691]. \t  -1764.3256972698305 \t -0.0105007464795961\n",
      "62     \t [ 0.98149748 -1.26261417]. \t  -495.4863362533916 \t -0.0105007464795961\n",
      "63     \t [0.96238417 0.93918237]. \t  -0.018312564324216435 \t -0.0105007464795961\n",
      "64     \t [-0.83953917  0.4039758 ]. \t  -12.434989412544088 \t -0.0105007464795961\n",
      "65     \t [-0.44755408 -1.96706802]. \t  -471.8458404542322 \t -0.0105007464795961\n",
      "66     \t [1.09210023 1.19998299]. \t  -0.01381154921942595 \t -0.0105007464795961\n",
      "67     \t [0.83759372 0.14908133]. \t  -30.550001785808465 \t -0.0105007464795961\n",
      "68     \t [0.98456094 0.25075815]. \t  -51.63913704506476 \t -0.0105007464795961\n",
      "69     \t [ 0.73246608 -1.77368981]. \t  -533.772297993229 \t -0.0105007464795961\n",
      "70     \t [-1.58563016  1.17593792]. \t  -185.78617810527686 \t -0.0105007464795961\n",
      "71     \t [0.79168256 1.72100923]. \t  -119.78125431659336 \t -0.0105007464795961\n",
      "72     \t [-0.81686469  0.1363013 ]. \t  -31.49355347048604 \t -0.0105007464795961\n",
      "73     \t [1.49237833 1.5409657 ]. \t  -47.333238345706455 \t -0.0105007464795961\n",
      "74     \t [0.77181157 0.30847436]. \t  -8.301530181457267 \t -0.0105007464795961\n",
      "75     \t [1.21385442 1.46075685]. \t  -0.061826425282636384 \t -0.0105007464795961\n",
      "76     \t [ 1.10650968 -0.76968109]. \t  -397.6327937907719 \t -0.0105007464795961\n",
      "77     \t [1.15363453 1.32068037]. \t  -0.03399177213583851 \t -0.0105007464795961\n",
      "78     \t [1.14062434 1.29181335]. \t  -0.028258585702706535 \t -0.0105007464795961\n",
      "79     \t [ 0.29947585 -1.35759449]. \t  -209.95275381756917 \t -0.0105007464795961\n",
      "80     \t [0.41354938 1.88425379]. \t  -293.8598650763438 \t -0.0105007464795961\n",
      "81     \t [ 1.16142564 -0.78263878]. \t  -454.37587569895425 \t -0.0105007464795961\n",
      "82     \t [1.10070234 1.20525414]. \t  -0.014099260547559497 \t -0.0105007464795961\n",
      "83     \t [-1.55106656  1.40530115]. \t  -106.60922967669718 \t -0.0105007464795961\n",
      "84     \t [-0.66643241 -1.82568483]. \t  -517.9839136256584 \t -0.0105007464795961\n",
      "85     \t [-1.30003343  0.94185481]. \t  -61.27528154653576 \t -0.0105007464795961\n",
      "86     \t [-1.77977978  0.80529652]. \t  -565.7825354395953 \t -0.0105007464795961\n",
      "87     \t [-1.17277124 -0.21195015]. \t  -256.686568077995 \t -0.0105007464795961\n",
      "88     \t [-0.37738586  2.03176769]. \t  -358.8606279912727 \t -0.0105007464795961\n",
      "89     \t [ 1.09585673 -0.96871735]. \t  -470.7339851117273 \t -0.0105007464795961\n",
      "90     \t [0.71029964 0.38376261]. \t  -1.5422957760661784 \t -0.0105007464795961\n",
      "91     \t [1.26013997 2.01043038]. \t  -17.91640790402721 \t -0.0105007464795961\n",
      "92     \t [1.1202444  1.24692499]. \t  -0.020894794702690515 \t -0.0105007464795961\n",
      "93     \t [-0.60869851  1.45333029]. \t  -119.83704824326959 \t -0.0105007464795961\n",
      "94     \t [-0.37949809 -1.97403528]. \t  -450.51832415832763 \t -0.0105007464795961\n",
      "95     \t [-0.03532646  0.20508262]. \t  -5.226757806966602 \t -0.0105007464795961\n",
      "96     \t [-1.44592539  0.65227005]. \t  -212.89069487972304 \t -0.0105007464795961\n",
      "97     \t [0.55016118 0.30517732]. \t  -0.20297995737585128 \t -0.0105007464795961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98     \t [-1.10398428  1.61088889]. \t  -19.801586554812506 \t -0.0105007464795961\n",
      "99     \t [1.0275188  1.04328329]. \t  -0.016411293445117516 \t -0.0105007464795961\n",
      "100    \t [2.0161086 1.2859591]. \t  -773.1691858866905 \t -0.0105007464795961\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\n",
    "\n",
    "np.random.seed(run_num_13)\n",
    "surrogate_loser_13 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_13 = GPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_13.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.6043691  0.02928512]. \t  -11.444570460525577 \t -4.306489127802793\n",
      "init   \t [0.11608629 1.6231842 ]. \t  -259.89734211112767 \t -4.306489127802793\n",
      "init   \t [0.81916392 0.87776093]. \t  -4.306489127802793 \t -4.306489127802793\n",
      "init   \t [ 0.89021801 -1.13533148]. \t  -371.66089065122344 \t -4.306489127802793\n",
      "init   \t [-1.33056707 -0.17677726]. \t  -384.58487399689074 \t -4.306489127802793\n",
      "1      \t [1.24752015 0.50762648]. \t  -110.03425123612769 \t -4.306489127802793\n",
      "2      \t [0.40905629 0.43067185]. \t  -7.284263221937693 \t -4.306489127802793\n",
      "3      \t [0.58960722 0.36786723]. \t  \u001b[92m-0.20934976693068252\u001b[0m \t -0.20934976693068252\n",
      "4      \t [1.50467572 1.85422513]. \t  -17.050259355443057 \t -0.20934976693068252\n",
      "5      \t [0.3895931  0.08380615]. \t  -0.8346788131126889 \t -0.20934976693068252\n",
      "6      \t [1.04319355 1.15819176]. \t  -0.4910117331202918 \t -0.20934976693068252\n",
      "7      \t [0.74112232 1.67535394]. \t  -126.87525800509205 \t -0.20934976693068252\n",
      "8      \t [1.81101547 1.70033276]. \t  -250.12216497315885 \t -0.20934976693068252\n",
      "9      \t [-1.39501341  1.96440775]. \t  -5.769744345411383 \t -0.20934976693068252\n",
      "10     \t [-1.11817079  1.50613291]. \t  -11.031392810522732 \t -0.20934976693068252\n",
      "11     \t [-1.51846679 -1.34386354]. \t  -1338.30428218148 \t -0.20934976693068252\n",
      "12     \t [0.84108936 1.35159914]. \t  -41.52047090373132 \t -0.20934976693068252\n",
      "13     \t [1.3550637 2.048    ]. \t  -4.612095149890622 \t -0.20934976693068252\n",
      "14     \t [-1.36955464  0.96694894]. \t  -88.19398624809719 \t -0.20934976693068252\n",
      "15     \t [1.27185276 1.7394737 ]. \t  -1.5589936860446165 \t -0.20934976693068252\n",
      "16     \t [-0.44917478  0.37965668]. \t  -5.264902319297189 \t -0.20934976693068252\n",
      "17     \t [-1.24756713  2.03358832]. \t  -27.820161630826867 \t -0.20934976693068252\n",
      "18     \t [-1.42485847  1.7555478 ]. \t  -13.424511198869107 \t -0.20934976693068252\n",
      "19     \t [-0.75252175  0.77409873]. \t  -7.389821627924743 \t -0.20934976693068252\n",
      "20     \t [0.36778814 0.19018003]. \t  -0.7012236754080831 \t -0.20934976693068252\n",
      "21     \t [1.28496257 1.04363211]. \t  -36.98642785984151 \t -0.20934976693068252\n",
      "22     \t [-0.20172275 -0.83419603]. \t  -77.98705544652253 \t -0.20934976693068252\n",
      "23     \t [1.45747924 2.048     ]. \t  -0.790628312566114 \t -0.20934976693068252\n",
      "24     \t [-1.71316048  0.26234352]. \t  -721.6271162631949 \t -0.20934976693068252\n",
      "25     \t [-0.50565574 -0.16992768]. \t  -20.381846978906378 \t -0.20934976693068252\n",
      "26     \t [-0.71991397  0.34792118]. \t  -5.8601846697037985 \t -0.20934976693068252\n",
      "27     \t [-1.13379775  1.22251051]. \t  -4.94982702281166 \t -0.20934976693068252\n",
      "28     \t [0.08710804 1.04892057]. \t  -109.27076301525734 \t -0.20934976693068252\n",
      "29     \t [ 0.08063829 -0.60628709]. \t  -38.39633775902093 \t -0.20934976693068252\n",
      "30     \t [-1.39992158  0.49737864]. \t  -219.62152666592806 \t -0.20934976693068252\n",
      "31     \t [-1.08945402 -0.16533092]. \t  -187.22138818008403 \t -0.20934976693068252\n",
      "32     \t [1.62780113 0.05607663]. \t  -673.101292312616 \t -0.20934976693068252\n",
      "33     \t [-1.16085581  0.21366414]. \t  -133.24722419089474 \t -0.20934976693068252\n",
      "34     \t [-0.92423914  0.76059627]. \t  -4.579198957579712 \t -0.20934976693068252\n",
      "35     \t [0.7476354  1.17273707]. \t  -37.736077303702444 \t -0.20934976693068252\n",
      "36     \t [ 0.77205549 -1.24169292]. \t  -337.7890947457284 \t -0.20934976693068252\n",
      "37     \t [0.95678488 0.89197313]. \t  \u001b[92m-0.05692429160489036\u001b[0m \t -0.05692429160489036\n",
      "38     \t [ 1.47847777 -1.31991067]. \t  -1229.297341811999 \t -0.05692429160489036\n",
      "39     \t [0.24445474 0.67058581]. \t  -37.88189592221282 \t -0.05692429160489036\n",
      "40     \t [0.96823241 0.92580636]. \t  \u001b[92m-0.014622547272366887\u001b[0m \t -0.014622547272366887\n",
      "41     \t [0.45014774 1.91013468]. \t  -291.8585425470606 \t -0.014622547272366887\n",
      "42     \t [1.43367697 0.32061112]. \t  -301.14760728437517 \t -0.014622547272366887\n",
      "43     \t [1.38584151 1.92644639]. \t  -0.15234251260283777 \t -0.014622547272366887\n",
      "44     \t [-1.5182108   0.80987152]. \t  -229.87155168550814 \t -0.014622547272366887\n",
      "45     \t [ 0.32514034 -1.28652307]. \t  -194.28846499360978 \t -0.014622547272366887\n",
      "46     \t [-1.9703253   1.59067058]. \t  -533.9251830288297 \t -0.014622547272366887\n",
      "47     \t [ 1.91370402 -1.69217447]. \t  -2867.835007825963 \t -0.014622547272366887\n",
      "48     \t [-1.73379435 -0.73116528]. \t  -1404.1460948150188 \t -0.014622547272366887\n",
      "49     \t [-0.03095389 -0.07465241]. \t  -1.6345614317988844 \t -0.014622547272366887\n",
      "50     \t [-0.05142272  2.04169095]. \t  -416.87661347692347 \t -0.014622547272366887\n",
      "51     \t [-2.00496167  0.62367447]. \t  -1162.4450734371903 \t -0.014622547272366887\n",
      "52     \t [ 0.29888104 -1.46818963]. \t  -243.07826902833955 \t -0.014622547272366887\n",
      "53     \t [-1.65210435 -0.0990571 ]. \t  -807.0782056667098 \t -0.014622547272366887\n",
      "54     \t [0.05155122 0.37314437]. \t  -14.625605019530663 \t -0.014622547272366887\n",
      "55     \t [-1.6060395  -1.08628769]. \t  -1350.4908402004257 \t -0.014622547272366887\n",
      "56     \t [ 0.33061555 -1.7446117 ]. \t  -344.14939823471684 \t -0.014622547272366887\n",
      "57     \t [0.52884692 0.2721379 ]. \t  -0.22767214315800485 \t -0.014622547272366887\n",
      "58     \t [-0.87484484  1.09494647]. \t  -14.37819674086421 \t -0.014622547272366887\n",
      "59     \t [1.11981129 2.01294109]. \t  -57.61695416628044 \t -0.014622547272366887\n",
      "60     \t [-1.32088165  0.18350019]. \t  -249.12981988210257 \t -0.014622547272366887\n",
      "61     \t [-0.98387027 -1.93324359]. \t  -845.6575912879017 \t -0.014622547272366887\n",
      "62     \t [0.50452257 0.22088028]. \t  -0.35881591436259663 \t -0.014622547272366887\n",
      "63     \t [1.38889767 1.93392397]. \t  -0.1536299034083029 \t -0.014622547272366887\n",
      "64     \t [ 1.43432849 -0.71886116]. \t  -770.8947222516093 \t -0.014622547272366887\n",
      "65     \t [1.74145888 0.47871748]. \t  -652.8217275890095 \t -0.014622547272366887\n",
      "66     \t [1.1008322  1.20953052]. \t  \u001b[92m-0.01069660301017262\u001b[0m \t -0.01069660301017262\n",
      "67     \t [ 1.33553529 -1.94373514]. \t  -1389.4559372659976 \t -0.01069660301017262\n",
      "68     \t [ 2.03794533 -1.12526205]. \t  -2787.3158341879744 \t -0.01069660301017262\n",
      "69     \t [0.56708569 0.31248964]. \t  -0.19568950560666498 \t -0.01069660301017262\n",
      "70     \t [ 1.67728183 -0.22584165]. \t  -924.0813148512133 \t -0.01069660301017262\n",
      "71     \t [1.38924299 1.93427859]. \t  -0.1533440992702526 \t -0.01069660301017262\n",
      "72     \t [-1.19208035 -1.95536668]. \t  -1144.827932674559 \t -0.01069660301017262\n",
      "73     \t [-2.02108705  1.69906427]. \t  -578.2970575781673 \t -0.01069660301017262\n",
      "74     \t [1.07956962 1.16235714]. \t  \u001b[92m-0.00730066986647933\u001b[0m \t -0.00730066986647933\n",
      "75     \t [-0.67274238 -1.56222325]. \t  -408.7422158525488 \t -0.00730066986647933\n",
      "76     \t [ 1.68856139 -1.69422831]. \t  -2066.601950465006 \t -0.00730066986647933\n",
      "77     \t [ 0.65265246 -1.88587041]. \t  -534.5744305473129 \t -0.00730066986647933\n",
      "78     \t [-1.69721438 -1.92572466]. \t  -2317.2897352832306 \t -0.00730066986647933\n",
      "79     \t [ 1.38138552 -1.7334792 ]. \t  -1326.3470913776823 \t -0.00730066986647933\n",
      "80     \t [-1.18472947  0.67165453]. \t  -58.34510613785976 \t -0.00730066986647933\n",
      "81     \t [-1.49436433  0.52273651]. \t  -298.7646462028464 \t -0.00730066986647933\n",
      "82     \t [-1.06147932 -1.21766938]. \t  -553.8744545919657 \t -0.00730066986647933\n",
      "83     \t [-0.48247986 -1.08389492]. \t  -175.56282593029763 \t -0.00730066986647933\n",
      "84     \t [ 1.39245159 -1.35409851]. \t  -1084.5520546572018 \t -0.00730066986647933\n",
      "85     \t [1.38753819 1.92259198]. \t  -0.1508988694596794 \t -0.00730066986647933\n",
      "86     \t [1.01643062 0.60629059]. \t  -18.21956078873848 \t -0.00730066986647933\n",
      "87     \t [1.3625676  0.35762958]. \t  -224.81982638961267 \t -0.00730066986647933\n",
      "88     \t [1.67259036 0.60769277]. \t  -480.00357033406283 \t -0.00730066986647933\n",
      "89     \t [-0.10675584  1.4505127 ]. \t  -208.3303635018325 \t -0.00730066986647933\n",
      "90     \t [ 1.66960399 -0.90521397]. \t  -1364.119242511162 \t -0.00730066986647933\n",
      "91     \t [1.00698279 1.00630881]. \t  \u001b[92m-0.00598628680283637\u001b[0m \t -0.00598628680283637\n",
      "92     \t [1.31011778 1.71844037]. \t  -0.09658584701169247 \t -0.00598628680283637\n",
      "93     \t [ 1.00089176 -0.74819103]. \t  -306.24137151272816 \t -0.00598628680283637\n",
      "94     \t [-0.31594662  0.30611815]. \t  -5.987514954531987 \t -0.00598628680283637\n",
      "95     \t [0.51232447 0.86448493]. \t  -36.479259130122884 \t -0.00598628680283637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [ 1.19759156 -0.75635892]. \t  -479.90507167496264 \t -0.00598628680283637\n",
      "97     \t [1.06233966 1.11046425]. \t  -0.0366519316149771 \t -0.00598628680283637\n",
      "98     \t [ 1.98684688 -0.24356647]. \t  -1757.5284212307308 \t -0.00598628680283637\n",
      "99     \t [ 1.8417731  -0.26493655]. \t  -1338.1207981665664 \t -0.00598628680283637\n",
      "100    \t [-0.67578045 -1.95670776]. \t  -585.2519126932482 \t -0.00598628680283637\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\n",
    "\n",
    "np.random.seed(run_num_14)\n",
    "surrogate_loser_14 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_14 = GPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_14.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.31900409 -1.00146655]. \t  -122.17543449532224 \t -6.867717811955245\n",
      "init   \t [-1.01364994  0.85976824]. \t  -6.867717811955245 \t -6.867717811955245\n",
      "init   \t [-0.21482552 -1.11844164]. \t  -137.1031718367486 \t -6.867717811955245\n",
      "init   \t [-0.40259762  1.56572768]. \t  -198.98860193376754 \t -6.867717811955245\n",
      "init   \t [-0.25717364  1.55002954]. \t  -221.77381285664336 \t -6.867717811955245\n",
      "1      \t [-1.81759642  0.69595325]. \t  -687.9505990059421 \t -6.867717811955245\n",
      "2      \t [-0.42723667  0.39237061]. \t  \u001b[92m-6.440263368743487\u001b[0m \t -6.440263368743487\n",
      "3      \t [-0.68138009  0.77658894]. \t  -12.58079932586099 \t -6.440263368743487\n",
      "4      \t [1.30644486 2.048     ]. \t  -11.735777297828214 \t -6.440263368743487\n",
      "5      \t [-0.63826815  0.03497162]. \t  -16.55318639544221 \t -6.440263368743487\n",
      "6      \t [1.19322693 0.38983581]. \t  -106.94356890411369 \t -6.440263368743487\n",
      "7      \t [-0.279325    0.00863746]. \t  \u001b[92m-2.1181002226537706\u001b[0m \t -2.1181002226537706\n",
      "8      \t [-0.60558284  0.34534271]. \t  -2.6236403205886574 \t -2.1181002226537706\n",
      "9      \t [-1.21471885  1.51432563]. \t  -5.055397447061675 \t -2.1181002226537706\n",
      "10     \t [ 1.69570655 -2.048     ]. \t  -2424.4911648953716 \t -2.1181002226537706\n",
      "11     \t [0.37314826 0.06116715]. \t  \u001b[92m-1.0024742369439816\u001b[0m \t -1.0024742369439816\n",
      "12     \t [0.20502039 1.38416858]. \t  -180.76468746907915 \t -1.0024742369439816\n",
      "13     \t [ 0.15533006 -0.02400782]. \t  \u001b[92m-0.9451674680240623\u001b[0m \t -0.9451674680240623\n",
      "14     \t [-1.06147738  1.21644242]. \t  -5.054444803060877 \t -0.9451674680240623\n",
      "15     \t [-1.35058142  2.048     ]. \t  -10.539690329788293 \t -0.9451674680240623\n",
      "16     \t [2.048 2.048]. \t  -461.7603900415999 \t -0.9451674680240623\n",
      "17     \t [0.46087486 0.32701969]. \t  -1.604294043954632 \t -0.9451674680240623\n",
      "18     \t [0.92606754 2.048     ]. \t  -141.7104228359623 \t -0.9451674680240623\n",
      "19     \t [-1.00960544 -1.69868823]. \t  -742.7862207882696 \t -0.9451674680240623\n",
      "20     \t [ 0.00721609 -0.30368694]. \t  -10.211358856081647 \t -0.9451674680240623\n",
      "21     \t [0.28443126 0.17799633]. \t  -1.4547862680960644 \t -0.9451674680240623\n",
      "22     \t [1.17563233 1.44384491]. \t  \u001b[92m-0.41194952080250125\u001b[0m \t -0.41194952080250125\n",
      "23     \t [-1.28718132  1.3649696 ]. \t  -13.749783661926397 \t -0.41194952080250125\n",
      "24     \t [0.72818365 1.90109329]. \t  -187.99462602351747 \t -0.41194952080250125\n",
      "25     \t [1.89352989 1.45619731]. \t  -454.1724149262043 \t -0.41194952080250125\n",
      "26     \t [ 0.22401697 -1.35838773]. \t  -199.00946979239325 \t -0.41194952080250125\n",
      "27     \t [ 1.42645813 -1.38249747]. \t  -1167.962308583028 \t -0.41194952080250125\n",
      "28     \t [1.27654895 0.99584516]. \t  -40.2381133304632 \t -0.41194952080250125\n",
      "29     \t [0.97727194 1.05809814]. \t  -1.062193377121161 \t -0.41194952080250125\n",
      "30     \t [ 0.97642433 -0.04462397]. \t  -99.60663494896336 \t -0.41194952080250125\n",
      "31     \t [0.20243607 1.33620308]. \t  -168.3962972454647 \t -0.41194952080250125\n",
      "32     \t [ 1.86202408 -0.41052955]. \t  -1504.3702930072584 \t -0.41194952080250125\n",
      "33     \t [-1.87844345  1.70426946]. \t  -341.08531238943704 \t -0.41194952080250125\n",
      "34     \t [-0.69688178  0.87300274]. \t  -17.884070497534147 \t -0.41194952080250125\n",
      "35     \t [0.830135   0.61139408]. \t  -0.6330501033647371 \t -0.41194952080250125\n",
      "36     \t [0.55182392 0.26399945]. \t  \u001b[92m-0.3649692831176702\u001b[0m \t -0.3649692831176702\n",
      "37     \t [ 0.94346704 -0.1780317 ]. \t  -114.10015004857898 \t -0.3649692831176702\n",
      "38     \t [-0.20019587  0.38612538]. \t  -13.415322219203915 \t -0.3649692831176702\n",
      "39     \t [-0.9926861   0.51774171]. \t  -25.843628897374533 \t -0.3649692831176702\n",
      "40     \t [1.15381854 1.09155764]. \t  -5.77116687300655 \t -0.3649692831176702\n",
      "41     \t [-1.33649111 -0.36559919]. \t  -468.48682209390284 \t -0.3649692831176702\n",
      "42     \t [-0.45892964  0.92975848]. \t  -53.84500608764505 \t -0.3649692831176702\n",
      "43     \t [0.98128879 0.91133416]. \t  \u001b[92m-0.26653920343508974\u001b[0m \t -0.26653920343508974\n",
      "44     \t [0.17414949 0.71603838]. \t  -47.70189533400468 \t -0.26653920343508974\n",
      "45     \t [ 1.68741734 -1.4058258 ]. \t  -1809.4462006025503 \t -0.26653920343508974\n",
      "46     \t [0.87307096 1.35901039]. \t  -35.6280596638206 \t -0.26653920343508974\n",
      "47     \t [ 0.07770192 -0.08699998]. \t  -1.7162325778799612 \t -0.26653920343508974\n",
      "48     \t [1.35710706 1.88281138]. \t  -0.29621477368128035 \t -0.26653920343508974\n",
      "49     \t [-1.12522902 -1.26760128]. \t  -646.5012649272198 \t -0.26653920343508974\n",
      "50     \t [1.28738432 1.76000009]. \t  -1.136121701875308 \t -0.26653920343508974\n",
      "51     \t [1.82117592 0.02005112]. \t  -1087.4516611715662 \t -0.26653920343508974\n",
      "52     \t [-0.97041722 -0.56868483]. \t  -232.01166970174543 \t -0.26653920343508974\n",
      "53     \t [-1.98750058 -0.69836789]. \t  -2169.804975144425 \t -0.26653920343508974\n",
      "54     \t [ 0.05025156 -1.881929  ]. \t  -356.01879130785295 \t -0.26653920343508974\n",
      "55     \t [ 0.13604281 -1.96151768]. \t  -392.79644976636814 \t -0.26653920343508974\n",
      "56     \t [0.49790348 1.06233932]. \t  -66.5819590662643 \t -0.26653920343508974\n",
      "57     \t [0.64401272 0.36646301]. \t  -0.359913286067773 \t -0.26653920343508974\n",
      "58     \t [-0.42475128  1.02612634]. \t  -73.55291025568293 \t -0.26653920343508974\n",
      "59     \t [-0.89795156 -0.02881667]. \t  -73.34704401720438 \t -0.26653920343508974\n",
      "60     \t [ 2.02435328 -0.77141525]. \t  -2372.1758244197367 \t -0.26653920343508974\n",
      "61     \t [0.49676181 0.3065502 ]. \t  -0.6105884926302754 \t -0.26653920343508974\n",
      "62     \t [ 0.74638391 -0.94930964]. \t  -226.98799123775052 \t -0.26653920343508974\n",
      "63     \t [-1.85923427  0.72504786]. \t  -754.3960043276296 \t -0.26653920343508974\n",
      "64     \t [ 0.59307306 -0.6666248 ]. \t  -103.87139014582638 \t -0.26653920343508974\n",
      "65     \t [-1.0754649   0.43522195]. \t  -56.34975499227283 \t -0.26653920343508974\n",
      "66     \t [0.83582424 0.69112011]. \t  \u001b[92m-0.032551796759837756\u001b[0m \t -0.032551796759837756\n",
      "67     \t [0.71328651 0.49137061]. \t  -0.11250513742056896 \t -0.032551796759837756\n",
      "68     \t [0.85295802 0.7190723 ]. \t  \u001b[92m-0.028787118217305247\u001b[0m \t -0.028787118217305247\n",
      "69     \t [0.99943212 1.00428487]. \t  \u001b[92m-0.0029382945008131363\u001b[0m \t -0.0029382945008131363\n",
      "70     \t [1.00630484 1.01727835]. \t  \u001b[92m-0.0021824393687718793\u001b[0m \t -0.0021824393687718793\n",
      "71     \t [-0.0198923  -0.92816516]. \t  -87.26270799406798 \t -0.0021824393687718793\n",
      "72     \t [1.01021726 0.99828821]. \t  -0.049613736625678376 \t -0.0021824393687718793\n",
      "73     \t [-0.88726298  1.60797499]. \t  -70.92307559057677 \t -0.0021824393687718793\n",
      "74     \t [-1.9100255   0.72643737]. \t  -862.1364214861749 \t -0.0021824393687718793\n",
      "75     \t [ 0.56475636 -1.4631005 ]. \t  -317.7597443470307 \t -0.0021824393687718793\n",
      "76     \t [ 0.82312635 -1.53627469]. \t  -490.1274956390867 \t -0.0021824393687718793\n",
      "77     \t [-0.50112241  0.72246393]. \t  -24.46953284373991 \t -0.0021824393687718793\n",
      "78     \t [ 2.0139713  -0.14158256]. \t  -1763.065573072827 \t -0.0021824393687718793\n",
      "79     \t [-0.22101996 -1.38284999]. \t  -206.46732600589422 \t -0.0021824393687718793\n",
      "80     \t [0.6547521  0.44081045]. \t  -0.1338616449430302 \t -0.0021824393687718793\n",
      "81     \t [1.58678507 2.0096641 ]. \t  -26.173355388676914 \t -0.0021824393687718793\n",
      "82     \t [0.93562776 0.86152371]. \t  -0.023396993412763373 \t -0.0021824393687718793\n",
      "83     \t [-0.06708015 -1.62779617]. \t  -267.5776570316121 \t -0.0021824393687718793\n",
      "84     \t [0.68517674 0.17783462]. \t  -8.604067714590572 \t -0.0021824393687718793\n",
      "85     \t [0.39495926 1.89434764]. \t  -302.55382616645153 \t -0.0021824393687718793\n",
      "86     \t [0.79810778 0.68684508]. \t  -0.28945273468813654 \t -0.0021824393687718793\n",
      "87     \t [-1.69201713 -0.00320477]. \t  -828.7152142879839 \t -0.0021824393687718793\n",
      "88     \t [0.73203987 0.55168381]. \t  -0.09677117549492316 \t -0.0021824393687718793\n",
      "89     \t [-0.40366291 -0.83154994]. \t  -100.87203805527996 \t -0.0021824393687718793\n",
      "90     \t [-0.59352026 -0.2073231 ]. \t  -33.85333716244417 \t -0.0021824393687718793\n",
      "91     \t [1.32553206 0.05199988]. \t  -290.8205294941111 \t -0.0021824393687718793\n",
      "92     \t [-0.83237225  0.07587352]. \t  -41.422790641693126 \t -0.0021824393687718793\n",
      "93     \t [-1.85018417  1.29794549]. \t  -459.7863391163911 \t -0.0021824393687718793\n",
      "94     \t [1.53159452 0.88600749]. \t  -213.37668811417538 \t -0.0021824393687718793\n",
      "95     \t [1.43160823 2.048     ]. \t  -0.18651129808219385 \t -0.0021824393687718793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [ 1.52335898 -1.74116497]. \t  -1650.0857154121345 \t -0.0021824393687718793\n",
      "97     \t [0.87720362 0.79027476]. \t  -0.05829541102554585 \t -0.0021824393687718793\n",
      "98     \t [ 0.83029518 -0.30582201]. \t  -99.07350907469653 \t -0.0021824393687718793\n",
      "99     \t [-0.87332185 -0.51619905]. \t  -167.06532525331025 \t -0.0021824393687718793\n",
      "100    \t [-1.01396943 -1.98510142]. \t  -912.0148419647545 \t -0.0021824393687718793\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\n",
    "\n",
    "np.random.seed(run_num_15)\n",
    "surrogate_loser_15 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_15 = GPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_15.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.49073237 -0.75216486]. \t  -884.9748202573794 \t -21.690996320546372\n",
      "init   \t [0.70512959 0.03240619]. \t  -21.690996320546372 \t -21.690996320546372\n",
      "init   \t [ 1.15368111 -0.88603985]. \t  -491.54136063574043 \t -21.690996320546372\n",
      "init   \t [-1.09072882  0.26132352]. \t  -90.5574614167091 \t -21.690996320546372\n",
      "init   \t [1.5360998 0.8967902]. \t  -214.26941031258198 \t -21.690996320546372\n",
      "1      \t [ 0.01925236 -1.03263826]. \t  -107.67260830173007 \t -21.690996320546372\n",
      "2      \t [0.24241732 0.64168896]. \t  -34.55383093366369 \t -21.690996320546372\n",
      "3      \t [ 0.32122442 -0.08712064]. \t  \u001b[92m-4.0823647468742\u001b[0m \t -4.0823647468742\n",
      "4      \t [-2.048       0.58434354]. \t  -1312.4717561464336 \t -4.0823647468742\n",
      "5      \t [-0.67957819  0.06097666]. \t  -18.889043986844815 \t -4.0823647468742\n",
      "6      \t [ 0.44883143 -2.048     ]. \t  -506.3061619406781 \t -4.0823647468742\n",
      "7      \t [-0.57582745  0.77096427]. \t  -21.789327264593773 \t -4.0823647468742\n",
      "8      \t [ 0.42941509 -0.34734148]. \t  -28.600181717840076 \t -4.0823647468742\n",
      "9      \t [0.51073814 0.28090377]. \t  \u001b[92m-0.2795786907744215\u001b[0m \t -0.2795786907744215\n",
      "10     \t [-1.36096431 -1.46986084]. \t  -1109.1988316865343 \t -0.2795786907744215\n",
      "11     \t [0.22563357 0.02868629]. \t  -0.6490349679910357 \t -0.2795786907744215\n",
      "12     \t [-0.55348631  0.39689309]. \t  -3.2331774270464315 \t -0.2795786907744215\n",
      "13     \t [-0.34728917  2.048     ]. \t  -373.2984981879673 \t -0.2795786907744215\n",
      "14     \t [0.38766628 0.15411658]. \t  -0.37642057905717796 \t -0.2795786907744215\n",
      "15     \t [0.44758652 1.78406243]. \t  -251.12483395628925 \t -0.2795786907744215\n",
      "16     \t [-1.52930951 -1.01038496]. \t  -1128.0930833336422 \t -0.2795786907744215\n",
      "17     \t [-0.48561958 -1.32420417]. \t  -245.5765963866809 \t -0.2795786907744215\n",
      "18     \t [1.28959433 1.43567933]. \t  -5.2537672055258255 \t -0.2795786907744215\n",
      "19     \t [1.64144774 2.048     ]. \t  -42.18837383697442 \t -0.2795786907744215\n",
      "20     \t [1.14214396 1.42611369]. \t  -1.4993683564463458 \t -0.2795786907744215\n",
      "21     \t [0.68986958 1.53523052]. \t  -112.31005072817484 \t -0.2795786907744215\n",
      "22     \t [-1.09066884  1.80179203]. \t  -41.85388301514736 \t -0.2795786907744215\n",
      "23     \t [1.25311608 1.8460984 ]. \t  -7.670547811694383 \t -0.2795786907744215\n",
      "24     \t [1.26689908 1.61194891]. \t  \u001b[92m-0.07601770859797806\u001b[0m \t -0.07601770859797806\n",
      "25     \t [0.2693997  1.07008029]. \t  -100.03521822356028 \t -0.07601770859797806\n",
      "26     \t [-0.28520894  1.99131112]. \t  -366.44914700758426 \t -0.07601770859797806\n",
      "27     \t [ 0.67906291 -1.46483459]. \t  -371.0355872844689 \t -0.07601770859797806\n",
      "28     \t [-1.99274381  0.30019054]. \t  -1356.4611941865257 \t -0.07601770859797806\n",
      "29     \t [-1.08543823  1.22942607]. \t  -4.611708008148876 \t -0.07601770859797806\n",
      "30     \t [-0.83656932  0.66120205]. \t  -3.522339594473351 \t -0.07601770859797806\n",
      "31     \t [ 0.79193639 -1.45821261]. \t  -434.9225345066252 \t -0.07601770859797806\n",
      "32     \t [-0.40342255 -1.92282347]. \t  -436.9311602155533 \t -0.07601770859797806\n",
      "33     \t [0.44858559 0.14463035]. \t  -0.6243989373758893 \t -0.07601770859797806\n",
      "34     \t [-1.22722795 -0.38785209]. \t  -363.66161351077665 \t -0.07601770859797806\n",
      "35     \t [-1.50651717  1.90888302]. \t  -19.293867303130988 \t -0.07601770859797806\n",
      "36     \t [ 0.23034354 -0.58857712]. \t  -41.761952530407115 \t -0.07601770859797806\n",
      "37     \t [-0.29731575  0.14691306]. \t  -2.0254451751424076 \t -0.07601770859797806\n",
      "38     \t [-1.12743965  1.91984752]. \t  -46.610718111637404 \t -0.07601770859797806\n",
      "39     \t [-1.69585202 -1.39566601]. \t  -1831.9072668479507 \t -0.07601770859797806\n",
      "40     \t [ 0.07572925 -2.0285615 ]. \t  -414.69046851086 \t -0.07601770859797806\n",
      "41     \t [-0.33921788  2.02645588]. \t  -367.13357261498413 \t -0.07601770859797806\n",
      "42     \t [1.51433978 2.00771194]. \t  -8.416314764337956 \t -0.07601770859797806\n",
      "43     \t [ 1.27618886 -1.3273556 ]. \t  -873.8779230470094 \t -0.07601770859797806\n",
      "44     \t [-1.46050973  2.048     ]. \t  -6.7781160869402655 \t -0.07601770859797806\n",
      "45     \t [-0.61286995 -1.65116907]. \t  -413.3845153643779 \t -0.07601770859797806\n",
      "46     \t [0.67686122 1.11927547]. \t  -43.81428231245156 \t -0.07601770859797806\n",
      "47     \t [-0.22066691 -0.38020652]. \t  -19.885583106297887 \t -0.07601770859797806\n",
      "48     \t [1.35015396 2.04109485]. \t  -4.8828212273132925 \t -0.07601770859797806\n",
      "49     \t [1.39051078 1.8553051 ]. \t  -0.7642594133898644 \t -0.07601770859797806\n",
      "50     \t [-0.03078867 -0.10215075]. \t  -2.1254592892524284 \t -0.07601770859797806\n",
      "51     \t [-1.64505925  0.84496156]. \t  -353.4246094702246 \t -0.07601770859797806\n",
      "52     \t [-0.67517123 -1.00998659]. \t  -217.67570332419444 \t -0.07601770859797806\n",
      "53     \t [-1.27228905 -1.86451717]. \t  -1218.457017748896 \t -0.07601770859797806\n",
      "54     \t [ 0.82517819 -1.33677215]. \t  -407.1383404308359 \t -0.07601770859797806\n",
      "55     \t [-0.64063064 -1.4676662 ]. \t  -355.40779388843447 \t -0.07601770859797806\n",
      "56     \t [ 1.44253993 -0.83813951]. \t  -852.2875350360233 \t -0.07601770859797806\n",
      "57     \t [-1.20340356  0.65695278]. \t  -67.45905943685975 \t -0.07601770859797806\n",
      "58     \t [ 1.74567009 -1.89656171]. \t  -2444.796244157968 \t -0.07601770859797806\n",
      "59     \t [0.02346288 1.9066815 ]. \t  -364.2871597262516 \t -0.07601770859797806\n",
      "60     \t [0.00200424 1.56567155]. \t  -246.12747811354427 \t -0.07601770859797806\n",
      "61     \t [-0.78139034  0.53479172]. \t  -3.747599573318423 \t -0.07601770859797806\n",
      "62     \t [ 1.35482936 -0.40326941]. \t  -501.3627715286027 \t -0.07601770859797806\n",
      "63     \t [-0.19401555 -0.5657402 ]. \t  -37.83268513239098 \t -0.07601770859797806\n",
      "64     \t [-1.88164839 -0.66326923]. \t  -1775.5560972745068 \t -0.07601770859797806\n",
      "65     \t [-0.35956237  1.98789795]. \t  -347.292581304873 \t -0.07601770859797806\n",
      "66     \t [-1.5604096  -1.72930828]. \t  -1740.6005414601545 \t -0.07601770859797806\n",
      "67     \t [-0.51937017 -1.56237223]. \t  -337.97397823813054 \t -0.07601770859797806\n",
      "68     \t [ 1.47032429 -0.11759852]. \t  -519.8113640209032 \t -0.07601770859797806\n",
      "69     \t [1.29801797 1.061399  ]. \t  -38.95801006777799 \t -0.07601770859797806\n",
      "70     \t [ 1.27704013 -0.05560941]. \t  -284.48504812707824 \t -0.07601770859797806\n",
      "71     \t [1.02552828 1.05397326]. \t  \u001b[92m-0.001164720931605901\u001b[0m \t -0.001164720931605901\n",
      "72     \t [-0.20521007 -0.20636756]. \t  -7.626699547511159 \t -0.001164720931605901\n",
      "73     \t [1.10154235 1.21162875]. \t  -0.010623004296493281 \t -0.001164720931605901\n",
      "74     \t [1.09899821 1.2058827 ]. \t  -0.010167130240381185 \t -0.001164720931605901\n",
      "75     \t [-1.33548973  0.16839956]. \t  -266.3200555030319 \t -0.001164720931605901\n",
      "76     \t [-1.13398124  1.03745487]. \t  -10.727043227510059 \t -0.001164720931605901\n",
      "77     \t [-1.27488388  0.72116571]. \t  -86.92620525398058 \t -0.001164720931605901\n",
      "78     \t [-0.0209748   0.02390357]. \t  -1.097443739468415 \t -0.001164720931605901\n",
      "79     \t [ 1.95609107 -0.37593049]. \t  -1766.7817341777504 \t -0.001164720931605901\n",
      "80     \t [1.91253661 0.98388592]. \t  -715.8123979341622 \t -0.001164720931605901\n",
      "81     \t [-1.97445608 -2.00286589]. \t  -3491.4319350150254 \t -0.001164720931605901\n",
      "82     \t [ 1.40973898 -1.13316608]. \t  -973.9386714775475 \t -0.001164720931605901\n",
      "83     \t [ 1.67192921 -1.86254559]. \t  -2170.0480973974777 \t -0.001164720931605901\n",
      "84     \t [ 0.51738178 -0.67274971]. \t  -88.67445975258771 \t -0.001164720931605901\n",
      "85     \t [-0.72281118 -1.29658803]. \t  -333.8601983413895 \t -0.001164720931605901\n",
      "86     \t [0.56832542 0.35812604]. \t  -0.3097705252017462 \t -0.001164720931605901\n",
      "87     \t [ 0.52141901 -1.73419679]. \t  -402.66255897447763 \t -0.001164720931605901\n",
      "88     \t [0.49183668 0.24545175]. \t  -0.259489104774664 \t -0.001164720931605901\n",
      "89     \t [0.12518397 0.9027846 ]. \t  -79.46235216551702 \t -0.001164720931605901\n",
      "90     \t [ 0.27307719 -1.38925742]. \t  -214.80782634268718 \t -0.001164720931605901\n",
      "91     \t [1.04560963 1.57958228]. \t  -23.64917436500178 \t -0.001164720931605901\n",
      "92     \t [-0.07028959 -1.58607329]. \t  -254.27804647879904 \t -0.001164720931605901\n",
      "93     \t [-1.5765528  -1.26771701]. \t  -1415.3164776664685 \t -0.001164720931605901\n",
      "94     \t [-0.93985372  1.36091761]. \t  -26.572501212386467 \t -0.001164720931605901\n",
      "95     \t [-0.6762613   1.75609806]. \t  -171.48987023474416 \t -0.001164720931605901\n",
      "96     \t [ 0.61478593 -0.26500832]. \t  -41.48944069562792 \t -0.001164720931605901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [-1.16529491  1.41904107]. \t  -5.062175677754481 \t -0.001164720931605901\n",
      "98     \t [1.03446245 1.09305449]. \t  -0.05382082520092298 \t -0.001164720931605901\n",
      "99     \t [-1.45038161  1.64952399]. \t  -26.623491401680013 \t -0.001164720931605901\n",
      "100    \t [-1.31115942  1.94872672]. \t  -10.612508665163395 \t -0.001164720931605901\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\n",
    "\n",
    "np.random.seed(run_num_16)\n",
    "surrogate_loser_16 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_16 = GPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_16.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.62187122 0.94420838]. \t  -31.22188590191926 \t -31.22188590191926\n",
      "init   \t [-1.25596728  0.08059797]. \t  -229.14713198618372 \t -31.22188590191926\n",
      "init   \t [0.72246586 1.2470062 ]. \t  -52.64667111223406 \t -31.22188590191926\n",
      "init   \t [0.13726869 1.671308  ]. \t  -273.80846401311214 \t -31.22188590191926\n",
      "init   \t [-0.2315013  -0.99953783]. \t  -112.42501801714917 \t -31.22188590191926\n",
      "1      \t [1.52002911 0.72258891]. \t  -252.41293686650607 \t -31.22188590191926\n",
      "2      \t [ 1.67350791 -1.98930425]. \t  -2294.799412623314 \t -31.22188590191926\n",
      "3      \t [-1.38699609 -2.028939  ]. \t  -1568.0792396357292 \t -31.22188590191926\n",
      "4      \t [-0.04878061 -0.25273843]. \t  \u001b[92m-7.608458763831756\u001b[0m \t -7.608458763831756\n",
      "5      \t [ 0.98301962 -1.01648991]. \t  -393.1568044044206 \t -7.608458763831756\n",
      "6      \t [ 0.39397944 -0.54423186]. \t  -49.290523610417054 \t -7.608458763831756\n",
      "7      \t [ 1.03714737 -0.87052167]. \t  -378.7694005688127 \t -7.608458763831756\n",
      "8      \t [-0.16593024  0.33478887]. \t  -10.800020230462335 \t -7.608458763831756\n",
      "9      \t [-0.66906229 -0.37411569]. \t  -70.31472418109783 \t -7.608458763831756\n",
      "10     \t [ 0.14552358 -0.78866312]. \t  -66.31425067583925 \t -7.608458763831756\n",
      "11     \t [1.57306667 2.048     ]. \t  -18.521936747116193 \t -7.608458763831756\n",
      "12     \t [0.27474746 0.2298561 ]. \t  \u001b[92m-2.908998860402267\u001b[0m \t -2.908998860402267\n",
      "13     \t [0.01774961 0.0851444 ]. \t  \u001b[92m-1.6844177853784323\u001b[0m \t -1.6844177853784323\n",
      "14     \t [-1.67099134  1.44532198]. \t  -188.54548800317096 \t -1.6844177853784323\n",
      "15     \t [-0.20917042  1.10710212]. \t  -114.53338518007739 \t -1.6844177853784323\n",
      "16     \t [-1.64824302 -0.21314914]. \t  -865.4177536465476 \t -1.6844177853784323\n",
      "17     \t [-0.94649806  0.73098383]. \t  -6.507223156858549 \t -1.6844177853784323\n",
      "18     \t [-0.6353006   0.23973332]. \t  -5.359661549356731 \t -1.6844177853784323\n",
      "19     \t [1.92350377 1.85305989]. \t  -341.92241734077754 \t -1.6844177853784323\n",
      "20     \t [-1.41432614  1.892902  ]. \t  -6.982799998635764 \t -1.6844177853784323\n",
      "21     \t [-1.29830636  2.048     ]. \t  -18.415631194251745 \t -1.6844177853784323\n",
      "22     \t [1.18817453 2.048     ]. \t  -40.5157063586203 \t -1.6844177853784323\n",
      "23     \t [-1.98012465  2.04178599]. \t  -361.9857001577987 \t -1.6844177853784323\n",
      "24     \t [-1.21317419  1.66561736]. \t  -8.65498186194672 \t -1.6844177853784323\n",
      "25     \t [0.85774365 0.02614855]. \t  -50.36999313130531 \t -1.6844177853784323\n",
      "26     \t [-1.0962071   0.44784178]. \t  -61.219782983337005 \t -1.6844177853784323\n",
      "27     \t [-0.52252074 -0.35167814]. \t  -41.34383566617724 \t -1.6844177853784323\n",
      "28     \t [1.68058629 1.72362185]. \t  -121.62790411263364 \t -1.6844177853784323\n",
      "29     \t [1.24354472 1.52412822]. \t  \u001b[92m-0.10893270336515468\u001b[0m \t -0.10893270336515468\n",
      "30     \t [-1.31910352  1.21186882]. \t  -33.27409734432689 \t -0.10893270336515468\n",
      "31     \t [-0.61373384 -0.38423968]. \t  -60.50237428910913 \t -0.10893270336515468\n",
      "32     \t [-1.40875573  1.6289175 ]. \t  -18.452590617769218 \t -0.10893270336515468\n",
      "33     \t [0.24730585 0.03044269]. \t  -0.6609049324232796 \t -0.10893270336515468\n",
      "34     \t [1.74080353 1.54362102]. \t  -221.59905021424652 \t -0.10893270336515468\n",
      "35     \t [1.3657064  1.85806843]. \t  -0.13876166779705879 \t -0.10893270336515468\n",
      "36     \t [-0.63748997 -1.38065512]. \t  -322.03563613815305 \t -0.10893270336515468\n",
      "37     \t [-1.5044638  -1.07565267]. \t  -1121.2071802953471 \t -0.10893270336515468\n",
      "38     \t [-0.35326329 -0.03337889]. \t  -4.333218142660941 \t -0.10893270336515468\n",
      "39     \t [-0.93743739 -1.51691736]. \t  -577.6944968849152 \t -0.10893270336515468\n",
      "40     \t [0.67325011 0.42533977]. \t  -0.18475131754841478 \t -0.10893270336515468\n",
      "41     \t [-0.53458742 -0.72927028]. \t  -105.38841833205716 \t -0.10893270336515468\n",
      "42     \t [-1.44728885  0.73940159]. \t  -189.65769942715474 \t -0.10893270336515468\n",
      "43     \t [-0.09976551  1.90814023]. \t  -361.5209006963194 \t -0.10893270336515468\n",
      "44     \t [-0.99610818  0.19881197]. \t  -66.93590511449064 \t -0.10893270336515468\n",
      "45     \t [0.50362477 0.21983198]. \t  -0.36067241414956924 \t -0.10893270336515468\n",
      "46     \t [1.75456992 1.83954575]. \t  -154.07400851032006 \t -0.10893270336515468\n",
      "47     \t [ 0.15483644 -1.32420254]. \t  -182.47238793527598 \t -0.10893270336515468\n",
      "48     \t [ 1.47443898 -1.28631293]. \t  -1197.5811107866384 \t -0.10893270336515468\n",
      "49     \t [-0.3415817  1.4835984]. \t  -188.6469634120826 \t -0.10893270336515468\n",
      "50     \t [ 0.80102667 -0.25698711]. \t  -80.79332915429475 \t -0.10893270336515468\n",
      "51     \t [1.73420509 1.46014498]. \t  -239.95969685251026 \t -0.10893270336515468\n",
      "52     \t [-1.81682755 -1.25327162]. \t  -2081.9481213711797 \t -0.10893270336515468\n",
      "53     \t [1.43037403 2.048     ]. \t  -0.18563395016082818 \t -0.10893270336515468\n",
      "54     \t [-0.5832036  -0.39844562]. \t  -57.055402148986836 \t -0.10893270336515468\n",
      "55     \t [-1.9677794  0.3980393]. \t  -1215.756244906381 \t -0.10893270336515468\n",
      "56     \t [ 0.3935953  -1.31480281]. \t  -216.3754355014879 \t -0.10893270336515468\n",
      "57     \t [-1.9069261   0.97514391]. \t  -716.6611417020337 \t -0.10893270336515468\n",
      "58     \t [-1.39981897  1.24181038]. \t  -57.265984360090215 \t -0.10893270336515468\n",
      "59     \t [ 0.00784076 -0.97890301]. \t  -96.821526355202 \t -0.10893270336515468\n",
      "60     \t [-1.12769857  1.00116836]. \t  -11.846057670026436 \t -0.10893270336515468\n",
      "61     \t [1.7954402  0.87939316]. \t  -550.1658746391516 \t -0.10893270336515468\n",
      "62     \t [-1.8409285   0.12421796]. \t  -1073.9626300129419 \t -0.10893270336515468\n",
      "63     \t [0.95663178 1.56151027]. \t  -41.78076889114262 \t -0.10893270336515468\n",
      "64     \t [1.02191855 1.02091545]. \t  \u001b[92m-0.055246093826293696\u001b[0m \t -0.055246093826293696\n",
      "65     \t [0.605327   0.36611034]. \t  -0.15577641270288978 \t -0.055246093826293696\n",
      "66     \t [1.12581227 1.25147644]. \t  \u001b[92m-0.04135465339877588\u001b[0m \t -0.04135465339877588\n",
      "67     \t [-1.90526878  1.91562327]. \t  -302.36618322749564 \t -0.04135465339877588\n",
      "68     \t [-1.8590975  -0.03748674]. \t  -1228.789541338643 \t -0.04135465339877588\n",
      "69     \t [1.50847337 0.24772284]. \t  -411.4432843590957 \t -0.04135465339877588\n",
      "70     \t [-0.03744522  1.13452397]. \t  -129.47279988809532 \t -0.04135465339877588\n",
      "71     \t [1.16794486 1.37444498]. \t  \u001b[92m-0.0389172856546883\u001b[0m \t -0.0389172856546883\n",
      "72     \t [1.65108357 1.7624896 ]. \t  -93.27396602248619 \t -0.0389172856546883\n",
      "73     \t [-0.8309511   0.38541964]. \t  -12.65854777429919 \t -0.0389172856546883\n",
      "74     \t [-0.62039625 -0.28322156]. \t  -47.26318975428057 \t -0.0389172856546883\n",
      "75     \t [1.58093254 0.269114  ]. \t  -497.73172264831635 \t -0.0389172856546883\n",
      "76     \t [-1.88671546  1.04551085]. \t  -640.4454340967029 \t -0.0389172856546883\n",
      "77     \t [-1.10915018  0.20551825]. \t  -109.44867834043086 \t -0.0389172856546883\n",
      "78     \t [-1.01927465  1.69726372]. \t  -47.41900715611397 \t -0.0389172856546883\n",
      "79     \t [1.92040529 0.19413672]. \t  -1221.52480784025 \t -0.0389172856546883\n",
      "80     \t [1.00947053 0.26619503]. \t  -56.67625089578919 \t -0.0389172856546883\n",
      "81     \t [0.3186804  1.09882346]. \t  -99.9181963257552 \t -0.0389172856546883\n",
      "82     \t [1.94725409 1.95571794]. \t  -338.01646995656824 \t -0.0389172856546883\n",
      "83     \t [ 0.75218183 -1.40652874]. \t  -389.0606101266476 \t -0.0389172856546883\n",
      "84     \t [-1.60862375  1.17313899]. \t  -206.89481918097397 \t -0.0389172856546883\n",
      "85     \t [1.942115   1.69760421]. \t  -431.12082590076807 \t -0.0389172856546883\n",
      "86     \t [-0.74083596 -1.65416337]. \t  -488.351976952918 \t -0.0389172856546883\n",
      "87     \t [-0.45545945  1.64796546]. \t  -209.62876938362476 \t -0.0389172856546883\n",
      "88     \t [-0.56896611 -0.73121945]. \t  -113.7518930266233 \t -0.0389172856546883\n",
      "89     \t [1.27427815 0.38180957]. \t  -154.3254793068761 \t -0.0389172856546883\n",
      "90     \t [0.49538467 0.25080293]. \t  -0.2575493460016409 \t -0.0389172856546883\n",
      "91     \t [ 0.6677092  -1.23963205]. \t  -284.19052973424647 \t -0.0389172856546883\n",
      "92     \t [-1.00700917 -1.26368828]. \t  -522.8452122917694 \t -0.0389172856546883\n",
      "93     \t [0.16015336 0.24262124]. \t  -5.413033502014389 \t -0.0389172856546883\n",
      "94     \t [0.48206965 0.23766799]. \t  -0.2710363611556548 \t -0.0389172856546883\n",
      "95     \t [1.1647046  1.34701788]. \t  \u001b[92m-0.036188593283794895\u001b[0m \t -0.036188593283794895\n",
      "96     \t [-1.91187252 -1.00609437]. \t  -2181.2982218772822 \t -0.036188593283794895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [-0.32671966 -0.20386439]. \t  -11.40804985655089 \t -0.036188593283794895\n",
      "98     \t [1.39696587 0.04097297]. \t  -365.1741489099554 \t -0.036188593283794895\n",
      "99     \t [1.676243   1.94336126]. \t  -75.52728652939464 \t -0.036188593283794895\n",
      "100    \t [0.53826131 1.57679642]. \t  -165.8684252955217 \t -0.036188593283794895\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\n",
    "\n",
    "np.random.seed(run_num_17)\n",
    "surrogate_loser_17 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_17 = GPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_17.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [1.52956683 1.91914255]. \t  -17.95675982372887 \t -1.7663579664225912\n",
      "init   \t [1.51222084 0.12638491]. \t  -467.0068292530542 \t -1.7663579664225912\n",
      "init   \t [-1.09474477 -2.0013105 ]. \t  -1028.244988519411 \t -1.7663579664225912\n",
      "init   \t [-0.28479972 -0.39996883]. \t  -24.79447905426233 \t -1.7663579664225912\n",
      "init   \t [ 0.09287545 -0.0885072 ]. \t  -1.7663579664225912 \t -1.7663579664225912\n",
      "1      \t [-0.5838592   0.42107488]. \t  -3.151546324395398 \t -1.7663579664225912\n",
      "2      \t [-0.25047818  0.02972492]. \t  \u001b[92m-1.6726906902126766\u001b[0m \t -1.6726906902126766\n",
      "3      \t [-1.31652822  1.37453189]. \t  -18.23392347915735 \t -1.6726906902126766\n",
      "4      \t [ 0.21720923 -0.54450454]. \t  -35.62180259164732 \t -1.6726906902126766\n",
      "5      \t [0.39893756 2.048     ]. \t  -357.13626394766504 \t -1.6726906902126766\n",
      "6      \t [2.048 2.048]. \t  -461.7603900415999 \t -1.6726906902126766\n",
      "7      \t [-1.6592448   0.61224628]. \t  -465.3941770270854 \t -1.6726906902126766\n",
      "8      \t [1.16380436 1.47061005]. \t  \u001b[92m-1.3763662634648268\u001b[0m \t -1.3763662634648268\n",
      "9      \t [-0.77922126  1.10699248]. \t  -28.146303051691017 \t -1.3763662634648268\n",
      "10     \t [-1.7520212  2.048    ]. \t  -111.93583684829115 \t -1.3763662634648268\n",
      "11     \t [1.21848011 2.048     ]. \t  -31.779122161790156 \t -1.3763662634648268\n",
      "12     \t [-0.96084287  1.00652407]. \t  -4.5388778732143615 \t -1.3763662634648268\n",
      "13     \t [-1.21805618 -0.74376773]. \t  -501.0635869544814 \t -1.3763662634648268\n",
      "14     \t [1.47996912 1.30758724]. \t  -78.15006826502355 \t -1.3763662634648268\n",
      "15     \t [1.32517167 1.71572575]. \t  \u001b[92m-0.2685827692038145\u001b[0m \t -0.2685827692038145\n",
      "16     \t [0.70899277 0.70790763]. \t  -4.296902837614261 \t -0.2685827692038145\n",
      "17     \t [1.0223785  1.10788087]. \t  -0.3926656474370077 \t -0.2685827692038145\n",
      "18     \t [0.00842168 1.21627265]. \t  -148.89789227870298 \t -0.2685827692038145\n",
      "19     \t [-0.06107749 -0.04401587]. \t  -1.3538566565061956 \t -0.2685827692038145\n",
      "20     \t [0.87621562 1.02633812]. \t  -6.701906934254677 \t -0.2685827692038145\n",
      "21     \t [1.36465468 1.79040632]. \t  -0.6495901307152054 \t -0.2685827692038145\n",
      "22     \t [0.37102679 0.20769264]. \t  -0.886052037304184 \t -0.2685827692038145\n",
      "23     \t [ 1.78433219 -1.76828436]. \t  -2452.970093434316 \t -0.2685827692038145\n",
      "24     \t [1.42976536 2.048     ]. \t  \u001b[92m-0.186120315685995\u001b[0m \t -0.186120315685995\n",
      "25     \t [0.61275137 0.31832019]. \t  -0.47650571394358315 \t -0.186120315685995\n",
      "26     \t [-0.50301419  0.51412736]. \t  -9.076586301267312 \t -0.186120315685995\n",
      "27     \t [0.58674508 0.36628632]. \t  -0.21925235875589705 \t -0.186120315685995\n",
      "28     \t [-1.7750829  0.9792554]. \t  -479.3134904673417 \t -0.186120315685995\n",
      "29     \t [ 1.30212754 -1.04704589]. \t  -752.2668928051232 \t -0.186120315685995\n",
      "30     \t [1.2241202  1.43137903]. \t  -0.5003533030563203 \t -0.186120315685995\n",
      "31     \t [-2.00155577 -1.73617063]. \t  -3306.5206802629314 \t -0.186120315685995\n",
      "32     \t [-0.51379973 -1.63690855]. \t  -363.6331805481531 \t -0.186120315685995\n",
      "33     \t [-0.3086102 -1.5223783]. \t  -263.3814376804697 \t -0.186120315685995\n",
      "34     \t [ 0.70410932 -2.03075123]. \t  -638.4184743565811 \t -0.186120315685995\n",
      "35     \t [0.54120142 1.47148362]. \t  -139.1166704926515 \t -0.186120315685995\n",
      "36     \t [-0.74895875  1.57222134]. \t  -105.32801185836587 \t -0.186120315685995\n",
      "37     \t [-1.83696341 -1.25620347]. \t  -2152.32922596957 \t -0.186120315685995\n",
      "38     \t [1.8550285  0.74680791]. \t  -726.6686225385389 \t -0.186120315685995\n",
      "39     \t [-1.27560641  1.69777836]. \t  -5.676914496877277 \t -0.186120315685995\n",
      "40     \t [-1.44206756 -1.21484429]. \t  -1091.2728928664756 \t -0.186120315685995\n",
      "41     \t [1.068591   0.91240867]. \t  -5.270722615947526 \t -0.186120315685995\n",
      "42     \t [-0.29874947  0.69036876]. \t  -37.820977350770235 \t -0.186120315685995\n",
      "43     \t [0.51472498 0.82805609]. \t  -31.945262563754127 \t -0.186120315685995\n",
      "44     \t [1.03427819 1.06711928]. \t  \u001b[92m-0.001857295385662538\u001b[0m \t -0.001857295385662538\n",
      "45     \t [ 0.34533537 -0.98658915]. \t  -122.71805110077798 \t -0.001857295385662538\n",
      "46     \t [1.01924627 1.03716979]. \t  \u001b[92m-0.0006570993390599014\u001b[0m \t -0.0006570993390599014\n",
      "47     \t [-1.16965222  1.44478982]. \t  -5.295733522302386 \t -0.0006570993390599014\n",
      "48     \t [-0.33344666  1.70336935]. \t  -255.28264766624903 \t -0.0006570993390599014\n",
      "49     \t [1.39119913 1.93327325]. \t  -0.15350408826844805 \t -0.0006570993390599014\n",
      "50     \t [1.01501892 1.03156513]. \t  \u001b[92m-0.00039501496185426996\u001b[0m \t -0.00039501496185426996\n",
      "51     \t [ 0.22431893 -0.01304156]. \t  -1.0031369957016563 \t -0.00039501496185426996\n",
      "52     \t [1.61404428 1.32600028]. \t  -163.99662342852554 \t -0.00039501496185426996\n",
      "53     \t [-1.94029947 -0.86847316]. \t  -2155.3322056729285 \t -0.00039501496185426996\n",
      "54     \t [0.07464172 1.83765711]. \t  -336.51009897136794 \t -0.00039501496185426996\n",
      "55     \t [0.51311545 0.0998633 ]. \t  -2.9078022004534447 \t -0.00039501496185426996\n",
      "56     \t [-0.9724487  -0.53938542]. \t  -224.42549533519733 \t -0.00039501496185426996\n",
      "57     \t [1.03697633 1.0319875 ]. \t  -0.18913702011945493 \t -0.00039501496185426996\n",
      "58     \t [1.02773691 1.05729113]. \t  -0.0008791593523144197 \t -0.00039501496185426996\n",
      "59     \t [-0.6656559   1.80874007]. \t  -189.27229594052525 \t -0.00039501496185426996\n",
      "60     \t [ 0.66227292 -0.70944906]. \t  -131.9169686429098 \t -0.00039501496185426996\n",
      "61     \t [ 1.91918506 -0.9411309 ]. \t  -2139.354445757398 \t -0.00039501496185426996\n",
      "62     \t [ 0.61530074 -0.04220243]. \t  -17.855040930280474 \t -0.00039501496185426996\n",
      "63     \t [-1.26278418  1.72912074]. \t  -6.929132761748157 \t -0.00039501496185426996\n",
      "64     \t [1.08652946 1.22040262]. \t  -0.16634022798350892 \t -0.00039501496185426996\n",
      "65     \t [-1.3258731  -1.69927707]. \t  -1200.6443181943291 \t -0.00039501496185426996\n",
      "66     \t [0.71180344 0.48073656]. \t  -0.15028119964701456 \t -0.00039501496185426996\n",
      "67     \t [ 1.94377297 -1.45285214]. \t  -2737.3371698894134 \t -0.00039501496185426996\n",
      "68     \t [1.25395039 1.49810155]. \t  -0.6163916927328872 \t -0.00039501496185426996\n",
      "69     \t [-1.26602996  1.28388001]. \t  -15.30791956683049 \t -0.00039501496185426996\n",
      "70     \t [-1.92633866 -2.01001904]. \t  -3281.3183646001717 \t -0.00039501496185426996\n",
      "71     \t [-1.66391243 -0.01085516]. \t  -779.6360682164172 \t -0.00039501496185426996\n",
      "72     \t [-0.84373096  1.94458006]. \t  -155.35381258533093 \t -0.00039501496185426996\n",
      "73     \t [-1.84375537 -1.86727236]. \t  -2781.9064022820107 \t -0.00039501496185426996\n",
      "74     \t [-0.78666941 -1.10787842]. \t  -301.3508649798184 \t -0.00039501496185426996\n",
      "75     \t [0.33148238 0.90152338]. \t  -63.11674969328898 \t -0.00039501496185426996\n",
      "76     \t [1.6702811  1.70236723]. \t  -118.70875420772408 \t -0.00039501496185426996\n",
      "77     \t [ 0.11668496 -1.96729657]. \t  -393.1814597329686 \t -0.00039501496185426996\n",
      "78     \t [-0.88247338  2.04742146]. \t  -164.49408042071573 \t -0.00039501496185426996\n",
      "79     \t [-0.43549238 -2.01846151]. \t  -489.6378795741759 \t -0.00039501496185426996\n",
      "80     \t [-0.01091106  0.37740414]. \t  -15.256345137726763 \t -0.00039501496185426996\n",
      "81     \t [-0.1208149   1.48729322]. \t  -218.1398643921702 \t -0.00039501496185426996\n",
      "82     \t [1.07927832 1.16554396]. \t  -0.0063343698946076105 \t -0.00039501496185426996\n",
      "83     \t [0.03857045 1.98165671]. \t  -393.0312840335908 \t -0.00039501496185426996\n",
      "84     \t [1.20764837 0.79814724]. \t  -43.63841506633272 \t -0.00039501496185426996\n",
      "85     \t [ 0.38269592 -1.23938671]. \t  -192.4371118418537 \t -0.00039501496185426996\n",
      "86     \t [ 0.61123068 -0.5270834 ]. \t  -81.27473123256354 \t -0.00039501496185426996\n",
      "87     \t [-0.11353625  0.93397012]. \t  -86.07873293752077 \t -0.00039501496185426996\n",
      "88     \t [-0.92679023 -1.14150193]. \t  -403.8893634877983 \t -0.00039501496185426996\n",
      "89     \t [0.78318802 0.59121723]. \t  -0.09614165737818745 \t -0.00039501496185426996\n",
      "90     \t [1.39924314 1.96076222]. \t  -0.16022501559781602 \t -0.00039501496185426996\n",
      "91     \t [-0.2896775 -0.7808088]. \t  -76.43765662694076 \t -0.00039501496185426996\n",
      "92     \t [-1.14294969 -0.48032313]. \t  -323.8066030564311 \t -0.00039501496185426996\n",
      "93     \t [-0.87056293 -0.71276528]. \t  -219.7787060538371 \t -0.00039501496185426996\n",
      "94     \t [1.58263691 1.77828318]. \t  -53.11335976600227 \t -0.00039501496185426996\n",
      "95     \t [-0.3000354 -0.4367719]. \t  -29.441193255529072 \t -0.00039501496185426996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [ 1.6314697  -1.10691842]. \t  -1420.642250098064 \t -0.00039501496185426996\n",
      "97     \t [1.10320842 1.21263853]. \t  -0.01261472276031042 \t -0.00039501496185426996\n",
      "98     \t [-0.97394672  1.79359404]. \t  -75.30265313975646 \t -0.00039501496185426996\n",
      "99     \t [ 2.00177263 -1.26549273]. \t  -2781.0202770904502 \t -0.00039501496185426996\n",
      "100    \t [-0.20113591  1.12016858]. \t  -118.02072811771028 \t -0.00039501496185426996\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\n",
    "\n",
    "np.random.seed(run_num_18)\n",
    "surrogate_loser_18 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_18 = GPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_18.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.24348908 1.07107119]. \t  -102.94304782477323 \t -4.219752052396591\n",
      "init   \t [-1.04639875  1.11284007]. \t  -4.219752052396591 \t -4.219752052396591\n",
      "init   \t [-0.59608235  1.21527255]. \t  -76.50031976976825 \t -4.219752052396591\n",
      "init   \t [-0.63078391 -0.22989031]. \t  -42.07005958957044 \t -4.219752052396591\n",
      "init   \t [-0.8056685  -0.23710138]. \t  -81.79603430667063 \t -4.219752052396591\n",
      "1      \t [ 0.4289548  -0.54692575]. \t  -53.75166279546546 \t -4.219752052396591\n",
      "2      \t [-2.01057158  1.54840386]. \t  -631.0642658014616 \t -4.219752052396591\n",
      "3      \t [-0.81640829  0.57003337]. \t  -4.230354265254039 \t -4.219752052396591\n",
      "4      \t [1.48975836 1.25183036]. \t  -93.85508647922497 \t -4.219752052396591\n",
      "5      \t [-0.08031195 -0.27675321]. \t  -9.187480026978836 \t -4.219752052396591\n",
      "6      \t [-0.20278538 -1.27079977]. \t  -173.56054293310598 \t -4.219752052396591\n",
      "7      \t [-1.00036302  0.86176304]. \t  -5.932527489682171 \t -4.219752052396591\n",
      "8      \t [0.78366374 0.40690276]. \t  -4.341066672074766 \t -4.219752052396591\n",
      "9      \t [ 1.88662176 -1.52272924]. \t  -2583.530575995578 \t -4.219752052396591\n",
      "10     \t [0.90780551 1.05472632]. \t  -5.3268501253444525 \t -4.219752052396591\n",
      "11     \t [ 1.883773   -0.96035514]. \t  -2033.8493574863483 \t -4.219752052396591\n",
      "12     \t [0.85073193 0.70907048]. \t  \u001b[92m-0.04381458802715288\u001b[0m \t -0.04381458802715288\n",
      "13     \t [-0.73934761  1.83511187]. \t  -169.04262503651273 \t -0.04381458802715288\n",
      "14     \t [-1.02326689  0.13551371]. \t  -87.18803054460024 \t -0.04381458802715288\n",
      "15     \t [-0.91779786  0.09679596]. \t  -59.26346567837915 \t -0.04381458802715288\n",
      "16     \t [ 1.22568239 -1.1696075 ]. \t  -713.9584663977281 \t -0.04381458802715288\n",
      "17     \t [ 0.32624751 -2.019548  ]. \t  -452.43535156259907 \t -0.04381458802715288\n",
      "18     \t [-0.06450438  0.16629939]. \t  -3.7620613850141296 \t -0.04381458802715288\n",
      "19     \t [1.06473705 2.048     ]. \t  -83.60504392331846 \t -0.04381458802715288\n",
      "20     \t [-0.94349482  0.99130426]. \t  -4.799733492202603 \t -0.04381458802715288\n",
      "21     \t [-1.99774998 -1.90184684]. \t  -3481.5567684995062 \t -0.04381458802715288\n",
      "22     \t [-0.39255273 -0.0113082 ]. \t  -4.6751125366412 \t -0.04381458802715288\n",
      "23     \t [1.45933798 1.02393232]. \t  -122.47598556447716 \t -0.04381458802715288\n",
      "24     \t [-1.31971342 -1.08305348]. \t  -803.2723765145327 \t -0.04381458802715288\n",
      "25     \t [ 1.38242446 -0.1308308 ]. \t  -417.0933217978137 \t -0.04381458802715288\n",
      "26     \t [0.5812708  0.38710501]. \t  -0.4176862677958069 \t -0.04381458802715288\n",
      "27     \t [0.75180032 0.57097289]. \t  -0.06493141378655598 \t -0.04381458802715288\n",
      "28     \t [0.17153394 0.02079416]. \t  -0.6938032391470994 \t -0.04381458802715288\n",
      "29     \t [-0.03142889  0.0074378 ]. \t  -1.0680058379718527 \t -0.04381458802715288\n",
      "30     \t [-0.20962842  0.03663851]. \t  -1.468538038697957 \t -0.04381458802715288\n",
      "31     \t [-1.53636148  0.48205596]. \t  -359.2532426805832 \t -0.04381458802715288\n",
      "32     \t [-0.62571956  1.4256426 ]. \t  -109.58289262533435 \t -0.04381458802715288\n",
      "33     \t [-0.61375131  0.89657007]. \t  -29.631652197295683 \t -0.04381458802715288\n",
      "34     \t [-0.5868938   1.12710118]. \t  -63.77340517813315 \t -0.04381458802715288\n",
      "35     \t [2.0381793 0.72216  ]. \t  -1178.9504173057226 \t -0.04381458802715288\n",
      "36     \t [1.111885   1.13756807]. \t  -0.9870855260166645 \t -0.04381458802715288\n",
      "37     \t [0.50673089 0.16869641]. \t  -1.0191193020423532 \t -0.04381458802715288\n",
      "38     \t [0.99761977 0.96281009]. \t  -0.10520937468045778 \t -0.04381458802715288\n",
      "39     \t [1.17092552 1.57273923]. \t  -4.096401225250451 \t -0.04381458802715288\n",
      "40     \t [ 0.959215  -1.3861733]. \t  -531.888274532088 \t -0.04381458802715288\n",
      "41     \t [ 0.963913   -1.67233212]. \t  -676.7609160489471 \t -0.04381458802715288\n",
      "42     \t [-0.24680134 -1.78126617]. \t  -340.9161515362546 \t -0.04381458802715288\n",
      "43     \t [-0.58202633 -0.52425901]. \t  -76.98206422017245 \t -0.04381458802715288\n",
      "44     \t [1.0893578  1.18241623]. \t  \u001b[92m-0.009820246873348799\u001b[0m \t -0.009820246873348799\n",
      "45     \t [ 0.42345259 -0.53876914]. \t  -51.896472941900235 \t -0.009820246873348799\n",
      "46     \t [ 1.79677526 -1.32817717]. \t  -2076.875614620295 \t -0.009820246873348799\n",
      "47     \t [0.79281754 0.70307785]. \t  -0.5982207655329881 \t -0.009820246873348799\n",
      "48     \t [-0.65978578 -1.76653712]. \t  -487.5711645016507 \t -0.009820246873348799\n",
      "49     \t [0.95554818 0.91196488]. \t  \u001b[92m-0.0020986079645110515\u001b[0m \t -0.0020986079645110515\n",
      "50     \t [ 1.02841419 -1.36419327]. \t  -586.5263844771054 \t -0.0020986079645110515\n",
      "51     \t [1.4252979  0.46022507]. \t  -247.06323074062726 \t -0.0020986079645110515\n",
      "52     \t [1.28390872 1.65478729]. \t  -0.08465634865059796 \t -0.0020986079645110515\n",
      "53     \t [1.42875409 0.43741702]. \t  -257.4401579992734 \t -0.0020986079645110515\n",
      "54     \t [1.50216865 0.12571883]. \t  -454.2795524743432 \t -0.0020986079645110515\n",
      "55     \t [-0.18601702 -1.73004712]. \t  -312.8054036447344 \t -0.0020986079645110515\n",
      "56     \t [-2.02404919  0.73184435]. \t  -1141.4207830370153 \t -0.0020986079645110515\n",
      "57     \t [1.20057254 1.42148384]. \t  -0.07979290349240192 \t -0.0020986079645110515\n",
      "58     \t [-1.46571663  2.048     ]. \t  -7.086274160521702 \t -0.0020986079645110515\n",
      "59     \t [0.76101456 0.58943797]. \t  -0.06771235104415185 \t -0.0020986079645110515\n",
      "60     \t [-1.14287721 -1.84974167]. \t  -1000.568697410595 \t -0.0020986079645110515\n",
      "61     \t [ 0.60120811 -0.72900613]. \t  -119.0687515408353 \t -0.0020986079645110515\n",
      "62     \t [-1.0803744   0.18974846]. \t  -99.8708387620765 \t -0.0020986079645110515\n",
      "63     \t [-1.07883665  0.83854152]. \t  -14.906629231210427 \t -0.0020986079645110515\n",
      "64     \t [-1.78051982  0.0386201 ]. \t  -988.4423965795083 \t -0.0020986079645110515\n",
      "65     \t [-1.74403802 -1.30179646]. \t  -1894.098633935269 \t -0.0020986079645110515\n",
      "66     \t [-1.28822841  1.7831024 ]. \t  -6.762942941341422 \t -0.0020986079645110515\n",
      "67     \t [ 1.84141782 -0.56448018]. \t  -1565.1476106907987 \t -0.0020986079645110515\n",
      "68     \t [0.29070172 0.83551013]. \t  -56.90360093054751 \t -0.0020986079645110515\n",
      "69     \t [ 0.77466228 -0.26976528]. \t  -75.71762526666582 \t -0.0020986079645110515\n",
      "70     \t [-1.77269767 -1.13558288]. \t  -1837.8503994612217 \t -0.0020986079645110515\n",
      "71     \t [0.11330097 1.90764284]. \t  -359.81511194786157 \t -0.0020986079645110515\n",
      "72     \t [1.00127548 0.98242266]. \t  -0.04052304804428198 \t -0.0020986079645110515\n",
      "73     \t [-0.41998961 -1.70756351]. \t  -356.9449335463213 \t -0.0020986079645110515\n",
      "74     \t [0.98970685 0.95722753]. \t  -0.049799799392199945 \t -0.0020986079645110515\n",
      "75     \t [ 1.54651295 -0.94071756]. \t  -1110.8008886824762 \t -0.0020986079645110515\n",
      "76     \t [1.22026773 1.09116792]. \t  -15.879797912924316 \t -0.0020986079645110515\n",
      "77     \t [0.94548383 0.88208101]. \t  -0.01703479846433533 \t -0.0020986079645110515\n",
      "78     \t [-0.25579785 -1.91213514]. \t  -392.65441879290506 \t -0.0020986079645110515\n",
      "79     \t [-1.01193613  1.27793013]. \t  -10.4951895211278 \t -0.0020986079645110515\n",
      "80     \t [-0.04833811  0.43081048]. \t  -19.4580015708361 \t -0.0020986079645110515\n",
      "81     \t [0.14672588 0.87963436]. \t  -74.36264551014955 \t -0.0020986079645110515\n",
      "82     \t [ 1.59712897 -0.86236332]. \t  -1165.3392525757588 \t -0.0020986079645110515\n",
      "83     \t [-0.98486741  0.68231356]. \t  -12.213965271784032 \t -0.0020986079645110515\n",
      "84     \t [-1.81862779  1.1758283 ]. \t  -462.3074627004391 \t -0.0020986079645110515\n",
      "85     \t [-0.24366068  1.84452526]. \t  -320.2244345913622 \t -0.0020986079645110515\n",
      "86     \t [-1.22222237  1.39634206]. \t  -5.88861363497911 \t -0.0020986079645110515\n",
      "87     \t [0.87499096 0.74409058]. \t  -0.0619322451656779 \t -0.0020986079645110515\n",
      "88     \t [-0.14002292 -1.35041328]. \t  -188.99504911871696 \t -0.0020986079645110515\n",
      "89     \t [-0.57263001  0.90454645]. \t  -35.72468675050905 \t -0.0020986079645110515\n",
      "90     \t [-0.52357218  0.25964062]. \t  -2.342260135583863 \t -0.0020986079645110515\n",
      "91     \t [0.01824425 1.73847894]. \t  -303.07902757254914 \t -0.0020986079645110515\n",
      "92     \t [1.81914489 1.51624507]. \t  -322.17134307118727 \t -0.0020986079645110515\n",
      "93     \t [ 0.88411369 -0.31217853]. \t  -119.66104849291484 \t -0.0020986079645110515\n",
      "94     \t [0.88798542 0.19356752]. \t  -35.40916619948873 \t -0.0020986079645110515\n",
      "95     \t [-0.93516627 -0.76413114]. \t  -272.2678550483519 \t -0.0020986079645110515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [-0.87974617 -1.97633035]. \t  -759.9394729222546 \t -0.0020986079645110515\n",
      "97     \t [1.22201364 0.65736029]. \t  -69.93170963058354 \t -0.0020986079645110515\n",
      "98     \t [-1.51314065  0.48297648]. \t  -332.70279227581244 \t -0.0020986079645110515\n",
      "99     \t [-2.02472409  1.55710344]. \t  -655.530863928991 \t -0.0020986079645110515\n",
      "100    \t [-0.20069779  1.79982045]. \t  -311.04007298950785 \t -0.0020986079645110515\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\n",
    "\n",
    "np.random.seed(run_num_19)\n",
    "surrogate_loser_19 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_19 = GPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_19.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.2888388  0.26430978]. \t  -3.777577453542735 \t -3.777577453542735\n",
      "init   \t [-0.04734225 -0.66978712]. \t  -46.25914429040042 \t -3.777577453542735\n",
      "init   \t [-0.50844394  0.13121892]. \t  -3.895838569726608 \t -3.777577453542735\n",
      "init   \t [-1.76903664  0.34623103]. \t  -782.3209705533411 \t -3.777577453542735\n",
      "init   \t [-1.07357076 -1.38954104]. \t  -650.524506936468 \t -3.777577453542735\n",
      "1      \t [-0.1332695   0.13267709]. \t  \u001b[92m-2.6048761032215726\u001b[0m \t -2.6048761032215726\n",
      "2      \t [1.52601541 0.49362155]. \t  -337.0364385207258 \t -2.6048761032215726\n",
      "3      \t [0.06323371 1.41284359]. \t  -199.36197910212854 \t -2.6048761032215726\n",
      "4      \t [ 0.93462177 -1.1475014 ]. \t  -408.4561526610949 \t -2.6048761032215726\n",
      "5      \t [-0.37159194 -0.18318702]. \t  -12.202550786317964 \t -2.6048761032215726\n",
      "6      \t [-1.42024952  1.13391263]. \t  -83.86113639867138 \t -2.6048761032215726\n",
      "7      \t [-1.41345518  2.048     ]. \t  -6.07621247720745 \t -2.6048761032215726\n",
      "8      \t [-0.97540746  1.26259133]. \t  -13.585012192506847 \t -2.6048761032215726\n",
      "9      \t [-1.88679977  2.048     ]. \t  -236.95205926553626 \t -2.6048761032215726\n",
      "10     \t [-0.29297393  0.05695057]. \t  \u001b[92m-1.7552052274073262\u001b[0m \t -1.7552052274073262\n",
      "11     \t [-1.19530118  2.048     ]. \t  -43.16703336468801 \t -1.7552052274073262\n",
      "12     \t [0.53419478 0.81149925]. \t  -27.898798372583414 \t -1.7552052274073262\n",
      "13     \t [-1.30576919  1.59694943]. \t  -6.4847809016286755 \t -1.7552052274073262\n",
      "14     \t [-0.78860716  0.73563946]. \t  -4.492753503759733 \t -1.7552052274073262\n",
      "15     \t [-0.3983729  -1.64194288]. \t  -326.1872740695626 \t -1.7552052274073262\n",
      "16     \t [0.32302253 0.4999506 ]. \t  -16.108791642617973 \t -1.7552052274073262\n",
      "17     \t [1.31747016 2.048     ]. \t  -9.852191432704268 \t -1.7552052274073262\n",
      "18     \t [ 0.25799003 -0.24867382]. \t  -10.4877429368878 \t -1.7552052274073262\n",
      "19     \t [-1.10943324  1.18089624]. \t  -4.699167739297667 \t -1.7552052274073262\n",
      "20     \t [0.18615436 0.02312019]. \t  \u001b[92m-0.675646330080072\u001b[0m \t -0.675646330080072\n",
      "21     \t [0.95289162 1.25393753]. \t  -11.969327146174747 \t -0.675646330080072\n",
      "22     \t [-0.19580929  0.08150855]. \t  -1.616301184436927 \t -0.675646330080072\n",
      "23     \t [ 1.66764852 -1.71888722]. \t  -2025.3906879916842 \t -0.675646330080072\n",
      "24     \t [-1.69537024 -1.39278119]. \t  -1828.0463414122016 \t -0.675646330080072\n",
      "25     \t [0.23677012 0.4709882 ]. \t  -17.799052926002027 \t -0.675646330080072\n",
      "26     \t [ 0.8501451  -0.35546572]. \t  -116.27665665103598 \t -0.675646330080072\n",
      "27     \t [ 0.14364922 -0.89507096]. \t  -84.58509496095981 \t -0.675646330080072\n",
      "28     \t [1.80318636 1.94411307]. \t  -171.56621188478246 \t -0.675646330080072\n",
      "29     \t [1.1229608  1.95377589]. \t  -48.00328822004864 \t -0.675646330080072\n",
      "30     \t [-1.1342216  -0.37006358]. \t  -278.9614916305373 \t -0.675646330080072\n",
      "31     \t [0.43348374 0.24789285]. \t  -0.6807571136465769 \t -0.675646330080072\n",
      "32     \t [1.20459609 1.79563618]. \t  -11.915702706226021 \t -0.675646330080072\n",
      "33     \t [-0.47830041 -1.52576977]. \t  -310.02680065959424 \t -0.675646330080072\n",
      "34     \t [-1.20550988 -0.05889056]. \t  -233.52241158111184 \t -0.675646330080072\n",
      "35     \t [-0.52674853  0.54617333]. \t  -9.55143065929903 \t -0.675646330080072\n",
      "36     \t [0.74304255 0.56832733]. \t  \u001b[92m-0.09232009178814232\u001b[0m \t -0.09232009178814232\n",
      "37     \t [1.32115648 1.18152797]. \t  -31.904447673878682 \t -0.09232009178814232\n",
      "38     \t [-0.3040362   0.76544083]. \t  -46.9937903703847 \t -0.09232009178814232\n",
      "39     \t [0.85087995 0.77008763]. \t  -0.23467415583651066 \t -0.09232009178814232\n",
      "40     \t [1.22350821 1.48006613]. \t  \u001b[92m-0.07853789152412906\u001b[0m \t -0.07853789152412906\n",
      "41     \t [0.71711674 0.58197332]. \t  -0.5385807862798063 \t -0.07853789152412906\n",
      "42     \t [-0.4068628  -1.28073464]. \t  -211.14952512344544 \t -0.07853789152412906\n",
      "43     \t [0.17529204 1.15916279]. \t  -128.0168083834562 \t -0.07853789152412906\n",
      "44     \t [0.48382867 1.77067549]. \t  -236.37587193199764 \t -0.07853789152412906\n",
      "45     \t [0.47964708 1.26569985]. \t  -107.52548316444184 \t -0.07853789152412906\n",
      "46     \t [-1.50941603 -1.10529415]. \t  -1151.1929674840562 \t -0.07853789152412906\n",
      "47     \t [ 1.27091858 -1.70898462]. \t  -1105.116363693183 \t -0.07853789152412906\n",
      "48     \t [1.51949793 0.87348176]. \t  -206.30495661161856 \t -0.07853789152412906\n",
      "49     \t [-1.86573362  0.37119973]. \t  -975.2745320400953 \t -0.07853789152412906\n",
      "50     \t [0.67134476 0.46935239]. \t  -0.14279133921161213 \t -0.07853789152412906\n",
      "51     \t [1.13740342 1.29280045]. \t  \u001b[92m-0.018958216595401665\u001b[0m \t -0.018958216595401665\n",
      "52     \t [1.95650081 1.38439042]. \t  -597.9865725043503 \t -0.018958216595401665\n",
      "53     \t [ 1.71219045 -1.9877464 ]. \t  -2420.500308270193 \t -0.018958216595401665\n",
      "54     \t [0.46888773 1.61398374]. \t  -194.64137800239578 \t -0.018958216595401665\n",
      "55     \t [ 1.15212581 -1.08914448]. \t  -583.9889106648615 \t -0.018958216595401665\n",
      "56     \t [1.10529464 0.8664695 ]. \t  -12.628270272037758 \t -0.018958216595401665\n",
      "57     \t [1.06989476 1.13508979]. \t  \u001b[92m-0.01407250852536994\u001b[0m \t -0.01407250852536994\n",
      "58     \t [-1.66676799  0.66994422]. \t  -451.5502786295779 \t -0.01407250852536994\n",
      "59     \t [-0.99523387  0.24252604]. \t  -59.926034914967154 \t -0.01407250852536994\n",
      "60     \t [1.17397494 1.40563929]. \t  -0.10546463932844857 \t -0.01407250852536994\n",
      "61     \t [0.56691931 0.32727801]. \t  -0.19101691259248388 \t -0.01407250852536994\n",
      "62     \t [-0.94001591  2.04528928]. \t  -138.70891112332072 \t -0.01407250852536994\n",
      "63     \t [ 0.57187407 -1.66012158]. \t  -395.0643876039098 \t -0.01407250852536994\n",
      "64     \t [0.44374421 1.12386373]. \t  -86.23394152062177 \t -0.01407250852536994\n",
      "65     \t [1.18674958 1.09267993]. \t  -10.00118618641607 \t -0.01407250852536994\n",
      "66     \t [-0.91082315 -1.26232069]. \t  -441.26396375882774 \t -0.01407250852536994\n",
      "67     \t [0.0821836  0.58251462]. \t  -33.99239946621883 \t -0.01407250852536994\n",
      "68     \t [1.15112364 0.41629861]. \t  -82.61222519369333 \t -0.01407250852536994\n",
      "69     \t [0.59012764 0.32547946]. \t  -0.2198479842540385 \t -0.01407250852536994\n",
      "70     \t [0.5989424 0.3403831]. \t  -0.19451541762401342 \t -0.01407250852536994\n",
      "71     \t [1.13604239 1.29132354]. \t  -0.01856100064466296 \t -0.01407250852536994\n",
      "72     \t [2.02236401 0.80552465]. \t  -1079.7942702821092 \t -0.01407250852536994\n",
      "73     \t [-0.62886151 -1.17512401]. \t  -249.32873851716164 \t -0.01407250852536994\n",
      "74     \t [1.52137197 0.73026457]. \t  -251.27504800338738 \t -0.01407250852536994\n",
      "75     \t [0.38581887 1.73789702]. \t  -252.88229059851358 \t -0.01407250852536994\n",
      "76     \t [-1.96944417 -0.5409523 ]. \t  -1962.1593894476853 \t -0.01407250852536994\n",
      "77     \t [ 1.154773   -0.95401997]. \t  -523.2990278949036 \t -0.01407250852536994\n",
      "78     \t [-0.51051861  1.68444812]. \t  -205.0076833315466 \t -0.01407250852536994\n",
      "79     \t [-1.06095289 -0.39755859]. \t  -236.25514667039812 \t -0.01407250852536994\n",
      "80     \t [ 0.36031694 -0.00897675]. \t  -2.335878478975027 \t -0.01407250852536994\n",
      "81     \t [0.50321713 0.25309516]. \t  -0.24679496978740006 \t -0.01407250852536994\n",
      "82     \t [-0.82949027  1.47402342]. \t  -65.12181013587157 \t -0.01407250852536994\n",
      "83     \t [ 0.10348957 -1.23911708]. \t  -157.01052777221707 \t -0.01407250852536994\n",
      "84     \t [1.78402586 0.47746595]. \t  -732.4699348470266 \t -0.01407250852536994\n",
      "85     \t [-1.05457877  1.81289119]. \t  -53.32702467490017 \t -0.01407250852536994\n",
      "86     \t [-0.85440648 -1.43697053]. \t  -473.019468218265 \t -0.01407250852536994\n",
      "87     \t [-1.23659192 -0.58198804]. \t  -450.69676966562554 \t -0.01407250852536994\n",
      "88     \t [-1.42980548  1.22539506]. \t  -72.97164514750482 \t -0.01407250852536994\n",
      "89     \t [-1.88312686  1.82949245]. \t  -303.00949927111895 \t -0.01407250852536994\n",
      "90     \t [-1.4660533   1.66046065]. \t  -29.979010274014193 \t -0.01407250852536994\n",
      "91     \t [-0.0655655   0.64578957]. \t  -42.28646587945757 \t -0.01407250852536994\n",
      "92     \t [1.59988677 0.83088525]. \t  -299.2183568062418 \t -0.01407250852536994\n",
      "93     \t [-0.18625368  0.67477663]. \t  -42.37823194204345 \t -0.01407250852536994\n",
      "94     \t [1.19295637 0.88925704]. \t  -28.540856130884436 \t -0.01407250852536994\n",
      "95     \t [ 1.25493264 -0.61556837]. \t  -479.8608501814786 \t -0.01407250852536994\n",
      "96     \t [1.5768618  1.87960322]. \t  -37.1643059079143 \t -0.01407250852536994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [0.6027097  1.82522772]. \t  -213.89309756821692 \t -0.01407250852536994\n",
      "98     \t [-1.99286282 -1.79067893]. \t  -3329.230399486352 \t -0.01407250852536994\n",
      "99     \t [ 1.5240691  -0.14424668]. \t  -608.8999803784616 \t -0.01407250852536994\n",
      "100    \t [-0.71917525  1.86097463]. \t  -183.5250858961645 \t -0.01407250852536994\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\n",
    "\n",
    "np.random.seed(run_num_20)\n",
    "surrogate_loser_20 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_20 = GPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_20.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.45944904 -1.35549029]. \t  -245.71064611316496 \t -108.5713485785257\n",
      "init   \t [-0.26190226  1.10289909]. \t  -108.5713485785257 \t -108.5713485785257\n",
      "init   \t [-0.83834755 -1.43702853]. \t  -461.2775269355244 \t -108.5713485785257\n",
      "init   \t [-1.95592878 -0.32676048]. \t  -1732.9949421003257 \t -108.5713485785257\n",
      "init   \t [-1.07035795 -0.66496024]. \t  -332.12317010404104 \t -108.5713485785257\n",
      "1      \t [-0.04757389 -0.15629853]. \t  \u001b[92m-3.6115955377645625\u001b[0m \t -3.6115955377645625\n",
      "2      \t [2.048 2.048]. \t  -461.7603900415999 \t -3.6115955377645625\n",
      "3      \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -3.6115955377645625\n",
      "4      \t [-1.82647858  2.048     ]. \t  -173.88955994266465 \t -3.6115955377645625\n",
      "5      \t [0.32457066 2.048     ]. \t  -377.84661618509784 \t -3.6115955377645625\n",
      "6      \t [2.048      0.44714657]. \t  -1405.2171822883681 \t -3.6115955377645625\n",
      "7      \t [-0.81571824  2.048     ]. \t  -194.4561476167204 \t -3.6115955377645625\n",
      "8      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -3.6115955377645625\n",
      "9      \t [-0.25388743 -0.99587226]. \t  -114.00243471736708 \t -3.6115955377645625\n",
      "10     \t [-0.00783991 -2.048     ]. \t  -420.4713174231713 \t -3.6115955377645625\n",
      "11     \t [-1.13833341  0.75726035]. \t  -33.57528230320979 \t -3.6115955377645625\n",
      "12     \t [ 0.89746019 -0.29412603]. \t  -120.91391442609597 \t -3.6115955377645625\n",
      "13     \t [0.78590154 0.83702965]. \t  -4.858965881958569 \t -3.6115955377645625\n",
      "14     \t [-2.048      1.2478483]. \t  -877.4504217256525 \t -3.6115955377645625\n",
      "15     \t [-0.65264532  0.3298894 ]. \t  -3.653922009888557 \t -3.6115955377645625\n",
      "16     \t [0.45036859 0.32772482]. \t  \u001b[92m-1.8619197031435337\u001b[0m \t -1.8619197031435337\n",
      "17     \t [1.21403932 2.048     ]. \t  -33.00587234265651 \t -1.8619197031435337\n",
      "18     \t [-1.10355449  1.42974168]. \t  -8.915491376367836 \t -1.8619197031435337\n",
      "19     \t [1.02428094 1.52050683]. \t  -22.21817938214378 \t -1.8619197031435337\n",
      "20     \t [ 0.43612282 -0.55224314]. \t  -55.440601665047254 \t -1.8619197031435337\n",
      "21     \t [ 2.048      -0.66616107]. \t  -2363.51037594521 \t -1.8619197031435337\n",
      "22     \t [-0.85546616  0.9898224 ]. \t  -10.099156999181202 \t -1.8619197031435337\n",
      "23     \t [-1.36241202  2.048     ]. \t  -9.260999023061864 \t -1.8619197031435337\n",
      "24     \t [0.60055774 1.24145676]. \t  -77.73815743345634 \t -1.8619197031435337\n",
      "25     \t [-0.58714343 -0.25652723]. \t  -38.67094021549694 \t -1.8619197031435337\n",
      "26     \t [ 0.72523555 -2.048     ]. \t  -662.6059022693962 \t -1.8619197031435337\n",
      "27     \t [0.85709342 0.29429615]. \t  -19.407974687482124 \t -1.8619197031435337\n",
      "28     \t [1.48521148 1.45847105]. \t  -56.093431320299075 \t -1.8619197031435337\n",
      "29     \t [1.12523219 1.15067717]. \t  \u001b[92m-1.3490223157568586\u001b[0m \t -1.3490223157568586\n",
      "30     \t [-0.25385771  0.27325276]. \t  -5.932280064142519 \t -1.3490223157568586\n",
      "31     \t [1.4880405 2.048    ]. \t  -3.0025726240715245 \t -1.3490223157568586\n",
      "32     \t [1.34859454 1.7650937 ]. \t  \u001b[92m-0.40895932848280436\u001b[0m \t -0.40895932848280436\n",
      "33     \t [0.52188027 0.04048901]. \t  -5.604968284000457 \t -0.40895932848280436\n",
      "34     \t [-1.35911858  1.79572368]. \t  -5.830455593694279 \t -0.40895932848280436\n",
      "35     \t [-0.7837085 -2.048    ]. \t  -711.9119712450065 \t -0.40895932848280436\n",
      "36     \t [ 1.14838358 -1.26035183]. \t  -665.2166151732523 \t -0.40895932848280436\n",
      "37     \t [-1.20586185  1.79552054]. \t  -16.522433053596565 \t -0.40895932848280436\n",
      "38     \t [1.20210808 1.41934665]. \t  \u001b[92m-0.10698501504533742\u001b[0m \t -0.10698501504533742\n",
      "39     \t [0.76820011 0.54576892]. \t  -0.25053422085826027 \t -0.10698501504533742\n",
      "40     \t [0.1262288  0.05835603]. \t  -0.9434414508754212 \t -0.10698501504533742\n",
      "41     \t [-2.048      -1.14747217]. \t  -2862.747567526279 \t -0.10698501504533742\n",
      "42     \t [-0.82379853  0.65716559]. \t  -3.3723733893758054 \t -0.10698501504533742\n",
      "43     \t [0.64354656 0.37414114]. \t  -0.28714741369266317 \t -0.10698501504533742\n",
      "44     \t [-1.10549618  1.13658565]. \t  -5.164757586490186 \t -0.10698501504533742\n",
      "45     \t [1.40339078 2.005     ]. \t  -0.2887088152448173 \t -0.10698501504533742\n",
      "46     \t [0.95588283 0.91827493]. \t  \u001b[92m-0.004028370072109002\u001b[0m \t -0.004028370072109002\n",
      "47     \t [1.15486667 1.35885184]. \t  -0.08715956188767633 \t -0.004028370072109002\n",
      "48     \t [0.74011057 0.52625572]. \t  -0.11380165758929997 \t -0.004028370072109002\n",
      "49     \t [-0.04859507  0.01154645]. \t  -1.1079879941988073 \t -0.004028370072109002\n",
      "50     \t [1.21162589 1.47330434]. \t  -0.04755970077911809 \t -0.004028370072109002\n",
      "51     \t [1.13354151 1.28081704]. \t  -0.01951377519024375 \t -0.004028370072109002\n",
      "52     \t [1.23935603 1.57860413]. \t  -0.23877382129059024 \t -0.004028370072109002\n",
      "53     \t [1.22201424 1.500232  ]. \t  -0.05406955016556009 \t -0.004028370072109002\n",
      "54     \t [0.89724676 0.81246046]. \t  -0.016047116584036004 \t -0.004028370072109002\n",
      "55     \t [1.03114023 1.06459166]. \t  \u001b[92m-0.0011496731274758514\u001b[0m \t -0.0011496731274758514\n",
      "56     \t [0.9473155  0.87625408]. \t  -0.047518795394768804 \t -0.0011496731274758514\n",
      "57     \t [1.39359055 2.048     ]. \t  -1.276508182642282 \t -0.0011496731274758514\n",
      "58     \t [0.7223935  0.50310321]. \t  -0.11221845984956219 \t -0.0011496731274758514\n",
      "59     \t [-0.32038368 -1.67090134]. \t  -316.2903233780781 \t -0.0011496731274758514\n",
      "60     \t [1.07990267 1.19802661]. \t  -0.10774279391716568 \t -0.0011496731274758514\n",
      "61     \t [1.09771727 1.23254626]. \t  -0.0855208126694734 \t -0.0011496731274758514\n",
      "62     \t [0.79807423 0.61003261]. \t  -0.11308053520097211 \t -0.0011496731274758514\n",
      "63     \t [1.07211987 1.12488744]. \t  -0.06548911696712242 \t -0.0011496731274758514\n",
      "64     \t [1.10970209 1.24497905]. \t  -0.030368568332775833 \t -0.0011496731274758514\n",
      "65     \t [1.10384394 1.21902109]. \t  -0.010813775294479654 \t -0.0011496731274758514\n",
      "66     \t [0.89170083 0.77752425]. \t  -0.042726254319586036 \t -0.0011496731274758514\n",
      "67     \t [0.97700172 0.95994904]. \t  -0.003462956054298013 \t -0.0011496731274758514\n",
      "68     \t [0.58030087 0.31242672]. \t  -0.23530518580621423 \t -0.0011496731274758514\n",
      "69     \t [1.44172766 2.04799726]. \t  -0.2886454787092266 \t -0.0011496731274758514\n",
      "70     \t [0.65543578 0.41333235]. \t  -0.14517534019414116 \t -0.0011496731274758514\n",
      "71     \t [0.78555823 0.60711308]. \t  -0.05596258622388977 \t -0.0011496731274758514\n",
      "72     \t [1.37349546 1.91225966]. \t  -0.20590759363388506 \t -0.0011496731274758514\n",
      "73     \t [0.90314362 0.81451483]. \t  -0.009514230220218979 \t -0.0011496731274758514\n",
      "74     \t [1.16065533 1.33082789]. \t  -0.052355977020322866 \t -0.0011496731274758514\n",
      "75     \t [0.83142714 0.68847819]. \t  -0.02919683650964434 \t -0.0011496731274758514\n",
      "76     \t [1.36521289 1.89164795]. \t  -0.2108965991990589 \t -0.0011496731274758514\n",
      "77     \t [1.10267561 1.26709966]. \t  -0.27274943676613517 \t -0.0011496731274758514\n",
      "78     \t [1.1875375  1.41143772]. \t  -0.035312496020648745 \t -0.0011496731274758514\n",
      "79     \t [1.05800624 1.12656362]. \t  -0.008529194632076523 \t -0.0011496731274758514\n",
      "80     \t [0.84883771 0.73423048]. \t  -0.04163278338684258 \t -0.0011496731274758514\n",
      "81     \t [1.26939235 1.63611123]. \t  -0.1338497367910915 \t -0.0011496731274758514\n",
      "82     \t [0.75501018 0.5484404 ]. \t  -0.10667590799424821 \t -0.0011496731274758514\n",
      "83     \t [0.80777678 0.66365278]. \t  -0.049380795879166925 \t -0.0011496731274758514\n",
      "84     \t [0.70875884 0.48073008]. \t  -0.13151632450815054 \t -0.0011496731274758514\n",
      "85     \t [1.15102354 1.34784478]. \t  -0.0756601751615748 \t -0.0011496731274758514\n",
      "86     \t [1.38860147 1.93382876]. \t  -0.15416360473973248 \t -0.0011496731274758514\n",
      "87     \t [0.13928768 0.04768773]. \t  -0.8208392991406159 \t -0.0011496731274758514\n",
      "88     \t [0.991068   0.98601613]. \t  -0.0015240393724211241 \t -0.0011496731274758514\n",
      "89     \t [1.40708073 2.00199591]. \t  -0.21464291888221887 \t -0.0011496731274758514\n",
      "90     \t [0.9360637  0.91687952]. \t  -0.16944607039081133 \t -0.0011496731274758514\n",
      "91     \t [1.10372089 1.22968058]. \t  -0.02393883420333884 \t -0.0011496731274758514\n",
      "92     \t [0.79390733 0.59935331]. \t  -0.1381748995748267 \t -0.0011496731274758514\n",
      "93     \t [1.10094383 1.23765521]. \t  -0.07561252258556139 \t -0.0011496731274758514\n",
      "94     \t [1.21294954 1.45548125]. \t  -0.07020207968721875 \t -0.0011496731274758514\n",
      "95     \t [1.40633584 2.048     ]. \t  -0.658186541257736 \t -0.0011496731274758514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [0.75935908 0.55847335]. \t  -0.09086065873880891 \t -0.0011496731274758514\n",
      "97     \t [1.05498367 1.12829349]. \t  -0.026441198026249054 \t -0.0011496731274758514\n",
      "98     \t [0.57518423 0.30904843]. \t  -0.22794217333150774 \t -0.0011496731274758514\n",
      "99     \t [1.07761783 1.16056011]. \t  -0.00607353709599396 \t -0.0011496731274758514\n",
      "100    \t [0.32625449 0.08849534]. \t  -0.486141253071374 \t -0.0011496731274758514\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \n",
    "\n",
    "np.random.seed(run_num_1)\n",
    "surrogate_winner_1 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_1 = GPGO(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_1.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.44173258 -1.74529086]. \t  -1462.4037722675753 \t -1.3013277264983028\n",
      "init   \t [ 1.6176405  -0.26012243]. \t  -828.0271838688934 \t -1.3013277264983028\n",
      "init   \t [-1.525032    0.31071385]. \t  -412.40181251788414 \t -1.3013277264983028\n",
      "init   \t [ 1.39456889 -0.26574622]. \t  -488.8170397507174 \t -1.3013277264983028\n",
      "init   \t [0.80244966 0.75627765]. \t  -1.3013277264983028 \t -1.3013277264983028\n",
      "1      \t [ 0.2151418  -0.29115095]. \t  -12.002371376824911 \t -1.3013277264983028\n",
      "2      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -1.3013277264983028\n",
      "3      \t [-2.048  2.048]. \t  -469.9523900415999 \t -1.3013277264983028\n",
      "4      \t [2.048 2.048]. \t  -461.7603900415999 \t -1.3013277264983028\n",
      "5      \t [-0.17657938  2.048     ]. \t  -408.1405189772543 \t -1.3013277264983028\n",
      "6      \t [-0.50646008  0.90367384]. \t  -44.15258516858773 \t -1.3013277264983028\n",
      "7      \t [0.55715909 0.18166696]. \t  -1.854003571843027 \t -1.3013277264983028\n",
      "8      \t [ 0.03112259 -2.048     ]. \t  -420.7659621055259 \t -1.3013277264983028\n",
      "9      \t [0.95787902 2.048     ]. \t  -127.79751646573513 \t -1.3013277264983028\n",
      "10     \t [-2.048       1.03527138]. \t  -1007.2390125242589 \t -1.3013277264983028\n",
      "11     \t [-0.72295932 -0.13374834]. \t  -46.057114397236845 \t -1.3013277264983028\n",
      "12     \t [1.66877271 1.11020912]. \t  -280.8735070771222 \t -1.3013277264983028\n",
      "13     \t [-1.16911006  2.048     ]. \t  -51.10588452508277 \t -1.3013277264983028\n",
      "14     \t [-0.25819424 -1.11129564]. \t  -140.3420076717116 \t -1.3013277264983028\n",
      "15     \t [ 0.41454883 -1.04795628]. \t  -149.1356670992928 \t -1.3013277264983028\n",
      "16     \t [-1.00721398  1.32041176]. \t  -13.388331985727348 \t -1.3013277264983028\n",
      "17     \t [-0.88466625  0.59472874]. \t  -7.082819866677806 \t -1.3013277264983028\n",
      "18     \t [1.11301394 1.4670464 ]. \t  -5.222412529424875 \t -1.3013277264983028\n",
      "19     \t [ 0.69062362 -0.18162583]. \t  -43.46937273488329 \t -1.3013277264983028\n",
      "20     \t [-0.39428589  0.14395809]. \t  -1.9572656914409672 \t -1.3013277264983028\n",
      "21     \t [1.01841649 0.4509822 ]. \t  -34.36220418624841 \t -1.3013277264983028\n",
      "22     \t [-2.048      -0.58840889]. \t  -2296.724564625251 \t -1.3013277264983028\n",
      "23     \t [-0.18581695 -0.47920934]. \t  -27.79876105478509 \t -1.3013277264983028\n",
      "24     \t [1.42242223 2.048     ]. \t  \u001b[92m-0.23952372410407272\u001b[0m \t -0.23952372410407272\n",
      "25     \t [-0.84603675  1.64200051]. \t  -89.19663103705712 \t -0.23952372410407272\n",
      "26     \t [-0.66769613 -2.048     ]. \t  -624.6940925319569 \t -0.23952372410407272\n",
      "27     \t [1.35579772 1.70179905]. \t  -1.9867717985699886 \t -0.23952372410407272\n",
      "28     \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -0.23952372410407272\n",
      "29     \t [1.16394839 1.09670903]. \t  -6.686727693382198 \t -0.23952372410407272\n",
      "30     \t [ 0.77990457 -2.048     ]. \t  -705.6154520976722 \t -0.23952372410407272\n",
      "31     \t [-1.4013068   1.75250227]. \t  -10.22506451376745 \t -0.23952372410407272\n",
      "32     \t [-1.47328778  2.048     ]. \t  -7.619661337708507 \t -0.23952372410407272\n",
      "33     \t [-0.94976754 -1.07838442]. \t  -396.0169629570128 \t -0.23952372410407272\n",
      "34     \t [0.08557412 0.09003683]. \t  -1.5203336199031847 \t -0.23952372410407272\n",
      "35     \t [0.17266036 1.34018246]. \t  -172.39167110438052 \t -0.23952372410407272\n",
      "36     \t [ 1.03491198 -1.00518154]. \t  -431.07197262038784 \t -0.23952372410407272\n",
      "37     \t [-1.29071081  1.80985457]. \t  -7.318657665839204 \t -0.23952372410407272\n",
      "38     \t [1.34637593 2.048     ]. \t  -5.655260437048025 \t -0.23952372410407272\n",
      "39     \t [1.1937363  1.39460255]. \t  \u001b[92m-0.12997293353449196\u001b[0m \t -0.12997293353449196\n",
      "40     \t [ 2.048      -1.11700873]. \t  -2822.102598577501 \t -0.12997293353449196\n",
      "41     \t [-0.11340574 -0.09855051]. \t  -2.4809216945380808 \t -0.12997293353449196\n",
      "42     \t [1.00847304 1.07278215]. \t  -0.3110373581691921 \t -0.12997293353449196\n",
      "43     \t [-1.22236838  1.08517182]. \t  -21.66805568930669 \t -0.12997293353449196\n",
      "44     \t [1.31027037 1.82630609]. \t  -1.2952413935247338 \t -0.12997293353449196\n",
      "45     \t [0.68159132 0.4656017 ]. \t  \u001b[92m-0.1014912003424842\u001b[0m \t -0.1014912003424842\n",
      "46     \t [0.91645013 0.83940501]. \t  \u001b[92m-0.007003222380125737\u001b[0m \t -0.007003222380125737\n",
      "47     \t [-2.048      -1.36423918]. \t  -3099.0305321383767 \t -0.007003222380125737\n",
      "48     \t [1.22418031 1.52537846]. \t  -0.1218720755355241 \t -0.007003222380125737\n",
      "49     \t [0.68573015 0.47425933]. \t  -0.10039244286075107 \t -0.007003222380125737\n",
      "50     \t [-1.23052691  1.49106343]. \t  -5.028764140024073 \t -0.007003222380125737\n",
      "51     \t [1.05914411 1.12345495]. \t  \u001b[92m-0.0037764840180860245\u001b[0m \t -0.0037764840180860245\n",
      "52     \t [0.81404476 0.60729661]. \t  -0.34118804682314946 \t -0.0037764840180860245\n",
      "53     \t [1.13813843 1.25707698]. \t  -0.16563413330187376 \t -0.0037764840180860245\n",
      "54     \t [1.23782391 1.55286389]. \t  -0.09922666424942889 \t -0.0037764840180860245\n",
      "55     \t [1.0984034  1.23754025]. \t  -0.106094846697602 \t -0.0037764840180860245\n",
      "56     \t [1.32738228 1.77681439]. \t  -0.1292928273932377 \t -0.0037764840180860245\n",
      "57     \t [0.72451591 0.50584297]. \t  -0.11229740544369254 \t -0.0037764840180860245\n",
      "58     \t [-1.33966702 -2.048     ]. \t  -1482.114313660182 \t -0.0037764840180860245\n",
      "59     \t [0.891525   0.78113088]. \t  -0.030497308967723978 \t -0.0037764840180860245\n",
      "60     \t [0.84919612 0.71749556]. \t  -0.02406567118521888 \t -0.0037764840180860245\n",
      "61     \t [ 0.42581754 -1.65234839]. \t  -336.56387532176325 \t -0.0037764840180860245\n",
      "62     \t [1.3687509 1.8655455]. \t  -0.1422712952295526 \t -0.0037764840180860245\n",
      "63     \t [1.23741048 1.54503163]. \t  -0.0755375100251203 \t -0.0037764840180860245\n",
      "64     \t [1.09539226 1.18025532]. \t  -0.04762899047253522 \t -0.0037764840180860245\n",
      "65     \t [1.14397735 1.26897092]. \t  -0.1784438320050241 \t -0.0037764840180860245\n",
      "66     \t [0.68156754 0.41441418]. \t  -0.35260194913433773 \t -0.0037764840180860245\n",
      "67     \t [1.19603519 1.43185707]. \t  -0.038613911746643415 \t -0.0037764840180860245\n",
      "68     \t [0.86250781 0.71728328]. \t  -0.08985411708174797 \t -0.0037764840180860245\n",
      "69     \t [0.67563608 0.46197262]. \t  -0.10822431696090255 \t -0.0037764840180860245\n",
      "70     \t [-1.37006842 -0.57948418]. \t  -609.0916497206207 \t -0.0037764840180860245\n",
      "71     \t [1.15341432 1.34768241]. \t  -0.053526658123331905 \t -0.0037764840180860245\n",
      "72     \t [0.73606167 0.51616739]. \t  -0.13529878409359009 \t -0.0037764840180860245\n",
      "73     \t [1.07478583 1.17239997]. \t  -0.03529879678952775 \t -0.0037764840180860245\n",
      "74     \t [1.10940854 1.24136266]. \t  -0.02315400929824435 \t -0.0037764840180860245\n",
      "75     \t [0.84361018 0.69447156]. \t  -0.05406439001359476 \t -0.0037764840180860245\n",
      "76     \t [0.92195879 0.83453693]. \t  -0.03002587374095603 \t -0.0037764840180860245\n",
      "77     \t [1.08728867 1.18787152]. \t  -0.010839729458438878 \t -0.0037764840180860245\n",
      "78     \t [1.0134958  1.02342237]. \t  \u001b[92m-0.0015894153949581036\u001b[0m \t -0.0015894153949581036\n",
      "79     \t [1.11309216 1.27126684]. \t  -0.11707165828917988 \t -0.0015894153949581036\n",
      "80     \t [0.67748012 0.42199761]. \t  -0.24078371973675278 \t -0.0015894153949581036\n",
      "81     \t [1.13634128 1.2971012 ]. \t  -0.021987466241895886 \t -0.0015894153949581036\n",
      "82     \t [1.00064555 1.06371229]. \t  -0.3896356004505905 \t -0.0015894153949581036\n",
      "83     \t [1.21169502 1.46440311]. \t  -0.04626008747749812 \t -0.0015894153949581036\n",
      "84     \t [0.82756875 0.61990479]. \t  -0.45178086747973345 \t -0.0015894153949581036\n",
      "85     \t [0.88016194 0.77063995]. \t  -0.015997436481588453 \t -0.0015894153949581036\n",
      "86     \t [1.36672641 1.85115795]. \t  -0.16265560543102367 \t -0.0015894153949581036\n",
      "87     \t [1.16671909 1.37850542]. \t  -0.0576273595735223 \t -0.0015894153949581036\n",
      "88     \t [1.36713168 1.88055194]. \t  -0.14801737150532937 \t -0.0015894153949581036\n",
      "89     \t [0.77909245 0.59568852]. \t  -0.061561308449043894 \t -0.0015894153949581036\n",
      "90     \t [1.33124371 1.75909777]. \t  -0.12691500309019937 \t -0.0015894153949581036\n",
      "91     \t [0.83058384 0.70775573]. \t  -0.06069346055556625 \t -0.0015894153949581036\n",
      "92     \t [1.0856513  1.17863197]. \t  -0.007336148943403674 \t -0.0015894153949581036\n",
      "93     \t [1.12185188 1.26102667]. \t  -0.01546046251913213 \t -0.0015894153949581036\n",
      "94     \t [1.02063221 1.02822413]. \t  -0.018558942182288884 \t -0.0015894153949581036\n",
      "95     \t [1.29073321 1.66657341]. \t  -0.08455957831018188 \t -0.0015894153949581036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [0.76351854 0.55564279]. \t  -0.13054956663758327 \t -0.0015894153949581036\n",
      "97     \t [1.06227973 1.12378761]. \t  -0.006041592904805986 \t -0.0015894153949581036\n",
      "98     \t [1.20526067 1.44762972]. \t  -0.044655567148992374 \t -0.0015894153949581036\n",
      "99     \t [1.11385218 1.26951941]. \t  -0.09621034091318478 \t -0.0015894153949581036\n",
      "100    \t [1.16633081 1.35051573]. \t  -0.0372931459388302 \t -0.0015894153949581036\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \n",
    "\n",
    "np.random.seed(run_num_2)\n",
    "surrogate_winner_2 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_2 = GPGO(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_2.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.03045369 -1.60050678]. \t  -708.8071948417274 \t -1.118465165857483\n",
      "init   \t [-0.05684724 -0.0006915 ]. \t  -1.118465165857483 \t -1.118465165857483\n",
      "init   \t [ 0.64352256 -1.08181622]. \t  -223.90997740226942 \t -1.118465165857483\n",
      "init   \t [ 0.46200787 -1.55790376]. \t  -314.05929962166397 \t -1.118465165857483\n",
      "init   \t [ 0.86918061 -0.52199202]. \t  -163.20929387450548 \t -1.118465165857483\n",
      "1      \t [-0.80188962 -1.1499399 ]. \t  -324.7198272125228 \t -1.118465165857483\n",
      "2      \t [-2.048  2.048]. \t  -469.9523900415999 \t -1.118465165857483\n",
      "3      \t [2.048 2.048]. \t  -461.7603900415999 \t -1.118465165857483\n",
      "4      \t [-0.00371746  2.048     ]. \t  -420.4321882836924 \t -1.118465165857483\n",
      "5      \t [-2.048       0.19837058]. \t  -1606.0386967364334 \t -1.118465165857483\n",
      "6      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -1.118465165857483\n",
      "7      \t [2.048      0.42327172]. \t  -1423.1667485129908 \t -1.118465165857483\n",
      "8      \t [-0.15195757 -0.75688452]. \t  -62.16320317709471 \t -1.118465165857483\n",
      "9      \t [0.46991783 0.92490623]. \t  -49.854339304580556 \t -1.118465165857483\n",
      "10     \t [-0.60334406  1.10202773]. \t  -57.035653979171684 \t -1.118465165857483\n",
      "11     \t [-1.04501042  2.048     ]. \t  -95.56672299123782 \t -1.118465165857483\n",
      "12     \t [-0.34713703 -2.048     ]. \t  -472.05578983656295 \t -1.118465165857483\n",
      "13     \t [0.06300525 0.63642891]. \t  -40.878429570258696 \t -1.118465165857483\n",
      "14     \t [1.12453642 2.048     ]. \t  -61.38985931720812 \t -1.118465165857483\n",
      "15     \t [-0.78712041 -0.0260104 ]. \t  -44.86972423558241 \t -1.118465165857483\n",
      "16     \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -1.118465165857483\n",
      "17     \t [-1.43088621  1.36897892]. \t  -51.93951822677084 \t -1.118465165857483\n",
      "18     \t [-0.56021211 -0.45044531]. \t  -60.847098316925376 \t -1.118465165857483\n",
      "19     \t [-0.93037188  0.74652401]. \t  -5.1440500062119145 \t -1.118465165857483\n",
      "20     \t [1.2909542  1.46144559]. \t  -4.291958895081695 \t -1.118465165857483\n",
      "21     \t [0.90419528 1.57747063]. \t  -57.754212570877726 \t -1.118465165857483\n",
      "22     \t [0.76750168 0.43828787]. \t  -2.327243394972086 \t -1.118465165857483\n",
      "23     \t [-1.12184435  1.53349587]. \t  -12.06258578636136 \t -1.118465165857483\n",
      "24     \t [-0.52731017  0.33045043]. \t  -2.607193772860254 \t -1.118465165857483\n",
      "25     \t [0.45444268 0.07393316]. \t  -2.0555107133815973 \t -1.118465165857483\n",
      "26     \t [ 2.048      -0.73791508]. \t  -2433.7768089397186 \t -1.118465165857483\n",
      "27     \t [1.04695552 1.05878065]. \t  \u001b[92m-0.14159660480109967\u001b[0m \t -0.14159660480109967\n",
      "28     \t [-2.048      -0.84879049]. \t  -2552.570504972789 \t -0.14159660480109967\n",
      "29     \t [-1.47678379  2.048     ]. \t  -7.900442932341298 \t -0.14159660480109967\n",
      "30     \t [2.048      1.38105926]. \t  -792.5329010566697 \t -0.14159660480109967\n",
      "31     \t [ 0.61928686 -2.048     ]. \t  -591.3720511944849 \t -0.14159660480109967\n",
      "32     \t [1.44834512 2.048     ]. \t  -0.44805809788695466 \t -0.14159660480109967\n",
      "33     \t [1.31607965 1.80363637]. \t  -0.6121433671971704 \t -0.14159660480109967\n",
      "34     \t [-1.37137033  1.80489948]. \t  -6.197310867214726 \t -0.14159660480109967\n",
      "35     \t [-1.0179407 -2.048    ]. \t  -955.303070808243 \t -0.14159660480109967\n",
      "36     \t [1.13992265 1.3884176 ]. \t  -0.8115705270387346 \t -0.14159660480109967\n",
      "37     \t [-1.12948269  1.25555097]. \t  -4.575420531816724 \t -0.14159660480109967\n",
      "38     \t [0.99407494 0.32646851]. \t  -43.78690391726423 \t -0.14159660480109967\n",
      "39     \t [-0.40521604  0.04470707]. \t  -3.4024890289203773 \t -0.14159660480109967\n",
      "40     \t [ 0.18070737 -0.29316791]. \t  -11.287306991802936 \t -0.14159660480109967\n",
      "41     \t [-2.048       1.23296639]. \t  -886.24235092793 \t -0.14159660480109967\n",
      "42     \t [1.38387109 2.04531289]. \t  -1.8429178980113183 \t -0.14159660480109967\n",
      "43     \t [-0.44717603 -1.54970166]. \t  -308.2281514003932 \t -0.14159660480109967\n",
      "44     \t [-0.71173384  0.39593068]. \t  -4.154029210581269 \t -0.14159660480109967\n",
      "45     \t [-1.34371516  1.98741073]. \t  -8.799589740731644 \t -0.14159660480109967\n",
      "46     \t [1.17666095 1.39285421]. \t  \u001b[92m-0.0381366842428807\u001b[0m \t -0.0381366842428807\n",
      "47     \t [1.10247606 1.24642007]. \t  -0.10639439104255988 \t -0.0381366842428807\n",
      "48     \t [1.06863193 1.03726977]. \t  -1.101012239242952 \t -0.0381366842428807\n",
      "49     \t [0.98907181 0.98802388]. \t  \u001b[92m-0.009646825139328978\u001b[0m \t -0.009646825139328978\n",
      "50     \t [1.24717987 1.5614018 ]. \t  -0.0646312114367664 \t -0.009646825139328978\n",
      "51     \t [1.13545239 1.28679684]. \t  -0.01895019077623132 \t -0.009646825139328978\n",
      "52     \t [0.83877186 0.70104018]. \t  -0.026618537029335657 \t -0.009646825139328978\n",
      "53     \t [1.35539097 1.78980966]. \t  -0.34979553669619284 \t -0.009646825139328978\n",
      "54     \t [1.03975998 1.09349756]. \t  -0.016948762050168015 \t -0.009646825139328978\n",
      "55     \t [1.15713069 1.35392896]. \t  -0.04712266845641574 \t -0.009646825139328978\n",
      "56     \t [-0.15556084  0.03496553]. \t  -1.3469122870005632 \t -0.009646825139328978\n",
      "57     \t [1.31279575 1.73951445]. \t  -0.12370349738971334 \t -0.009646825139328978\n",
      "58     \t [0.94045448 0.86493583]. \t  -0.041643986977396234 \t -0.009646825139328978\n",
      "59     \t [1.29368566 1.67426376]. \t  -0.08629237820770021 \t -0.009646825139328978\n",
      "60     \t [1.08274793 1.17937248]. \t  -0.011788459029337608 \t -0.009646825139328978\n",
      "61     \t [0.81456657 0.65549692]. \t  -0.04082045478636227 \t -0.009646825139328978\n",
      "62     \t [0.94078916 0.82915556]. \t  -0.3163076531244843 \t -0.009646825139328978\n",
      "63     \t [1.40517888 1.99655031]. \t  -0.21266952052386218 \t -0.009646825139328978\n",
      "64     \t [0.98390977 0.95875747]. \t  \u001b[92m-0.008946943218295374\u001b[0m \t -0.008946943218295374\n",
      "65     \t [1.33899938 1.80187338]. \t  -0.122938063112419 \t -0.008946943218295374\n",
      "66     \t [0.97354241 0.9652618 ]. \t  -0.03124448882975726 \t -0.008946943218295374\n",
      "67     \t [0.95204876 0.89489822]. \t  -0.01552112839824647 \t -0.008946943218295374\n",
      "68     \t [1.24796507 1.58522591]. \t  -0.13882125776896576 \t -0.008946943218295374\n",
      "69     \t [1.08242906 1.20393354]. \t  -0.11100004184662911 \t -0.008946943218295374\n",
      "70     \t [0.94141874 0.87832711]. \t  -0.009739505270454925 \t -0.008946943218295374\n",
      "71     \t [1.26308978 1.63452918]. \t  -0.22235843424364615 \t -0.008946943218295374\n",
      "72     \t [1.42038958 2.04799999]. \t  -0.2697124080318372 \t -0.008946943218295374\n",
      "73     \t [1.24945597 1.5668673 ]. \t  -0.06550821858538328 \t -0.008946943218295374\n",
      "74     \t [1.26203692 1.60315527]. \t  -0.07951700365806255 \t -0.008946943218295374\n",
      "75     \t [1.07504496 1.15794469]. \t  \u001b[92m-0.0061259320425086465\u001b[0m \t -0.0061259320425086465\n",
      "76     \t [1.06176258 1.1392823 ]. \t  -0.018077009257239162 \t -0.0061259320425086465\n",
      "77     \t [1.25748842 1.57596292]. \t  -0.06912434840425773 \t -0.0061259320425086465\n",
      "78     \t [0.87185127 0.7075062 ]. \t  -0.29329205388898416 \t -0.0061259320425086465\n",
      "79     \t [0.95501875 0.9170929 ]. \t  \u001b[92m-0.004555514079651278\u001b[0m \t -0.004555514079651278\n",
      "80     \t [0.66313556 0.42713634]. \t  -0.12938500645619588 \t -0.004555514079651278\n",
      "81     \t [0.31165214 0.11972383]. \t  -0.5248842212308629 \t -0.004555514079651278\n",
      "82     \t [1.1652292  1.39297679]. \t  -0.1513294159882382 \t -0.004555514079651278\n",
      "83     \t [0.82427858 0.62406242]. \t  -0.3374922322462087 \t -0.004555514079651278\n",
      "84     \t [0.44870001 0.1659131 ]. \t  -0.4293793773374193 \t -0.004555514079651278\n",
      "85     \t [0.97561834 0.92999332]. \t  -0.048283517582985325 \t -0.004555514079651278\n",
      "86     \t [0.99104841 0.9526858 ]. \t  -0.08705296581865195 \t -0.004555514079651278\n",
      "87     \t [1.03626496 1.03703837]. \t  -0.13678850587166796 \t -0.004555514079651278\n",
      "88     \t [1.05213906 1.13542138]. \t  -0.08351529095177387 \t -0.004555514079651278\n",
      "89     \t [1.10264418 1.21430003]. \t  -0.010768132113345692 \t -0.004555514079651278\n",
      "90     \t [1.21072934 1.46925306]. \t  -0.045554387448722373 \t -0.004555514079651278\n",
      "91     \t [1.04696   1.0439746]. \t  -0.27417428579301417 \t -0.004555514079651278\n",
      "92     \t [1.26727291 1.59993489]. \t  -0.07508990586177451 \t -0.004555514079651278\n",
      "93     \t [1.31796422 1.75613011]. \t  -0.13758388601028798 \t -0.004555514079651278\n",
      "94     \t [1.15364469 1.31159825]. \t  -0.060847254978418336 \t -0.004555514079651278\n",
      "95     \t [1.39565703 1.97600926]. \t  -0.23579072324668007 \t -0.004555514079651278\n",
      "96     \t [0.11891358 0.0432573 ]. \t  -0.8610924310726299 \t -0.004555514079651278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [0.83275675 0.66478787]. \t  -0.11031595826467755 \t -0.004555514079651278\n",
      "98     \t [1.05619544 1.09365023]. \t  -0.051112667850816085 \t -0.004555514079651278\n",
      "99     \t [1.25614747 1.58245068]. \t  -0.0676765161905657 \t -0.004555514079651278\n",
      "100    \t [1.19770054 1.38719553]. \t  -0.2627299474158723 \t -0.004555514079651278\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \n",
    "\n",
    "np.random.seed(run_num_3)\n",
    "surrogate_winner_3 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_3 = GPGO(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_3.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [1.39002285 1.31602418]. \t  -38.11488737154776 \t -12.122423820878506\n",
      "init   \t [0.58248055 0.68494384]. \t  -12.122423820878506 \t -12.122423820878506\n",
      "init   \t [-1.88768295 -1.7790059 ]. \t  -2862.4120613667637 \t -12.122423820878506\n",
      "init   \t [-0.91545338 -0.75218738]. \t  -256.5560079635026 \t -12.122423820878506\n",
      "init   \t [-1.25504548 -1.24070756]. \t  -797.98450090518 \t -12.122423820878506\n",
      "1      \t [ 0.30487229 -2.02003903]. \t  -446.9542441674814 \t -12.122423820878506\n",
      "2      \t [-2.048  2.048]. \t  -469.9523900415999 \t -12.122423820878506\n",
      "3      \t [ 2.048      -0.44896694]. \t  -2157.094810222332 \t -12.122423820878506\n",
      "4      \t [0.15979549 2.048     ]. \t  -409.7425737423387 \t -12.122423820878506\n",
      "5      \t [2.048 2.048]. \t  -461.7603900415999 \t -12.122423820878506\n",
      "6      \t [-2.048       0.42457869]. \t  -1430.3731915240685 \t -12.122423820878506\n",
      "7      \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -12.122423820878506\n",
      "8      \t [-0.32833209 -1.37897878]. \t  -222.816162951114 \t -12.122423820878506\n",
      "9      \t [-0.60565179  1.13763677]. \t  -61.99487730325764 \t -12.122423820878506\n",
      "10     \t [-0.97201383  2.048     ]. \t  -125.59145907837153 \t -12.122423820878506\n",
      "11     \t [2.048      0.94746742]. \t  -1055.2930813993391 \t -12.122423820878506\n",
      "12     \t [0.64674476 1.27110891]. \t  -72.8567120579421 \t -12.122423820878506\n",
      "13     \t [1.20946706 2.048     ]. \t  -34.288544072312526 \t -12.122423820878506\n",
      "14     \t [-0.42413195 -2.048     ]. \t  -498.37660810249025 \t -12.122423820878506\n",
      "15     \t [ 0.47003044 -0.43609659]. \t  -43.44907954633037 \t -12.122423820878506\n",
      "16     \t [-0.07561133  0.1663297 ]. \t  \u001b[92m-3.736581428664339\u001b[0m \t -3.736581428664339\n",
      "17     \t [-1.30732023  1.4377917 ]. \t  -12.68379569944149 \t -3.736581428664339\n",
      "18     \t [0.334979   0.00604323]. \t  \u001b[92m-1.5694109766288729\u001b[0m \t -1.5694109766288729\n",
      "19     \t [-0.89806215  0.58340094]. \t  -8.580656383202898 \t -1.5694109766288729\n",
      "20     \t [1.07140975 1.30911217]. \t  -2.60342828194462 \t -1.5694109766288729\n",
      "21     \t [1.34675012 1.68209325]. \t  -1.8532142019046884 \t -1.5694109766288729\n",
      "22     \t [-1.43444599  2.048     ]. \t  -5.935811190152463 \t -1.5694109766288729\n",
      "23     \t [-0.42616708  0.59929672]. \t  -19.479472165249238 \t -1.5694109766288729\n",
      "24     \t [ 0.08747448 -0.60446501]. \t  -38.30139974288017 \t -1.5694109766288729\n",
      "25     \t [-2.048      -0.62787328]. \t  -2334.6296800628393 \t -1.5694109766288729\n",
      "26     \t [-1.03796711  1.02624102]. \t  -4.41478572957835 \t -1.5694109766288729\n",
      "27     \t [-1.21201606  1.69785646]. \t  -10.131324500973648 \t -1.5694109766288729\n",
      "28     \t [1.109843  0.3226621]. \t  -82.65641631121264 \t -1.5694109766288729\n",
      "29     \t [-0.45325221 -0.01328757]. \t  -6.896010586231462 \t -1.5694109766288729\n",
      "30     \t [0.98004623 0.79717071]. \t  -2.667737183610228 \t -1.5694109766288729\n",
      "31     \t [ 0.42434686 -1.29052322]. \t  -216.5958952337476 \t -1.5694109766288729\n",
      "32     \t [1.47742145 2.048     ]. \t  -2.0443385326620307 \t -1.5694109766288729\n",
      "33     \t [ 1.00414671 -2.048     ]. \t  -934.1034809676511 \t -1.5694109766288729\n",
      "34     \t [ 1.24628698 -1.10392694]. \t  -706.1096127647595 \t -1.5694109766288729\n",
      "35     \t [0.69981697 0.33889491]. \t  -2.3656483895789857 \t -1.5694109766288729\n",
      "36     \t [-0.14278606 -0.1945638 ]. \t  -5.926381754211827 \t -1.5694109766288729\n",
      "37     \t [-1.13260145 -2.048     ]. \t  -1113.9615618999226 \t -1.5694109766288729\n",
      "38     \t [-2.048       1.35629732]. \t  -814.7184932873878 \t -1.5694109766288729\n",
      "39     \t [1.20536914 1.41763345]. \t  \u001b[92m-0.16665366121208108\u001b[0m \t -0.16665366121208108\n",
      "40     \t [-0.66342967  0.38915345]. \t  -3.026950078887559 \t -0.16665366121208108\n",
      "41     \t [1.11943633 1.21856852]. \t  \u001b[92m-0.13376790903895636\u001b[0m \t -0.13376790903895636\n",
      "42     \t [-1.21318632 -0.1004741 ]. \t  -252.10939727050922 \t -0.13376790903895636\n",
      "43     \t [-1.09452836  1.2617177 ]. \t  -4.793141269162467 \t -0.13376790903895636\n",
      "44     \t [ 0.09031629 -0.03862135]. \t  -1.0463461681182216 \t -0.13376790903895636\n",
      "45     \t [1.09751034 1.18519488]. \t  \u001b[92m-0.04688884618601827\u001b[0m \t -0.04688884618601827\n",
      "46     \t [1.36590781 2.048     ]. \t  -3.4570660002424765 \t -0.04688884618601827\n",
      "47     \t [1.29467312 1.70550776]. \t  -0.17285288460415393 \t -0.04688884618601827\n",
      "48     \t [1.23615789 1.53469099]. \t  -0.06013271245065668 \t -0.04688884618601827\n",
      "49     \t [-1.38545445  1.90308501]. \t  -5.7172857046706405 \t -0.04688884618601827\n",
      "50     \t [1.17116949 1.33190659]. \t  -0.18715732936167612 \t -0.04688884618601827\n",
      "51     \t [1.35646804 1.81429058]. \t  -0.19319540339405072 \t -0.04688884618601827\n",
      "52     \t [1.39805395 1.96600869]. \t  -0.17156597305316906 \t -0.04688884618601827\n",
      "53     \t [1.22922982 1.45040373]. \t  -0.41980913392289204 \t -0.04688884618601827\n",
      "54     \t [0.44061087 0.23170726]. \t  -0.45406160651714594 \t -0.04688884618601827\n",
      "55     \t [1.23627993 1.52155077]. \t  -0.06050304782457475 \t -0.04688884618601827\n",
      "56     \t [ 2.048      -1.29318325]. \t  -3012.3499324892255 \t -0.04688884618601827\n",
      "57     \t [0.42380396 0.20775586]. \t  -0.4112219920271912 \t -0.04688884618601827\n",
      "58     \t [0.13006512 0.02744098]. \t  -0.7678622453333579 \t -0.04688884618601827\n",
      "59     \t [-1.3529882   2.03934704]. \t  -9.895043481190744 \t -0.04688884618601827\n",
      "60     \t [0.77388466 0.53384203]. \t  -0.4743490569625878 \t -0.04688884618601827\n",
      "61     \t [1.41071845 1.93700444]. \t  -0.45088553528480535 \t -0.04688884618601827\n",
      "62     \t [1.43975002 2.048     ]. \t  -0.25528215952051764 \t -0.04688884618601827\n",
      "63     \t [1.38768251 1.91334334]. \t  -0.16547448974649323 \t -0.04688884618601827\n",
      "64     \t [1.23119729 1.49215198]. \t  -0.1095964747883953 \t -0.04688884618601827\n",
      "65     \t [1.30394075 1.67906273]. \t  -0.13731866903832252 \t -0.04688884618601827\n",
      "66     \t [1.17514464 1.35898427]. \t  -0.07899052446356664 \t -0.04688884618601827\n",
      "67     \t [0.74490525 0.52299491]. \t  -0.16676364639466046 \t -0.04688884618601827\n",
      "68     \t [1.32756161 1.79076841]. \t  -0.18766075974868524 \t -0.04688884618601827\n",
      "69     \t [ 0.11199079 -0.02181226]. \t  -0.9065814564271162 \t -0.04688884618601827\n",
      "70     \t [1.38792241 1.96442648]. \t  -0.29562861608116253 \t -0.04688884618601827\n",
      "71     \t [0.65006721 0.37997674]. \t  -0.30401961022231394 \t -0.04688884618601827\n",
      "72     \t [1.05762634 1.11449011]. \t  \u001b[92m-0.004988174771491823\u001b[0m \t -0.004988174771491823\n",
      "73     \t [ 0.0580282  -0.00343096]. \t  -0.8919324688417662 \t -0.004988174771491823\n",
      "74     \t [1.41374822 2.04799493]. \t  -0.41434410353328355 \t -0.004988174771491823\n",
      "75     \t [0.63810865 0.41600756]. \t  -0.13875325554177154 \t -0.004988174771491823\n",
      "76     \t [1.40057695 2.048     ]. \t  -0.9066850104561426 \t -0.004988174771491823\n",
      "77     \t [0.58867376 0.32625442]. \t  -0.21032674921822156 \t -0.004988174771491823\n",
      "78     \t [1.34334067 1.79211916]. \t  -0.13337063264987578 \t -0.004988174771491823\n",
      "79     \t [1.30784114 1.71241816]. \t  -0.09515414646784807 \t -0.004988174771491823\n",
      "80     \t [1.39975606 1.96744245]. \t  -0.16640715504073772 \t -0.004988174771491823\n",
      "81     \t [1.01342808 1.0222473 ]. \t  \u001b[92m-0.0024739250851452937\u001b[0m \t -0.0024739250851452937\n",
      "82     \t [1.08616415 1.14274249]. \t  -0.14439879966699534 \t -0.0024739250851452937\n",
      "83     \t [1.16837108 1.33289777]. \t  -0.13198915610648293 \t -0.0024739250851452937\n",
      "84     \t [0.60668051 0.34538201]. \t  -0.2061350160862993 \t -0.0024739250851452937\n",
      "85     \t [1.33673546 1.79567254]. \t  -0.12115388664907668 \t -0.0024739250851452937\n",
      "86     \t [1.39939298 1.968476  ]. \t  -0.16986839900458076 \t -0.0024739250851452937\n",
      "87     \t [0.73143445 0.52524363]. \t  -0.08163903256701913 \t -0.0024739250851452937\n",
      "88     \t [1.24694088 1.5051608 ]. \t  -0.3079962450702708 \t -0.0024739250851452937\n",
      "89     \t [1.16331679 1.34952871]. \t  -0.02809912648193984 \t -0.0024739250851452937\n",
      "90     \t [1.10472486 1.16728846]. \t  -0.2932316513902215 \t -0.0024739250851452937\n",
      "91     \t [0.50783077 0.26695183]. \t  -0.25043843926202697 \t -0.0024739250851452937\n",
      "92     \t [0.52237585 0.28434135]. \t  -0.24126903815073042 \t -0.0024739250851452937\n",
      "93     \t [0.64496694 0.39137609]. \t  -0.18659529456290147 \t -0.0024739250851452937\n",
      "94     \t [1.26173472 1.58715198]. \t  -0.07083074714259031 \t -0.0024739250851452937\n",
      "95     \t [0.55797217 0.26751831]. \t  -0.3873608463842475 \t -0.0024739250851452937\n",
      "96     \t [0.48271367 0.21828148]. \t  -0.2892854098988309 \t -0.0024739250851452937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [1.36926115 1.8832657 ]. \t  -0.1433923492887205 \t -0.0024739250851452937\n",
      "98     \t [0.61523429 0.33829531]. \t  -0.3097927779466676 \t -0.0024739250851452937\n",
      "99     \t [0.63678095 0.37855153]. \t  -0.20449602637921732 \t -0.0024739250851452937\n",
      "100    \t [1.24019056 1.50517118]. \t  -0.165942040598321 \t -0.0024739250851452937\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \n",
    "\n",
    "np.random.seed(run_num_4)\n",
    "surrogate_winner_4 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_4 = GPGO(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_4.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.23491901 0.17105153]. \t  -1.9278091788796494 \t -1.9278091788796494\n",
      "init   \t [ 0.24749582 -0.48313101]. \t  -30.201785940713883 \t -1.9278091788796494\n",
      "init   \t [-1.56019024  1.41721238]. \t  -109.97965348801837 \t -1.9278091788796494\n",
      "init   \t [0.98198164 1.77093658]. \t  -65.06852657321838 \t -1.9278091788796494\n",
      "init   \t [1.47180752 0.85239655]. \t  -172.83511538815276 \t -1.9278091788796494\n",
      "1      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -1.9278091788796494\n",
      "2      \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -1.9278091788796494\n",
      "3      \t [2.048 2.048]. \t  -461.7603900415999 \t -1.9278091788796494\n",
      "4      \t [-0.38969692  2.048     ]. \t  -361.46454857295794 \t -1.9278091788796494\n",
      "5      \t [-2.048      0.2857494]. \t  -1536.9702130275637 \t -1.9278091788796494\n",
      "6      \t [-2.048  2.048]. \t  -469.9523900415999 \t -1.9278091788796494\n",
      "7      \t [-0.01453783 -2.048     ]. \t  -420.54625976497556 \t -1.9278091788796494\n",
      "8      \t [-0.61875687  0.87941486]. \t  -27.27703988627891 \t -1.9278091788796494\n",
      "9      \t [ 2.048      -0.04806219]. \t  -1800.865396729416 \t -1.9278091788796494\n",
      "10     \t [-0.49511734 -0.59755253]. \t  -73.2486437406289 \t -1.9278091788796494\n",
      "11     \t [0.56329255 1.08150396]. \t  -58.591711426818065 \t -1.9278091788796494\n",
      "12     \t [-1.22645659  2.048     ]. \t  -34.52941393759355 \t -1.9278091788796494\n",
      "13     \t [-0.20894402 -0.14933761]. \t  -5.186260953584945 \t -1.9278091788796494\n",
      "14     \t [-0.12905692 -1.12909751]. \t  -132.54980986699516 \t -1.9278091788796494\n",
      "15     \t [-0.99997268  1.48278352]. \t  -27.31315967971894 \t -1.9278091788796494\n",
      "16     \t [2.048      1.27529484]. \t  -853.1597537181763 \t -1.9278091788796494\n",
      "17     \t [0.90271178 0.60312925]. \t  -4.493665646471144 \t -1.9278091788796494\n",
      "18     \t [1.16691341 1.26665229]. \t  \u001b[92m-0.9310180435270131\u001b[0m \t -0.9310180435270131\n",
      "19     \t [1.34224992 2.048     ]. \t  -6.186713853564422 \t -0.9310180435270131\n",
      "20     \t [-0.64798331 -2.048     ]. \t  -611.7601877496991 \t -0.9310180435270131\n",
      "21     \t [-1.11376078  0.85633001]. \t  -19.223806119708765 \t -0.9310180435270131\n",
      "22     \t [-0.07560563 -0.48269908]. \t  -25.011877567281232 \t -0.9310180435270131\n",
      "23     \t [-0.75356372  0.29115789]. \t  -10.731296338855467 \t -0.9310180435270131\n",
      "24     \t [0.98558831 0.96530354]. \t  \u001b[92m-0.0039052937748983185\u001b[0m \t -0.0039052937748983185\n",
      "25     \t [ 0.57677588 -0.02214463]. \t  -12.768490266513574 \t -0.0039052937748983185\n",
      "26     \t [-2.048      -0.81729405]. \t  -2520.9018039828197 \t -0.0039052937748983185\n",
      "27     \t [ 0.86630233 -2.048     ]. \t  -783.1667575900534 \t -0.0039052937748983185\n",
      "28     \t [ 1.11802856 -0.99204683]. \t  -502.68588747545107 \t -0.0039052937748983185\n",
      "29     \t [-0.4501822   0.37211699]. \t  -4.974459514846899 \t -0.0039052937748983185\n",
      "30     \t [-1.35687212  1.74384665]. \t  -6.500705195700178 \t -0.0039052937748983185\n",
      "31     \t [ 0.54183993 -1.34451059]. \t  -268.54743054861524 \t -0.0039052937748983185\n",
      "32     \t [-1.15683703  1.29053445]. \t  -4.879832558694864 \t -0.0039052937748983185\n",
      "33     \t [1.33528104 1.80157753]. \t  -0.14701709209508798 \t -0.0039052937748983185\n",
      "34     \t [-0.82757579  0.67070238]. \t  -3.3601385251351266 \t -0.0039052937748983185\n",
      "35     \t [-1.48752676  2.048     ]. \t  -8.901580182439117 \t -0.0039052937748983185\n",
      "36     \t [0.57179386 0.46640779]. \t  -2.1282577662488635 \t -0.0039052937748983185\n",
      "37     \t [ 2.048     -1.0498679]. \t  -2751.2321959294086 \t -0.0039052937748983185\n",
      "38     \t [-1.10893316 -1.42377874]. \t  -708.5599296477582 \t -0.0039052937748983185\n",
      "39     \t [0.56515507 2.048     ]. \t  -298.99479843810985 \t -0.0039052937748983185\n",
      "40     \t [1.51725395 2.048     ]. \t  -6.722177698116065 \t -0.0039052937748983185\n",
      "41     \t [-0.03511693  1.48873611]. \t  -222.33795733639303 \t -0.0039052937748983185\n",
      "42     \t [ 1.14404779 -0.28755754]. \t  -254.8709677524394 \t -0.0039052937748983185\n",
      "43     \t [-1.31695193 -0.45395852]. \t  -484.2431026958251 \t -0.0039052937748983185\n",
      "44     \t [-0.04068723  0.02708114]. \t  -1.1476764929326506 \t -0.0039052937748983185\n",
      "45     \t [1.27606046 1.67099981]. \t  -0.2582780304790525 \t -0.0039052937748983185\n",
      "46     \t [1.15407637 1.33151202]. \t  -0.023753987442141553 \t -0.0039052937748983185\n",
      "47     \t [0.95419405 0.89267007]. \t  -0.03383995460222321 \t -0.0039052937748983185\n",
      "48     \t [1.1759348  1.42888315]. \t  -0.24311002716215138 \t -0.0039052937748983185\n",
      "49     \t [0.90590386 0.80651435]. \t  -0.028869111585765855 \t -0.0039052937748983185\n",
      "50     \t [1.39793631 1.9641239 ]. \t  -0.16815030534821007 \t -0.0039052937748983185\n",
      "51     \t [1.08736793 1.16587498]. \t  -0.034838508272876795 \t -0.0039052937748983185\n",
      "52     \t [1.05026904 1.0990391 ]. \t  -0.004147808410937012 \t -0.0039052937748983185\n",
      "53     \t [1.25170267 1.66057122]. \t  -0.9434167122519628 \t -0.0039052937748983185\n",
      "54     \t [1.34099049 1.84919419]. \t  -0.3757496400152717 \t -0.0039052937748983185\n",
      "55     \t [1.20243364 1.47016292]. \t  -0.10010741011148892 \t -0.0039052937748983185\n",
      "56     \t [1.21490376 1.5305348 ]. \t  -0.34368458437045957 \t -0.0039052937748983185\n",
      "57     \t [1.41232292 2.03304927]. \t  -0.31741421471840614 \t -0.0039052937748983185\n",
      "58     \t [1.06458953 1.12489862]. \t  -0.011315834411248102 \t -0.0039052937748983185\n",
      "59     \t [0.91601943 0.81666694]. \t  -0.057339289627772114 \t -0.0039052937748983185\n",
      "60     \t [0.97007199 0.92758567]. \t  -0.018996677313089502 \t -0.0039052937748983185\n",
      "61     \t [1.11996142 1.24478355]. \t  -0.023472910316187326 \t -0.0039052937748983185\n",
      "62     \t [1.10822527 1.20994325]. \t  -0.044909587193354535 \t -0.0039052937748983185\n",
      "63     \t [-1.30740643 -2.048     ]. \t  -1417.0631501427658 \t -0.0039052937748983185\n",
      "64     \t [-0.14206989  0.06595054]. \t  -1.5137825670968748 \t -0.0039052937748983185\n",
      "65     \t [1.42365358 2.02236452]. \t  -0.18144042607059507 \t -0.0039052937748983185\n",
      "66     \t [0.74850376 0.56421386]. \t  -0.06481533596883919 \t -0.0039052937748983185\n",
      "67     \t [0.76586841 0.59568905]. \t  -0.06316174680884154 \t -0.0039052937748983185\n",
      "68     \t [1.2912699  1.68128989]. \t  -0.10419234045839458 \t -0.0039052937748983185\n",
      "69     \t [1.0663604  1.11487567]. \t  -0.05390473996714007 \t -0.0039052937748983185\n",
      "70     \t [-2.048       1.06177073]. \t  -990.566770450407 \t -0.0039052937748983185\n",
      "71     \t [1.28045591 1.69133453]. \t  -0.34663978299840303 \t -0.0039052937748983185\n",
      "72     \t [0.91624067 0.80592886]. \t  -0.11969743057270182 \t -0.0039052937748983185\n",
      "73     \t [1.39880044 2.0409062 ]. \t  -0.8690761664121838 \t -0.0039052937748983185\n",
      "74     \t [0.86945515 0.74543978]. \t  -0.02809316493167495 \t -0.0039052937748983185\n",
      "75     \t [1.00075643 0.97002522]. \t  -0.09915126817275734 \t -0.0039052937748983185\n",
      "76     \t [1.19742091 1.47291019]. \t  -0.19180399492954334 \t -0.0039052937748983185\n",
      "77     \t [0.7557599  0.56882779]. \t  -0.060203239660118334 \t -0.0039052937748983185\n",
      "78     \t [1.42002457 2.048     ]. \t  -0.2758361544698091 \t -0.0039052937748983185\n",
      "79     \t [0.83582681 0.69158554]. \t  -0.031882154413764276 \t -0.0039052937748983185\n",
      "80     \t [1.25719588 1.61683189]. \t  -0.19784919177797916 \t -0.0039052937748983185\n",
      "81     \t [0.98050355 0.92868161]. \t  -0.10734573158740816 \t -0.0039052937748983185\n",
      "82     \t [0.83392349 0.65254455]. \t  -0.21148373707383905 \t -0.0039052937748983185\n",
      "83     \t [0.67138398 0.5358664 ]. \t  -0.83235899926899 \t -0.0039052937748983185\n",
      "84     \t [1.03841066 1.08124531]. \t  \u001b[92m-0.002344807926231091\u001b[0m \t -0.002344807926231091\n",
      "85     \t [1.4315096 2.0479837]. \t  -0.18635331589262558 \t -0.002344807926231091\n",
      "86     \t [1.10368599 1.20495319]. \t  -0.028094531038072253 \t -0.002344807926231091\n",
      "87     \t [1.31223783 1.73413786]. \t  -0.11230268033049756 \t -0.002344807926231091\n",
      "88     \t [1.06143217 1.06113102]. \t  -0.4328936010962588 \t -0.002344807926231091\n",
      "89     \t [1.04503047 1.07402707]. \t  -0.03464989437773185 \t -0.002344807926231091\n",
      "90     \t [1.09513682 1.16603463]. \t  -0.11987352626725699 \t -0.002344807926231091\n",
      "91     \t [0.61169817 0.44304074]. \t  -0.6250322711648464 \t -0.002344807926231091\n",
      "92     \t [1.4013311 1.9661261]. \t  -0.16164133466061348 \t -0.002344807926231091\n",
      "93     \t [1.38965189 1.9533977 ]. \t  -0.20140309821295707 \t -0.002344807926231091\n",
      "94     \t [0.92587285 0.83305454]. \t  -0.06399105243606627 \t -0.002344807926231091\n",
      "95     \t [0.86410961 0.70297616]. \t  -0.2095162252036572 \t -0.002344807926231091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [1.1129961  1.27139509]. \t  -0.11927097833886371 \t -0.002344807926231091\n",
      "97     \t [1.23431255 1.54114146]. \t  -0.08592760374405638 \t -0.002344807926231091\n",
      "98     \t [0.92816795 0.84965478]. \t  -0.01918067405046733 \t -0.002344807926231091\n",
      "99     \t [0.37371628 0.14161045]. \t  -0.3926102193081854 \t -0.002344807926231091\n",
      "100    \t [0.72551722 0.50675341]. \t  -0.11384242788222904 \t -0.002344807926231091\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \n",
    "\n",
    "np.random.seed(run_num_5)\n",
    "surrogate_winner_5 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_5 = GPGO(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_5.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.60915518 -0.68821072]. \t  -1074.6314195925434 \t -3.0269049669752817\n",
      "init   \t [ 1.31575449 -1.87721062]. \t  -1302.169546932896 \t -3.0269049669752817\n",
      "init   \t [-1.60703824  0.38933325]. \t  -487.8262244570432 \t -3.0269049669752817\n",
      "init   \t [ 0.12213192 -0.33256477]. \t  -12.844955340992902 \t -3.0269049669752817\n",
      "init   \t [-0.67416945  0.50183959]. \t  -3.0269049669752817 \t -3.0269049669752817\n",
      "1      \t [0.43194847 1.45942665]. \t  -162.33667328228017 \t -3.0269049669752817\n",
      "2      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -3.0269049669752817\n",
      "3      \t [2.048 2.048]. \t  -461.7603900415999 \t -3.0269049669752817\n",
      "4      \t [-2.048  2.048]. \t  -469.9523900415999 \t -3.0269049669752817\n",
      "5      \t [-0.57338707  2.048     ]. \t  -298.049786719953 \t -3.0269049669752817\n",
      "6      \t [2.048      0.72858569]. \t  -1202.2186423119783 \t -3.0269049669752817\n",
      "7      \t [0.18993578 0.45572636]. \t  -18.26687996831763 \t -3.0269049669752817\n",
      "8      \t [-0.0703773 -2.048    ]. \t  -422.60729521270326 \t -3.0269049669752817\n",
      "9      \t [-0.61381857 -0.39120093]. \t  -61.58284157868169 \t -3.0269049669752817\n",
      "10     \t [0.82932433 2.048     ]. \t  -185.04929042303755 \t -3.0269049669752817\n",
      "11     \t [-0.2780338   0.07475543]. \t  \u001b[92m-1.634019308188203\u001b[0m \t -1.634019308188203\n",
      "12     \t [-0.12784474 -1.07155366]. \t  -119.62422547905427 \t -1.634019308188203\n",
      "13     \t [-1.07748604  1.30825912]. \t  -6.48517524839198 \t -1.634019308188203\n",
      "14     \t [-0.43343091  1.12275856]. \t  -89.45781566585373 \t -1.634019308188203\n",
      "15     \t [1.00676933 1.03855382]. \t  \u001b[92m-0.06239256286263441\u001b[0m \t -0.06239256286263441\n",
      "16     \t [0.8088238  0.46409419]. \t  -3.6504161499857797 \t -0.06239256286263441\n",
      "17     \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -0.06239256286263441\n",
      "18     \t [ 0.5445527 -1.4589307]. \t  -308.37434127253863 \t -0.06239256286263441\n",
      "19     \t [-2.048       1.18924106]. \t  -912.3306295721836 \t -0.06239256286263441\n",
      "20     \t [-1.29177426  2.048     ]. \t  -19.64053884297834 \t -0.06239256286263441\n",
      "21     \t [ 0.56307517 -2.048     ]. \t  -559.5387790222273 \t -0.06239256286263441\n",
      "22     \t [-2.048     -0.5233487]. \t  -2234.9150023316574 \t -0.06239256286263441\n",
      "23     \t [0.6939665  0.84232613]. \t  -13.106748504672536 \t -0.06239256286263441\n",
      "24     \t [1.22354739 1.6789087 ]. \t  -3.356569711734655 \t -0.06239256286263441\n",
      "25     \t [ 0.77874778 -0.40201519]. \t  -101.74877422324663 \t -0.06239256286263441\n",
      "26     \t [0.46447024 0.12026002]. \t  -1.1982934032542334 \t -0.06239256286263441\n",
      "27     \t [-0.8483844 -2.048    ]. \t  -769.463906740018 \t -0.06239256286263441\n",
      "28     \t [-0.96919402  0.91366766]. \t  -3.9436168351191245 \t -0.06239256286263441\n",
      "29     \t [1.40888356 2.048     ]. \t  -0.564679711424077 \t -0.06239256286263441\n",
      "30     \t [-0.811122   -1.24143448]. \t  -364.034487149882 \t -0.06239256286263441\n",
      "31     \t [-1.08379872  1.73536604]. \t  -35.78586573566607 \t -0.06239256286263441\n",
      "32     \t [-1.41251343  1.75508388]. \t  -11.585517517655168 \t -0.06239256286263441\n",
      "33     \t [1.53457906 1.53833543]. \t  -66.96891811002953 \t -0.06239256286263441\n",
      "34     \t [-0.88656613  0.33155827]. \t  -24.210815573621918 \t -0.06239256286263441\n",
      "35     \t [1.3775392 1.8395114]. \t  -0.48012991712865627 \t -0.06239256286263441\n",
      "36     \t [-1.4859629  2.048    ]. \t  -8.742756188043664 \t -0.06239256286263441\n",
      "37     \t [1.21545931 1.4652293 ]. \t  \u001b[92m-0.06109283952170815\u001b[0m \t -0.06109283952170815\n",
      "38     \t [0.71253773 0.50293746]. \t  -0.08491228161332413 \t -0.06109283952170815\n",
      "39     \t [ 0.0530931  -0.01128274]. \t  -0.9165182271073684 \t -0.06109283952170815\n",
      "40     \t [ 2.048      -0.19335065]. \t  -1926.2496356604588 \t -0.06109283952170815\n",
      "41     \t [-0.20721673 -0.21970879]. \t  -8.355746460565305 \t -0.06109283952170815\n",
      "42     \t [-1.22228271  1.19727757]. \t  -13.74147884275403 \t -0.06109283952170815\n",
      "43     \t [0.86683964 0.73707036]. \t  \u001b[92m-0.03829694912263361\u001b[0m \t -0.03829694912263361\n",
      "44     \t [ 2.048      -1.24454545]. \t  -2959.206640719453 \t -0.03829694912263361\n",
      "45     \t [1.3552664 2.048    ]. \t  -4.5889965266284864 \t -0.03829694912263361\n",
      "46     \t [-0.39992032 -1.64215854]. \t  -326.714343658102 \t -0.03829694912263361\n",
      "47     \t [1.3203156  1.73322949]. \t  -0.11260966154570798 \t -0.03829694912263361\n",
      "48     \t [0.77503222 0.61129433]. \t  -0.06188762780265682 \t -0.03829694912263361\n",
      "49     \t [1.26311696 1.51413667]. \t  -0.7306512942970098 \t -0.03829694912263361\n",
      "50     \t [0.60534913 0.36004624]. \t  -0.15984700544233416 \t -0.03829694912263361\n",
      "51     \t [0.99421737 1.04183926]. \t  -0.28488060669318593 \t -0.03829694912263361\n",
      "52     \t [1.28464358 1.62892566]. \t  -0.12674719672886697 \t -0.03829694912263361\n",
      "53     \t [1.26901394 1.59873281]. \t  -0.08597239104630192 \t -0.03829694912263361\n",
      "54     \t [0.67593643 0.48931333]. \t  -0.21014408903880866 \t -0.03829694912263361\n",
      "55     \t [1.30179004 1.64702969]. \t  -0.31791622215197113 \t -0.03829694912263361\n",
      "56     \t [0.18732875 0.06457692]. \t  -0.7473702796996772 \t -0.03829694912263361\n",
      "57     \t [0.64850608 0.40654524]. \t  -0.14318969334240494 \t -0.03829694912263361\n",
      "58     \t [0.71947603 0.53704215]. \t  -0.11631567176164663 \t -0.03829694912263361\n",
      "59     \t [1.19211569 1.38640293]. \t  -0.15757365090469558 \t -0.03829694912263361\n",
      "60     \t [0.90421127 0.87835386]. \t  -0.37830267090692143 \t -0.03829694912263361\n",
      "61     \t [0.71488514 0.51605154]. \t  -0.08378126640552915 \t -0.03829694912263361\n",
      "62     \t [1.3136266  1.70084405]. \t  -0.15972084197274664 \t -0.03829694912263361\n",
      "63     \t [1.28816371 1.64764937]. \t  -0.09676569284381095 \t -0.03829694912263361\n",
      "64     \t [1.15668781 1.33464311]. \t  \u001b[92m-0.02562925484048015\u001b[0m \t -0.02562925484048015\n",
      "65     \t [0.96276979 0.94631457]. \t  -0.03897904038072258 \t -0.02562925484048015\n",
      "66     \t [1.2121726  1.47672163]. \t  -0.05043301495345337 \t -0.02562925484048015\n",
      "67     \t [0.79677565 0.66833448]. \t  -0.15341149316271518 \t -0.02562925484048015\n",
      "68     \t [1.26045603 1.61465398]. \t  -0.13494201927539973 \t -0.02562925484048015\n",
      "69     \t [1.2262867  1.50642668]. \t  -0.051906647845931554 \t -0.02562925484048015\n",
      "70     \t [-1.29230632  1.71930125]. \t  -5.497181366979375 \t -0.02562925484048015\n",
      "71     \t [1.34114695 1.79700387]. \t  -0.11666056042315233 \t -0.02562925484048015\n",
      "72     \t [1.26590336 1.6236663 ]. \t  -0.11545794349535371 \t -0.02562925484048015\n",
      "73     \t [0.61515481 0.36171149]. \t  -0.17600803525601003 \t -0.02562925484048015\n",
      "74     \t [1.17931545 1.38759056]. \t  -0.03317442143276073 \t -0.02562925484048015\n",
      "75     \t [1.09043406 1.23156941]. \t  -0.18899867428581185 \t -0.02562925484048015\n",
      "76     \t [0.68546566 0.46831083]. \t  -0.09917282786479258 \t -0.02562925484048015\n",
      "77     \t [1.31389352 1.69902415]. \t  -0.17301461096386167 \t -0.02562925484048015\n",
      "78     \t [0.89517739 0.79675073]. \t  \u001b[92m-0.013096260711605336\u001b[0m \t -0.013096260711605336\n",
      "79     \t [0.685873   0.49656582]. \t  -0.16702686448985365 \t -0.013096260711605336\n",
      "80     \t [1.08973484 1.20255324]. \t  -0.030646081183221093 \t -0.013096260711605336\n",
      "81     \t [0.07053512 0.04764411]. \t  -1.0459685315202778 \t -0.013096260711605336\n",
      "82     \t [1.08374619 1.19912952]. \t  -0.06764615832389456 \t -0.013096260711605336\n",
      "83     \t [0.66975161 0.45602927]. \t  -0.11463222060463235 \t -0.013096260711605336\n",
      "84     \t [0.85061866 0.72472246]. \t  -0.022451755768582854 \t -0.013096260711605336\n",
      "85     \t [0.86817612 0.76618126]. \t  -0.03288147220644727 \t -0.013096260711605336\n",
      "86     \t [0.81991831 0.69534882]. \t  -0.08571086865854241 \t -0.013096260711605336\n",
      "87     \t [1.26702661 1.59519213]. \t  -0.08163451921166415 \t -0.013096260711605336\n",
      "88     \t [0.68075009 0.45149575]. \t  -0.1161409135918533 \t -0.013096260711605336\n",
      "89     \t [0.62205006 0.36654114]. \t  -0.18448310826018174 \t -0.013096260711605336\n",
      "90     \t [0.80696297 0.621519  ]. \t  -0.12529561581640905 \t -0.013096260711605336\n",
      "91     \t [1.09782169 1.22921313]. \t  -0.06717232351902844 \t -0.013096260711605336\n",
      "92     \t [0.74712511 0.59221554]. \t  -0.17967906143476312 \t -0.013096260711605336\n",
      "93     \t [0.15660129 0.05633675]. \t  -0.8125267214691946 \t -0.013096260711605336\n",
      "94     \t [0.6804843  0.45629025]. \t  -0.10667171036880008 \t -0.013096260711605336\n",
      "95     \t [1.34555492 1.79746659]. \t  -0.13644228596449628 \t -0.013096260711605336\n",
      "96     \t [1.22306393 1.49655819]. \t  -0.049802784952531966 \t -0.013096260711605336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [1.05498828 1.1200053 ]. \t  \u001b[92m-0.007930769858199791\u001b[0m \t -0.007930769858199791\n",
      "98     \t [0.68825983 0.4507629 ]. \t  -0.14980027203826463 \t -0.007930769858199791\n",
      "99     \t [0.94405384 0.92628073]. \t  -0.1259317692654256 \t -0.007930769858199791\n",
      "100    \t [1.18665886 1.41757699]. \t  -0.04371092381636191 \t -0.007930769858199791\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \n",
    "\n",
    "np.random.seed(run_num_6)\n",
    "surrogate_winner_6 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_6 = GPGO(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_6.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-1.42268934 -0.80954733]. \t  -808.7939502616841 \t -2.0077595729598063\n",
      "init   \t [-1.79389885 -0.16441204]. \t  -1151.9264213711547 \t -2.0077595729598063\n",
      "init   \t [1.37319786 1.74897991]. \t  -2.0077595729598063 \t -2.0077595729598063\n",
      "init   \t [0.92974688 1.09976052]. \t  -5.543015908052858 \t -2.0077595729598063\n",
      "init   \t [-0.94533605  0.58994398]. \t  -13.008689168416776 \t -2.0077595729598063\n",
      "1      \t [-0.57538407  1.53443439]. \t  -147.29118277315794 \t -2.0077595729598063\n",
      "2      \t [0.08522312 0.00582546]. \t  \u001b[92m-0.8370233883999405\u001b[0m \t -0.8370233883999405\n",
      "3      \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -0.8370233883999405\n",
      "4      \t [-2.048  2.048]. \t  -469.9523900415999 \t -0.8370233883999405\n",
      "5      \t [2.048      0.69903628]. \t  -1222.7879496277433 \t -0.8370233883999405\n",
      "6      \t [-0.63448561 -2.048     ]. \t  -603.2018505946053 \t -0.8370233883999405\n",
      "7      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -0.8370233883999405\n",
      "8      \t [-0.35301077 -0.94535999]. \t  -116.31562974774884 \t -0.8370233883999405\n",
      "9      \t [0.54409477 2.048     ]. \t  -307.1445429754385 \t -0.8370233883999405\n",
      "10     \t [-0.10410823  0.78255348]. \t  -60.773452386814185 \t -0.8370233883999405\n",
      "11     \t [2.048 2.048]. \t  -461.7603900415999 \t -0.8370233883999405\n",
      "12     \t [-0.64230508 -0.17997135]. \t  -37.80600958150815 \t -0.8370233883999405\n",
      "13     \t [ 0.23622429 -2.048     ]. \t  -443.18160369686 \t -0.8370233883999405\n",
      "14     \t [ 1.08503053 -0.3386984 ]. \t  -229.82969172723583 \t -0.8370233883999405\n",
      "15     \t [-1.1803369  2.048    ]. \t  -47.630803032866105 \t -0.8370233883999405\n",
      "16     \t [-1.34908578  1.39178406]. \t  -23.85787170140006 \t -0.8370233883999405\n",
      "17     \t [0.7699793  0.30701283]. \t  -8.224234252851602 \t -0.8370233883999405\n",
      "18     \t [1.24212237 2.048     ]. \t  -25.57445929919348 \t -0.8370233883999405\n",
      "19     \t [-0.8944956  1.0864255]. \t  -11.786061204587988 \t -0.8370233883999405\n",
      "20     \t [-2.048       1.08393705]. \t  -976.7285599454033 \t -0.8370233883999405\n",
      "21     \t [ 0.48287779 -0.7874219 ]. \t  -104.4283938493415 \t -0.8370233883999405\n",
      "22     \t [0.93963943 1.56604935]. \t  -46.66990567849711 \t -0.8370233883999405\n",
      "23     \t [-0.5190931   0.36001816]. \t  -3.1277645107706427 \t -0.8370233883999405\n",
      "24     \t [0.4352162  0.44147803]. \t  -6.672651829757315 \t -0.8370233883999405\n",
      "25     \t [ 0.51606982 -0.22320439]. \t  -24.198389776787813 \t -0.8370233883999405\n",
      "26     \t [ 2.048     -0.4733619]. \t  -2179.8088000235957 \t -0.8370233883999405\n",
      "27     \t [-0.0763228  -1.57113869]. \t  -249.8399714930716 \t -0.8370233883999405\n",
      "28     \t [-0.61267113  2.048     ]. \t  -282.3711858910841 \t -0.8370233883999405\n",
      "29     \t [-1.19533139  1.63608582]. \t  -9.115510762324359 \t -0.8370233883999405\n",
      "30     \t [1.21389151 1.34085554]. \t  -1.8060694910864974 \t -0.8370233883999405\n",
      "31     \t [ 0.91049365 -1.63373216]. \t  -606.5123384074955 \t -0.8370233883999405\n",
      "32     \t [-1.11830692  1.10896929]. \t  -6.493443847257936 \t -0.8370233883999405\n",
      "33     \t [-1.47547745  2.048     ]. \t  -7.792958252978263 \t -0.8370233883999405\n",
      "34     \t [1.47917518 2.048     ]. \t  -2.188467062687695 \t -0.8370233883999405\n",
      "35     \t [-0.80306454 -1.33535978]. \t  -395.3989334265941 \t -0.8370233883999405\n",
      "36     \t [-1.38377262  1.85267898]. \t  -6.068605117208174 \t -0.8370233883999405\n",
      "37     \t [0.40971407 0.15771082]. \t  \u001b[92m-0.35874948374597954\u001b[0m \t -0.35874948374597954\n",
      "38     \t [0.99442988 0.8281372 ]. \t  -2.584202268569655 \t -0.35874948374597954\n",
      "39     \t [-2.048      -1.03742077]. \t  -2746.3847128332104 \t -0.35874948374597954\n",
      "40     \t [ 1.48565932 -1.16998844]. \t  -1140.7649699605 \t -0.35874948374597954\n",
      "41     \t [2.048      1.43293354]. \t  -763.6149854064046 \t -0.35874948374597954\n",
      "42     \t [1.22336539 1.53924765]. \t  \u001b[92m-0.23157916751524435\u001b[0m \t -0.23157916751524435\n",
      "43     \t [0.7269077 0.5128569]. \t  \u001b[92m-0.09872206541485622\u001b[0m \t -0.09872206541485622\n",
      "44     \t [-0.80060635  0.66769477]. \t  -3.3136017741173966 \t -0.09872206541485622\n",
      "45     \t [1.38214347 1.94888243]. \t  -0.2947353239104018 \t -0.09872206541485622\n",
      "46     \t [ 1.13127196 -2.048     ]. \t  -1107.4267125714046 \t -0.09872206541485622\n",
      "47     \t [-1.27368485 -2.048     ]. \t  -1352.2601091696733 \t -0.09872206541485622\n",
      "48     \t [1.40851501 2.048     ]. \t  -0.5775793540754135 \t -0.09872206541485622\n",
      "49     \t [-0.69942831  0.32155611]. \t  -5.6985027957674355 \t -0.09872206541485622\n",
      "50     \t [0.88578895 0.78898676]. \t  \u001b[92m-0.014949217433375229\u001b[0m \t -0.014949217433375229\n",
      "51     \t [1.11146796 1.23600177]. \t  \u001b[92m-0.01246616089075497\u001b[0m \t -0.01246616089075497\n",
      "52     \t [-0.19877736  0.01707865]. \t  -1.4873946411944425 \t -0.01246616089075497\n",
      "53     \t [0.66412945 0.4498684 ]. \t  -0.12055385387038067 \t -0.01246616089075497\n",
      "54     \t [0.67651799 0.44816222]. \t  -0.11369294357614187 \t -0.01246616089075497\n",
      "55     \t [0.594204   0.28752093]. \t  -0.5944485242840383 \t -0.01246616089075497\n",
      "56     \t [1.18286447 1.3783027 ]. \t  -0.07697696459155087 \t -0.01246616089075497\n",
      "57     \t [1.34772973 1.87857827]. \t  -0.5078354826838103 \t -0.01246616089075497\n",
      "58     \t [1.42140656 2.048     ]. \t  -0.25377819081261477 \t -0.01246616089075497\n",
      "59     \t [1.14770372 1.3166032 ]. \t  -0.02185490781364569 \t -0.01246616089075497\n",
      "60     \t [1.23460307 1.52719224]. \t  -0.055907373853585 \t -0.01246616089075497\n",
      "61     \t [1.01316081 1.05170648]. \t  -0.06373594222442892 \t -0.01246616089075497\n",
      "62     \t [1.14169524 1.28814586]. \t  -0.04355440747764882 \t -0.01246616089075497\n",
      "63     \t [0.92821821 0.86238608]. \t  \u001b[92m-0.0052161501998060624\u001b[0m \t -0.0052161501998060624\n",
      "64     \t [1.24670379 1.58717524]. \t  -0.16913598945303032 \t -0.0052161501998060624\n",
      "65     \t [0.97108911 0.96030652]. \t  -0.030738761196914842 \t -0.0052161501998060624\n",
      "66     \t [0.9893409 0.9655526]. \t  -0.017650821030332026 \t -0.0052161501998060624\n",
      "67     \t [0.06572843 0.08020293]. \t  -1.4486818344175603 \t -0.0052161501998060624\n",
      "68     \t [1.35504885 1.85915187]. \t  -0.17893433327832972 \t -0.0052161501998060624\n",
      "69     \t [1.40502422 2.0018436 ]. \t  -0.24105387738426723 \t -0.0052161501998060624\n",
      "70     \t [1.20560418 1.44347074]. \t  -0.05229449908398244 \t -0.0052161501998060624\n",
      "71     \t [0.92566015 0.85227345]. \t  -0.00761788386540911 \t -0.0052161501998060624\n",
      "72     \t [0.86335395 0.72058451]. \t  -0.08015393051932766 \t -0.0052161501998060624\n",
      "73     \t [0.61242373 0.36762315]. \t  -0.15575023679245903 \t -0.0052161501998060624\n",
      "74     \t [1.42150972 2.04799469]. \t  -0.25222574592416963 \t -0.0052161501998060624\n",
      "75     \t [0.95267125 0.89234911]. \t  -0.025445702449459654 \t -0.0052161501998060624\n",
      "76     \t [0.9737849  0.94938615]. \t  \u001b[92m-0.0008147253483149306\u001b[0m \t -0.0008147253483149306\n",
      "77     \t [0.6179851  0.35700425]. \t  -0.2079430196937005 \t -0.0008147253483149306\n",
      "78     \t [1.30781654 1.72959983]. \t  -0.13167546128035554 \t -0.0008147253483149306\n",
      "79     \t [1.03613652 1.03388204]. \t  -0.15888987957400896 \t -0.0008147253483149306\n",
      "80     \t [1.09593106 1.20714655]. \t  -0.012901441099543012 \t -0.0008147253483149306\n",
      "81     \t [0.53367755 0.25729132]. \t  -0.29319388436590743 \t -0.0008147253483149306\n",
      "82     \t [1.00434909 1.01149808]. \t  \u001b[92m-0.0007923010524946041\u001b[0m \t -0.0007923010524946041\n",
      "83     \t [0.27985148 0.12297738]. \t  -0.7180701376955408 \t -0.0007923010524946041\n",
      "84     \t [1.25698179 1.59456888]. \t  -0.08725546691512442 \t -0.0007923010524946041\n",
      "85     \t [1.12337446 1.26949303]. \t  -0.02088058379732137 \t -0.0007923010524946041\n",
      "86     \t [0.7125719  0.51110648]. \t  -0.08373566996047173 \t -0.0007923010524946041\n",
      "87     \t [1.28290225 1.62039552]. \t  -0.14476665831311963 \t -0.0007923010524946041\n",
      "88     \t [1.03664213 1.10611845]. \t  -0.10051441801062402 \t -0.0007923010524946041\n",
      "89     \t [1.02569239 1.07832956]. \t  -0.06974857757381787 \t -0.0007923010524946041\n",
      "90     \t [0.98786067 0.98318866]. \t  -0.005505551961439045 \t -0.0007923010524946041\n",
      "91     \t [1.3071582  1.71720394]. \t  -0.10164166976176249 \t -0.0007923010524946041\n",
      "92     \t [0.7427851  0.55471353]. \t  -0.067049831427108 \t -0.0007923010524946041\n",
      "93     \t [1.08926146 1.1547978 ]. \t  -0.10841054618602695 \t -0.0007923010524946041\n",
      "94     \t [1.36790481 1.91124805]. \t  -0.2960304448463151 \t -0.0007923010524946041\n",
      "95     \t [0.63364957 0.4142069 ]. \t  -0.15032923205076187 \t -0.0007923010524946041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [1.25542524 1.58194995]. \t  -0.06867297806356126 \t -0.0007923010524946041\n",
      "97     \t [1.06228701 1.14570636]. \t  -0.03364515387831668 \t -0.0007923010524946041\n",
      "98     \t [0.95280384 0.91723203]. \t  -0.011057589419378396 \t -0.0007923010524946041\n",
      "99     \t [1.08973818 1.19314635]. \t  -0.011208071006086423 \t -0.0007923010524946041\n",
      "100    \t [1.05414692 1.12016021]. \t  -0.010914392465534089 \t -0.0007923010524946041\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \n",
    "\n",
    "np.random.seed(run_num_7)\n",
    "surrogate_winner_7 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_7 = GPGO(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_7.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-1.59948851 -1.8750873 ]. \t  -1972.3059423071145 \t -7.238275380208551\n",
      "init   \t [ 0.40834083 -0.81972959]. \t  -97.66272647293047 \t -7.238275380208551\n",
      "init   \t [ 1.21492186 -0.54806067]. \t  -409.7425700197761 \t -7.238275380208551\n",
      "init   \t [ 1.55897178 -0.82804067]. \t  -1062.051446884513 \t -7.238275380208551\n",
      "init   \t [-0.69999303  0.28146449]. \t  -7.238275380208551 \t -7.238275380208551\n",
      "1      \t [0.71499755 0.68508596]. \t  \u001b[92m-3.104111715674435\u001b[0m \t -3.104111715674435\n",
      "2      \t [-2.048       1.93979489]. \t  -517.5714349057238 \t -3.104111715674435\n",
      "3      \t [2.048 2.048]. \t  -461.7603900415999 \t -3.104111715674435\n",
      "4      \t [ 0.36684304 -0.02926714]. \t  \u001b[92m-3.0852735746766697\u001b[0m \t -3.0852735746766697\n",
      "5      \t [-0.17742936  2.048     ]. \t  -408.0211556447173 \t -3.0852735746766697\n",
      "6      \t [-2.048       0.33844938]. \t  -1496.0517869319083 \t -3.0852735746766697\n",
      "7      \t [2.048      0.67872314]. \t  -1237.0291826823009 \t -3.0852735746766697\n",
      "8      \t [-0.40979296  0.95421569]. \t  -63.81199307361336 \t -3.0852735746766697\n",
      "9      \t [ 0.45576868 -2.048     ]. \t  -509.1257562182805 \t -3.0852735746766697\n",
      "10     \t [0.94892346 2.048     ]. \t  -131.6883951071148 \t -3.0852735746766697\n",
      "11     \t [0.47275554 1.31817358]. \t  -120.1094926731294 \t -3.0852735746766697\n",
      "12     \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -3.0852735746766697\n",
      "13     \t [-0.2703838 -2.048    ]. \t  -451.5235342630583 \t -3.0852735746766697\n",
      "14     \t [-0.12634999  0.44542993]. \t  -19.712735513530617 \t -3.0852735746766697\n",
      "15     \t [-0.52832508 -0.66362557]. \t  -91.21409145277038 \t -3.0852735746766697\n",
      "16     \t [-1.19333064  2.048     ]. \t  -43.74355579423761 \t -3.0852735746766697\n",
      "17     \t [-1.19882642  1.3935306 ]. \t  -5.0254064658807955 \t -3.0852735746766697\n",
      "18     \t [-0.25818402 -0.23997388]. \t  -10.985398812545492 \t -3.0852735746766697\n",
      "19     \t [1.21552664 1.4969985 ]. \t  \u001b[92m-0.08445133907396443\u001b[0m \t -0.08445133907396443\n",
      "20     \t [-2.048      -0.83136668]. \t  -2535.0268797503995 \t -0.08445133907396443\n",
      "21     \t [-0.09392858 -1.2375372 ]. \t  -156.53794801107858 \t -0.08445133907396443\n",
      "22     \t [-0.91273716  0.89364148]. \t  -4.025222176243098 \t -0.08445133907396443\n",
      "23     \t [-0.98227415  1.54636464]. \t  -37.74388477098686 \t -0.08445133907396443\n",
      "24     \t [1.42265872 2.048     ]. \t  -0.2364429090768784 \t -0.08445133907396443\n",
      "25     \t [1.265459   1.79156936]. \t  -3.6874214017281233 \t -0.08445133907396443\n",
      "26     \t [0.95336124 1.23968506]. \t  -10.944206308740409 \t -0.08445133907396443\n",
      "27     \t [0.45178395 0.62689415]. \t  -18.175291549538777 \t -0.08445133907396443\n",
      "28     \t [-0.52877246  0.37660345]. \t  -3.2781062296018613 \t -0.08445133907396443\n",
      "29     \t [0.6729039  0.27240855]. \t  -3.361087433301676 \t -0.08445133907396443\n",
      "30     \t [ 2.048      -0.31921277]. \t  -2038.2816691985036 \t -0.08445133907396443\n",
      "31     \t [-2.048       1.23941035]. \t  -882.4299514785281 \t -0.08445133907396443\n",
      "32     \t [ 0.78967475 -1.35561093]. \t  -391.7663687525557 \t -0.08445133907396443\n",
      "33     \t [-1.48165745  2.048     ]. \t  -8.328611884971636 \t -0.08445133907396443\n",
      "34     \t [-1.34579808  1.80747975]. \t  -5.5041322679176385 \t -0.08445133907396443\n",
      "35     \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -0.08445133907396443\n",
      "36     \t [-0.76133406 -1.51553732]. \t  -442.07472177437717 \t -0.08445133907396443\n",
      "37     \t [-0.36203892  0.06331326]. \t  -2.3142771064901386 \t -0.08445133907396443\n",
      "38     \t [1.08466197 1.05717712]. \t  -1.4307618853558546 \t -0.08445133907396443\n",
      "39     \t [-0.95214753 -2.048     ]. \t  -876.7680809499246 \t -0.08445133907396443\n",
      "40     \t [0.40329723 0.22175306]. \t  -0.7053873157397585 \t -0.08445133907396443\n",
      "41     \t [-1.1984224   0.82807283]. \t  -41.816901240774165 \t -0.08445133907396443\n",
      "42     \t [ 1.22352953 -2.048     ]. \t  -1256.7698450226055 \t -0.08445133907396443\n",
      "43     \t [2.048      1.48681097]. \t  -734.1501564709399 \t -0.08445133907396443\n",
      "44     \t [1.33759027 2.048     ]. \t  -6.814417418770952 \t -0.08445133907396443\n",
      "45     \t [-1.24298583 -0.7048205 ]. \t  -511.20640552436817 \t -0.08445133907396443\n",
      "46     \t [1.19245381 1.39998769]. \t  -0.08525560593018253 \t -0.08445133907396443\n",
      "47     \t [1.38307777 1.86079889]. \t  -0.41824400337200374 \t -0.08445133907396443\n",
      "48     \t [0.62639418 0.38501231]. \t  -0.14499437401484563 \t -0.08445133907396443\n",
      "49     \t [0.94112112 0.91501   ]. \t  -0.08932179218768996 \t -0.08445133907396443\n",
      "50     \t [ 0.0414456  -0.02070215]. \t  -0.9690916630101495 \t -0.08445133907396443\n",
      "51     \t [0.81117876 0.71023018]. \t  -0.30833791317305187 \t -0.08445133907396443\n",
      "52     \t [0.50255873 0.23207922]. \t  -0.28941564527290975 \t -0.08445133907396443\n",
      "53     \t [1.08561663 1.23733127]. \t  -0.3526955964565842 \t -0.08445133907396443\n",
      "54     \t [1.0521649  1.11258737]. \t  \u001b[92m-0.005786335521019823\u001b[0m \t -0.005786335521019823\n",
      "55     \t [1.22897026 1.50593227]. \t  -0.05439486517548172 \t -0.005786335521019823\n",
      "56     \t [1.1158964 1.2628597]. \t  -0.04453104125177797 \t -0.005786335521019823\n",
      "57     \t [1.37947934 1.92644552]. \t  -0.1991463191079093 \t -0.005786335521019823\n",
      "58     \t [0.55552658 0.3090326 ]. \t  -0.1975744971816228 \t -0.005786335521019823\n",
      "59     \t [1.03985063 1.11921441]. \t  -0.1454191615922816 \t -0.005786335521019823\n",
      "60     \t [1.07756112 1.15347602]. \t  -0.011886276718517757 \t -0.005786335521019823\n",
      "61     \t [0.95266889 0.90343918]. \t  \u001b[92m-0.0039532192080686205\u001b[0m \t -0.0039532192080686205\n",
      "62     \t [1.14151095 1.30601632]. \t  -0.020906889346689286 \t -0.0039532192080686205\n",
      "63     \t [1.33404983 1.76546808]. \t  -0.13181263792137307 \t -0.0039532192080686205\n",
      "64     \t [0.52673434 0.29379769]. \t  -0.25070815729977464 \t -0.0039532192080686205\n",
      "65     \t [1.05925207 1.13842232]. \t  -0.03043103371651237 \t -0.0039532192080686205\n",
      "66     \t [0.59135168 0.33547944]. \t  -0.1872068068625224 \t -0.0039532192080686205\n",
      "67     \t [1.06567787 1.16261335]. \t  -0.07691163691582632 \t -0.0039532192080686205\n",
      "68     \t [0.85077586 0.73229627]. \t  -0.029453302926332452 \t -0.0039532192080686205\n",
      "69     \t [0.66499768 0.44846152]. \t  -0.11611983064644714 \t -0.0039532192080686205\n",
      "70     \t [0.52273596 0.26113505]. \t  -0.24246515776460817 \t -0.0039532192080686205\n",
      "71     \t [1.05170391 1.10051846]. \t  -0.005767602172921991 \t -0.0039532192080686205\n",
      "72     \t [1.30857919 1.70700525]. \t  -0.09810936656722519 \t -0.0039532192080686205\n",
      "73     \t [1.03712128 1.0729572 ]. \t  \u001b[92m-0.002087330507328505\u001b[0m \t -0.002087330507328505\n",
      "74     \t [1.19884214 1.43670868]. \t  -0.03956459728059611 \t -0.002087330507328505\n",
      "75     \t [0.98251148 0.99850606]. \t  -0.1103788522990577 \t -0.002087330507328505\n",
      "76     \t [1.16644765 1.36492468]. \t  -0.029575008401823564 \t -0.002087330507328505\n",
      "77     \t [0.53782262 0.26026987]. \t  -0.2976111404586791 \t -0.002087330507328505\n",
      "78     \t [0.83874507 0.71206074]. \t  -0.03334325212719472 \t -0.002087330507328505\n",
      "79     \t [1.40834569 2.04799978]. \t  -0.583573776366164 \t -0.002087330507328505\n",
      "80     \t [1.0000264  1.02673331]. \t  -0.07118494403050381 \t -0.002087330507328505\n",
      "81     \t [1.1173811  1.26882869]. \t  -0.054939330223047714 \t -0.002087330507328505\n",
      "82     \t [0.99284354 1.00563487]. \t  -0.03963862611880532 \t -0.002087330507328505\n",
      "83     \t [1.16164647 1.33929551]. \t  -0.03638518460486336 \t -0.002087330507328505\n",
      "84     \t [0.59195819 0.32962643]. \t  -0.20971252182038674 \t -0.002087330507328505\n",
      "85     \t [1.40450901 2.00929424]. \t  -0.2979400630450199 \t -0.002087330507328505\n",
      "86     \t [1.40193357 1.98984372]. \t  -0.22121350081449476 \t -0.002087330507328505\n",
      "87     \t [0.6390535 0.4123454]. \t  -0.13184738665084259 \t -0.002087330507328505\n",
      "88     \t [1.01451228 1.02976135]. \t  \u001b[92m-0.00023829404064100053\u001b[0m \t -0.00023829404064100053\n",
      "89     \t [1.09557978 1.20028984]. \t  -0.0091354962488884 \t -0.00023829404064100053\n",
      "90     \t [0.87854513 0.78718838]. \t  -0.038303798987500415 \t -0.00023829404064100053\n",
      "91     \t [1.07094059 1.1361847 ]. \t  -0.01654381510945217 \t -0.00023829404064100053\n",
      "92     \t [0.96303821 0.95463126]. \t  -0.07528853390582055 \t -0.00023829404064100053\n",
      "93     \t [1.1079618 1.2409805]. \t  -0.02961482548869465 \t -0.00023829404064100053\n",
      "94     \t [1.05894963 1.14663586]. \t  -0.06728957591409507 \t -0.00023829404064100053\n",
      "95     \t [0.61828713 0.37626818]. \t  -0.14931767862491127 \t -0.00023829404064100053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [1.0428519  1.07546416]. \t  -0.016419107689296858 \t -0.00023829404064100053\n",
      "97     \t [1.03679434 1.1193491 ]. \t  -0.19854845854912648 \t -0.00023829404064100053\n",
      "98     \t [0.59024324 0.3605499 ]. \t  -0.18269401004764285 \t -0.00023829404064100053\n",
      "99     \t [1.18217523 1.36700244]. \t  -0.12643157396717353 \t -0.00023829404064100053\n",
      "100    \t [0.98911453 1.02435853]. \t  -0.21181944861896926 \t -0.00023829404064100053\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \n",
    "\n",
    "np.random.seed(run_num_8)\n",
    "surrogate_winner_8 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_8 = GPGO(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_8.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.70565298 -0.04883088]. \t  -29.9831488845538 \t -29.9831488845538\n",
      "init   \t [ 1.33322823 -1.9191956 ]. \t  -1366.6650412963722 \t -29.9831488845538\n",
      "init   \t [1.26177265 0.26876895]. \t  -175.18114988457984 \t -29.9831488845538\n",
      "init   \t [-0.82893825 -1.85673433]. \t  -650.4739702903224 \t -29.9831488845538\n",
      "init   \t [ 2.00960983 -2.0200418 ]. \t  -3671.650548034952 \t -29.9831488845538\n",
      "1      \t [-2.048  2.048]. \t  -469.9523900415999 \t -29.9831488845538\n",
      "2      \t [2.048 2.048]. \t  -461.7603900415999 \t -29.9831488845538\n",
      "3      \t [-0.01557951  2.048     ]. \t  -420.36238904091795 \t -29.9831488845538\n",
      "4      \t [-2.048      -0.00515431]. \t  -1772.8353116183025 \t -29.9831488845538\n",
      "5      \t [ 0.37124007 -1.53068659]. \t  -278.7864911444023 \t -29.9831488845538\n",
      "6      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -29.9831488845538\n",
      "7      \t [-0.29942263  0.67850487]. \t  -36.36304386319816 \t -29.9831488845538\n",
      "8      \t [ 0.15222813 -2.048     ]. \t  -429.6946439022117 \t -29.9831488845538\n",
      "9      \t [-0.53427227 -0.51496973]. \t  -66.420663611586 \t -29.9831488845538\n",
      "10     \t [0.79601041 1.21798903]. \t  -34.188858823732275 \t -29.9831488845538\n",
      "11     \t [2.048      0.95839145]. \t  -1048.2113090377761 \t -29.9831488845538\n",
      "12     \t [1.04276077 2.048     ]. \t  -92.28666778780699 \t -29.9831488845538\n",
      "13     \t [-1.17188819  1.37912143]. \t  \u001b[92m-4.720461719984191\u001b[0m \t -4.720461719984191\n",
      "14     \t [ 1.2691152  -0.67441224]. \t  -522.224913977718 \t -4.720461719984191\n",
      "15     \t [-1.11647199  2.048     ]. \t  -68.71812257828113 \t -4.720461719984191\n",
      "16     \t [0.51495522 0.59028568]. \t  -10.804711572592412 \t -4.720461719984191\n",
      "17     \t [-0.6118099   1.39518448]. \t  -106.81612563420748 \t -4.720461719984191\n",
      "18     \t [-0.92162473  0.45570571]. \t  -19.19154304956175 \t -4.720461719984191\n",
      "19     \t [-0.32203733  0.02819743]. \t  \u001b[92m-2.3179679624881704\u001b[0m \t -2.3179679624881704\n",
      "20     \t [-2.048       1.10947479]. \t  -960.9074293919875 \t -2.3179679624881704\n",
      "21     \t [-0.85894429  0.8949027 ]. \t  -5.924261707349892 \t -2.3179679624881704\n",
      "22     \t [ 2.048      -0.11147275]. \t  -1855.0696460490594 \t -2.3179679624881704\n",
      "23     \t [-0.62185396  0.21904277]. \t  -5.441383976804397 \t -2.3179679624881704\n",
      "24     \t [1.36328227 1.51648053]. \t  -11.832342820015889 \t -2.3179679624881704\n",
      "25     \t [-0.33997159 -1.26061609]. \t  -191.18727889788698 \t -2.3179679624881704\n",
      "26     \t [1.09676361 0.93665401]. \t  -7.097546014598946 \t -2.3179679624881704\n",
      "27     \t [1.45528061 2.048     ]. \t  \u001b[92m-0.695066177829029\u001b[0m \t -0.695066177829029\n",
      "28     \t [ 0.27024971 -0.72155387]. \t  -63.66966730806086 \t -0.695066177829029\n",
      "29     \t [1.28753573 1.78103888]. \t  -1.602734485293594 \t -0.695066177829029\n",
      "30     \t [-1.4829744  2.048    ]. \t  -8.451700853878156 \t -0.695066177829029\n",
      "31     \t [-1.32838317  1.80214137]. \t  -5.562289688351578 \t -0.695066177829029\n",
      "32     \t [-1.36234528 -1.03473147]. \t  -841.2046540555882 \t -0.695066177829029\n",
      "33     \t [0.19119148 0.09801653]. \t  -1.0319332135171337 \t -0.695066177829029\n",
      "34     \t [ 0.70310759 -2.048     ]. \t  -646.4477256272789 \t -0.695066177829029\n",
      "35     \t [ 0.74879991 -0.99946768]. \t  -243.47582911190838 \t -0.695066177829029\n",
      "36     \t [ 0.04300583 -0.2080599 ]. \t  -5.3220334802926565 \t -0.695066177829029\n",
      "37     \t [0.79255895 0.45688904]. \t  -2.9760526047563793 \t -0.695066177829029\n",
      "38     \t [1.15952633 1.3488087 ]. \t  \u001b[92m-0.027304006202123778\u001b[0m \t -0.027304006202123778\n",
      "39     \t [-2.048      -0.94655955]. \t  -2652.138106840796 \t -0.027304006202123778\n",
      "40     \t [-1.1029224  -0.22553812]. \t  -212.35174699846584 \t -0.027304006202123778\n",
      "41     \t [0.91601525 0.87508717]. \t  -0.13667666898605801 \t -0.027304006202123778\n",
      "42     \t [ 2.048      -1.20800059]. \t  -2919.5877961656815 \t -0.027304006202123778\n",
      "43     \t [-0.03918689  0.07691056]. \t  -1.6480476387697187 \t -0.027304006202123778\n",
      "44     \t [1.36268415 1.92897963]. \t  -0.6509705641308459 \t -0.027304006202123778\n",
      "45     \t [-1.06946377  1.00371105]. \t  -6.243848546730456 \t -0.027304006202123778\n",
      "46     \t [1.37005962 2.048     ]. \t  -3.058877279732004 \t -0.027304006202123778\n",
      "47     \t [1.11220773 1.20965959]. \t  -0.0873734161592408 \t -0.027304006202123778\n",
      "48     \t [-1.38379945  2.048     ]. \t  -7.454036279570451 \t -0.027304006202123778\n",
      "49     \t [-1.39604413  1.46425106]. \t  -29.233288195434252 \t -0.027304006202123778\n",
      "50     \t [1.26071653 1.63182443]. \t  -0.2479039257314386 \t -0.027304006202123778\n",
      "51     \t [0.95369749 0.9549156 ]. \t  -0.2080483927142317 \t -0.027304006202123778\n",
      "52     \t [1.25584652 1.57524341]. \t  -0.06582113762520723 \t -0.027304006202123778\n",
      "53     \t [1.05225163 1.1143161 ]. \t  \u001b[92m-0.0077465651097730695\u001b[0m \t -0.0077465651097730695\n",
      "54     \t [1.29119937 1.6449839 ]. \t  -0.13413405849028745 \t -0.0077465651097730695\n",
      "55     \t [0.85547558 0.75829134]. \t  -0.0908627074961871 \t -0.0077465651097730695\n",
      "56     \t [1.25809415 1.57480637]. \t  -0.0730037986976113 \t -0.0077465651097730695\n",
      "57     \t [1.38656421 1.89195082]. \t  -0.24312590084323532 \t -0.0077465651097730695\n",
      "58     \t [-0.01360932  0.02560752]. \t  -1.0920332261629748 \t -0.0077465651097730695\n",
      "59     \t [0.87580843 0.73125931]. \t  -0.14345224632334738 \t -0.0077465651097730695\n",
      "60     \t [0.92748208 0.83999312]. \t  -0.04618366989811665 \t -0.0077465651097730695\n",
      "61     \t [1.38737758 1.94208395]. \t  -0.17987772719774542 \t -0.0077465651097730695\n",
      "62     \t [0.9323067  0.87978586]. \t  -0.015797333838923518 \t -0.0077465651097730695\n",
      "63     \t [1.06211933 1.14848667]. \t  -0.04543071676481379 \t -0.0077465651097730695\n",
      "64     \t [1.26148131 1.58650153]. \t  -0.07070881533421856 \t -0.0077465651097730695\n",
      "65     \t [1.18740324 1.39347692]. \t  -0.06217866165918139 \t -0.0077465651097730695\n",
      "66     \t [1.09811851 1.20028288]. \t  -0.012742419205544725 \t -0.0077465651097730695\n",
      "67     \t [-0.68624293  0.4991852 ]. \t  -2.9232544876991096 \t -0.0077465651097730695\n",
      "68     \t [1.16841783 1.34290192]. \t  -0.07808597984025155 \t -0.0077465651097730695\n",
      "69     \t [1.18884375 1.4198259 ]. \t  -0.03985640097688931 \t -0.0077465651097730695\n",
      "70     \t [0.80539885 0.69276401]. \t  -0.23232148640760342 \t -0.0077465651097730695\n",
      "71     \t [0.98376525 0.93589715]. \t  -0.10200487127314702 \t -0.0077465651097730695\n",
      "72     \t [1.20202016 1.42713053]. \t  -0.07221885149017365 \t -0.0077465651097730695\n",
      "73     \t [1.39500661 2.048     ]. \t  -1.1955444455500248 \t -0.0077465651097730695\n",
      "74     \t [0.81202317 0.64667696]. \t  -0.051476132145674994 \t -0.0077465651097730695\n",
      "75     \t [1.07934597 1.15310369]. \t  -0.02041880365457724 \t -0.0077465651097730695\n",
      "76     \t [0.96076464 0.89600029]. \t  -0.07480928395830529 \t -0.0077465651097730695\n",
      "77     \t [0.81461897 0.64916221]. \t  -0.05522282730712497 \t -0.0077465651097730695\n",
      "78     \t [1.16152915 1.33470959]. \t  -0.0469441309788604 \t -0.0077465651097730695\n",
      "79     \t [0.99205598 0.98874803]. \t  \u001b[92m-0.0021542991380423897\u001b[0m \t -0.0021542991380423897\n",
      "80     \t [0.78397354 0.55642731]. \t  -0.38524252524803043 \t -0.0021542991380423897\n",
      "81     \t [0.97525659 0.98291757]. \t  -0.10168630604995428 \t -0.0021542991380423897\n",
      "82     \t [1.4445271  2.04798077]. \t  -0.347201251390322 \t -0.0021542991380423897\n",
      "83     \t [1.17800016 1.36520871]. \t  -0.08219965515088404 \t -0.0021542991380423897\n",
      "84     \t [0.9512592  0.89009216]. \t  -0.024285276923990406 \t -0.0021542991380423897\n",
      "85     \t [0.96682521 0.90466486]. \t  -0.09161800924775552 \t -0.0021542991380423897\n",
      "86     \t [0.89428718 0.78003866]. \t  -0.050027172622709684 \t -0.0021542991380423897\n",
      "87     \t [0.88616526 0.79825608]. \t  -0.029773189106116486 \t -0.0021542991380423897\n",
      "88     \t [1.22272913 1.4598754 ]. \t  -0.17344975151168524 \t -0.0021542991380423897\n",
      "89     \t [1.37909449 1.90142518]. \t  -0.1437353323953796 \t -0.0021542991380423897\n",
      "90     \t [-0.4451329 -2.048    ]. \t  -506.60438271323824 \t -0.0021542991380423897\n",
      "91     \t [0.98415499 0.98231491]. \t  -0.019167969680712393 \t -0.0021542991380423897\n",
      "92     \t [1.08307841 1.16743675]. \t  -0.010062821917795606 \t -0.0021542991380423897\n",
      "93     \t [1.23207737 1.48167626]. \t  -0.18590769377482197 \t -0.0021542991380423897\n",
      "94     \t [0.78843308 0.6420231 ]. \t  -0.08636177937284992 \t -0.0021542991380423897\n",
      "95     \t [0.78765917 0.6282211 ]. \t  -0.051194709950640405 \t -0.0021542991380423897\n",
      "96     \t [0.86492866 0.72198977]. \t  -0.08642698654038637 \t -0.0021542991380423897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [0.8832368  0.76596053]. \t  -0.03364659184886295 \t -0.0021542991380423897\n",
      "98     \t [0.95891626 0.88868863]. \t  -0.09674763656541835 \t -0.0021542991380423897\n",
      "99     \t [0.83874822 0.67230098]. \t  -0.1233311243936223 \t -0.0021542991380423897\n",
      "100    \t [1.03966417 1.05722436]. \t  -0.05763436514377697 \t -0.0021542991380423897\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \n",
    "\n",
    "np.random.seed(run_num_9)\n",
    "surrogate_winner_9 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_9 = GPGO(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_9.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.62910294 -1.57693156]. \t  -389.29291138113445 \t -8.580376531587937\n",
      "init   \t [ 1.84435861 -0.07294402]. \t  -1207.999341247548 \t -8.580376531587937\n",
      "init   \t [ 1.5256557  -1.17828534]. \t  -1229.4172568797705 \t -8.580376531587937\n",
      "init   \t [-1.88125338 -0.42109149]. \t  -1576.6245828972787 \t -8.580376531587937\n",
      "init   \t [-1.09309052  1.39977001]. \t  -8.580376531587937 \t -8.580376531587937\n",
      "1      \t [0.28236156 2.048     ]. \t  -387.92445174082155 \t -8.580376531587937\n",
      "2      \t [-2.048  2.048]. \t  -469.9523900415999 \t -8.580376531587937\n",
      "3      \t [-0.54449946 -2.048     ]. \t  -552.0439694146817 \t -8.580376531587937\n",
      "4      \t [2.048 2.048]. \t  -461.7603900415999 \t -8.580376531587937\n",
      "5      \t [-0.13348148  0.63226755]. \t  -39.039690810996696 \t -8.580376531587937\n",
      "6      \t [-0.79901125  2.048     ]. \t  -201.92830584391655 \t -8.580376531587937\n",
      "7      \t [-0.51117984  1.17490439]. \t  -85.75008014510576 \t -8.580376531587937\n",
      "8      \t [-0.00831908 -0.48702543]. \t  -24.7428258131562 \t -8.580376531587937\n",
      "9      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -8.580376531587937\n",
      "10     \t [-0.79614966  0.31568659]. \t  -13.349221476876368 \t -8.580376531587937\n",
      "11     \t [-2.048       0.88122681]. \t  -1106.9383475412005 \t -8.580376531587937\n",
      "12     \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -8.580376531587937\n",
      "13     \t [ 0.73331163 -0.48552561]. \t  -104.77959152740026 \t -8.580376531587937\n",
      "14     \t [1.10555355 1.29397949]. \t  \u001b[92m-0.5256728186046916\u001b[0m \t -0.5256728186046916\n",
      "15     \t [1.20298937 2.048     ]. \t  -36.1392597992779 \t -0.5256728186046916\n",
      "16     \t [ 0.00388126 -1.26679461]. \t  -161.47292783174586 \t -0.5256728186046916\n",
      "17     \t [ 0.07789514 -2.048     ]. \t  -422.76966941204586 \t -0.5256728186046916\n",
      "18     \t [2.048      1.19268884]. \t  -902.0676592319492 \t -0.5256728186046916\n",
      "19     \t [0.81688797 1.50518842]. \t  -70.23823161230219 \t -0.5256728186046916\n",
      "20     \t [0.75371662 0.61583002]. \t  \u001b[92m-0.28857851062883444\u001b[0m \t -0.28857851062883444\n",
      "21     \t [-0.72057669 -0.6069651 ]. \t  -129.79209796406573 \t -0.28857851062883444\n",
      "22     \t [-0.42821692  0.00107308]. \t  -5.363010438553201 \t -0.28857851062883444\n",
      "23     \t [-0.84312578  0.88340778]. \t  -6.3743492047393975 \t -0.28857851062883444\n",
      "24     \t [0.77008654 0.95702244]. \t  -13.3016702957796 \t -0.28857851062883444\n",
      "25     \t [0.33640065 0.20606475]. \t  -1.3033929571444332 \t -0.28857851062883444\n",
      "26     \t [-1.39303233  2.048     ]. \t  -6.88138859504863 \t -0.28857851062883444\n",
      "27     \t [1.28973466 1.70116278]. \t  \u001b[92m-0.22643189336888145\u001b[0m \t -0.22643189336888145\n",
      "28     \t [-1.24080446  1.80501847]. \t  -12.066128689159441 \t -0.22643189336888145\n",
      "29     \t [ 0.4569329  -0.91792506]. \t  -127.24308103889277 \t -0.22643189336888145\n",
      "30     \t [0.54107174 0.45200049]. \t  -2.7464121430547466 \t -0.22643189336888145\n",
      "31     \t [1.48312872 2.048     ]. \t  -2.533816969707869 \t -0.22643189336888145\n",
      "32     \t [-1.45318436  1.5589624 ]. \t  -36.57495125897168 \t -0.22643189336888145\n",
      "33     \t [-1.14852372  0.83926127]. \t  -27.641321115701846 \t -0.22643189336888145\n",
      "34     \t [-0.94661933 -1.39960327]. \t  -530.8092405074501 \t -0.22643189336888145\n",
      "35     \t [ 0.99073328 -2.048     ]. \t  -917.8188839810192 \t -0.22643189336888145\n",
      "36     \t [-0.51942198  0.33922722]. \t  -2.7906682884186647 \t -0.22643189336888145\n",
      "37     \t [1.40889623 2.048     ]. \t  -0.5642399201373143 \t -0.22643189336888145\n",
      "38     \t [-0.4441541 -1.0799876]. \t  -165.22501030522653 \t -0.22643189336888145\n",
      "39     \t [0.4997552  0.04697068]. \t  -4.362403446789425 \t -0.22643189336888145\n",
      "40     \t [-2.048      -1.18941771]. \t  -2907.736246813871 \t -0.22643189336888145\n",
      "41     \t [ 2.048      -0.82342426]. \t  -2518.8579889535 \t -0.22643189336888145\n",
      "42     \t [ 0.02666437 -0.0289854 ]. \t  -1.0355697892312794 \t -0.22643189336888145\n",
      "43     \t [1.12551762 0.94356678]. \t  -10.463073608077764 \t -0.22643189336888145\n",
      "44     \t [0.71546567 0.40272644]. \t  -1.2726524061511966 \t -0.22643189336888145\n",
      "45     \t [-1.22487473  1.30928819]. \t  -8.599310803258412 \t -0.22643189336888145\n",
      "46     \t [-1.23939618 -2.048     ]. \t  -1289.5942459404691 \t -0.22643189336888145\n",
      "47     \t [1.28969772 1.75677562]. \t  -0.9573161690793959 \t -0.22643189336888145\n",
      "48     \t [1.3677301  1.91268438]. \t  -0.3116148736233677 \t -0.22643189336888145\n",
      "49     \t [0.99621325 0.9759489 ]. \t  \u001b[92m-0.027212720838110355\u001b[0m \t -0.027212720838110355\n",
      "50     \t [0.71477806 0.49157349]. \t  -0.11873262125536047 \t -0.027212720838110355\n",
      "51     \t [1.18340556 1.41972595]. \t  -0.07079877113688031 \t -0.027212720838110355\n",
      "52     \t [0.99398976 0.98739765]. \t  \u001b[92m-7.431532463798308e-05\u001b[0m \t -7.431532463798308e-05\n",
      "53     \t [0.9265138  0.83932387]. \t  -0.0418963372683776 \t -7.431532463798308e-05\n",
      "54     \t [0.69528176 0.47540593]. \t  -0.09927049888919665 \t -7.431532463798308e-05\n",
      "55     \t [0.23048924 0.07053262]. \t  -0.6224483332413009 \t -7.431532463798308e-05\n",
      "56     \t [1.11490019 1.25882541]. \t  -0.03823870450766435 \t -7.431532463798308e-05\n",
      "57     \t [0.87465272 0.77202533]. \t  -0.020623062822287493 \t -7.431532463798308e-05\n",
      "58     \t [0.68989719 0.47116395]. \t  -0.09846217569711073 \t -7.431532463798308e-05\n",
      "59     \t [0.81796659 0.64247126]. \t  -0.10388197557743298 \t -7.431532463798308e-05\n",
      "60     \t [1.3689561 1.9113963]. \t  -0.27567196933328936 \t -7.431532463798308e-05\n",
      "61     \t [-1.43076805  1.90044055]. \t  -8.05945134790211 \t -7.431532463798308e-05\n",
      "62     \t [1.4153968 2.048    ]. \t  -0.3719336342231504 \t -7.431532463798308e-05\n",
      "63     \t [0.91077507 0.8121244 ]. \t  -0.03819126130681627 \t -7.431532463798308e-05\n",
      "64     \t [0.91388574 0.87655893]. \t  -0.17857812384788313 \t -7.431532463798308e-05\n",
      "65     \t [1.41511627 2.04799903]. \t  -0.37884608962557165 \t -7.431532463798308e-05\n",
      "66     \t [0.8983428  0.80668827]. \t  -0.010345176529770697 \t -7.431532463798308e-05\n",
      "67     \t [1.08865651 1.22581122]. \t  -0.1730065102681037 \t -7.431532463798308e-05\n",
      "68     \t [1.02448661 1.05726468]. \t  -0.0065160705383762365 \t -7.431532463798308e-05\n",
      "69     \t [0.51974109 0.2658499 ]. \t  -0.23248122823985984 \t -7.431532463798308e-05\n",
      "70     \t [1.15990786 1.3694144 ]. \t  -0.08330576314684496 \t -7.431532463798308e-05\n",
      "71     \t [0.74222152 0.52200002]. \t  -0.14992893581291578 \t -7.431532463798308e-05\n",
      "72     \t [0.55409611 0.32595356]. \t  -0.23466879729326148 \t -7.431532463798308e-05\n",
      "73     \t [1.13514695 1.31824747]. \t  -0.10640763850360463 \t -7.431532463798308e-05\n",
      "74     \t [0.66742644 0.42371593]. \t  -0.15787718284439484 \t -7.431532463798308e-05\n",
      "75     \t [1.15955171 1.38982366]. \t  -0.23033501953743302 \t -7.431532463798308e-05\n",
      "76     \t [ 0.14942348 -0.00469611]. \t  -0.796507294911936 \t -7.431532463798308e-05\n",
      "77     \t [1.03019696 1.06277778]. \t  -0.0011285338415212753 \t -7.431532463798308e-05\n",
      "78     \t [0.86407134 0.7229448 ]. \t  -0.07452471555804466 \t -7.431532463798308e-05\n",
      "79     \t [1.08989644 1.2172482 ]. \t  -0.09436425664631541 \t -7.431532463798308e-05\n",
      "80     \t [0.81744983 0.63383604]. \t  -0.15157935458386976 \t -7.431532463798308e-05\n",
      "81     \t [0.92915366 0.87510113]. \t  -0.018883348507709945 \t -7.431532463798308e-05\n",
      "82     \t [1.01318874 1.02500493]. \t  -0.0004131066084014707 \t -7.431532463798308e-05\n",
      "83     \t [0.86030666 0.74169366]. \t  -0.019759501281731928 \t -7.431532463798308e-05\n",
      "84     \t [0.93293501 0.90391066]. \t  -0.11701051986194459 \t -7.431532463798308e-05\n",
      "85     \t [0.71317557 0.4864473 ]. \t  -0.13142844291690123 \t -7.431532463798308e-05\n",
      "86     \t [1.41539286 2.048     ]. \t  -0.3720300585797115 \t -7.431532463798308e-05\n",
      "87     \t [0.92272749 0.86423037]. \t  -0.02236620124863568 \t -7.431532463798308e-05\n",
      "88     \t [1.09940539 1.22074925]. \t  -0.024418635368187384 \t -7.431532463798308e-05\n",
      "89     \t [0.7392988  0.53871283]. \t  -0.07412719151393302 \t -7.431532463798308e-05\n",
      "90     \t [1.01082264 1.0467674 ]. \t  -0.06264205804972182 \t -7.431532463798308e-05\n",
      "91     \t [0.72083875 0.50782753]. \t  -0.09181013903033493 \t -7.431532463798308e-05\n",
      "92     \t [0.4361904  0.20623261]. \t  -0.3433870728956371 \t -7.431532463798308e-05\n",
      "93     \t [0.58256255 0.33289561]. \t  -0.17845762797942366 \t -7.431532463798308e-05\n",
      "94     \t [0.65095127 0.40212367]. \t  -0.16855101501610376 \t -7.431532463798308e-05\n",
      "95     \t [0.80416535 0.70592563]. \t  -0.38933312162127764 \t -7.431532463798308e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [0.70079851 0.47927926]. \t  -0.10353843209702612 \t -7.431532463798308e-05\n",
      "97     \t [0.51572994 0.25559286]. \t  -0.245301295158225 \t -7.431532463798308e-05\n",
      "98     \t [0.99226038 1.00897933]. \t  -0.059589467811990336 \t -7.431532463798308e-05\n",
      "99     \t [0.985173   0.91871099]. \t  -0.2691123524806223 \t -7.431532463798308e-05\n",
      "100    \t [0.84462281 0.68092495]. \t  -0.12952504862496467 \t -7.431532463798308e-05\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \n",
    "\n",
    "np.random.seed(run_num_10)\n",
    "surrogate_winner_10 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_10 = GPGO(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_10.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [1.97032207 1.31583119]. \t  -659.5505379977991 \t -45.00854175284071\n",
      "init   \t [ 0.59758024 -0.3125739 ]. \t  -45.00854175284071 \t -45.00854175284071\n",
      "init   \t [-1.21933423 -0.03314987]. \t  -235.94289844157058 \t -45.00854175284071\n",
      "init   \t [-1.48046507 -0.19447384]. \t  -575.5719351651762 \t -45.00854175284071\n",
      "init   \t [-1.58214297 -2.0360213 ]. \t  -2067.0989998073865 \t -45.00854175284071\n",
      "1      \t [-0.66465003  1.26538735]. \t  -70.60731635160307 \t -45.00854175284071\n",
      "2      \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -45.00854175284071\n",
      "3      \t [-2.048  2.048]. \t  -469.9523900415999 \t -45.00854175284071\n",
      "4      \t [-0.00749175  0.40950805]. \t  \u001b[92m-17.780127375638536\u001b[0m \t -17.780127375638536\n",
      "5      \t [0.48859746 2.048     ]. \t  -327.60823977182605 \t -17.780127375638536\n",
      "6      \t [-0.16418368 -0.74859751]. \t  -61.50369140489272 \t -17.780127375638536\n",
      "7      \t [-2.048       0.86068517]. \t  -1120.5917552528663 \t -17.780127375638536\n",
      "8      \t [2.048      0.06555541]. \t  -1705.7547975982973 \t -17.780127375638536\n",
      "9      \t [-0.79729916  2.048     ]. \t  -202.6933835701079 \t -17.780127375638536\n",
      "10     \t [2.048 2.048]. \t  -461.7603900415999 \t -17.780127375638536\n",
      "11     \t [ 0.14186794 -2.048     ]. \t  -428.45111824793906 \t -17.780127375638536\n",
      "12     \t [-0.47929137  0.1334501 ]. \t  \u001b[92m-3.1150964723520573\u001b[0m \t -3.1150964723520573\n",
      "13     \t [0.85811185 1.04599177]. \t  -9.607566665897995 \t -3.1150964723520573\n",
      "14     \t [ 0.39555038 -1.08383388]. \t  -154.1982765723566 \t -3.1150964723520573\n",
      "15     \t [0.17703178 1.2823358 ]. \t  -157.17626255257233 \t -3.1150964723520573\n",
      "16     \t [0.69630279 0.4784467 ]. \t  \u001b[92m-0.09631632826586743\u001b[0m \t -0.09631632826586743\n",
      "17     \t [-0.84187865  0.60361937]. \t  -4.497965060988889 \t -0.09631632826586743\n",
      "18     \t [ 0.1880384  -0.31567749]. \t  -12.981903960182537 \t -0.09631632826586743\n",
      "19     \t [1.29274902 2.048     ]. \t  -14.283524299402474 \t -0.09631632826586743\n",
      "20     \t [-1.31682833  1.77348943]. \t  -5.523344119851874 \t -0.09631632826586743\n",
      "21     \t [-1.33450094  2.048     ]. \t  -12.584522711505226 \t -0.09631632826586743\n",
      "22     \t [1.08628318 1.67629856]. \t  -24.637565362997435 \t -0.09631632826586743\n",
      "23     \t [0.50574864 0.67547684]. \t  -17.858686904990126 \t -0.09631632826586743\n",
      "24     \t [-0.23707268 -1.57427801]. \t  -267.37732875599687 \t -0.09631632826586743\n",
      "25     \t [-0.61737493  0.43599827]. \t  -2.9167151172017536 \t -0.09631632826586743\n",
      "26     \t [-0.97925321 -0.90806313]. \t  -352.48633201060073 \t -0.09631632826586743\n",
      "27     \t [0.42419699 0.11302991]. \t  -0.779286401333513 \t -0.09631632826586743\n",
      "28     \t [-1.12611817  1.27679109]. \t  -4.527858926601309 \t -0.09631632826586743\n",
      "29     \t [-2.048     -1.3575599]. \t  -3091.609576780293 \t -0.09631632826586743\n",
      "30     \t [-0.580367 -2.048   ]. \t  -571.2369965208718 \t -0.09631632826586743\n",
      "31     \t [ 0.93161828 -2.048     ]. \t  -850.2593125306798 \t -0.09631632826586743\n",
      "32     \t [1.38516442 1.63418882]. \t  -8.24190143096325 \t -0.09631632826586743\n",
      "33     \t [ 1.66015807 -0.99743158]. \t  -1409.3543554453577 \t -0.09631632826586743\n",
      "34     \t [1.13003133 1.12702409]. \t  -2.265310137704041 \t -0.09631632826586743\n",
      "35     \t [-1.14514767  1.64206897]. \t  -15.53829076751901 \t -0.09631632826586743\n",
      "36     \t [-0.82004007  0.84761106]. \t  -6.38013494855667 \t -0.09631632826586743\n",
      "37     \t [1.30967525 1.77277696]. \t  -0.42684233170682273 \t -0.09631632826586743\n",
      "38     \t [1.04664311 1.21692043]. \t  -1.4773954213169354 \t -0.09631632826586743\n",
      "39     \t [0.88838826 0.82416635]. \t  -0.13448622284801592 \t -0.09631632826586743\n",
      "40     \t [ 0.02283414 -0.00402187]. \t  -0.9569172422950212 \t -0.09631632826586743\n",
      "41     \t [1.25750625 1.66591111]. \t  -0.7818417333028779 \t -0.09631632826586743\n",
      "42     \t [ 1.01147756 -1.3729438 ]. \t  -574.0964169996784 \t -0.09631632826586743\n",
      "43     \t [1.21514489 1.46505347]. \t  \u001b[92m-0.059566718853529856\u001b[0m \t -0.059566718853529856\n",
      "44     \t [1.518821 2.048   ]. \t  -6.967811574921519 \t -0.059566718853529856\n",
      "45     \t [-1.50979439  2.048     ]. \t  -11.657325598381929 \t -0.059566718853529856\n",
      "46     \t [0.32569857 0.15063614]. \t  -0.653211325595008 \t -0.059566718853529856\n",
      "47     \t [-2.048       0.09609572]. \t  -1688.821411366932 \t -0.059566718853529856\n",
      "48     \t [1.21683012 0.13311066]. \t  -181.64012557930442 \t -0.059566718853529856\n",
      "49     \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -0.059566718853529856\n",
      "50     \t [ 0.48058473 -1.69741552]. \t  -372.1336548809944 \t -0.059566718853529856\n",
      "51     \t [0.73263982 0.48892134]. \t  -0.3003458310645143 \t -0.059566718853529856\n",
      "52     \t [1.31634744 1.69583652]. \t  -0.2364881854824932 \t -0.059566718853529856\n",
      "53     \t [1.13136659 1.27008787]. \t  \u001b[92m-0.02706310369157111\u001b[0m \t -0.02706310369157111\n",
      "54     \t [1.41209753 2.0035458 ]. \t  -0.1788995281956901 \t -0.02706310369157111\n",
      "55     \t [1.4442295 2.048    ]. \t  -0.340215105653506 \t -0.02706310369157111\n",
      "56     \t [0.77722005 0.57273144]. \t  -0.1478478088153503 \t -0.02706310369157111\n",
      "57     \t [0.1369857  0.07623192]. \t  -1.0750373702767686 \t -0.02706310369157111\n",
      "58     \t [0.85790622 0.72924107]. \t  \u001b[92m-0.024763117963193526\u001b[0m \t -0.024763117963193526\n",
      "59     \t [1.33897945 1.81064717]. \t  -0.14652421232575605 \t -0.024763117963193526\n",
      "60     \t [0.92761219 0.88466484]. \t  -0.06380622718153994 \t -0.024763117963193526\n",
      "61     \t [1.19864154 1.44587924]. \t  -0.04780822972669397 \t -0.024763117963193526\n",
      "62     \t [1.41126807 2.03576093]. \t  -0.3634755998080804 \t -0.024763117963193526\n",
      "63     \t [1.25570265 1.58041037]. \t  -0.0666951719427166 \t -0.024763117963193526\n",
      "64     \t [-0.86720882 -1.56500497]. \t  -540.3613680147644 \t -0.024763117963193526\n",
      "65     \t [1.38530645 1.88614241]. \t  -0.2569098273430356 \t -0.024763117963193526\n",
      "66     \t [0.8658882  0.74681853]. \t  \u001b[92m-0.018852595211866243\u001b[0m \t -0.018852595211866243\n",
      "67     \t [0.69009365 0.45829684]. \t  -0.12819906490075966 \t -0.018852595211866243\n",
      "68     \t [0.95853142 0.91313094]. \t  \u001b[92m-0.004913638547392398\u001b[0m \t -0.004913638547392398\n",
      "69     \t [1.38875036 1.90440087]. \t  -0.20982013604078686 \t -0.004913638547392398\n",
      "70     \t [0.69073045 0.50141074]. \t  -0.1547072577710833 \t -0.004913638547392398\n",
      "71     \t [0.73480081 0.49096445]. \t  -0.31011493844956334 \t -0.004913638547392398\n",
      "72     \t [1.23528053 1.53957237]. \t  -0.07400113586258704 \t -0.004913638547392398\n",
      "73     \t [1.43740449 2.048     ]. \t  -0.22419847599273532 \t -0.004913638547392398\n",
      "74     \t [1.14551198 1.31755043]. \t  -0.02403890675410352 \t -0.004913638547392398\n",
      "75     \t [1.13027469 1.28507003]. \t  -0.022670470621979785 \t -0.004913638547392398\n",
      "76     \t [1.25480037 1.60500141]. \t  -0.15781074356676333 \t -0.004913638547392398\n",
      "77     \t [1.2115109 1.4908733]. \t  -0.09816547395133339 \t -0.004913638547392398\n",
      "78     \t [0.7568037  0.53715169]. \t  -0.18588153987946873 \t -0.004913638547392398\n",
      "79     \t [0.5874974  0.32290215]. \t  -0.21966933667668637 \t -0.004913638547392398\n",
      "80     \t [0.69415388 0.43782496]. \t  -0.2873588160320784 \t -0.004913638547392398\n",
      "81     \t [1.06478306 1.14294374]. \t  -0.012625488089716804 \t -0.004913638547392398\n",
      "82     \t [1.1512273  1.32962967]. \t  -0.024723323972262706 \t -0.004913638547392398\n",
      "83     \t [1.09989808 1.22669907]. \t  -0.03861935032365326 \t -0.004913638547392398\n",
      "84     \t [0.96417333 0.92385273]. \t  \u001b[92m-0.004621488953779099\u001b[0m \t -0.004621488953779099\n",
      "85     \t [0.93859127 0.83713352]. \t  -0.19579075244399227 \t -0.004621488953779099\n",
      "86     \t [0.82718501 0.68252594]. \t  -0.030157125076915654 \t -0.004621488953779099\n",
      "87     \t [1.4251691  2.04799455]. \t  -0.209287798871872 \t -0.004621488953779099\n",
      "88     \t [1.28083494 1.58337541]. \t  -0.4056261549357417 \t -0.004621488953779099\n",
      "89     \t [0.81004244 0.62466287]. \t  -0.13534596295469448 \t -0.004621488953779099\n",
      "90     \t [0.75746808 0.57594385]. \t  -0.059299570607204644 \t -0.004621488953779099\n",
      "91     \t [0.77065411 0.56572724]. \t  -0.13201369544726577 \t -0.004621488953779099\n",
      "92     \t [0.91417977 0.82233385]. \t  -0.02529648272641634 \t -0.004621488953779099\n",
      "93     \t [1.27603155 1.63494501]. \t  -0.08066700135961559 \t -0.004621488953779099\n",
      "94     \t [0.87506023 0.76476994]. \t  -0.015702194919404933 \t -0.004621488953779099\n",
      "95     \t [-0.00404947 -0.01965386]. \t  -1.0468072178503507 \t -0.004621488953779099\n",
      "96     \t [0.43303736 0.19834454]. \t  -0.33316076729052146 \t -0.004621488953779099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [0.78257143 0.58961738]. \t  -0.09926221824424765 \t -0.004621488953779099\n",
      "98     \t [0.21894805 0.05894289]. \t  -0.6221523539346598 \t -0.004621488953779099\n",
      "99     \t [1.43887676 2.048     ]. \t  -0.24263805586316367 \t -0.004621488953779099\n",
      "100    \t [0.13093239 0.00411587]. \t  -0.7722498756759665 \t -0.004621488953779099\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \n",
    "\n",
    "np.random.seed(run_num_11)\n",
    "surrogate_winner_11 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_11 = GPGO(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_11.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-1.26353633  0.50015753]. \t  -125.32555445527122 \t -19.52113145175031\n",
      "init   \t [-0.25506718  1.16882876]. \t  -123.40590278793995 \t -19.52113145175031\n",
      "init   \t [ 1.14678091 -0.93146069]. \t  -504.72793807789463 \t -19.52113145175031\n",
      "init   \t [-0.91560241  1.23646844]. \t  -19.52113145175031 \t -19.52113145175031\n",
      "init   \t [1.87653879 1.53982007]. \t  -393.43336554061835 \t -19.52113145175031\n",
      "1      \t [-2.048  2.048]. \t  -469.9523900415999 \t -19.52113145175031\n",
      "2      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -19.52113145175031\n",
      "3      \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -19.52113145175031\n",
      "4      \t [ 0.1269517 -0.439017 ]. \t  -21.476885343312116 \t -19.52113145175031\n",
      "5      \t [1.43660771 0.1888325 ]. \t  -351.7565823207636 \t -19.52113145175031\n",
      "6      \t [0.42180439 2.048     ]. \t  -350.05462457470617 \t -19.52113145175031\n",
      "7      \t [ 0.05536453 -2.048     ]. \t  -421.57919432585794 \t -19.52113145175031\n",
      "8      \t [-2.048       0.77482567]. \t  -1178.5735058721948 \t -19.52113145175031\n",
      "9      \t [-0.76867236  2.048     ]. \t  -215.45471748610504 \t -19.52113145175031\n",
      "10     \t [-0.58443061  0.31613009]. \t  \u001b[92m-2.575084009536273\u001b[0m \t -2.575084009536273\n",
      "11     \t [-0.61909152 -0.83351933]. \t  -150.68013350633782 \t -2.575084009536273\n",
      "12     \t [ 0.17703947 -1.13687652]. \t  -137.15094274714852 \t -2.575084009536273\n",
      "13     \t [2.048 2.048]. \t  -461.7603900415999 \t -2.575084009536273\n",
      "14     \t [ 2.048     -0.1305046]. \t  -1871.4952465673546 \t -2.575084009536273\n",
      "15     \t [0.84822934 0.82445139]. \t  \u001b[92m-1.1246602885743735\u001b[0m \t -1.1246602885743735\n",
      "16     \t [ 0.78453641 -0.16371794]. \t  -60.76407577238636 \t -1.1246602885743735\n",
      "17     \t [-0.90694067 -0.19304749]. \t  -106.77849862195427 \t -1.1246602885743735\n",
      "18     \t [1.14181881 1.4736311 ]. \t  -2.9060653148237643 \t -1.1246602885743735\n",
      "19     \t [1.19183202 1.00915958]. \t  -16.95389537829341 \t -1.1246602885743735\n",
      "20     \t [1.28632736 2.048     ]. \t  -15.555343330123844 \t -1.1246602885743735\n",
      "21     \t [-2.048      -0.44918167]. \t  -2165.4862232560963 \t -1.1246602885743735\n",
      "22     \t [-1.38410656  2.048     ]. \t  -7.43294470610676 \t -1.1246602885743735\n",
      "23     \t [-0.32752743 -0.52743481]. \t  -42.047883943776014 \t -1.1246602885743735\n",
      "24     \t [-0.61854218 -2.048     ]. \t  -593.3986052143619 \t -1.1246602885743735\n",
      "25     \t [-0.851134    0.71861793]. \t  -3.430074036033941 \t -1.1246602885743735\n",
      "26     \t [-1.33778373  1.62582377]. \t  -8.149637558860611 \t -1.1246602885743735\n",
      "27     \t [ 0.86101125 -2.048     ]. \t  -778.0612911148048 \t -1.1246602885743735\n",
      "28     \t [0.17923516 0.37083254]. \t  -12.145918039829958 \t -1.1246602885743735\n",
      "29     \t [-1.2111623   1.78956444]. \t  -15.29956263084538 \t -1.1246602885743735\n",
      "30     \t [-1.15903806  1.15566737]. \t  -8.184643949883899 \t -1.1246602885743735\n",
      "31     \t [0.88176489 0.47550765]. \t  -9.134480981114358 \t -1.1246602885743735\n",
      "32     \t [2.048      0.78940706]. \t  -1160.4306245056118 \t -1.1246602885743735\n",
      "33     \t [1.35769307 1.78280416]. \t  \u001b[92m-0.4942878719889602\u001b[0m \t -0.4942878719889602\n",
      "34     \t [ 0.55852572 -0.79657795]. \t  -123.07853803203818 \t -0.4942878719889602\n",
      "35     \t [-0.04361883 -0.04223998]. \t  -1.2839970251579924 \t -0.4942878719889602\n",
      "36     \t [1.26147102 1.76304328]. \t  -3.017628635393057 \t -0.4942878719889602\n",
      "37     \t [-0.79007861  0.32059677]. \t  -12.423343513888305 \t -0.4942878719889602\n",
      "38     \t [0.99836749 0.96664937]. \t  \u001b[92m-0.09053305829715477\u001b[0m \t -0.09053305829715477\n",
      "39     \t [1.26052593 1.53086697]. \t  -0.40495448863840855 \t -0.09053305829715477\n",
      "40     \t [-0.36029034 -1.53666801]. \t  -279.5649933211827 \t -0.09053305829715477\n",
      "41     \t [-1.28531432 -1.51980911]. \t  -1011.2808368938692 \t -0.09053305829715477\n",
      "42     \t [-0.23234933  0.14580229]. \t  -2.361704131196227 \t -0.09053305829715477\n",
      "43     \t [ 2.048      -1.18451793]. \t  -2894.270843913547 \t -0.09053305829715477\n",
      "44     \t [-2.048      -1.25371592]. \t  -2977.382410062803 \t -0.09053305829715477\n",
      "45     \t [1.45302702 2.048     ]. \t  -0.6057645094857407 \t -0.09053305829715477\n",
      "46     \t [ 0.6503046  -1.57187576]. \t  -398.0337532420957 \t -0.09053305829715477\n",
      "47     \t [1.0067019  1.04816396]. \t  -0.12055968804658823 \t -0.09053305829715477\n",
      "48     \t [1.00275562 1.00025021]. \t  \u001b[92m-0.0027834374847643477\u001b[0m \t -0.0027834374847643477\n",
      "49     \t [0.97850599 0.92501523]. \t  -0.10581904307963745 \t -0.0027834374847643477\n",
      "50     \t [1.18446811 1.37418651]. \t  -0.11684688443065952 \t -0.0027834374847643477\n",
      "51     \t [-1.17553482  1.39534778]. \t  -4.751084165936533 \t -0.0027834374847643477\n",
      "52     \t [1.26030481 1.57442912]. \t  -0.0871884520979103 \t -0.0027834374847643477\n",
      "53     \t [1.43894163 2.04799916]. \t  -0.24353730355686903 \t -0.0027834374847643477\n",
      "54     \t [-1.5542961  2.048    ]. \t  -20.05478696203792 \t -0.0027834374847643477\n",
      "55     \t [1.26680188 1.60317795]. \t  -0.07144214187203106 \t -0.0027834374847643477\n",
      "56     \t [-0.29839748  0.00950819]. \t  -2.318383733949334 \t -0.0027834374847643477\n",
      "57     \t [1.4171114  2.04757032]. \t  -0.3289468928177716 \t -0.0027834374847643477\n",
      "58     \t [0.92474014 0.83621789]. \t  -0.041485090127639 \t -0.0027834374847643477\n",
      "59     \t [1.08923331 1.24770123]. \t  -0.38338871182684287 \t -0.0027834374847643477\n",
      "60     \t [0.95167913 0.86930163]. \t  -0.134769237590747 \t -0.0027834374847643477\n",
      "61     \t [1.16541857 1.35033339]. \t  -0.03355234602034125 \t -0.0027834374847643477\n",
      "62     \t [0.99416961 0.99303535]. \t  \u001b[92m-0.00220754470493691\u001b[0m \t -0.00220754470493691\n",
      "63     \t [1.2024425  1.41694684]. \t  -0.12462607938376304 \t -0.00220754470493691\n",
      "64     \t [1.01970314 1.04122034]. \t  \u001b[92m-0.0005915190090261315\u001b[0m \t -0.0005915190090261315\n",
      "65     \t [1.31196979 1.672227  ]. \t  -0.33779496824977007 \t -0.0005915190090261315\n",
      "66     \t [0.84766961 0.66313134]. \t  -0.3302582951981658 \t -0.0005915190090261315\n",
      "67     \t [1.43471203 2.048     ]. \t  -0.19978764927822285 \t -0.0005915190090261315\n",
      "68     \t [0.95932657 0.93511119]. \t  -0.023569370633485513 \t -0.0005915190090261315\n",
      "69     \t [0.97718677 0.97829002]. \t  -0.05525787603742925 \t -0.0005915190090261315\n",
      "70     \t [0.73785873 0.60260999]. \t  -0.4071451624099248 \t -0.0005915190090261315\n",
      "71     \t [0.88758152 0.75624888]. \t  -0.1121912774275376 \t -0.0005915190090261315\n",
      "72     \t [1.37083625 1.88203984]. \t  -0.13833053001845225 \t -0.0005915190090261315\n",
      "73     \t [1.18638848 1.40579506]. \t  -0.035037393533105324 \t -0.0005915190090261315\n",
      "74     \t [1.29200634 1.66625638]. \t  -0.08618215770273947 \t -0.0005915190090261315\n",
      "75     \t [0.91732761 0.80510846]. \t  -0.1391959353587088 \t -0.0005915190090261315\n",
      "76     \t [1.14980288 1.33235866]. \t  -0.033074636557707045 \t -0.0005915190090261315\n",
      "77     \t [-0.16319293 -0.04094566]. \t  -1.80969096321807 \t -0.0005915190090261315\n",
      "78     \t [0.87514843 0.76087287]. \t  -0.018099832031098204 \t -0.0005915190090261315\n",
      "79     \t [1.22354304 1.49018539]. \t  -0.054694183127833106 \t -0.0005915190090261315\n",
      "80     \t [0.84723215 0.70522941]. \t  -0.0391458189256809 \t -0.0005915190090261315\n",
      "81     \t [1.32203546 1.71483384]. \t  -0.21223702708157008 \t -0.0005915190090261315\n",
      "82     \t [1.2075491  1.40725864]. \t  -0.3023225239607045 \t -0.0005915190090261315\n",
      "83     \t [0.89401251 0.83069925]. \t  -0.11008618555953033 \t -0.0005915190090261315\n",
      "84     \t [0.3643611 0.1706002]. \t  -0.5472323947789317 \t -0.0005915190090261315\n",
      "85     \t [1.40821278 1.98054748]. \t  -0.16727058445428808 \t -0.0005915190090261315\n",
      "86     \t [0.59075068 0.3705354 ]. \t  -0.21392106764824198 \t -0.0005915190090261315\n",
      "87     \t [0.95115313 0.90638088]. \t  -0.0026711546260692227 \t -0.0005915190090261315\n",
      "88     \t [0.80706004 0.65347198]. \t  -0.037677848755711445 \t -0.0005915190090261315\n",
      "89     \t [1.06686862 1.13359987]. \t  -0.006595489539783417 \t -0.0005915190090261315\n",
      "90     \t [1.01555546 1.0254904 ]. \t  -0.0036788532597339294 \t -0.0005915190090261315\n",
      "91     \t [0.81110723 0.64843134]. \t  -0.044636460774637274 \t -0.0005915190090261315\n",
      "92     \t [0.77754671 0.60149939]. \t  -0.05043379303132194 \t -0.0005915190090261315\n",
      "93     \t [0.78124323 0.60732887]. \t  -0.048761806768545086 \t -0.0005915190090261315\n",
      "94     \t [0.94263259 0.90175014]. \t  -0.020699049788295294 \t -0.0005915190090261315\n",
      "95     \t [1.38918894 1.95261974]. \t  -0.2033328123713055 \t -0.0005915190090261315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [1.06404256 1.12566173]. \t  -0.008358798632652344 \t -0.0005915190090261315\n",
      "97     \t [1.25593037 1.53762148]. \t  -0.22342410496569226 \t -0.0005915190090261315\n",
      "98     \t [1.25622301 1.57708813]. \t  -0.06575186377981443 \t -0.0005915190090261315\n",
      "99     \t [1.0698743  1.11851917]. \t  -0.07306527314683166 \t -0.0005915190090261315\n",
      "100    \t [0.59693961 0.33796437]. \t  -0.19621265531100684 \t -0.0005915190090261315\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\n",
    "\n",
    "np.random.seed(run_num_12)\n",
    "surrogate_winner_12 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_12 = GPGO(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_12.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.65799909 -0.35389173]. \t  -62.0309701572776 \t -62.0309701572776\n",
      "init   \t [ 0.68854808 -1.10615174]. \t  -249.81607110978882 \t -62.0309701572776\n",
      "init   \t [1.26025046 0.56040843]. \t  -105.7097019264073 \t -62.0309701572776\n",
      "init   \t [-1.34269398 -0.98145948]. \t  -780.713409392333 \t -62.0309701572776\n",
      "init   \t [ 1.70115068 -0.15230767]. \t  -928.4380405275942 \t -62.0309701572776\n",
      "1      \t [0.25012924 0.96841573]. \t  -82.61892654081512 \t -62.0309701572776\n",
      "2      \t [1.86282284 2.048     ]. \t  -202.9838499431739 \t -62.0309701572776\n",
      "3      \t [-2.048  2.048]. \t  -469.9523900415999 \t -62.0309701572776\n",
      "4      \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -62.0309701572776\n",
      "5      \t [-0.28215227 -2.048     ]. \t  -454.31630420646803 \t -62.0309701572776\n",
      "6      \t [-2.048      0.5035011]. \t  -1371.492909412917 \t -62.0309701572776\n",
      "7      \t [0.30083592 2.048     ]. \t  -383.66857368086596 \t -62.0309701572776\n",
      "8      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -62.0309701572776\n",
      "9      \t [-0.27773416 -0.7418218 ]. \t  -68.7018358606437 \t -62.0309701572776\n",
      "10     \t [-0.88126026  2.048     ]. \t  -165.1799389335729 \t -62.0309701572776\n",
      "11     \t [2.048      1.15813851]. \t  -922.9283942931684 \t -62.0309701572776\n",
      "12     \t [-0.80188168  0.17452676]. \t  \u001b[92m-25.19482895120515\u001b[0m \t -25.19482895120515\n",
      "13     \t [-0.84972458  1.16075217]. \t  \u001b[92m-22.66903179806273\u001b[0m \t -22.66903179806273\n",
      "14     \t [1.10110333 1.57251346]. \t  \u001b[92m-12.976337206967356\u001b[0m \t -12.976337206967356\n",
      "15     \t [ 0.38909408 -2.048     ]. \t  -484.10669128048096 \t -12.976337206967356\n",
      "16     \t [1.21932104 2.048     ]. \t  -31.548952777652374 \t -12.976337206967356\n",
      "17     \t [0.79941585 0.69341989]. \t  \u001b[92m-0.33567179128476365\u001b[0m \t -0.33567179128476365\n",
      "18     \t [ 0.08052851 -1.2970919 ]. \t  -170.7766605756672 \t -0.33567179128476365\n",
      "19     \t [-0.32324706  0.40643011]. \t  -10.867846391492488 \t -0.33567179128476365\n",
      "20     \t [-0.49812011  1.25603982]. \t  -103.83386405341051 \t -0.33567179128476365\n",
      "21     \t [-1.31160214  1.69891061]. \t  -5.389255730513989 \t -0.33567179128476365\n",
      "22     \t [-1.39308007  2.048     ]. \t  -6.878760679906267 \t -0.33567179128476365\n",
      "23     \t [-2.048      -0.53263172]. \t  -2243.68243512414 \t -0.33567179128476365\n",
      "24     \t [-0.74090544 -0.58464607]. \t  -131.5326881206104 \t -0.33567179128476365\n",
      "25     \t [0.93423032 1.09502618]. \t  -4.943382450550217 \t -0.33567179128476365\n",
      "26     \t [ 0.14656758 -0.28285567]. \t  -9.99049192782985 \t -0.33567179128476365\n",
      "27     \t [-0.72669998  0.59928272]. \t  -3.4882924925019205 \t -0.33567179128476365\n",
      "28     \t [0.77250277 1.5468072 ]. \t  -90.31062470383644 \t -0.33567179128476365\n",
      "29     \t [ 0.29139458 -0.71838666]. \t  -65.03080260586438 \t -0.33567179128476365\n",
      "30     \t [-0.72601134 -1.44702186]. \t  -392.6918541793305 \t -0.33567179128476365\n",
      "31     \t [-0.37819587 -0.0248753 ]. \t  -4.718713985420786 \t -0.33567179128476365\n",
      "32     \t [-1.21916694  1.0311991 ]. \t  -25.64257813676997 \t -0.33567179128476365\n",
      "33     \t [0.30683257 0.14961219]. \t  -0.7881284286756383 \t -0.33567179128476365\n",
      "34     \t [ 2.048      -1.05301905]. \t  -2754.5382242946694 \t -0.33567179128476365\n",
      "35     \t [1.46125697 2.048     ]. \t  -0.9743968544273565 \t -0.33567179128476365\n",
      "36     \t [-0.03669973  0.04569144]. \t  -1.2713903768240014 \t -0.33567179128476365\n",
      "37     \t [0.72917238 0.30192831]. \t  -5.352499523539196 \t -0.33567179128476365\n",
      "38     \t [-2.048       1.39112082]. \t  -795.073900837429 \t -0.33567179128476365\n",
      "39     \t [-1.1569966   1.42132601]. \t  -5.336313368711879 \t -0.33567179128476365\n",
      "40     \t [-1.10247257 -2.048     ]. \t  -1069.4282144979204 \t -0.33567179128476365\n",
      "41     \t [ 1.13229639 -2.048     ]. \t  -1108.9708503718584 \t -0.33567179128476365\n",
      "42     \t [1.02463117 0.97044236]. \t  -0.6314663244035994 \t -0.33567179128476365\n",
      "43     \t [1.36278616 1.88699176]. \t  \u001b[92m-0.22045135806372002\u001b[0m \t -0.22045135806372002\n",
      "44     \t [-1.20959455  1.7708009 ]. \t  -14.349124899292555 \t -0.22045135806372002\n",
      "45     \t [2.048      0.30246389]. \t  -1515.7402449499334 \t -0.22045135806372002\n",
      "46     \t [0.82750554 0.65732266]. \t  \u001b[92m-0.10506483082667936\u001b[0m \t -0.10506483082667936\n",
      "47     \t [1.25464532 1.46278585]. \t  -1.3047048564191401 \t -0.10506483082667936\n",
      "48     \t [1.17598539 1.38019334]. \t  \u001b[92m-0.03172617592651884\u001b[0m \t -0.03172617592651884\n",
      "49     \t [1.12445565 1.26684508]. \t  \u001b[92m-0.01608680193277684\u001b[0m \t -0.01608680193277684\n",
      "50     \t [0.99154583 0.94236256]. \t  -0.16654007055423986 \t -0.01608680193277684\n",
      "51     \t [1.36981151 1.88204538]. \t  -0.13996615828572379 \t -0.01608680193277684\n",
      "52     \t [1.40974249 2.048     ]. \t  -0.5354413052047818 \t -0.01608680193277684\n",
      "53     \t [0.03123355 0.01579223]. \t  -0.9604618783374796 \t -0.01608680193277684\n",
      "54     \t [-0.5071881   0.26193871]. \t  -2.2738239790662305 \t -0.01608680193277684\n",
      "55     \t [0.87405784 0.72374857]. \t  -0.17769503320285363 \t -0.01608680193277684\n",
      "56     \t [1.10237662 1.23199833]. \t  -0.03858453758467204 \t -0.01608680193277684\n",
      "57     \t [1.31531588 1.7119495 ]. \t  -0.13220815993473573 \t -0.01608680193277684\n",
      "58     \t [1.04803576 1.09094406]. \t  \u001b[92m-0.007835200971567704\u001b[0m \t -0.007835200971567704\n",
      "59     \t [1.37757028 1.9221221 ]. \t  -0.20220385122681994 \t -0.007835200971567704\n",
      "60     \t [1.02667718 1.06361964]. \t  -0.009838808168734791 \t -0.007835200971567704\n",
      "61     \t [1.14826974 1.33178772]. \t  -0.03957814553715962 \t -0.007835200971567704\n",
      "62     \t [0.81574146 0.66575125]. \t  -0.03396126722417688 \t -0.007835200971567704\n",
      "63     \t [1.30026853 1.70013604]. \t  -0.09906839640382627 \t -0.007835200971567704\n",
      "64     \t [1.20897309 1.48267031]. \t  -0.08799843647689301 \t -0.007835200971567704\n",
      "65     \t [0.89220943 0.76108871]. \t  -0.13376182598105998 \t -0.007835200971567704\n",
      "66     \t [1.14749189 1.32416939]. \t  -0.027276959931268056 \t -0.007835200971567704\n",
      "67     \t [-0.01677107  0.00769969]. \t  -1.0393267039539247 \t -0.007835200971567704\n",
      "68     \t [0.93644952 0.86034755]. \t  -0.03156197876922888 \t -0.007835200971567704\n",
      "69     \t [0.85666443 0.7228473 ]. \t  -0.03270378640471279 \t -0.007835200971567704\n",
      "70     \t [1.24452651 1.55543582]. \t  -0.06413549740011533 \t -0.007835200971567704\n",
      "71     \t [1.05032934 1.08590027]. \t  -0.032432520616589945 \t -0.007835200971567704\n",
      "72     \t [0.95243945 0.88877656]. \t  -0.03598689467131538 \t -0.007835200971567704\n",
      "73     \t [0.79854442 0.6355508 ]. \t  -0.04103480557621734 \t -0.007835200971567704\n",
      "74     \t [1.24113674 1.58521748]. \t  -0.25882458891331334 \t -0.007835200971567704\n",
      "75     \t [1.24081099 1.5565031 ]. \t  -0.08652118845467885 \t -0.007835200971567704\n",
      "76     \t [1.12410825 1.28799731]. \t  -0.0748313096775569 \t -0.007835200971567704\n",
      "77     \t [0.95946354 0.92152517]. \t  \u001b[92m-0.0017343851760034874\u001b[0m \t -0.0017343851760034874\n",
      "78     \t [0.93861748 0.84861727]. \t  -0.10864988775907772 \t -0.0017343851760034874\n",
      "79     \t [0.96421229 0.92401686]. \t  -0.004516641007913384 \t -0.0017343851760034874\n",
      "80     \t [1.43507706 2.04799968]. \t  -0.2023942609989698 \t -0.0017343851760034874\n",
      "81     \t [1.3719773  1.84408919]. \t  -0.2845396452406148 \t -0.0017343851760034874\n",
      "82     \t [1.099743   1.21029986]. \t  -0.010023522509316448 \t -0.0017343851760034874\n",
      "83     \t [1.30838489 1.7383114 ]. \t  -0.16501065661455 \t -0.0017343851760034874\n",
      "84     \t [1.43017611 2.04770479]. \t  -0.18558098458525224 \t -0.0017343851760034874\n",
      "85     \t [0.9998685  1.02815802]. \t  -0.08077540077743019 \t -0.0017343851760034874\n",
      "86     \t [1.25947997 1.53676821]. \t  -0.3125685417608862 \t -0.0017343851760034874\n",
      "87     \t [1.14347759 1.33084842]. \t  -0.07490943016760221 \t -0.0017343851760034874\n",
      "88     \t [0.92099072 0.83124719]. \t  -0.03506337105983359 \t -0.0017343851760034874\n",
      "89     \t [0.9318996  0.86189744]. \t  -0.008914060468888227 \t -0.0017343851760034874\n",
      "90     \t [0.90442594 0.82707097]. \t  -0.017387558131411966 \t -0.0017343851760034874\n",
      "91     \t [-2.048      -1.34245621]. \t  -3074.861661108655 \t -0.0017343851760034874\n",
      "92     \t [1.06458687 1.12425307]. \t  -0.01243816832708276 \t -0.0017343851760034874\n",
      "93     \t [1.21108335 1.50113972]. \t  -0.16300803700569255 \t -0.0017343851760034874\n",
      "94     \t [1.148348   1.31067549]. \t  -0.028451434822816504 \t -0.0017343851760034874\n",
      "95     \t [1.05192025 1.08406641]. \t  -0.053184942887331436 \t -0.0017343851760034874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [1.21046203 1.49642971]. \t  -0.14170926471066314 \t -0.0017343851760034874\n",
      "97     \t [1.31112912 1.69758   ]. \t  -0.14293855544280087 \t -0.0017343851760034874\n",
      "98     \t [0.90912841 0.77471977]. \t  -0.2765266889799562 \t -0.0017343851760034874\n",
      "99     \t [1.0275188  1.04328329]. \t  -0.016411293445117516 \t -0.0017343851760034874\n",
      "100    \t [0.92025682 0.82763058]. \t  -0.04338458929634957 \t -0.0017343851760034874\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \n",
    "\n",
    "np.random.seed(run_num_13)\n",
    "surrogate_winner_13 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_13 = GPGO(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_13.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.6043691  0.02928512]. \t  -11.444570460525577 \t -4.306489127802793\n",
      "init   \t [0.11608629 1.6231842 ]. \t  -259.89734211112767 \t -4.306489127802793\n",
      "init   \t [0.81916392 0.87776093]. \t  -4.306489127802793 \t -4.306489127802793\n",
      "init   \t [ 0.89021801 -1.13533148]. \t  -371.66089065122344 \t -4.306489127802793\n",
      "init   \t [-1.33056707 -0.17677726]. \t  -384.58487399689074 \t -4.306489127802793\n",
      "1      \t [2.048      0.58414529]. \t  -1304.4228953030417 \t -4.306489127802793\n",
      "2      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -4.306489127802793\n",
      "3      \t [-2.048  2.048]. \t  -469.9523900415999 \t -4.306489127802793\n",
      "4      \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -4.306489127802793\n",
      "5      \t [1.58005535 2.048     ]. \t  -20.458409183467854 \t -4.306489127802793\n",
      "6      \t [-0.05496885 -1.13689199]. \t  -131.05325273811664 \t -4.306489127802793\n",
      "7      \t [-2.048       0.68682467]. \t  -1239.5314261595772 \t -4.306489127802793\n",
      "8      \t [-0.39266657  0.40854089]. \t  -8.40910874717278 \t -4.306489127802793\n",
      "9      \t [ 0.19006107 -2.048     ]. \t  -435.01295635033864 \t -4.306489127802793\n",
      "10     \t [-0.99198101  2.048     ]. \t  -117.17198670582633 \t -4.306489127802793\n",
      "11     \t [0.90018589 2.048     ]. \t  -153.1915202659922 \t -4.306489127802793\n",
      "12     \t [-0.17169376 -0.2776752 ]. \t  -10.80722134044388 \t -4.306489127802793\n",
      "13     \t [2.048 2.048]. \t  -461.7603900415999 \t -4.306489127802793\n",
      "14     \t [ 0.40242883 -0.72672546]. \t  -79.33131372346837 \t -4.306489127802793\n",
      "15     \t [0.24109163 0.62424938]. \t  -32.62560376503897 \t -4.306489127802793\n",
      "16     \t [-0.92498627  1.33363788]. \t  -26.557632122449135 \t -4.306489127802793\n",
      "17     \t [ 1.4775221  -0.41083765]. \t  -673.0645158650996 \t -4.306489127802793\n",
      "18     \t [1.26159234 1.59973279]. \t  \u001b[92m-0.07502004008597457\u001b[0m \t -0.07502004008597457\n",
      "19     \t [-0.68795476 -0.44201826]. \t  -86.62660202124063 \t -0.07502004008597457\n",
      "20     \t [-0.57631877 -2.048     ]. \t  -568.9930042411154 \t -0.07502004008597457\n",
      "21     \t [-0.60535167  1.4237342 ]. \t  -114.36200584389465 \t -0.07502004008597457\n",
      "22     \t [-0.82064459  0.20947444]. \t  -24.842778778083797 \t -0.07502004008597457\n",
      "23     \t [-1.29567839  1.73136745]. \t  -5.546657040136472 \t -0.07502004008597457\n",
      "24     \t [0.88014597 1.41932559]. \t  -41.57413247089759 \t -0.07502004008597457\n",
      "25     \t [-0.43185592  0.04069919]. \t  -4.175985294010568 \t -0.07502004008597457\n",
      "26     \t [-1.43586375  2.048     ]. \t  -5.952214080873607 \t -0.07502004008597457\n",
      "27     \t [1.35313104 1.94283696]. \t  -1.3762659896499667 \t -0.07502004008597457\n",
      "28     \t [-2.048      -0.74331528]. \t  -2447.2987189440337 \t -0.07502004008597457\n",
      "29     \t [0.21847629 0.07762582]. \t  -0.7001440164388446 \t -0.07502004008597457\n",
      "30     \t [1.11714331 1.20720712]. \t  -0.18020336319743951 \t -0.07502004008597457\n",
      "31     \t [ 2.048      -0.98565944]. \t  -2684.3004259227114 \t -0.07502004008597457\n",
      "32     \t [ 0.90683313 -2.048     ]. \t  -823.8974864700205 \t -0.07502004008597457\n",
      "33     \t [0.95324652 0.48231279]. \t  -18.180995146680736 \t -0.07502004008597457\n",
      "34     \t [-0.90697889 -1.39129177]. \t  -493.7729870020379 \t -0.07502004008597457\n",
      "35     \t [-1.25417162  1.86415189]. \t  -13.561350380639787 \t -0.07502004008597457\n",
      "36     \t [-1.12541287  0.87803745]. \t  -19.6119012487517 \t -0.07502004008597457\n",
      "37     \t [1.40605579 2.048     ]. \t  -0.6690825316900546 \t -0.07502004008597457\n",
      "38     \t [-0.10288486  0.05954755]. \t  -1.4560852214397175 \t -0.07502004008597457\n",
      "39     \t [ 0.43726193 -1.5318207 ]. \t  -297.19601520495365 \t -0.07502004008597457\n",
      "40     \t [-0.79107986  0.76699293]. \t  -5.2013040072821495 \t -0.07502004008597457\n",
      "41     \t [0.6491321  0.40996638]. \t  -0.13611820115947657 \t -0.07502004008597457\n",
      "42     \t [1.16731672 1.49676699]. \t  -1.8273130455268078 \t -0.07502004008597457\n",
      "43     \t [-2.048       1.42729525]. \t  -774.9240483199652 \t -0.07502004008597457\n",
      "44     \t [0.97404453 0.9839229 ]. \t  -0.12429729041461622 \t -0.07502004008597457\n",
      "45     \t [-1.25168452 -2.048     ]. \t  -1311.6859063903657 \t -0.07502004008597457\n",
      "46     \t [-1.20145709  1.3549263 ]. \t  -5.630927843971223 \t -0.07502004008597457\n",
      "47     \t [2.048      1.39491071]. \t  -784.7585828114064 \t -0.07502004008597457\n",
      "48     \t [0.90660552 0.80591434]. \t  \u001b[92m-0.034384113624159426\u001b[0m \t -0.034384113624159426\n",
      "49     \t [1.04668454 1.17225453]. \t  -0.5905604434727487 \t -0.034384113624159426\n",
      "50     \t [1.34313877 1.73949894]. \t  -0.5340635785538819 \t -0.034384113624159426\n",
      "51     \t [1.30888395 1.73974047]. \t  -0.16597009346647745 \t -0.034384113624159426\n",
      "52     \t [1.34335156 1.79879791]. \t  -0.12124909968390207 \t -0.034384113624159426\n",
      "53     \t [1.41703467 2.048     ]. \t  -0.3340199025671131 \t -0.034384113624159426\n",
      "54     \t [0.69383962 0.45052261]. \t  -0.1891584043317213 \t -0.034384113624159426\n",
      "55     \t [0.94605377 0.91699683]. \t  -0.05121821652114814 \t -0.034384113624159426\n",
      "56     \t [1.44287559 2.04236807]. \t  -0.3523367876548943 \t -0.034384113624159426\n",
      "57     \t [0.66261784 0.40862004]. \t  -0.20650043789379185 \t -0.034384113624159426\n",
      "58     \t [1.05411861 1.17494152]. \t  -0.4096598199578848 \t -0.034384113624159426\n",
      "59     \t [0.72692338 0.51473587]. \t  -0.09328982472529002 \t -0.034384113624159426\n",
      "60     \t [0.77041724 0.57958485]. \t  -0.07219046759945165 \t -0.034384113624159426\n",
      "61     \t [0.81034872 0.64888635]. \t  -0.042018425083540256 \t -0.034384113624159426\n",
      "62     \t [0.71502587 0.49919079]. \t  -0.09578165474981755 \t -0.034384113624159426\n",
      "63     \t [1.40898308 1.97859941]. \t  -0.17166803227059463 \t -0.034384113624159426\n",
      "64     \t [-0.22547978  0.04453971]. \t  -1.5057714938547373 \t -0.034384113624159426\n",
      "65     \t [0.83442332 0.68222994]. \t  -0.04710625924548768 \t -0.034384113624159426\n",
      "66     \t [0.87085309 0.7681512 ]. \t  \u001b[92m-0.026216560099116472\u001b[0m \t -0.026216560099116472\n",
      "67     \t [1.39997354 1.93158014]. \t  -0.2403271917793778 \t -0.026216560099116472\n",
      "68     \t [1.21706249 1.49894133]. \t  -0.07844594791360557 \t -0.026216560099116472\n",
      "69     \t [1.40192077 1.95179127]. \t  -0.18001065680745187 \t -0.026216560099116472\n",
      "70     \t [0.82935723 0.68064199]. \t  -0.034290616431366897 \t -0.026216560099116472\n",
      "71     \t [1.11867817 1.27228285]. \t  -0.05752340405293301 \t -0.026216560099116472\n",
      "72     \t [1.04259119 1.12939313]. \t  -0.18156237921447665 \t -0.026216560099116472\n",
      "73     \t [0.83828937 0.66943776]. \t  -0.13698146480564832 \t -0.026216560099116472\n",
      "74     \t [1.04450524 1.10745451]. \t  -0.029084815087996294 \t -0.026216560099116472\n",
      "75     \t [-0.103665   -0.00615787]. \t  -1.2466519951622037 \t -0.026216560099116472\n",
      "76     \t [1.30024654 1.69354562]. \t  -0.09099162704997707 \t -0.026216560099116472\n",
      "77     \t [1.033802   1.08770395]. \t  -0.03708076677604493 \t -0.026216560099116472\n",
      "78     \t [0.77240592 0.59831718]. \t  -0.052090206417226136 \t -0.026216560099116472\n",
      "79     \t [1.03424605 1.12680147]. \t  -0.3276315845292741 \t -0.026216560099116472\n",
      "80     \t [0.94831029 0.88567318]. \t  \u001b[92m-0.021220174224503112\u001b[0m \t -0.021220174224503112\n",
      "81     \t [1.37909488 1.87601685]. \t  -0.21072059846598742 \t -0.021220174224503112\n",
      "82     \t [0.75495416 0.53546794]. \t  -0.1789885436627302 \t -0.021220174224503112\n",
      "83     \t [1.02652167 1.0897814 ]. \t  -0.1305531571795682 \t -0.021220174224503112\n",
      "84     \t [1.38054018 1.87317247]. \t  -0.25186231101335 \t -0.021220174224503112\n",
      "85     \t [1.38753678 1.92259036]. \t  -0.1508965675555905 \t -0.021220174224503112\n",
      "86     \t [0.82932793 0.68029775]. \t  -0.0347345688441793 \t -0.021220174224503112\n",
      "87     \t [0.99432614 1.01251861]. \t  -0.05683876446688509 \t -0.021220174224503112\n",
      "88     \t [0.79619416 0.58346902]. \t  -0.2961187291653494 \t -0.021220174224503112\n",
      "89     \t [0.80309637 0.63161196]. \t  -0.05659815709153845 \t -0.021220174224503112\n",
      "90     \t [1.14535737 1.35926796]. \t  -0.24603672264348717 \t -0.021220174224503112\n",
      "91     \t [0.93642086 0.89016113]. \t  -0.02167045163915377 \t -0.021220174224503112\n",
      "92     \t [1.00328257 1.03939958]. \t  -0.10775006175800365 \t -0.021220174224503112\n",
      "93     \t [1.02430484 1.09316002]. \t  -0.19383548923721294 \t -0.021220174224503112\n",
      "94     \t [1.02334597 1.07503356]. \t  -0.07781006093979044 \t -0.021220174224503112\n",
      "95     \t [0.79317975 0.61087787]. \t  -0.07610369227547908 \t -0.021220174224503112\n",
      "96     \t [0.64161472 0.34838705]. \t  -0.5289061354781215 \t -0.021220174224503112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [0.70798548 0.49119707]. \t  -0.09536543655423504 \t -0.021220174224503112\n",
      "98     \t [1.09739694 1.22434522]. \t  -0.04974728057805382 \t -0.021220174224503112\n",
      "99     \t [0.77662933 0.55687649]. \t  -0.2640471509919797 \t -0.021220174224503112\n",
      "100    \t [0.99189589 1.04205859]. \t  -0.3388029170759022 \t -0.021220174224503112\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \n",
    "\n",
    "np.random.seed(run_num_14)\n",
    "surrogate_winner_14 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_14 = GPGO(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_14.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.31900409 -1.00146655]. \t  -122.17543449532224 \t -6.867717811955245\n",
      "init   \t [-1.01364994  0.85976824]. \t  -6.867717811955245 \t -6.867717811955245\n",
      "init   \t [-0.21482552 -1.11844164]. \t  -137.1031718367486 \t -6.867717811955245\n",
      "init   \t [-0.40259762  1.56572768]. \t  -198.98860193376754 \t -6.867717811955245\n",
      "init   \t [-0.25717364  1.55002954]. \t  -221.77381285664336 \t -6.867717811955245\n",
      "1      \t [-2.048       0.74386023]. \t  -1199.8465265691564 \t -6.867717811955245\n",
      "2      \t [-0.36033437  0.2698833 ]. \t  \u001b[92m-3.8116981639463265\u001b[0m \t -3.8116981639463265\n",
      "3      \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -3.8116981639463265\n",
      "4      \t [2.048 2.048]. \t  -461.7603900415999 \t -3.8116981639463265\n",
      "5      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -3.8116981639463265\n",
      "6      \t [2.048      0.43111301]. \t  -1417.2589441611515 \t -3.8116981639463265\n",
      "7      \t [-2.048  2.048]. \t  -469.9523900415999 \t -3.8116981639463265\n",
      "8      \t [ 0.0850719 -2.048    ]. \t  -423.2371001336605 \t -3.8116981639463265\n",
      "9      \t [0.93038598 2.048     ]. \t  -139.80754728627048 \t -3.8116981639463265\n",
      "10     \t [0.7031344  0.57471018]. \t  \u001b[92m-0.7331340492544802\u001b[0m \t -0.7331340492544802\n",
      "11     \t [ 0.4204836  -0.14208841]. \t  -10.505232978342613 \t -0.7331340492544802\n",
      "12     \t [-1.20355028 -0.21479018]. \t  -281.520130260179 \t -0.7331340492544802\n",
      "13     \t [1.09048384 1.36552717]. \t  -3.1189011796775796 \t -0.7331340492544802\n",
      "14     \t [-1.20616968  2.048     ]. \t  -40.050433746096644 \t -0.7331340492544802\n",
      "15     \t [-0.15953572 -0.45705376]. \t  -24.625669808359323 \t -0.7331340492544802\n",
      "16     \t [0.42768081 0.93035388]. \t  -56.19465356092197 \t -0.7331340492544802\n",
      "17     \t [-0.85542533  0.35182095]. \t  -17.877401835865022 \t -0.7331340492544802\n",
      "18     \t [ 1.27568691 -0.45769874]. \t  -434.83012058847 \t -0.7331340492544802\n",
      "19     \t [-1.2899965   1.56116786]. \t  -6.303400526073779 \t -0.7331340492544802\n",
      "20     \t [0.0152819 2.048    ]. \t  -420.3044186742271 \t -0.7331340492544802\n",
      "21     \t [-0.71534127  0.89493315]. \t  -17.628153374967926 \t -0.7331340492544802\n",
      "22     \t [0.82732531 1.47642389]. \t  -62.74936072450405 \t -0.7331340492544802\n",
      "23     \t [1.36960066 1.81276137]. \t  \u001b[92m-0.5340668205048236\u001b[0m \t -0.5340668205048236\n",
      "24     \t [-0.63783096 -2.048     ]. \t  -605.3007070461653 \t -0.5340668205048236\n",
      "25     \t [-2.048      -0.45842347]. \t  -2174.0775944375996 \t -0.5340668205048236\n",
      "26     \t [-0.64074197 -0.32019723]. \t  -56.09122520852657 \t -0.5340668205048236\n",
      "27     \t [ 2.048      -0.77046247]. \t  -2465.9889114238076 \t -0.5340668205048236\n",
      "28     \t [1.41445471 2.048     ]. \t  \u001b[92m-0.3956708822998886\u001b[0m \t -0.3956708822998886\n",
      "29     \t [0.95168076 0.18388957]. \t  -52.10282423363889 \t -0.3956708822998886\n",
      "30     \t [ 0.90792124 -2.048     ]. \t  -825.0312590737026 \t -0.3956708822998886\n",
      "31     \t [-1.06769757 -1.20222022]. \t  -552.864668742424 \t -0.3956708822998886\n",
      "32     \t [0.94299234 0.86617567]. \t  \u001b[92m-0.0564210573038873\u001b[0m \t -0.0564210573038873\n",
      "33     \t [-1.49180828  2.048     ]. \t  -9.35944775004165 \t -0.0564210573038873\n",
      "34     \t [0.05518081 0.05880686]. \t  -1.203622619945308 \t -0.0564210573038873\n",
      "35     \t [-1.32104285  1.83786356]. \t  -6.246742125186614 \t -0.0564210573038873\n",
      "36     \t [1.28392503 1.85291639]. \t  -4.260712956023827 \t -0.0564210573038873\n",
      "37     \t [-1.0726971   1.33850668]. \t  -7.823994261047766 \t -0.0564210573038873\n",
      "38     \t [ 1.02975663 -1.29832291]. \t  -556.3576544701461 \t -0.0564210573038873\n",
      "39     \t [2.048      1.33035838]. \t  -821.3167566345487 \t -0.0564210573038873\n",
      "40     \t [0.87251971 0.921742  ]. \t  -2.5907147822935843 \t -0.0564210573038873\n",
      "41     \t [0.10213816 0.29250622]. \t  -8.762730723550055 \t -0.0564210573038873\n",
      "42     \t [-2.048      -1.25103373]. \t  -2974.460604114087 \t -0.0564210573038873\n",
      "43     \t [0.82667068 0.67296014]. \t  \u001b[92m-0.04090959498876012\u001b[0m \t -0.04090959498876012\n",
      "44     \t [-0.74239163  0.56727182]. \t  -3.06193494118464 \t -0.04090959498876012\n",
      "45     \t [1.30767075 1.72697782]. \t  -0.12347651054009115 \t -0.04090959498876012\n",
      "46     \t [ 0.41697768 -1.65802726]. \t  -335.92481320421916 \t -0.04090959498876012\n",
      "47     \t [1.25941259 1.5995104 ]. \t  -0.08522496450873446 \t -0.04090959498876012\n",
      "48     \t [0.81043566 0.65596483]. \t  \u001b[92m-0.036005388413570955\u001b[0m \t -0.036005388413570955\n",
      "49     \t [-0.06957604  0.01110781]. \t  -1.14792041496333 \t -0.036005388413570955\n",
      "50     \t [-2.048       1.50096946]. \t  -734.6953972467888 \t -0.036005388413570955\n",
      "51     \t [0.79817864 0.63811774]. \t  -0.04083766118405466 \t -0.036005388413570955\n",
      "52     \t [0.89864248 0.833872  ]. \t  -0.07951433381220126 \t -0.036005388413570955\n",
      "53     \t [1.31259644 1.75633839]. \t  -0.20946621077988192 \t -0.036005388413570955\n",
      "54     \t [1.40280503 2.048     ]. \t  -0.8044624002654706 \t -0.036005388413570955\n",
      "55     \t [1.1657569  1.37384784]. \t  -0.049553418373893196 \t -0.036005388413570955\n",
      "56     \t [1.26731827 1.62663762]. \t  -0.11365655123493128 \t -0.036005388413570955\n",
      "57     \t [0.80687666 0.63347728]. \t  -0.06817649053247375 \t -0.036005388413570955\n",
      "58     \t [0.81893648 0.70350263]. \t  -0.14066778763724225 \t -0.036005388413570955\n",
      "59     \t [0.7696272  0.64341019]. \t  -0.3140308050296561 \t -0.036005388413570955\n",
      "60     \t [1.37759101 1.85358201]. \t  -0.3377178129886377 \t -0.036005388413570955\n",
      "61     \t [0.58374383 0.24643873]. \t  -1.0628600608584942 \t -0.036005388413570955\n",
      "62     \t [1.04893351 1.15969434]. \t  -0.35562066850792895 \t -0.036005388413570955\n",
      "63     \t [-1.31779224 -2.048     ]. \t  -1437.6739952505905 \t -0.036005388413570955\n",
      "64     \t [1.29422912 1.65370397]. \t  -0.13204656406780957 \t -0.036005388413570955\n",
      "65     \t [1.0450827  1.01799058]. \t  -0.5527043170694723 \t -0.036005388413570955\n",
      "66     \t [0.84286139 0.70701295]. \t  \u001b[92m-0.02585016135909729\u001b[0m \t -0.02585016135909729\n",
      "67     \t [1.0005208  0.99993241]. \t  \u001b[92m-0.00012336277564526372\u001b[0m \t -0.00012336277564526372\n",
      "68     \t [0.96582491 0.94273216]. \t  -0.010997489813224338 \t -0.00012336277564526372\n",
      "69     \t [0.99943212 1.00428487]. \t  -0.0029382945008131363 \t -0.00012336277564526372\n",
      "70     \t [1.0459632  1.11706143]. \t  -0.055115739225021804 \t -0.00012336277564526372\n",
      "71     \t [1.20452284 1.45228143]. \t  -0.04202732096030925 \t -0.00012336277564526372\n",
      "72     \t [0.89781488 0.81617715]. \t  -0.02065407389440943 \t -0.00012336277564526372\n",
      "73     \t [1.34456485 1.84716521]. \t  -0.2732571077823587 \t -0.00012336277564526372\n",
      "74     \t [0.97442715 0.97300603]. \t  -0.05586842810995383 \t -0.00012336277564526372\n",
      "75     \t [0.95362511 0.92101894]. \t  -0.015648650265201097 \t -0.00012336277564526372\n",
      "76     \t [1.0092025  1.03181566]. \t  -0.0178428759368939 \t -0.00012336277564526372\n",
      "77     \t [0.82215307 0.66833396]. \t  -0.037408128351122276 \t -0.00012336277564526372\n",
      "78     \t [1.01087171 1.04204551]. \t  -0.04085713174691989 \t -0.00012336277564526372\n",
      "79     \t [0.87966468 0.78174554]. \t  -0.020777934933718083 \t -0.00012336277564526372\n",
      "80     \t [0.65475905 0.44081847]. \t  -0.1338542388189151 \t -0.00012336277564526372\n",
      "81     \t [0.8911281  0.80243663]. \t  -0.018787543857095634 \t -0.00012336277564526372\n",
      "82     \t [0.93562838 0.86152437]. \t  -0.023398305346105333 \t -0.00012336277564526372\n",
      "83     \t [0.80239937 0.64487809]. \t  -0.039152790094919454 \t -0.00012336277564526372\n",
      "84     \t [0.50964908 0.2772024 ]. \t  -0.27092992361706625 \t -0.00012336277564526372\n",
      "85     \t [1.07507266 1.16920095]. \t  -0.023644813034020384 \t -0.00012336277564526372\n",
      "86     \t [1.01533676 1.06927439]. \t  -0.14742752243320867 \t -0.00012336277564526372\n",
      "87     \t [-0.22580856  0.08703171]. \t  -1.6325106713840283 \t -0.00012336277564526372\n",
      "88     \t [1.0013985  1.01999097]. \t  -0.029558499103946353 \t -0.00012336277564526372\n",
      "89     \t [0.64483849 0.37758296]. \t  -0.27232134483032533 \t -0.00012336277564526372\n",
      "90     \t [0.75260742 0.5517861 ]. \t  -0.08261210975413877 \t -0.00012336277564526372\n",
      "91     \t [1.19819984 1.44928175]. \t  -0.05777616687746094 \t -0.00012336277564526372\n",
      "92     \t [1.28217238 1.65870919]. \t  -0.10135735956518965 \t -0.00012336277564526372\n",
      "93     \t [1.17621639 1.38324153]. \t  -0.031058144522059676 \t -0.00012336277564526372\n",
      "94     \t [1.11502751 1.26256345]. \t  -0.050392021996074676 \t -0.00012336277564526372\n",
      "95     \t [1.3455909 1.8146021]. \t  -0.12102287582893566 \t -0.00012336277564526372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [1.01387062 1.06086215]. \t  -0.10862111088402861 \t -0.00012336277564526372\n",
      "97     \t [0.87720529 0.79027663]. \t  -0.05829059888504143 \t -0.00012336277564526372\n",
      "98     \t [0.65891554 0.38784132]. \t  -0.33097044395279207 \t -0.00012336277564526372\n",
      "99     \t [1.27249165 1.57605488]. \t  -0.26070390647200337 \t -0.00012336277564526372\n",
      "100    \t [0.89231309 0.7997787 ]. \t  -0.012861013018602705 \t -0.00012336277564526372\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \n",
    "\n",
    "np.random.seed(run_num_15)\n",
    "surrogate_winner_15 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_15 = GPGO(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_15.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.49073237 -0.75216486]. \t  -884.9748202573794 \t -21.690996320546372\n",
      "init   \t [0.70512959 0.03240619]. \t  -21.690996320546372 \t -21.690996320546372\n",
      "init   \t [ 1.15368111 -0.88603985]. \t  -491.54136063574043 \t -21.690996320546372\n",
      "init   \t [-1.09072882  0.26132352]. \t  -90.5574614167091 \t -21.690996320546372\n",
      "init   \t [1.5360998 0.8967902]. \t  -214.26941031258198 \t -21.690996320546372\n",
      "1      \t [-0.10511709 -1.21466192]. \t  -151.4581580344606 \t -21.690996320546372\n",
      "2      \t [0.14462392 1.45090606]. \t  -205.21880283745847 \t -21.690996320546372\n",
      "3      \t [-2.048  2.048]. \t  -469.9523900415999 \t -21.690996320546372\n",
      "4      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -21.690996320546372\n",
      "5      \t [2.048 2.048]. \t  -461.7603900415999 \t -21.690996320546372\n",
      "6      \t [ 0.74851858 -2.048     ]. \t  -680.3757363137149 \t -21.690996320546372\n",
      "7      \t [-0.04936234 -0.31974443]. \t  \u001b[92m-11.481225668389094\u001b[0m \t -11.481225668389094\n",
      "8      \t [-2.048       0.58386639]. \t  -1312.8162802735692 \t -11.481225668389094\n",
      "9      \t [-0.83390202  2.048     ]. \t  -186.3178805525869 \t -11.481225668389094\n",
      "10     \t [-0.5767954   0.82674092]. \t  -26.894625213924996 \t -11.481225668389094\n",
      "11     \t [0.94279397 2.048     ]. \t  -134.36371930449494 \t -11.481225668389094\n",
      "12     \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -11.481225668389094\n",
      "13     \t [2.048      0.40728343]. \t  -1435.2507812711915 \t -11.481225668389094\n",
      "14     \t [0.9576233  1.15251065]. \t  \u001b[92m-5.5463256509576215\u001b[0m \t -5.5463256509576215\n",
      "15     \t [-0.04212118 -2.048     ]. \t  -421.2434412189002 \t -5.5463256509576215\n",
      "16     \t [-0.82231742 -0.44675629]. \t  -129.42525650235743 \t -5.5463256509576215\n",
      "17     \t [-0.56071898  0.16933363]. \t  \u001b[92m-4.540436570074576\u001b[0m \t -4.540436570074576\n",
      "18     \t [1.37737089 1.53232359]. \t  -13.452281452498594 \t -4.540436570074576\n",
      "19     \t [ 0.38798353 -0.75822493]. \t  -82.95833843473123 \t -4.540436570074576\n",
      "20     \t [-1.1246944  1.3331957]. \t  -4.980244694184753 \t -4.540436570074576\n",
      "21     \t [0.00704557 2.048     ]. \t  -420.39602617861607 \t -4.540436570074576\n",
      "22     \t [-0.78570702  1.40674498]. \t  -65.50547790685226 \t -4.540436570074576\n",
      "23     \t [-1.40683802  2.048     ]. \t  -6.266306510749218 \t -4.540436570074576\n",
      "24     \t [0.33105593 0.50217803]. \t  -15.859391631551397 \t -4.540436570074576\n",
      "25     \t [-2.048      -0.60552784]. \t  -2313.1288709979394 \t -4.540436570074576\n",
      "26     \t [2.048      1.40998891]. \t  -776.3393537761007 \t -4.540436570074576\n",
      "27     \t [0.91031868 0.61202115]. \t  -4.702152969678399 \t -4.540436570074576\n",
      "28     \t [1.44034939 2.048     ]. \t  \u001b[92m-0.2646974998754031\u001b[0m \t -0.2646974998754031\n",
      "29     \t [-0.97889942  0.86731317]. \t  -4.742885868226628 \t -0.2646974998754031\n",
      "30     \t [-0.86886345 -2.048     ]. \t  -789.1307754529599 \t -0.2646974998754031\n",
      "31     \t [-1.29739029  1.78287728]. \t  -6.271128231292239 \t -0.2646974998754031\n",
      "32     \t [1.14654459 1.11011952]. \t  -4.20125059013306 \t -0.2646974998754031\n",
      "33     \t [-0.33448959 -0.5396917 ]. \t  -44.235858744415175 \t -0.2646974998754031\n",
      "34     \t [1.28609631 1.77474706]. \t  -1.5387804782237426 \t -0.2646974998754031\n",
      "35     \t [-0.76637994 -1.3266116 ]. \t  -369.44048406173613 \t -0.2646974998754031\n",
      "36     \t [0.35875064 0.053301  ]. \t  -0.9797321743940282 \t -0.2646974998754031\n",
      "37     \t [ 0.48775345 -1.5091534 ]. \t  -305.48315104103114 \t -0.2646974998754031\n",
      "38     \t [-0.05224993  0.10566643]. \t  -2.166819661318502 \t -0.2646974998754031\n",
      "39     \t [-0.77980971  0.57563038]. \t  -3.2731708668434916 \t -0.2646974998754031\n",
      "40     \t [-2.048       1.44067516]. \t  -767.5374818710724 \t -0.2646974998754031\n",
      "41     \t [1.39446252 1.91333644]. \t  \u001b[92m-0.25287772712634937\u001b[0m \t -0.25287772712634937\n",
      "42     \t [ 0.11524804 -0.01478793]. \t  -0.8615787404685346 \t -0.25287772712634937\n",
      "43     \t [-0.87263079  0.90566981]. \t  -5.5856864734916805 \t -0.25287772712634937\n",
      "44     \t [ 2.048      -1.23086215]. \t  -2944.3410794824777 \t -0.25287772712634937\n",
      "45     \t [0.85333115 0.75785542]. \t  \u001b[92m-0.10961006779087842\u001b[0m \t -0.10961006779087842\n",
      "46     \t [1.27202766 1.61910365]. \t  \u001b[92m-0.07410914710047342\u001b[0m \t -0.07410914710047342\n",
      "47     \t [-1.56341077 -1.33963625]. \t  -1438.3530496916078 \t -0.07410914710047342\n",
      "48     \t [-1.29681168  2.04492406]. \t  -18.467024775811957 \t -0.07410914710047342\n",
      "49     \t [0.57780208 0.34228109]. \t  -0.18535057288028994 \t -0.07410914710047342\n",
      "50     \t [0.60749349 0.32997843]. \t  -0.30670717787550733 \t -0.07410914710047342\n",
      "51     \t [1.12478789 1.27913747]. \t  \u001b[92m-0.035143115742366546\u001b[0m \t -0.035143115742366546\n",
      "52     \t [1.3691631 2.048    ]. \t  -3.1427742856427434 \t -0.035143115742366546\n",
      "53     \t [1.36150554 1.86093239]. \t  -0.13592084023185474 \t -0.035143115742366546\n",
      "54     \t [0.68674778 0.44958053]. \t  -0.146711848374524 \t -0.035143115742366546\n",
      "55     \t [0.81597989 0.65285169]. \t  -0.05068938918888488 \t -0.035143115742366546\n",
      "56     \t [1.2467396  1.55680881]. \t  -0.06148028262463166 \t -0.035143115742366546\n",
      "57     \t [1.35926715 1.83108774]. \t  -0.15636209544925495 \t -0.035143115742366546\n",
      "58     \t [1.12972814 1.26944202]. \t  \u001b[92m-0.021512933441029065\u001b[0m \t -0.021512933441029065\n",
      "59     \t [0.4371944  0.16262162]. \t  -0.39807391735082037 \t -0.021512933441029065\n",
      "60     \t [0.96608001 0.94276589]. \t  \u001b[92m-0.01009083920418431\u001b[0m \t -0.01009083920418431\n",
      "61     \t [1.025066   1.02756427]. \t  -0.054433943794578846 \t -0.01009083920418431\n",
      "62     \t [0.34851992 0.14036145]. \t  -0.46012960653720797 \t -0.01009083920418431\n",
      "63     \t [1.07022323 1.16994681]. \t  -0.06529509851610793 \t -0.01009083920418431\n",
      "64     \t [0.4875408  0.24021597]. \t  -0.26324943677422424 \t -0.01009083920418431\n",
      "65     \t [1.41692971 2.04799942]. \t  -0.33631681285629667 \t -0.01009083920418431\n",
      "66     \t [1.04613517 1.09203097]. \t  \u001b[92m-0.00268911563244869\u001b[0m \t -0.00268911563244869\n",
      "67     \t [0.87822035 0.78464036]. \t  -0.03270432508373207 \t -0.00268911563244869\n",
      "68     \t [ 1.30451619 -1.8138044 ]. \t  -1236.0137741363199 \t -0.00268911563244869\n",
      "69     \t [0.74213155 0.5296185 ]. \t  -0.11118917576992463 \t -0.00268911563244869\n",
      "70     \t [1.03910666 1.07925622]. \t  \u001b[92m-0.0015529934125343143\u001b[0m \t -0.0015529934125343143\n",
      "71     \t [1.4171612  2.03802881]. \t  -0.2621311581648526 \t -0.0015529934125343143\n",
      "72     \t [0.75078236 0.54669459]. \t  -0.09094001048975911 \t -0.0015529934125343143\n",
      "73     \t [1.19665603 1.40069031]. \t  -0.1366134859312435 \t -0.0015529934125343143\n",
      "74     \t [0.63055887 0.3750238 ]. \t  -0.18747547375484633 \t -0.0015529934125343143\n",
      "75     \t [1.19683136 1.4335338 ]. \t  -0.03886993484272421 \t -0.0015529934125343143\n",
      "76     \t [0.67602115 0.43979887]. \t  -0.1345660142014505 \t -0.0015529934125343143\n",
      "77     \t [1.03682644 1.07382799]. \t  \u001b[92m-0.0014956796279390254\u001b[0m \t -0.0014956796279390254\n",
      "78     \t [1.2431272  1.54174412]. \t  -0.06042209022361554 \t -0.0014956796279390254\n",
      "79     \t [1.41904786 2.048     ]. \t  -0.2932719551485785 \t -0.0014956796279390254\n",
      "80     \t [1.3045518  1.72649639]. \t  -0.15346957507689113 \t -0.0014956796279390254\n",
      "81     \t [1.04014492 1.09317822]. \t  -0.014328133027762622 \t -0.0014956796279390254\n",
      "82     \t [1.41904824 1.99911442]. \t  -0.19686921480185354 \t -0.0014956796279390254\n",
      "83     \t [0.64194983 0.4575617 ]. \t  -0.3348803111120094 \t -0.0014956796279390254\n",
      "84     \t [1.08247181 1.14826752]. \t  -0.06192185876165068 \t -0.0014956796279390254\n",
      "85     \t [1.13496921 1.25852063]. \t  -0.10603699831087789 \t -0.0014956796279390254\n",
      "86     \t [0.56832542 0.35812604]. \t  -0.3097705252017462 \t -0.0014956796279390254\n",
      "87     \t [1.0052607  1.01242718]. \t  \u001b[92m-0.00038039923677885\u001b[0m \t -0.00038039923677885\n",
      "88     \t [1.28377815 1.63336648]. \t  -0.10219746228994897 \t -0.00038039923677885\n",
      "89     \t [1.11261582 1.22465785]. \t  -0.030254805172820115 \t -0.00038039923677885\n",
      "90     \t [1.04618175 1.08060182]. \t  -0.02143827593040451 \t -0.00038039923677885\n",
      "91     \t [0.96017447 0.90634155]. \t  -0.025901709589214752 \t -0.00038039923677885\n",
      "92     \t [0.52742533 0.24908457]. \t  -0.3079665446100548 \t -0.00038039923677885\n",
      "93     \t [0.26828112 0.08269861]. \t  -0.5469126385536205 \t -0.00038039923677885\n",
      "94     \t [1.19596448 1.43212036]. \t  -0.038722247556733196 \t -0.00038039923677885\n",
      "95     \t [1.25559439 1.57249571]. \t  -0.06694579641798601 \t -0.00038039923677885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [1.09048897 1.21995089]. \t  -0.10295805017304945 \t -0.00038039923677885\n",
      "97     \t [0.94701923 0.90017704]. \t  -0.003916925072904123 \t -0.00038039923677885\n",
      "98     \t [1.03446245 1.09305449]. \t  -0.05382082520092298 \t -0.00038039923677885\n",
      "99     \t [1.10623273 1.19622435]. \t  -0.08705621191922293 \t -0.00038039923677885\n",
      "100    \t [0.58037137 0.3187701 ]. \t  -0.20870751829868253 \t -0.00038039923677885\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \n",
    "\n",
    "np.random.seed(run_num_16)\n",
    "surrogate_winner_16 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_16 = GPGO(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_16.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.62187122 0.94420838]. \t  -31.22188590191926 \t -31.22188590191926\n",
      "init   \t [-1.25596728  0.08059797]. \t  -229.14713198618372 \t -31.22188590191926\n",
      "init   \t [0.72246586 1.2470062 ]. \t  -52.64667111223406 \t -31.22188590191926\n",
      "init   \t [0.13726869 1.671308  ]. \t  -273.80846401311214 \t -31.22188590191926\n",
      "init   \t [-0.2315013  -0.99953783]. \t  -112.42501801714917 \t -31.22188590191926\n",
      "1      \t [1.69096619 0.6595077 ]. \t  -484.41538096570434 \t -31.22188590191926\n",
      "2      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -31.22188590191926\n",
      "3      \t [ 1.4157073 -2.048    ]. \t  -1642.2273058371322 \t -31.22188590191926\n",
      "4      \t [-2.048  2.048]. \t  -469.9523900415999 \t -31.22188590191926\n",
      "5      \t [2.048 2.048]. \t  -461.7603900415999 \t -31.22188590191926\n",
      "6      \t [-0.06095141 -0.1552655 ]. \t  \u001b[92m-3.653100148006159\u001b[0m \t -3.653100148006159\n",
      "7      \t [-2.048       0.73036808]. \t  -1209.175510441534 \t -3.653100148006159\n",
      "8      \t [ 2.048      -0.62632257]. \t  -2324.9423559180486 \t -3.653100148006159\n",
      "9      \t [-0.54961802  0.65882511]. \t  -15.128026087946708 \t -3.653100148006159\n",
      "10     \t [-1.00132637  2.048     ]. \t  -113.2800282842581 \t -3.653100148006159\n",
      "11     \t [ 0.00993325 -2.048     ]. \t  -420.4510481810895 \t -3.653100148006159\n",
      "12     \t [-0.60451978 -0.22161393]. \t  -37.03820385596254 \t -3.653100148006159\n",
      "13     \t [1.03812388 2.048     ]. \t  -94.14943136771777 \t -3.653100148006159\n",
      "14     \t [ 0.40201959 -1.15636156]. \t  -174.06505413918438 \t -3.653100148006159\n",
      "15     \t [-1.04354035  1.34043137]. \t  -10.499014158997088 \t -3.653100148006159\n",
      "16     \t [0.76343303 0.01421181]. \t  -32.38862832427562 \t -3.653100148006159\n",
      "17     \t [1.00519466 0.8055799 ]. \t  -4.195822440402551 \t -3.653100148006159\n",
      "18     \t [ 0.2037244  -0.54818378]. \t  -35.40717870413958 \t -3.653100148006159\n",
      "19     \t [0.53495254 0.26603636]. \t  \u001b[92m-0.2568224533984007\u001b[0m \t -0.2568224533984007\n",
      "20     \t [-2.048      -0.49839169]. \t  -2211.4295883239856 \t -0.2568224533984007\n",
      "21     \t [-0.91821482  0.48838413]. \t  -16.26319196006398 \t -0.2568224533984007\n",
      "22     \t [-0.74881718  1.41828811]. \t  -76.59943804174391 \t -0.2568224533984007\n",
      "23     \t [-1.32156786  1.80431228]. \t  -5.723422375776218 \t -0.2568224533984007\n",
      "24     \t [-0.70863289 -2.048     ]. \t  -653.2513235291814 \t -0.2568224533984007\n",
      "25     \t [-0.55290763  0.20028589]. \t  -3.522880096938912 \t -0.2568224533984007\n",
      "26     \t [1.48005878 1.46329022]. \t  -53.12462278076099 \t -0.2568224533984007\n",
      "27     \t [-1.41203986  2.048     ]. \t  -6.1110874742249415 \t -0.2568224533984007\n",
      "28     \t [2.048      1.35970356]. \t  -804.5942695738529 \t -0.2568224533984007\n",
      "29     \t [1.43864533 2.048     ]. \t  \u001b[92m-0.23950035028034322\u001b[0m \t -0.23950035028034322\n",
      "30     \t [1.29295425 1.73112075]. \t  -0.43854022837367396 \t -0.23950035028034322\n",
      "31     \t [1.16263365 1.25748127]. \t  -0.9144872472656903 \t -0.23950035028034322\n",
      "32     \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -0.23950035028034322\n",
      "33     \t [-0.98440683 -1.21342095]. \t  -480.25878653794604 \t -0.23950035028034322\n",
      "34     \t [ 0.65204367 -2.048     ]. \t  -611.7735779313113 \t -0.23950035028034322\n",
      "35     \t [ 0.9770338  -0.94430034]. \t  -360.5808953908338 \t -0.23950035028034322\n",
      "36     \t [ 0.30317072 -0.04278838]. \t  -2.300003457410994 \t -0.23950035028034322\n",
      "37     \t [-0.00376779  0.2404748 ]. \t  -6.789680192022562 \t -0.23950035028034322\n",
      "38     \t [-1.63054582  1.43355971]. \t  -157.01166597614207 \t -0.23950035028034322\n",
      "39     \t [-0.03897468  0.99909168]. \t  -100.59458843254805 \t -0.23950035028034322\n",
      "40     \t [0.77913991 0.58874023]. \t  \u001b[92m-0.08233690300431469\u001b[0m \t -0.08233690300431469\n",
      "41     \t [1.37591863 1.84050293]. \t  -0.41850796860376993 \t -0.08233690300431469\n",
      "42     \t [1.24273537 1.50911449]. \t  -0.18336501185832654 \t -0.08233690300431469\n",
      "43     \t [-0.42223945 -1.62682927]. \t  -327.8669366065667 \t -0.08233690300431469\n",
      "44     \t [ 0.0492388  -0.01864865]. \t  -0.9483544580742279 \t -0.08233690300431469\n",
      "45     \t [-2.048      -1.30048141]. \t  -3028.55697126421 \t -0.08233690300431469\n",
      "46     \t [1.40886023 2.04535364]. \t  -0.5327861729941776 \t -0.08233690300431469\n",
      "47     \t [-2.048       1.52517907]. \t  -721.7130922046509 \t -0.08233690300431469\n",
      "48     \t [-1.08484821  1.03226777]. \t  -6.43831406390094 \t -0.08233690300431469\n",
      "49     \t [0.98086031 0.98001879]. \t  \u001b[92m-0.032521465764424135\u001b[0m \t -0.032521465764424135\n",
      "50     \t [0.82305694 0.70191319]. \t  -0.09128712960743753 \t -0.032521465764424135\n",
      "51     \t [1.25557321 1.54048465]. \t  -0.19476970042506098 \t -0.032521465764424135\n",
      "52     \t [0.78611239 0.64270285]. \t  -0.10690605047899338 \t -0.032521465764424135\n",
      "53     \t [-0.31058367  0.12843364]. \t  -1.8198467805233696 \t -0.032521465764424135\n",
      "54     \t [1.06811548 1.14194079]. \t  \u001b[92m-0.004754233309075969\u001b[0m \t -0.004754233309075969\n",
      "55     \t [1.38240476 1.92642535]. \t  -0.16989532345213873 \t -0.004754233309075969\n",
      "56     \t [1.13144844 1.30121699]. \t  -0.06155280280614207 \t -0.004754233309075969\n",
      "57     \t [0.86430427 0.72161783]. \t  -0.08294989008708827 \t -0.004754233309075969\n",
      "58     \t [1.32486151 1.71939396]. \t  -0.2341580856462721 \t -0.004754233309075969\n",
      "59     \t [1.28493309 1.59084518]. \t  -0.4436854658574313 \t -0.004754233309075969\n",
      "60     \t [0.77139681 0.56248782]. \t  -0.15830871444878436 \t -0.004754233309075969\n",
      "61     \t [0.73263739 0.51349456]. \t  -0.12559940352862 \t -0.004754233309075969\n",
      "62     \t [0.996413   0.99443288]. \t  \u001b[92m-0.0002669561688166835\u001b[0m \t -0.0002669561688166835\n",
      "63     \t [0.95323272 0.9179463 ]. \t  -0.010824413450890153 \t -0.0002669561688166835\n",
      "64     \t [1.41770323 2.04799992]. \t  -0.31977015286462707 \t -0.0002669561688166835\n",
      "65     \t [0.87369473 0.73963134]. \t  -0.07217489486346867 \t -0.0002669561688166835\n",
      "66     \t [1.12289063 1.23820703]. \t  -0.06652373390445139 \t -0.0002669561688166835\n",
      "67     \t [1.40577724 2.01221923]. \t  -0.29432411684953125 \t -0.0002669561688166835\n",
      "68     \t [1.37593672 1.88483246]. \t  -0.14833308811692528 \t -0.0002669561688166835\n",
      "69     \t [0.92053844 0.88039493]. \t  -0.11524000079479184 \t -0.0002669561688166835\n",
      "70     \t [0.94090345 0.89169149]. \t  -0.007578399337066668 \t -0.0002669561688166835\n",
      "71     \t [1.24739921 1.50165645]. \t  -0.3565804804635223 \t -0.0002669561688166835\n",
      "72     \t [1.0971614  1.19188437]. \t  -0.023550844295586577 \t -0.0002669561688166835\n",
      "73     \t [0.96278837 0.91984833]. \t  -0.0064443375572073545 \t -0.0002669561688166835\n",
      "74     \t [1.09390186 1.17428407]. \t  -0.05871264353369784 \t -0.0002669561688166835\n",
      "75     \t [0.70668333 0.50517187]. \t  -0.08936458566883043 \t -0.0002669561688166835\n",
      "76     \t [0.68125528 0.4482528 ]. \t  -0.1267393222368912 \t -0.0002669561688166835\n",
      "77     \t [0.25541985 0.04504619]. \t  -0.5951757718276739 \t -0.0002669561688166835\n",
      "78     \t [1.3170769  1.67360375]. \t  -0.47370993676631745 \t -0.0002669561688166835\n",
      "79     \t [1.27155595 1.574528  ]. \t  -0.2528962306334378 \t -0.0002669561688166835\n",
      "80     \t [0.84547961 0.73490677]. \t  -0.06416104873234059 \t -0.0002669561688166835\n",
      "81     \t [1.39489439 2.048     ]. \t  -1.2018495521052752 \t -0.0002669561688166835\n",
      "82     \t [0.87797204 0.75792914]. \t  -0.031546699688524166 \t -0.0002669561688166835\n",
      "83     \t [1.42010013 2.048     ]. \t  -0.2745508651133094 \t -0.0002669561688166835\n",
      "84     \t [1.03799972 1.0927133 ]. \t  -0.024760895275008486 \t -0.0002669561688166835\n",
      "85     \t [0.94266338 0.86147061]. \t  -0.07696521637668234 \t -0.0002669561688166835\n",
      "86     \t [0.49644745 0.2499303 ]. \t  -0.2547694238479464 \t -0.0002669561688166835\n",
      "87     \t [0.69249667 0.47089673]. \t  -0.10204905454367183 \t -0.0002669561688166835\n",
      "88     \t [0.81227689 0.63661113]. \t  -0.08898336427923793 \t -0.0002669561688166835\n",
      "89     \t [0.82063253 0.64022831]. \t  -0.14245932593300406 \t -0.0002669561688166835\n",
      "90     \t [0.4690638  0.20677628]. \t  -0.2994351055852952 \t -0.0002669561688166835\n",
      "91     \t [1.22026435 1.47018273]. \t  -0.08409517823848289 \t -0.0002669561688166835\n",
      "92     \t [1.05144314 1.11476874]. \t  -0.011176890257458782 \t -0.0002669561688166835\n",
      "93     \t [1.4255132  2.04799457]. \t  -0.20636377841862913 \t -0.0002669561688166835\n",
      "94     \t [0.84508445 0.74732108]. \t  -0.1339132568038867 \t -0.0002669561688166835\n",
      "95     \t [0.94512289 0.92497789]. \t  -0.10363125148750948 \t -0.0002669561688166835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [0.99063142 1.00551029]. \t  -0.05845679587980423 \t -0.0002669561688166835\n",
      "97     \t [1.03831601 1.08374998]. \t  -0.004660204213668615 \t -0.0002669561688166835\n",
      "98     \t [0.71331213 0.50048094]. \t  -0.08913424720082086 \t -0.0002669561688166835\n",
      "99     \t [0.68079448 0.43294393]. \t  -0.19514420213170458 \t -0.0002669561688166835\n",
      "100    \t [0.76481533 0.57024167]. \t  -0.07692322486618723 \t -0.0002669561688166835\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \n",
    "\n",
    "np.random.seed(run_num_17)\n",
    "surrogate_winner_17 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_17 = GPGO(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_17.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [1.52956683 1.91914255]. \t  -17.95675982372887 \t -1.7663579664225912\n",
      "init   \t [1.51222084 0.12638491]. \t  -467.0068292530542 \t -1.7663579664225912\n",
      "init   \t [-1.09474477 -2.0013105 ]. \t  -1028.244988519411 \t -1.7663579664225912\n",
      "init   \t [-0.28479972 -0.39996883]. \t  -24.79447905426233 \t -1.7663579664225912\n",
      "init   \t [ 0.09287545 -0.0885072 ]. \t  -1.7663579664225912 \t -1.7663579664225912\n",
      "1      \t [-1.01632931  0.91131603]. \t  -5.544464481757653 \t -1.7663579664225912\n",
      "2      \t [-2.048  2.048]. \t  -469.9523900415999 \t -1.7663579664225912\n",
      "3      \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -1.7663579664225912\n",
      "4      \t [0.0241631 2.048    ]. \t  -420.143544609087 \t -1.7663579664225912\n",
      "5      \t [-2.048      -0.06978722]. \t  -1827.5376962889593 \t -1.7663579664225912\n",
      "6      \t [2.048      1.18604226]. \t  -906.0621763816237 \t -1.7663579664225912\n",
      "7      \t [-0.22591678  0.78137829]. \t  -54.842508667059356 \t -1.7663579664225912\n",
      "8      \t [ 0.21223294 -1.24947969]. \t  -168.1994292297452 \t -1.7663579664225912\n",
      "9      \t [-1.05834685  2.048     ]. \t  -90.33699407855629 \t -1.7663579664225912\n",
      "10     \t [2.048 2.048]. \t  -461.7603900415999 \t -1.7663579664225912\n",
      "11     \t [1.11309698 2.048     ]. \t  -65.46333600476132 \t -1.7663579664225912\n",
      "12     \t [0.93872881 1.09616766]. \t  -4.624357079566379 \t -1.7663579664225912\n",
      "13     \t [-0.84357039  1.40164688]. \t  -51.01370317277049 \t -1.7663579664225912\n",
      "14     \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -1.7663579664225912\n",
      "15     \t [-0.22578179 -2.048     ]. \t  -442.07315988499414 \t -1.7663579664225912\n",
      "16     \t [-0.42094588 -1.25558592]. \t  -207.30532793306674 \t -1.7663579664225912\n",
      "17     \t [ 0.74139889 -0.5489587 ]. \t  -120.76588301386799 \t -1.7663579664225912\n",
      "18     \t [-2.048       1.14365734]. \t  -939.934806722502 \t -1.7663579664225912\n",
      "19     \t [-0.63711196  0.41821513]. \t  -2.695273126110893 \t -1.7663579664225912\n",
      "20     \t [ 0.14291959 -0.65776137]. \t  -46.72839928115164 \t -1.7663579664225912\n",
      "21     \t [ 2.048      -0.55162023]. \t  -2253.4779875478316 \t -1.7663579664225912\n",
      "22     \t [0.80794142 0.37959373]. \t  -7.499377295519561 \t -1.7663579664225912\n",
      "23     \t [1.20980338 1.62398511]. \t  -2.615578985280753 \t -1.7663579664225912\n",
      "24     \t [ 0.71713167 -2.048     ]. \t  -656.6067861264175 \t -1.7663579664225912\n",
      "25     \t [1.20764636 0.93904889]. \t  -27.016684431402393 \t -1.7663579664225912\n",
      "26     \t [-1.10469918 -0.74897631]. \t  -392.2584152365734 \t -1.7663579664225912\n",
      "27     \t [-1.37006589  1.59982235]. \t  -13.304423166469821 \t -1.7663579664225912\n",
      "28     \t [-1.13033519  1.31025016]. \t  -4.644555291165263 \t -1.7663579664225912\n",
      "29     \t [-1.45306873  2.048     ]. \t  -6.41961307860619 \t -1.7663579664225912\n",
      "30     \t [1.42873763 2.048     ]. \t  \u001b[92m-0.18831674275551644\u001b[0m \t -0.18831674275551644\n",
      "31     \t [-0.93126307  0.04758924]. \t  -70.91430295172714 \t -0.18831674275551644\n",
      "32     \t [ 0.45742925 -0.16056138]. \t  -13.969801462813104 \t -0.18831674275551644\n",
      "33     \t [0.47003765 0.47058282]. \t  -6.513243603399485 \t -0.18831674275551644\n",
      "34     \t [0.89054495 0.7539601 ]. \t  \u001b[92m-0.16494120967079404\u001b[0m \t -0.16494120967079404\n",
      "35     \t [-1.29660155  1.81227587]. \t  -6.993107332587199 \t -0.16494120967079404\n",
      "36     \t [0.28263015 1.39046327]. \t  -172.27752173818197 \t -0.16494120967079404\n",
      "37     \t [1.28955306 1.53283636]. \t  -1.7767212839627335 \t -0.16494120967079404\n",
      "38     \t [ 1.00820523 -1.44342366]. \t  -605.1115827714126 \t -0.16494120967079404\n",
      "39     \t [-0.84631624  0.88135949]. \t  -6.134959074319458 \t -0.16494120967079404\n",
      "40     \t [1.36729651 1.89023699]. \t  -0.17791000775843938 \t -0.16494120967079404\n",
      "41     \t [-2.048      -1.07797715]. \t  -2788.9851541650596 \t -0.16494120967079404\n",
      "42     \t [0.28072828 0.11770333]. \t  -0.6686336139503176 \t -0.16494120967079404\n",
      "43     \t [-0.47650276  0.09286889]. \t  -3.980648466257751 \t -0.16494120967079404\n",
      "44     \t [2.048      0.31803315]. \t  -1503.6458751882874 \t -0.16494120967079404\n",
      "45     \t [1.04555224 1.07982693]. \t  \u001b[92m-0.01990409911114361\u001b[0m \t -0.01990409911114361\n",
      "46     \t [1.37251913 1.88468827]. \t  -0.13884785714998826 \t -0.01990409911114361\n",
      "47     \t [1.09957561 1.21340479]. \t  \u001b[92m-0.011797352600330678\u001b[0m \t -0.011797352600330678\n",
      "48     \t [1.057503   1.14750327]. \t  -0.08851619870612988 \t -0.011797352600330678\n",
      "49     \t [1.42339789 2.048     ]. \t  -0.2273953265910718 \t -0.011797352600330678\n",
      "50     \t [1.36355985 1.81848254]. \t  -0.29874515722372386 \t -0.011797352600330678\n",
      "51     \t [-1.11280818 -1.43791131]. \t  -720.6971627119611 \t -0.011797352600330678\n",
      "52     \t [0.14583115 0.02219359]. \t  -0.7296903273782025 \t -0.011797352600330678\n",
      "53     \t [0.84171344 0.7372362 ]. \t  -0.10773784070432568 \t -0.011797352600330678\n",
      "54     \t [0.14666239 0.02897258]. \t  -0.7337543024515056 \t -0.011797352600330678\n",
      "55     \t [0.92309789 0.82196595]. \t  -0.09677861000007032 \t -0.011797352600330678\n",
      "56     \t [0.95752328 0.90703669]. \t  \u001b[92m-0.011436019432254071\u001b[0m \t -0.011436019432254071\n",
      "57     \t [1.02744066 1.05524246]. \t  \u001b[92m-0.0007683453188639257\u001b[0m \t -0.0007683453188639257\n",
      "58     \t [1.36981434 1.84260969]. \t  -0.25088259128281176 \t -0.0007683453188639257\n",
      "59     \t [1.19821513 1.39554715]. \t  -0.20067100025007745 \t -0.0007683453188639257\n",
      "60     \t [0.89817667 0.8220935 ]. \t  -0.0339983473962223 \t -0.0007683453188639257\n",
      "61     \t [1.32851189 1.7650339 ]. \t  -0.1079208715870333 \t -0.0007683453188639257\n",
      "62     \t [0.96121563 0.85902639]. \t  -0.4228233812783134 \t -0.0007683453188639257\n",
      "63     \t [1.32844574 1.69929548]. \t  -0.5365428821512304 \t -0.0007683453188639257\n",
      "64     \t [1.00819496 1.00223369]. \t  -0.020297598399286233 \t -0.0007683453188639257\n",
      "65     \t [1.20030878 1.43824155]. \t  -0.0407484227082168 \t -0.0007683453188639257\n",
      "66     \t [1.42224487 2.04799862]. \t  -0.2418862510852331 \t -0.0007683453188639257\n",
      "67     \t [0.20280412 0.03294108]. \t  -0.6422263064927215 \t -0.0007683453188639257\n",
      "68     \t [1.00182303 1.02667312]. \t  -0.053012598441388586 \t -0.0007683453188639257\n",
      "69     \t [1.05118846 1.11726936]. \t  -0.01768093006306592 \t -0.0007683453188639257\n",
      "70     \t [1.42033416 2.04799925]. \t  -0.27062389840891554 \t -0.0007683453188639257\n",
      "71     \t [1.38694716 1.90327406]. \t  -0.19113369389105167 \t -0.0007683453188639257\n",
      "72     \t [1.2841166  1.64346863]. \t  -0.0837327730057113 \t -0.0007683453188639257\n",
      "73     \t [1.13278271 1.27837844]. \t  -0.01995278535764273 \t -0.0007683453188639257\n",
      "74     \t [1.26896164 1.59977843]. \t  -0.08333432625385477 \t -0.0007683453188639257\n",
      "75     \t [1.29990263 1.68602059]. \t  -0.09133009043291078 \t -0.0007683453188639257\n",
      "76     \t [1.41721043 2.04799916]. \t  -0.330198235804855 \t -0.0007683453188639257\n",
      "77     \t [0.98329257 0.97026397]. \t  -0.0014349212213450114 \t -0.0007683453188639257\n",
      "78     \t [1.07787591 1.17772596]. \t  -0.03137583405549337 \t -0.0007683453188639257\n",
      "79     \t [0.11560292 0.02862741]. \t  -0.8054552557618605 \t -0.0007683453188639257\n",
      "80     \t [0.27170648 0.04447973]. \t  -0.6165224624253803 \t -0.0007683453188639257\n",
      "81     \t [1.11665137 1.22627004]. \t  -0.05620951139372455 \t -0.0007683453188639257\n",
      "82     \t [1.05680787 1.10630357]. \t  -0.014334810942404018 \t -0.0007683453188639257\n",
      "83     \t [ 2.048      -1.37677076]. \t  -3104.7857038999277 \t -0.0007683453188639257\n",
      "84     \t [1.25627707 1.55911194]. \t  -0.1022358848862952 \t -0.0007683453188639257\n",
      "85     \t [1.3346344  1.77211724]. \t  -0.12031903992171211 \t -0.0007683453188639257\n",
      "86     \t [1.00254052 0.97020866]. \t  -0.12165980377544937 \t -0.0007683453188639257\n",
      "87     \t [0.90696496 0.81492917]. \t  -0.014517361652275358 \t -0.0007683453188639257\n",
      "88     \t [0.92019881 0.82839852]. \t  -0.0401041065011041 \t -0.0007683453188639257\n",
      "89     \t [1.04779559 1.08256261]. \t  -0.025733207977007115 \t -0.0007683453188639257\n",
      "90     \t [1.06994737 1.12888743]. \t  -0.030173457568843903 \t -0.0007683453188639257\n",
      "91     \t [0.93669343 0.87442095]. \t  -0.004891967648478197 \t -0.0007683453188639257\n",
      "92     \t [0.26622987 0.06421862]. \t  -0.5428537936296486 \t -0.0007683453188639257\n",
      "93     \t [1.23850892 1.50462284]. \t  -0.14262711348802348 \t -0.0007683453188639257\n",
      "94     \t [1.23178628 1.51007279]. \t  -0.058944439408219654 \t -0.0007683453188639257\n",
      "95     \t [1.07246189 1.1708765 ]. \t  -0.04810794099423877 \t -0.0007683453188639257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [ 0.22555408 -0.01561459]. \t  -1.041848279864148 \t -0.0007683453188639257\n",
      "97     \t [1.0311184 1.0708717]. \t  -0.006845935949633201 \t -0.0007683453188639257\n",
      "98     \t [0.97681403 0.95686982]. \t  -0.0012688474810880475 \t -0.0007683453188639257\n",
      "99     \t [1.38451716 1.88843129]. \t  -0.22883054471976888 \t -0.0007683453188639257\n",
      "100    \t [0.96727162 0.93167685]. \t  -0.0026215689891299246 \t -0.0007683453188639257\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \n",
    "\n",
    "np.random.seed(run_num_18)\n",
    "surrogate_winner_18 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_18 = GPGO(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_18.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.24348908 1.07107119]. \t  -102.94304782477323 \t -4.219752052396591\n",
      "init   \t [-1.04639875  1.11284007]. \t  -4.219752052396591 \t -4.219752052396591\n",
      "init   \t [-0.59608235  1.21527255]. \t  -76.50031976976825 \t -4.219752052396591\n",
      "init   \t [-0.63078391 -0.22989031]. \t  -42.07005958957044 \t -4.219752052396591\n",
      "init   \t [-0.8056685  -0.23710138]. \t  -81.79603430667063 \t -4.219752052396591\n",
      "1      \t [ 0.52580058 -0.62342624]. \t  -81.20551379007752 \t -4.219752052396591\n",
      "2      \t [-2.048       1.69587227]. \t  -633.5064159420247 \t -4.219752052396591\n",
      "3      \t [2.048      1.77765133]. \t  -585.1193145118535 \t -4.219752052396591\n",
      "4      \t [ 2.048 -2.048]. \t  -3897.734226841601 \t -4.219752052396591\n",
      "5      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -4.219752052396591\n",
      "6      \t [2.048      0.27090127]. \t  -1540.4071986163417 \t -4.219752052396591\n",
      "7      \t [-2.048       0.33609855]. \t  -1497.8652346086542 \t -4.219752052396591\n",
      "8      \t [0.72524383 2.048     ]. \t  -231.73040016144995 \t -4.219752052396591\n",
      "9      \t [ 0.00345113 -2.048     ]. \t  -420.4283881265478 \t -4.219752052396591\n",
      "10     \t [-1.0880965  2.048    ]. \t  -79.01769670837072 \t -4.219752052396591\n",
      "11     \t [-0.20284639 -1.09111371]. \t  -129.64819369918771 \t -4.219752052396591\n",
      "12     \t [0.69294568 0.27279575]. \t  -4.39484441460701 \t -4.219752052396591\n",
      "13     \t [1.10564062 1.20503735]. \t  \u001b[92m-0.04144924932713778\u001b[0m \t -0.04144924932713778\n",
      "14     \t [-0.7689053   0.48785071]. \t  -4.197451213462073 \t -0.04144924932713778\n",
      "15     \t [-0.23051164  2.048     ]. \t  -399.46254945107825 \t -0.04144924932713778\n",
      "16     \t [0.78578374 0.89239708]. \t  -7.605143697884743 \t -0.04144924932713778\n",
      "17     \t [ 0.2564735 -0.1632616]. \t  -5.798775551511055 \t -0.04144924932713778\n",
      "18     \t [1.4347585 2.048    ]. \t  -0.20010718492634902 \t -0.04144924932713778\n",
      "19     \t [1.10836723 1.63050597]. \t  -16.17439980318168 \t -0.04144924932713778\n",
      "20     \t [-0.6539846 -2.048    ]. \t  -615.6426644276153 \t -0.04144924932713778\n",
      "21     \t [-0.00174595  0.45490259]. \t  -21.69685414414532 \t -0.04144924932713778\n",
      "22     \t [-1.21773915  1.61663154]. \t  -6.707083635209077 \t -0.04144924932713778\n",
      "23     \t [-1.61279245  2.048     ]. \t  -37.41858717292372 \t -0.04144924932713778\n",
      "24     \t [ 2.048      -0.78931158]. \t  -2484.740728576623 \t -0.04144924932713778\n",
      "25     \t [-2.048      -0.78549011]. \t  -2489.125244837888 \t -0.04144924932713778\n",
      "26     \t [-0.79976098 -1.11188694]. \t  -310.01596399211195 \t -0.04144924932713778\n",
      "27     \t [ 0.53445553 -0.08356811]. \t  -13.848394904333684 \t -0.04144924932713778\n",
      "28     \t [ 0.72333367 -1.62892127]. \t  -463.2441373939544 \t -0.04144924932713778\n",
      "29     \t [0.44435392 0.29813616]. \t  -1.3225047595076598 \t -0.04144924932713778\n",
      "30     \t [1.35470146 1.73746011]. \t  -1.0814355096756798 \t -0.04144924932713778\n",
      "31     \t [-1.40704717  2.048     ]. \t  -6.259249209545584 \t -0.04144924932713778\n",
      "32     \t [1.31610889 2.048     ]. \t  -10.076514124974617 \t -0.04144924932713778\n",
      "33     \t [ 0.24320386 -1.41500085]. \t  -217.88425851577827 \t -0.04144924932713778\n",
      "34     \t [-0.7643177   0.78184835]. \t  -7.020033057301125 \t -0.04144924932713778\n",
      "35     \t [0.95391416 0.79535371]. \t  -1.3154059767190578 \t -0.04144924932713778\n",
      "36     \t [ 0.66039964 -2.048     ]. \t  -617.2043627991254 \t -0.04144924932713778\n",
      "37     \t [-0.44669697  0.1784854 ]. \t  -2.1372541089710277 \t -0.04144924932713778\n",
      "38     \t [0.05156799 0.04399632]. \t  -1.070398509881202 \t -0.04144924932713778\n",
      "39     \t [-1.04986287  1.34848855]. \t  -10.267149043636227 \t -0.04144924932713778\n",
      "40     \t [1.35732801 1.86936099]. \t  -0.2007002522168927 \t -0.04144924932713778\n",
      "41     \t [0.3578394  0.09941995]. \t  -0.4943326832841616 \t -0.04144924932713778\n",
      "42     \t [1.18133058 1.4439034 ]. \t  -0.26676390613198747 \t -0.04144924932713778\n",
      "43     \t [0.90447126 0.87000618]. \t  -0.2788804954355403 \t -0.04144924932713778\n",
      "44     \t [2.048      1.04274297]. \t  -994.3319967452708 \t -0.04144924932713778\n",
      "45     \t [0.58421917 0.33984021]. \t  -0.17309032725078416 \t -0.04144924932713778\n",
      "46     \t [0.98301341 0.97208897]. \t  \u001b[92m-0.0036220022517345456\u001b[0m \t -0.0036220022517345456\n",
      "47     \t [0.73850107 0.59346067]. \t  -0.29952000738086426 \t -0.0036220022517345456\n",
      "48     \t [1.327265  1.7619615]. \t  -0.10711321342107087 \t -0.0036220022517345456\n",
      "49     \t [1.30903326 1.73692056]. \t  -0.15003534634661406 \t -0.0036220022517345456\n",
      "50     \t [0.3949888  0.13338138]. \t  -0.41727184112445903 \t -0.0036220022517345456\n",
      "51     \t [1.23861891 1.56766128]. \t  -0.1690599935399824 \t -0.0036220022517345456\n",
      "52     \t [0.9347552  0.94295316]. \t  -0.48292559844686495 \t -0.0036220022517345456\n",
      "53     \t [-0.43486029 -1.61848374]. \t  -328.79597791092755 \t -0.0036220022517345456\n",
      "54     \t [1.23362041 1.54573089]. \t  -0.11175484094325339 \t -0.0036220022517345456\n",
      "55     \t [0.75484972 0.5598079 ]. \t  -0.0700790529994938 \t -0.0036220022517345456\n",
      "56     \t [1.42453985 2.04235855]. \t  -0.19725068235833498 \t -0.0036220022517345456\n",
      "57     \t [0.92093171 0.85029509]. \t  -0.006726978696160852 \t -0.0036220022517345456\n",
      "58     \t [0.82532056 0.70177626]. \t  -0.07304058640025801 \t -0.0036220022517345456\n",
      "59     \t [0.76101527 0.58943968]. \t  -0.067713288912378 \t -0.0036220022517345456\n",
      "60     \t [1.04838514 1.11490685]. \t  -0.027290768976375092 \t -0.0036220022517345456\n",
      "61     \t [0.86301735 0.76163324]. \t  -0.047103603905142635 \t -0.0036220022517345456\n",
      "62     \t [1.23998695 1.56341263]. \t  -0.12439012449082858 \t -0.0036220022517345456\n",
      "63     \t [1.12353668 1.28248899]. \t  -0.055881027171716766 \t -0.0036220022517345456\n",
      "64     \t [1.36093916 1.8478054 ]. \t  -0.13216932742625342 \t -0.0036220022517345456\n",
      "65     \t [0.69814326 0.44858315]. \t  -0.241823440617415 \t -0.0036220022517345456\n",
      "66     \t [0.87364511 0.76785388]. \t  -0.01807980280863374 \t -0.0036220022517345456\n",
      "67     \t [0.30012526 0.04513167]. \t  -0.6918164625281259 \t -0.0036220022517345456\n",
      "68     \t [0.81953728 0.66094023]. \t  -0.04401819814452548 \t -0.0036220022517345456\n",
      "69     \t [0.68843119 0.48067947]. \t  -0.10162052510958448 \t -0.0036220022517345456\n",
      "70     \t [0.51623288 0.23845744]. \t  -0.3126488801296115 \t -0.0036220022517345456\n",
      "71     \t [0.99734917 0.98486014]. \t  -0.009699878196582327 \t -0.0036220022517345456\n",
      "72     \t [0.94287887 0.90506076]. \t  -0.028991601964101558 \t -0.0036220022517345456\n",
      "73     \t [1.28316438 1.68960943]. \t  -0.26593104939660006 \t -0.0036220022517345456\n",
      "74     \t [0.84458681 0.70516282]. \t  -0.030818444821309066 \t -0.0036220022517345456\n",
      "75     \t [1.06814909 1.17462873]. \t  -0.11812072136378292 \t -0.0036220022517345456\n",
      "76     \t [1.29904316 1.70859805]. \t  -0.1338841985935758 \t -0.0036220022517345456\n",
      "77     \t [0.89834451 0.79957021]. \t  -0.01588802797560308 \t -0.0036220022517345456\n",
      "78     \t [0.92173886 0.9008204 ]. \t  -0.2684518962431433 \t -0.0036220022517345456\n",
      "79     \t [0.51108313 0.22547931]. \t  -0.36667909863500847 \t -0.0036220022517345456\n",
      "80     \t [0.81865032 0.67951891]. \t  -0.04159363902804111 \t -0.0036220022517345456\n",
      "81     \t [0.89834248 0.81869239]. \t  -0.023960543523228885 \t -0.0036220022517345456\n",
      "82     \t [0.68453311 0.44050689]. \t  -0.1783606347045502 \t -0.0036220022517345456\n",
      "83     \t [0.78190582 0.60544   ]. \t  -0.051089527852012574 \t -0.0036220022517345456\n",
      "84     \t [1.31125548 1.73156286]. \t  -0.11169557259938262 \t -0.0036220022517345456\n",
      "85     \t [0.8168266  0.66890216]. \t  -0.033840298738371195 \t -0.0036220022517345456\n",
      "86     \t [0.95713598 0.91776424]. \t  \u001b[92m-0.0021112106150526417\u001b[0m \t -0.0021112106150526417\n",
      "87     \t [0.87499239 0.74409277]. \t  -0.061933235481323616 \t -0.0021112106150526417\n",
      "88     \t [-1.33500614  1.87963455]. \t  -6.400796398396844 \t -0.0021112106150526417\n",
      "89     \t [0.4834477  0.16928727]. \t  -0.6820056202895582 \t -0.0021112106150526417\n",
      "90     \t [0.5232369  0.23692618]. \t  -0.3631002881263107 \t -0.0021112106150526417\n",
      "91     \t [0.96327387 0.95167143]. \t  -0.05787333892985715 \t -0.0021112106150526417\n",
      "92     \t [0.81707336 0.66531678]. \t  -0.033987525620263415 \t -0.0021112106150526417\n",
      "93     \t [0.81972522 0.672178  ]. \t  -0.032504218912683576 \t -0.0021112106150526417\n",
      "94     \t [1.36288361 1.88505113]. \t  -0.20785713337883166 \t -0.0021112106150526417\n",
      "95     \t [0.94267189 0.92991093]. \t  -0.17369566044886436 \t -0.0021112106150526417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     \t [0.92720949 0.86251875]. \t  -0.0060831896833492905 \t -0.0021112106150526417\n",
      "97     \t [1.0994129  1.21646995]. \t  -0.015906579388992467 \t -0.0021112106150526417\n",
      "98     \t [0.91023953 0.86270218]. \t  -0.12478972088525317 \t -0.0021112106150526417\n",
      "99     \t [1.27799505 1.65338842]. \t  -0.11775095684353555 \t -0.0021112106150526417\n",
      "100    \t [1.40568987 2.04157152]. \t  -0.5950189208176758 \t -0.0021112106150526417\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \n",
    "\n",
    "np.random.seed(run_num_19)\n",
    "surrogate_winner_19 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_19 = GPGO(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_19.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.2888388  0.26430978]. \t  -3.777577453542735 \t -3.777577453542735\n",
      "init   \t [-0.04734225 -0.66978712]. \t  -46.25914429040042 \t -3.777577453542735\n",
      "init   \t [-0.50844394  0.13121892]. \t  -3.895838569726608 \t -3.777577453542735\n",
      "init   \t [-1.76903664  0.34623103]. \t  -782.3209705533411 \t -3.777577453542735\n",
      "init   \t [-1.07357076 -1.38954104]. \t  -650.524506936468 \t -3.777577453542735\n",
      "1      \t [ 2.048 -2.048]. \t  -3897.7342268415996 \t -3.777577453542735\n",
      "2      \t [2.048 2.048]. \t  -461.7603900415999 \t -3.777577453542735\n",
      "3      \t [-0.4844159  2.048    ]. \t  -331.0241348921225 \t -3.777577453542735\n",
      "4      \t [-2.048  2.048]. \t  -469.9523900415999 \t -3.777577453542735\n",
      "5      \t [2.048      0.47281149]. \t  -1386.0489509799118 \t -3.777577453542735\n",
      "6      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -3.777577453542735\n",
      "7      \t [-0.13310922 -2.048     ]. \t  -428.0030489688999 \t -3.777577453542735\n",
      "8      \t [0.7422317 2.048    ]. \t  -224.194920701761 \t -3.777577453542735\n",
      "9      \t [0.07159435 1.12709968]. \t  -126.74448667008589 \t -3.777577453542735\n",
      "10     \t [-0.71980262 -0.64267806]. \t  -137.7019637828713 \t -3.777577453542735\n",
      "11     \t [-1.09179443  1.23171481]. \t  -4.533210871428655 \t -3.777577453542735\n",
      "12     \t [ 1.05321472 -0.50033313]. \t  -259.0822384580298 \t -3.777577453542735\n",
      "13     \t [-0.58272819  0.85618784]. \t  -29.19420641366766 \t -3.777577453542735\n",
      "14     \t [1.1023569  1.26338682]. \t  \u001b[92m-0.24276318975751696\u001b[0m \t -0.24276318975751696\n",
      "15     \t [-1.27675626  2.048     ]. \t  -22.64711377413745 \t -0.24276318975751696\n",
      "16     \t [-0.37769135 -1.3516768 ]. \t  -225.1995174974239 \t -0.24276318975751696\n",
      "17     \t [ 0.47771364 -0.28161274]. \t  -26.264738382292478 \t -0.24276318975751696\n",
      "18     \t [ 0.58665554 -1.30972065]. \t  -273.70453559200496 \t -0.24276318975751696\n",
      "19     \t [1.27625574 1.79471575]. \t  -2.828168290885923 \t -0.24276318975751696\n",
      "20     \t [-2.048       1.23473734]. \t  -885.1937834371561 \t -0.24276318975751696\n",
      "21     \t [-2.048      -0.62662108]. \t  -2333.42217055823 \t -0.24276318975751696\n",
      "22     \t [0.89673184 1.43294832]. \t  -39.55216356591837 \t -0.24276318975751696\n",
      "23     \t [1.3483247 2.048    ]. \t  -5.412273100010858 \t -0.24276318975751696\n",
      "24     \t [-0.99627958  1.61392095]. \t  -42.59245996676029 \t -0.24276318975751696\n",
      "25     \t [0.6661783  0.75326592]. \t  -9.688752820297372 \t -0.24276318975751696\n",
      "26     \t [-0.75340592 -2.048     ]. \t  -687.221483287256 \t -0.24276318975751696\n",
      "27     \t [ 0.5094424  -0.74093688]. \t  -100.33435673426658 \t -0.24276318975751696\n",
      "28     \t [-0.16694421 -0.1395251 ]. \t  -4.163883265139555 \t -0.24276318975751696\n",
      "29     \t [-0.9684683   0.66692604]. \t  -11.219227860478965 \t -0.24276318975751696\n",
      "30     \t [ 2.048      -0.69913433]. \t  -2395.672168573232 \t -0.24276318975751696\n",
      "31     \t [0.96429777 0.34503845]. \t  -34.20409146209782 \t -0.24276318975751696\n",
      "32     \t [ 0.70512537 -2.048     ]. \t  -647.8921628342968 \t -0.24276318975751696\n",
      "33     \t [-1.36767049  1.73123804]. \t  -7.545881879152088 \t -0.24276318975751696\n",
      "34     \t [-0.31963213  0.33619387]. \t  -7.218394309895578 \t -0.24276318975751696\n",
      "35     \t [0.63766156 0.27988636]. \t  -1.7372347068373277 \t -0.24276318975751696\n",
      "36     \t [2.048      1.38614341]. \t  -789.6748955006302 \t -0.24276318975751696\n",
      "37     \t [-1.49543851  2.048     ]. \t  -9.774271279209714 \t -0.24276318975751696\n",
      "38     \t [1.20092226 1.51026546]. \t  -0.503466174273691 \t -0.24276318975751696\n",
      "39     \t [ 1.31337529 -1.42062194]. \t  -989.5634207070934 \t -0.24276318975751696\n",
      "40     \t [-0.70672816  0.49119821]. \t  -2.919754465391216 \t -0.24276318975751696\n",
      "41     \t [0.85814613 0.70671777]. \t  \u001b[92m-0.10831381216606074\u001b[0m \t -0.10831381216606074\n",
      "42     \t [-1.19020436 -0.1301405 ]. \t  -244.03341030163588 \t -0.10831381216606074\n",
      "43     \t [ 0.0949999  -0.01826418]. \t  -0.8934950037942677 \t -0.10831381216606074\n",
      "44     \t [1.43984485 1.93935092]. \t  -1.9837683346177757 \t -0.10831381216606074\n",
      "45     \t [1.41895394 2.048     ]. \t  -0.29502889170219254 \t -0.10831381216606074\n",
      "46     \t [0.66983337 0.51488736]. \t  -0.5473945260347551 \t -0.10831381216606074\n",
      "47     \t [0.86170671 0.77035342]. \t  \u001b[92m-0.09649225468525922\u001b[0m \t -0.09649225468525922\n",
      "48     \t [0.9035913  0.89003783]. \t  -0.5504108460501982 \t -0.09649225468525922\n",
      "49     \t [0.66997923 0.4153224 ]. \t  -0.22147242908824266 \t -0.09649225468525922\n",
      "50     \t [-1.37154525  1.91067131]. \t  -5.711458198989387 \t -0.09649225468525922\n",
      "51     \t [0.82392837 0.62839591]. \t  -0.2856430649374941 \t -0.09649225468525922\n",
      "52     \t [1.31621746 1.74174216]. \t  -0.10866809443473138 \t -0.09649225468525922\n",
      "53     \t [0.68939654 0.46122284]. \t  -0.11620001921391875 \t -0.09649225468525922\n",
      "54     \t [0.78678453 0.61415599]. \t  \u001b[92m-0.04783633290953428\u001b[0m \t -0.04783633290953428\n",
      "55     \t [1.05564116 1.19263819]. \t  -0.6155575382276364 \t -0.04783633290953428\n",
      "56     \t [1.41126549 2.00587299]. \t  -0.189311028867757 \t -0.04783633290953428\n",
      "57     \t [-1.22900612  1.52049069]. \t  -4.978537702011321 \t -0.04783633290953428\n",
      "58     \t [1.40748821 1.97747432]. \t  -0.16730599918346237 \t -0.04783633290953428\n",
      "59     \t [0.87487054 0.75139135]. \t  \u001b[92m-0.03527731465469404\u001b[0m \t -0.03527731465469404\n",
      "60     \t [1.39407393 1.93992866]. \t  -0.15652871257057419 \t -0.03527731465469404\n",
      "61     \t [0.03117624 0.00435528]. \t  -0.9397641741134916 \t -0.03527731465469404\n",
      "62     \t [0.96505213 0.98018099]. \t  -0.23990625408179422 \t -0.03527731465469404\n",
      "63     \t [0.75948433 0.58932587]. \t  -0.07349635363277401 \t -0.03527731465469404\n",
      "64     \t [1.21145887 1.52548909]. \t  -0.37945227588730773 \t -0.03527731465469404\n",
      "65     \t [0.71840264 0.47708418]. \t  -0.23153882491221273 \t -0.03527731465469404\n",
      "66     \t [1.1735376 1.4572848]. \t  -0.6716248154015088 \t -0.03527731465469404\n",
      "67     \t [0.77041274 0.62110322]. \t  -0.12870659223190126 \t -0.03527731465469404\n",
      "68     \t [0.75551182 0.53698541]. \t  -0.1741043009123529 \t -0.03527731465469404\n",
      "69     \t [1.34290333 1.75564224]. \t  -0.3455614682782145 \t -0.03527731465469404\n",
      "70     \t [0.69152052 0.5159025 ]. \t  -0.2373026844914593 \t -0.03527731465469404\n",
      "71     \t [0.81196701 0.67177918]. \t  -0.05095331650155753 \t -0.03527731465469404\n",
      "72     \t [0.63055627 0.43784521]. \t  -0.2984465774091231 \t -0.03527731465469404\n",
      "73     \t [0.73422596 0.52716117]. \t  -0.08486019167950243 \t -0.03527731465469404\n",
      "74     \t [1.09253888 1.20403353]. \t  \u001b[92m-0.019363492099804456\u001b[0m \t -0.019363492099804456\n",
      "75     \t [0.68998034 0.4684556 ]. \t  -0.10191446775765169 \t -0.019363492099804456\n",
      "76     \t [ 1.31622697 -2.048     ]. \t  -1429.2828253263851 \t -0.019363492099804456\n",
      "77     \t [0.72847348 0.53564302]. \t  -0.0761961469221465 \t -0.019363492099804456\n",
      "78     \t [0.73631247 0.51382202]. \t  -0.14981288472644033 \t -0.019363492099804456\n",
      "79     \t [0.85031897 0.74436085]. \t  -0.06785221852095512 \t -0.019363492099804456\n",
      "80     \t [1.35038584 1.82423428]. \t  -0.1228181741038043 \t -0.019363492099804456\n",
      "81     \t [0.8762888  0.78565407]. \t  -0.046888905910122444 \t -0.019363492099804456\n",
      "82     \t [0.27506267 0.07892518]. \t  -0.5266006249553066 \t -0.019363492099804456\n",
      "83     \t [1.30753342 1.71402681]. \t  -0.09649802203556279 \t -0.019363492099804456\n",
      "84     \t [1.37420162 1.92418689]. \t  -0.26788171253257576 \t -0.019363492099804456\n",
      "85     \t [0.86797151 0.75583694]. \t  \u001b[92m-0.01803786149409551\u001b[0m \t -0.01803786149409551\n",
      "86     \t [0.7558981  0.54492518]. \t  -0.1295817365749956 \t -0.01803786149409551\n",
      "87     \t [1.30286057 1.66690612]. \t  -0.18499100652013073 \t -0.01803786149409551\n",
      "88     \t [1.21654219 1.52001066]. \t  -0.20717673904142755 \t -0.01803786149409551\n",
      "89     \t [0.94831962 0.94990096]. \t  -0.2586143777407267 \t -0.01803786149409551\n",
      "90     \t [1.08474096 1.22638523]. \t  -0.25441165444206254 \t -0.01803786149409551\n",
      "91     \t [0.86555099 0.77462948]. \t  -0.08285170984804187 \t -0.01803786149409551\n",
      "92     \t [0.90652415 0.86715072]. \t  -0.21453324241474156 \t -0.01803786149409551\n",
      "93     \t [1.16588435 1.33200266]. \t  -0.10195746467996326 \t -0.01803786149409551\n",
      "94     \t [ 0.10893929 -0.00353682]. \t  -0.817719322303277 \t -0.01803786149409551\n",
      "95     \t [1.31170819 1.7256594 ]. \t  -0.09974367602198016 \t -0.01803786149409551\n",
      "96     \t [1.18833933 1.44439135]. \t  -0.13941985063341916 \t -0.01803786149409551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     \t [1.31014469 1.74576988]. \t  -0.18198470089767937 \t -0.01803786149409551\n",
      "98     \t [0.75563139 0.54055524]. \t  -0.1522753527936998 \t -0.01803786149409551\n",
      "99     \t [0.87765391 0.82593939]. \t  -0.3248056132873941 \t -0.01803786149409551\n",
      "100    \t [1.32320089 1.76847911]. \t  -0.13550002603177946 \t -0.01803786149409551\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \n",
    "\n",
    "np.random.seed(run_num_20)\n",
    "surrogate_winner_20 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_20 = GPGO(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_20.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-7.7382985562695135, -6.768277613987485)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 1\n",
    "\n",
    "loser_output_1 = np.append(np.max(loser_1.GP.y[0:n_init]),loser_1.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_1 = np.append(np.max(winner_1.GP.y[0:n_init]),winner_1.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_1 = np.log(y_global_orig - loser_output_1)\n",
    "regret_winner_1 = np.log(y_global_orig - winner_output_1)\n",
    "\n",
    "train_regret_loser_1 = min_max_array(regret_loser_1)\n",
    "train_regret_winner_1 = min_max_array(regret_winner_1)\n",
    "\n",
    "min_train_regret_loser_1 = min(train_regret_loser_1)\n",
    "min_train_regret_winner_1 = min(train_regret_winner_1)\n",
    "\n",
    "min_train_regret_loser_1, min_train_regret_winner_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.578961856157758, -6.444389006486416)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 2\n",
    "\n",
    "loser_output_2 = np.append(np.max(loser_2.GP.y[0:n_init]),loser_2.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_2 = np.append(np.max(winner_2.GP.y[0:n_init]),winner_2.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_2 = np.log(y_global_orig - loser_output_2)\n",
    "regret_winner_2 = np.log(y_global_orig - winner_output_2)\n",
    "\n",
    "train_regret_loser_2 = min_max_array(regret_loser_2)\n",
    "train_regret_winner_2 = min_max_array(regret_winner_2)\n",
    "\n",
    "min_train_regret_loser_2 = min(train_regret_loser_2)\n",
    "min_train_regret_winner_2 = min(train_regret_winner_2)\n",
    "\n",
    "min_train_regret_loser_2, min_train_regret_winner_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.423724656799146, -5.391416894122287)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 3\n",
    "\n",
    "loser_output_3 = np.append(np.max(loser_3.GP.y[0:n_init]),loser_3.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_3 = np.append(np.max(winner_3.GP.y[0:n_init]),winner_3.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_3 = np.log(y_global_orig - loser_output_3)\n",
    "regret_winner_3 = np.log(y_global_orig - winner_output_3)\n",
    "\n",
    "train_regret_loser_3 = min_max_array(regret_loser_3)\n",
    "train_regret_winner_3 = min_max_array(regret_winner_3)\n",
    "\n",
    "min_train_regret_loser_3 = min(train_regret_loser_3)\n",
    "min_train_regret_winner_3 = min(train_regret_winner_3)\n",
    "\n",
    "min_train_regret_loser_3, min_train_regret_winner_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.805857922469597, -6.001949286333172)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 4\n",
    "\n",
    "loser_output_4 = np.append(np.max(loser_4.GP.y[0:n_init]),loser_4.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_4 = np.append(np.max(winner_4.GP.y[0:n_init]),winner_4.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_4 = np.log(y_global_orig - loser_output_4)\n",
    "regret_winner_4 = np.log(y_global_orig - winner_output_4)\n",
    "\n",
    "train_regret_loser_4 = min_max_array(regret_loser_4)\n",
    "train_regret_winner_4 = min_max_array(regret_winner_4)\n",
    "\n",
    "min_train_regret_loser_4 = min(train_regret_loser_4)\n",
    "min_train_regret_winner_4 = min(train_regret_winner_4)\n",
    "\n",
    "min_train_regret_loser_4, min_train_regret_winner_4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.163924777679048, -6.0555517882291)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 5\n",
    "\n",
    "loser_output_5 = np.append(np.max(loser_5.GP.y[0:n_init]),loser_5.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_5 = np.append(np.max(winner_5.GP.y[0:n_init]),winner_5.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_5 = np.log(y_global_orig - loser_output_5)\n",
    "regret_winner_5 = np.log(y_global_orig - winner_output_5)\n",
    "\n",
    "train_regret_loser_5 = min_max_array(regret_loser_5)\n",
    "train_regret_winner_5 = min_max_array(regret_winner_5)\n",
    "\n",
    "min_train_regret_loser_5 = min(train_regret_loser_5)\n",
    "min_train_regret_winner_5 = min(train_regret_winner_5)\n",
    "\n",
    "min_train_regret_loser_5, min_train_regret_winner_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.49470853304788, -4.837005166307307)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 6\n",
    "\n",
    "loser_output_6 = np.append(np.max(loser_6.GP.y[0:n_init]),loser_6.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_6 = np.append(np.max(winner_6.GP.y[0:n_init]),winner_6.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_6 = np.log(y_global_orig - loser_output_6)\n",
    "regret_winner_6 = np.log(y_global_orig - winner_output_6)\n",
    "\n",
    "train_regret_loser_6 = min_max_array(regret_loser_6)\n",
    "train_regret_winner_6 = min_max_array(regret_winner_6)\n",
    "\n",
    "min_train_regret_loser_6 = min(train_regret_loser_6)\n",
    "min_train_regret_winner_6 = min(train_regret_winner_6)\n",
    "\n",
    "min_train_regret_loser_6, min_train_regret_winner_6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-7.1406456930929565, -7.140569121589815)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 7\n",
    "\n",
    "loser_output_7 = np.append(np.max(loser_7.GP.y[0:n_init]),loser_7.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_7 = np.append(np.max(winner_7.GP.y[0:n_init]),winner_7.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_7 = np.log(y_global_orig - loser_output_7)\n",
    "regret_winner_7 = np.log(y_global_orig - winner_output_7)\n",
    "\n",
    "train_regret_loser_7 = min_max_array(regret_loser_7)\n",
    "train_regret_winner_7 = min_max_array(regret_winner_7)\n",
    "\n",
    "min_train_regret_loser_7 = min(train_regret_loser_7)\n",
    "min_train_regret_winner_7 = min(train_regret_winner_7)\n",
    "\n",
    "min_train_regret_loser_7, min_train_regret_winner_7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.519425086485754, -8.342005181973366)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 8\n",
    "\n",
    "loser_output_8 = np.append(np.max(loser_8.GP.y[0:n_init]),loser_8.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_8 = np.append(np.max(winner_8.GP.y[0:n_init]),winner_8.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_8 = np.log(y_global_orig - loser_output_8)\n",
    "regret_winner_8 = np.log(y_global_orig - winner_output_8)\n",
    "\n",
    "train_regret_loser_8 = min_max_array(regret_loser_8)\n",
    "train_regret_winner_8 = min_max_array(regret_winner_8)\n",
    "\n",
    "min_train_regret_loser_8 = min(train_regret_loser_8)\n",
    "min_train_regret_winner_8 = min(train_regret_winner_8)\n",
    "\n",
    "min_train_regret_loser_8, min_train_regret_winner_8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.629380121121366, -6.140289834290268)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 9\n",
    "\n",
    "loser_output_9 = np.append(np.max(loser_9.GP.y[0:n_init]),loser_9.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_9 = np.append(np.max(winner_9.GP.y[0:n_init]),winner_9.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_9 = np.log(y_global_orig - loser_output_9)\n",
    "regret_winner_9 = np.log(y_global_orig - winner_output_9)\n",
    "\n",
    "train_regret_loser_9 = min_max_array(regret_loser_9)\n",
    "train_regret_winner_9 = min_max_array(regret_winner_9)\n",
    "\n",
    "min_train_regret_loser_9 = min(train_regret_loser_9)\n",
    "min_train_regret_winner_9 = min(train_regret_winner_9)\n",
    "\n",
    "min_train_regret_loser_9, min_train_regret_winner_9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-6.174907200404692, -9.507193373968422)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 10\n",
    "\n",
    "loser_output_10 = np.append(np.max(loser_10.GP.y[0:n_init]),loser_10.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_10 = np.append(np.max(winner_10.GP.y[0:n_init]),winner_10.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_10 = np.log(y_global_orig - loser_output_10)\n",
    "regret_winner_10 = np.log(y_global_orig - winner_output_10)\n",
    "\n",
    "train_regret_loser_10 = min_max_array(regret_loser_10)\n",
    "train_regret_winner_10 = min_max_array(regret_winner_10)\n",
    "\n",
    "min_train_regret_loser_10 = min(train_regret_loser_10)\n",
    "min_train_regret_winner_10 = min(train_regret_winner_10)\n",
    "\n",
    "min_train_regret_loser_10, min_train_regret_winner_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.192581773904122, -5.37703834144317)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 11\n",
    "\n",
    "loser_output_11 = np.append(np.max(loser_11.GP.y[0:n_init]),loser_11.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_11 = np.append(np.max(winner_11.GP.y[0:n_init]),winner_11.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_11 = np.log(y_global_orig - loser_output_11)\n",
    "regret_winner_11 = np.log(y_global_orig - winner_output_11)\n",
    "\n",
    "train_regret_loser_11 = min_max_array(regret_loser_11)\n",
    "train_regret_winner_11 = min_max_array(regret_winner_11)\n",
    "\n",
    "min_train_regret_loser_11 = min(train_regret_loser_11)\n",
    "min_train_regret_winner_11 = min(train_regret_winner_11)\n",
    "\n",
    "min_train_regret_loser_11, min_train_regret_winner_11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.92524445582635, -7.432816738078053)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 12\n",
    "\n",
    "loser_output_12 = np.append(np.max(loser_12.GP.y[0:n_init]),loser_12.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_12 = np.append(np.max(winner_12.GP.y[0:n_init]),winner_12.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_12 = np.log(y_global_orig - loser_output_12)\n",
    "regret_winner_12 = np.log(y_global_orig - winner_output_12)\n",
    "\n",
    "train_regret_loser_12 = min_max_array(regret_loser_12)\n",
    "train_regret_winner_12 = min_max_array(regret_winner_12)\n",
    "\n",
    "min_train_regret_loser_12 = min(train_regret_loser_12)\n",
    "min_train_regret_winner_12 = min(train_regret_winner_12)\n",
    "\n",
    "min_train_regret_loser_12, min_train_regret_winner_12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.556308931050801, -6.357102293801433)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 13\n",
    "\n",
    "loser_output_13 = np.append(np.max(loser_13.GP.y[0:n_init]),loser_13.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_13 = np.append(np.max(winner_13.GP.y[0:n_init]),winner_13.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_13 = np.log(y_global_orig - loser_output_13)\n",
    "regret_winner_13 = np.log(y_global_orig - winner_output_13)\n",
    "\n",
    "train_regret_loser_13 = min_max_array(regret_loser_13)\n",
    "train_regret_winner_13 = min_max_array(regret_winner_13)\n",
    "\n",
    "min_train_regret_loser_13 = min(train_regret_loser_13)\n",
    "min_train_regret_winner_13 = min(train_regret_winner_13)\n",
    "\n",
    "min_train_regret_loser_13, min_train_regret_winner_13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.118283958431365, -3.852802935438718)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 14\n",
    "\n",
    "loser_output_14 = np.append(np.max(loser_14.GP.y[0:n_init]),loser_14.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_14 = np.append(np.max(winner_14.GP.y[0:n_init]),winner_14.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_14 = np.log(y_global_orig - loser_output_14)\n",
    "regret_winner_14 = np.log(y_global_orig - winner_output_14)\n",
    "\n",
    "train_regret_loser_14 = min_max_array(regret_loser_14)\n",
    "train_regret_winner_14 = min_max_array(regret_winner_14)\n",
    "\n",
    "min_train_regret_loser_14 = min(train_regret_loser_14)\n",
    "min_train_regret_winner_14 = min(train_regret_winner_14)\n",
    "\n",
    "min_train_regret_loser_14, min_train_regret_winner_14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-6.127312051267847, -9.00038114803546)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 15\n",
    "\n",
    "loser_output_15 = np.append(np.max(loser_15.GP.y[0:n_init]),loser_15.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_15 = np.append(np.max(winner_15.GP.y[0:n_init]),winner_15.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_15 = np.log(y_global_orig - loser_output_15)\n",
    "regret_winner_15 = np.log(y_global_orig - winner_output_15)\n",
    "\n",
    "train_regret_loser_15 = min_max_array(regret_loser_15)\n",
    "train_regret_winner_15 = min_max_array(regret_winner_15)\n",
    "\n",
    "min_train_regret_loser_15 = min(train_regret_loser_15)\n",
    "min_train_regret_winner_15 = min(train_regret_winner_15)\n",
    "\n",
    "min_train_regret_loser_15, min_train_regret_winner_15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-6.755273764345566, -7.874289233659799)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 16\n",
    "\n",
    "loser_output_16 = np.append(np.max(loser_16.GP.y[0:n_init]),loser_16.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_16 = np.append(np.max(winner_16.GP.y[0:n_init]),winner_16.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_16 = np.log(y_global_orig - loser_output_16)\n",
    "regret_winner_16 = np.log(y_global_orig - winner_output_16)\n",
    "\n",
    "train_regret_loser_16 = min_max_array(regret_loser_16)\n",
    "train_regret_winner_16 = min_max_array(regret_winner_16)\n",
    "\n",
    "min_train_regret_loser_16 = min(train_regret_loser_16)\n",
    "min_train_regret_winner_16 = min(train_regret_winner_16)\n",
    "\n",
    "min_train_regret_loser_16, min_train_regret_winner_16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.3190113124632847, -8.228426074775305)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 17\n",
    "\n",
    "loser_output_17 = np.append(np.max(loser_17.GP.y[0:n_init]),loser_17.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_17 = np.append(np.max(winner_17.GP.y[0:n_init]),winner_17.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_17 = np.log(y_global_orig - loser_output_17)\n",
    "regret_winner_17 = np.log(y_global_orig - winner_output_17)\n",
    "\n",
    "train_regret_loser_17 = min_max_array(regret_loser_17)\n",
    "train_regret_winner_17 = min_max_array(regret_winner_17)\n",
    "\n",
    "min_train_regret_loser_17 = min(train_regret_loser_17)\n",
    "min_train_regret_winner_17 = min(train_regret_winner_17)\n",
    "\n",
    "min_train_regret_loser_17, min_train_regret_winner_17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-7.836586915668434, -7.1712712919342465)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 18\n",
    "\n",
    "loser_output_18 = np.append(np.max(loser_18.GP.y[0:n_init]),loser_18.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_18 = np.append(np.max(winner_18.GP.y[0:n_init]),winner_18.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_18 = np.log(y_global_orig - loser_output_18)\n",
    "regret_winner_18 = np.log(y_global_orig - winner_output_18)\n",
    "\n",
    "train_regret_loser_18 = min_max_array(regret_loser_18)\n",
    "train_regret_winner_18 = min_max_array(regret_winner_18)\n",
    "\n",
    "min_train_regret_loser_18 = min(train_regret_loser_18)\n",
    "min_train_regret_winner_18 = min(train_regret_winner_18)\n",
    "\n",
    "min_train_regret_loser_18, min_train_regret_winner_18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-6.166481028093252, -6.160493744816674)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 19\n",
    "\n",
    "loser_output_19 = np.append(np.max(loser_19.GP.y[0:n_init]),loser_19.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_19 = np.append(np.max(winner_19.GP.y[0:n_init]),winner_19.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_19 = np.log(y_global_orig - loser_output_19)\n",
    "regret_winner_19 = np.log(y_global_orig - winner_output_19)\n",
    "\n",
    "train_regret_loser_19 = min_max_array(regret_loser_19)\n",
    "train_regret_winner_19 = min_max_array(regret_winner_19)\n",
    "\n",
    "min_train_regret_loser_19 = min(train_regret_loser_19)\n",
    "min_train_regret_winner_19 = min(train_regret_winner_19)\n",
    "\n",
    "min_train_regret_loser_19, min_train_regret_winner_19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.263532134808551, -4.015282313830275)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 20\n",
    "\n",
    "loser_output_20 = np.append(np.max(loser_20.GP.y[0:n_init]),loser_20.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_20 = np.append(np.max(winner_20.GP.y[0:n_init]),winner_20.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_20 = np.log(y_global_orig - loser_output_20)\n",
    "regret_winner_20 = np.log(y_global_orig - winner_output_20)\n",
    "\n",
    "train_regret_loser_20 = min_max_array(regret_loser_20)\n",
    "train_regret_winner_20 = min_max_array(regret_winner_20)\n",
    "\n",
    "min_train_regret_loser_20 = min(train_regret_loser_20)\n",
    "min_train_regret_winner_20 = min(train_regret_winner_20)\n",
    "\n",
    "min_train_regret_loser_20, min_train_regret_winner_20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration1 :\n",
    "\n",
    "slice1 = 0\n",
    "\n",
    "loser1 = [train_regret_loser_1[slice1],\n",
    "       train_regret_loser_2[slice1],\n",
    "       train_regret_loser_3[slice1],\n",
    "       train_regret_loser_4[slice1],\n",
    "       train_regret_loser_5[slice1],\n",
    "       train_regret_loser_6[slice1],\n",
    "       train_regret_loser_7[slice1],\n",
    "       train_regret_loser_8[slice1],\n",
    "       train_regret_loser_9[slice1],\n",
    "       train_regret_loser_10[slice1],\n",
    "       train_regret_loser_11[slice1],\n",
    "       train_regret_loser_12[slice1],\n",
    "       train_regret_loser_13[slice1],\n",
    "       train_regret_loser_14[slice1],\n",
    "       train_regret_loser_15[slice1],\n",
    "       train_regret_loser_16[slice1],\n",
    "       train_regret_loser_17[slice1],\n",
    "       train_regret_loser_18[slice1],\n",
    "       train_regret_loser_19[slice1],\n",
    "       train_regret_loser_20[slice1]]\n",
    "\n",
    "winner1 = [train_regret_winner_1[slice1],\n",
    "       train_regret_winner_2[slice1],\n",
    "       train_regret_winner_3[slice1],\n",
    "       train_regret_winner_4[slice1],\n",
    "       train_regret_winner_5[slice1],\n",
    "       train_regret_winner_6[slice1],\n",
    "       train_regret_winner_7[slice1],\n",
    "       train_regret_winner_8[slice1],\n",
    "       train_regret_winner_9[slice1],\n",
    "       train_regret_winner_10[slice1],\n",
    "       train_regret_winner_11[slice1],\n",
    "       train_regret_winner_12[slice1],\n",
    "       train_regret_winner_13[slice1],\n",
    "       train_regret_winner_14[slice1],\n",
    "       train_regret_winner_15[slice1],\n",
    "       train_regret_winner_16[slice1],\n",
    "       train_regret_winner_17[slice1],\n",
    "       train_regret_winner_18[slice1],\n",
    "       train_regret_winner_19[slice1],\n",
    "       train_regret_winner_20[slice1]]\n",
    "\n",
    "loser1_results = pd.DataFrame(loser1).sort_values(by=[0], ascending=False)\n",
    "winner1_results = pd.DataFrame(winner1).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser1 = np.asarray(loser1_results[4:5][0])[0]\n",
    "median_loser1 = np.asarray(loser1_results[9:10][0])[0]\n",
    "upper_loser1 = np.asarray(loser1_results[14:15][0])[0]\n",
    "\n",
    "lower_winner1 = np.asarray(winner1_results[4:5][0])[0]\n",
    "median_winner1 = np.asarray(winner1_results[9:10][0])[0]\n",
    "upper_winner1 = np.asarray(winner1_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration11 :\n",
    "\n",
    "slice11 = 10\n",
    "\n",
    "loser11 = [train_regret_loser_1[slice11],\n",
    "       train_regret_loser_2[slice11],\n",
    "       train_regret_loser_3[slice11],\n",
    "       train_regret_loser_4[slice11],\n",
    "       train_regret_loser_5[slice11],\n",
    "       train_regret_loser_6[slice11],\n",
    "       train_regret_loser_7[slice11],\n",
    "       train_regret_loser_8[slice11],\n",
    "       train_regret_loser_9[slice11],\n",
    "       train_regret_loser_10[slice11],\n",
    "       train_regret_loser_11[slice11],\n",
    "       train_regret_loser_12[slice11],\n",
    "       train_regret_loser_13[slice11],\n",
    "       train_regret_loser_14[slice11],\n",
    "       train_regret_loser_15[slice11],\n",
    "       train_regret_loser_16[slice11],\n",
    "       train_regret_loser_17[slice11],\n",
    "       train_regret_loser_18[slice11],\n",
    "       train_regret_loser_19[slice11],\n",
    "       train_regret_loser_20[slice11]]\n",
    "\n",
    "winner11 = [train_regret_winner_1[slice11],\n",
    "       train_regret_winner_2[slice11],\n",
    "       train_regret_winner_3[slice11],\n",
    "       train_regret_winner_4[slice11],\n",
    "       train_regret_winner_5[slice11],\n",
    "       train_regret_winner_6[slice11],\n",
    "       train_regret_winner_7[slice11],\n",
    "       train_regret_winner_8[slice11],\n",
    "       train_regret_winner_9[slice11],\n",
    "       train_regret_winner_10[slice11],\n",
    "       train_regret_winner_11[slice11],\n",
    "       train_regret_winner_12[slice11],\n",
    "       train_regret_winner_13[slice11],\n",
    "       train_regret_winner_14[slice11],\n",
    "       train_regret_winner_15[slice11],\n",
    "       train_regret_winner_16[slice11],\n",
    "       train_regret_winner_17[slice11],\n",
    "       train_regret_winner_18[slice11],\n",
    "       train_regret_winner_19[slice11],\n",
    "       train_regret_winner_20[slice11]]\n",
    "\n",
    "loser11_results = pd.DataFrame(loser11).sort_values(by=[0], ascending=False)\n",
    "winner11_results = pd.DataFrame(winner11).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser11 = np.asarray(loser11_results[4:5][0])[0]\n",
    "median_loser11 = np.asarray(loser11_results[9:10][0])[0]\n",
    "upper_loser11 = np.asarray(loser11_results[14:15][0])[0]\n",
    "\n",
    "lower_winner11 = np.asarray(winner11_results[4:5][0])[0]\n",
    "median_winner11 = np.asarray(winner11_results[9:10][0])[0]\n",
    "upper_winner11 = np.asarray(winner11_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration21 :\n",
    "\n",
    "slice21 = 20\n",
    "\n",
    "loser21 = [train_regret_loser_1[slice21],\n",
    "       train_regret_loser_2[slice21],\n",
    "       train_regret_loser_3[slice21],\n",
    "       train_regret_loser_4[slice21],\n",
    "       train_regret_loser_5[slice21],\n",
    "       train_regret_loser_6[slice21],\n",
    "       train_regret_loser_7[slice21],\n",
    "       train_regret_loser_8[slice21],\n",
    "       train_regret_loser_9[slice21],\n",
    "       train_regret_loser_10[slice21],\n",
    "       train_regret_loser_11[slice21],\n",
    "       train_regret_loser_12[slice21],\n",
    "       train_regret_loser_13[slice21],\n",
    "       train_regret_loser_14[slice21],\n",
    "       train_regret_loser_15[slice21],\n",
    "       train_regret_loser_16[slice21],\n",
    "       train_regret_loser_17[slice21],\n",
    "       train_regret_loser_18[slice21],\n",
    "       train_regret_loser_19[slice21],\n",
    "       train_regret_loser_20[slice21]]\n",
    "\n",
    "winner21 = [train_regret_winner_1[slice21],\n",
    "       train_regret_winner_2[slice21],\n",
    "       train_regret_winner_3[slice21],\n",
    "       train_regret_winner_4[slice21],\n",
    "       train_regret_winner_5[slice21],\n",
    "       train_regret_winner_6[slice21],\n",
    "       train_regret_winner_7[slice21],\n",
    "       train_regret_winner_8[slice21],\n",
    "       train_regret_winner_9[slice21],\n",
    "       train_regret_winner_10[slice21],\n",
    "       train_regret_winner_11[slice21],\n",
    "       train_regret_winner_12[slice21],\n",
    "       train_regret_winner_13[slice21],\n",
    "       train_regret_winner_14[slice21],\n",
    "       train_regret_winner_15[slice21],\n",
    "       train_regret_winner_16[slice21],\n",
    "       train_regret_winner_17[slice21],\n",
    "       train_regret_winner_18[slice21],\n",
    "       train_regret_winner_19[slice21],\n",
    "       train_regret_winner_20[slice21]]\n",
    "\n",
    "loser21_results = pd.DataFrame(loser21).sort_values(by=[0], ascending=False)\n",
    "winner21_results = pd.DataFrame(winner21).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser21 = np.asarray(loser21_results[4:5][0])[0]\n",
    "median_loser21 = np.asarray(loser21_results[9:10][0])[0]\n",
    "upper_loser21 = np.asarray(loser21_results[14:15][0])[0]\n",
    "\n",
    "lower_winner21 = np.asarray(winner21_results[4:5][0])[0]\n",
    "median_winner21 = np.asarray(winner21_results[9:10][0])[0]\n",
    "upper_winner21 = np.asarray(winner21_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration31 :\n",
    "\n",
    "slice31 = 30\n",
    "\n",
    "loser31 = [train_regret_loser_1[slice31],\n",
    "       train_regret_loser_2[slice31],\n",
    "       train_regret_loser_3[slice31],\n",
    "       train_regret_loser_4[slice31],\n",
    "       train_regret_loser_5[slice31],\n",
    "       train_regret_loser_6[slice31],\n",
    "       train_regret_loser_7[slice31],\n",
    "       train_regret_loser_8[slice31],\n",
    "       train_regret_loser_9[slice31],\n",
    "       train_regret_loser_10[slice31],\n",
    "       train_regret_loser_11[slice31],\n",
    "       train_regret_loser_12[slice31],\n",
    "       train_regret_loser_13[slice31],\n",
    "       train_regret_loser_14[slice31],\n",
    "       train_regret_loser_15[slice31],\n",
    "       train_regret_loser_16[slice31],\n",
    "       train_regret_loser_17[slice31],\n",
    "       train_regret_loser_18[slice31],\n",
    "       train_regret_loser_19[slice31],\n",
    "       train_regret_loser_20[slice31]]\n",
    "\n",
    "winner31 = [train_regret_winner_1[slice31],\n",
    "       train_regret_winner_2[slice31],\n",
    "       train_regret_winner_3[slice31],\n",
    "       train_regret_winner_4[slice31],\n",
    "       train_regret_winner_5[slice31],\n",
    "       train_regret_winner_6[slice31],\n",
    "       train_regret_winner_7[slice31],\n",
    "       train_regret_winner_8[slice31],\n",
    "       train_regret_winner_9[slice31],\n",
    "       train_regret_winner_10[slice31],\n",
    "       train_regret_winner_11[slice31],\n",
    "       train_regret_winner_12[slice31],\n",
    "       train_regret_winner_13[slice31],\n",
    "       train_regret_winner_14[slice31],\n",
    "       train_regret_winner_15[slice31],\n",
    "       train_regret_winner_16[slice31],\n",
    "       train_regret_winner_17[slice31],\n",
    "       train_regret_winner_18[slice31],\n",
    "       train_regret_winner_19[slice31],\n",
    "       train_regret_winner_20[slice31]]\n",
    "\n",
    "loser31_results = pd.DataFrame(loser31).sort_values(by=[0], ascending=False)\n",
    "winner31_results = pd.DataFrame(winner31).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser31 = np.asarray(loser31_results[4:5][0])[0]\n",
    "median_loser31 = np.asarray(loser31_results[9:10][0])[0]\n",
    "upper_loser31 = np.asarray(loser31_results[14:15][0])[0]\n",
    "\n",
    "lower_winner31 = np.asarray(winner31_results[4:5][0])[0]\n",
    "median_winner31 = np.asarray(winner31_results[9:10][0])[0]\n",
    "upper_winner31 = np.asarray(winner31_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration41 :\n",
    "\n",
    "slice41 = 40\n",
    "\n",
    "loser41 = [train_regret_loser_1[slice41],\n",
    "       train_regret_loser_2[slice41],\n",
    "       train_regret_loser_3[slice41],\n",
    "       train_regret_loser_4[slice41],\n",
    "       train_regret_loser_5[slice41],\n",
    "       train_regret_loser_6[slice41],\n",
    "       train_regret_loser_7[slice41],\n",
    "       train_regret_loser_8[slice41],\n",
    "       train_regret_loser_9[slice41],\n",
    "       train_regret_loser_10[slice41],\n",
    "       train_regret_loser_11[slice41],\n",
    "       train_regret_loser_12[slice41],\n",
    "       train_regret_loser_13[slice41],\n",
    "       train_regret_loser_14[slice41],\n",
    "       train_regret_loser_15[slice41],\n",
    "       train_regret_loser_16[slice41],\n",
    "       train_regret_loser_17[slice41],\n",
    "       train_regret_loser_18[slice41],\n",
    "       train_regret_loser_19[slice41],\n",
    "       train_regret_loser_20[slice41]]\n",
    "\n",
    "winner41 = [train_regret_winner_1[slice41],\n",
    "       train_regret_winner_2[slice41],\n",
    "       train_regret_winner_3[slice41],\n",
    "       train_regret_winner_4[slice41],\n",
    "       train_regret_winner_5[slice41],\n",
    "       train_regret_winner_6[slice41],\n",
    "       train_regret_winner_7[slice41],\n",
    "       train_regret_winner_8[slice41],\n",
    "       train_regret_winner_9[slice41],\n",
    "       train_regret_winner_10[slice41],\n",
    "       train_regret_winner_11[slice41],\n",
    "       train_regret_winner_12[slice41],\n",
    "       train_regret_winner_13[slice41],\n",
    "       train_regret_winner_14[slice41],\n",
    "       train_regret_winner_15[slice41],\n",
    "       train_regret_winner_16[slice41],\n",
    "       train_regret_winner_17[slice41],\n",
    "       train_regret_winner_18[slice41],\n",
    "       train_regret_winner_19[slice41],\n",
    "       train_regret_winner_20[slice41]]\n",
    "\n",
    "loser41_results = pd.DataFrame(loser41).sort_values(by=[0], ascending=False)\n",
    "winner41_results = pd.DataFrame(winner41).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser41 = np.asarray(loser41_results[4:5][0])[0]\n",
    "median_loser41 = np.asarray(loser41_results[9:10][0])[0]\n",
    "upper_loser41 = np.asarray(loser41_results[14:15][0])[0]\n",
    "\n",
    "lower_winner41 = np.asarray(winner41_results[4:5][0])[0]\n",
    "median_winner41 = np.asarray(winner41_results[9:10][0])[0]\n",
    "upper_winner41 = np.asarray(winner41_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration51 :\n",
    "\n",
    "slice51 = 50\n",
    "\n",
    "loser51 = [train_regret_loser_1[slice51],\n",
    "       train_regret_loser_2[slice51],\n",
    "       train_regret_loser_3[slice51],\n",
    "       train_regret_loser_4[slice51],\n",
    "       train_regret_loser_5[slice51],\n",
    "       train_regret_loser_6[slice51],\n",
    "       train_regret_loser_7[slice51],\n",
    "       train_regret_loser_8[slice51],\n",
    "       train_regret_loser_9[slice51],\n",
    "       train_regret_loser_10[slice51],\n",
    "       train_regret_loser_11[slice51],\n",
    "       train_regret_loser_12[slice51],\n",
    "       train_regret_loser_13[slice51],\n",
    "       train_regret_loser_14[slice51],\n",
    "       train_regret_loser_15[slice51],\n",
    "       train_regret_loser_16[slice51],\n",
    "       train_regret_loser_17[slice51],\n",
    "       train_regret_loser_18[slice51],\n",
    "       train_regret_loser_19[slice51],\n",
    "       train_regret_loser_20[slice51]]\n",
    "\n",
    "winner51 = [train_regret_winner_1[slice51],\n",
    "       train_regret_winner_2[slice51],\n",
    "       train_regret_winner_3[slice51],\n",
    "       train_regret_winner_4[slice51],\n",
    "       train_regret_winner_5[slice51],\n",
    "       train_regret_winner_6[slice51],\n",
    "       train_regret_winner_7[slice51],\n",
    "       train_regret_winner_8[slice51],\n",
    "       train_regret_winner_9[slice51],\n",
    "       train_regret_winner_10[slice51],\n",
    "       train_regret_winner_11[slice51],\n",
    "       train_regret_winner_12[slice51],\n",
    "       train_regret_winner_13[slice51],\n",
    "       train_regret_winner_14[slice51],\n",
    "       train_regret_winner_15[slice51],\n",
    "       train_regret_winner_16[slice51],\n",
    "       train_regret_winner_17[slice51],\n",
    "       train_regret_winner_18[slice51],\n",
    "       train_regret_winner_19[slice51],\n",
    "       train_regret_winner_20[slice51]]\n",
    "\n",
    "loser51_results = pd.DataFrame(loser51).sort_values(by=[0], ascending=False)\n",
    "winner51_results = pd.DataFrame(winner51).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser51 = np.asarray(loser51_results[4:5][0])[0]\n",
    "median_loser51 = np.asarray(loser51_results[9:10][0])[0]\n",
    "upper_loser51 = np.asarray(loser51_results[14:15][0])[0]\n",
    "\n",
    "lower_winner51 = np.asarray(winner51_results[4:5][0])[0]\n",
    "median_winner51 = np.asarray(winner51_results[9:10][0])[0]\n",
    "upper_winner51 = np.asarray(winner51_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration61 :\n",
    "\n",
    "slice61 = 60\n",
    "\n",
    "loser61 = [train_regret_loser_1[slice61],\n",
    "       train_regret_loser_2[slice61],\n",
    "       train_regret_loser_3[slice61],\n",
    "       train_regret_loser_4[slice61],\n",
    "       train_regret_loser_5[slice61],\n",
    "       train_regret_loser_6[slice61],\n",
    "       train_regret_loser_7[slice61],\n",
    "       train_regret_loser_8[slice61],\n",
    "       train_regret_loser_9[slice61],\n",
    "       train_regret_loser_10[slice61],\n",
    "       train_regret_loser_11[slice61],\n",
    "       train_regret_loser_12[slice61],\n",
    "       train_regret_loser_13[slice61],\n",
    "       train_regret_loser_14[slice61],\n",
    "       train_regret_loser_15[slice61],\n",
    "       train_regret_loser_16[slice61],\n",
    "       train_regret_loser_17[slice61],\n",
    "       train_regret_loser_18[slice61],\n",
    "       train_regret_loser_19[slice61],\n",
    "       train_regret_loser_20[slice61]]\n",
    "\n",
    "winner61 = [train_regret_winner_1[slice61],\n",
    "       train_regret_winner_2[slice61],\n",
    "       train_regret_winner_3[slice61],\n",
    "       train_regret_winner_4[slice61],\n",
    "       train_regret_winner_5[slice61],\n",
    "       train_regret_winner_6[slice61],\n",
    "       train_regret_winner_7[slice61],\n",
    "       train_regret_winner_8[slice61],\n",
    "       train_regret_winner_9[slice61],\n",
    "       train_regret_winner_10[slice61],\n",
    "       train_regret_winner_11[slice61],\n",
    "       train_regret_winner_12[slice61],\n",
    "       train_regret_winner_13[slice61],\n",
    "       train_regret_winner_14[slice61],\n",
    "       train_regret_winner_15[slice61],\n",
    "       train_regret_winner_16[slice61],\n",
    "       train_regret_winner_17[slice61],\n",
    "       train_regret_winner_18[slice61],\n",
    "       train_regret_winner_19[slice61],\n",
    "       train_regret_winner_20[slice61]]\n",
    "\n",
    "loser61_results = pd.DataFrame(loser61).sort_values(by=[0], ascending=False)\n",
    "winner61_results = pd.DataFrame(winner61).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser61 = np.asarray(loser61_results[4:5][0])[0]\n",
    "median_loser61 = np.asarray(loser61_results[9:10][0])[0]\n",
    "upper_loser61 = np.asarray(loser61_results[14:15][0])[0]\n",
    "\n",
    "lower_winner61 = np.asarray(winner61_results[4:5][0])[0]\n",
    "median_winner61 = np.asarray(winner61_results[9:10][0])[0]\n",
    "upper_winner61 = np.asarray(winner61_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration71 :\n",
    "\n",
    "slice71 = 70\n",
    "\n",
    "loser71 = [train_regret_loser_1[slice71],\n",
    "       train_regret_loser_2[slice71],\n",
    "       train_regret_loser_3[slice71],\n",
    "       train_regret_loser_4[slice71],\n",
    "       train_regret_loser_5[slice71],\n",
    "       train_regret_loser_6[slice71],\n",
    "       train_regret_loser_7[slice71],\n",
    "       train_regret_loser_8[slice71],\n",
    "       train_regret_loser_9[slice71],\n",
    "       train_regret_loser_10[slice71],\n",
    "       train_regret_loser_11[slice71],\n",
    "       train_regret_loser_12[slice71],\n",
    "       train_regret_loser_13[slice71],\n",
    "       train_regret_loser_14[slice71],\n",
    "       train_regret_loser_15[slice71],\n",
    "       train_regret_loser_16[slice71],\n",
    "       train_regret_loser_17[slice71],\n",
    "       train_regret_loser_18[slice71],\n",
    "       train_regret_loser_19[slice71],\n",
    "       train_regret_loser_20[slice71]]\n",
    "\n",
    "winner71 = [train_regret_winner_1[slice71],\n",
    "       train_regret_winner_2[slice71],\n",
    "       train_regret_winner_3[slice71],\n",
    "       train_regret_winner_4[slice71],\n",
    "       train_regret_winner_5[slice71],\n",
    "       train_regret_winner_6[slice71],\n",
    "       train_regret_winner_7[slice71],\n",
    "       train_regret_winner_8[slice71],\n",
    "       train_regret_winner_9[slice71],\n",
    "       train_regret_winner_10[slice71],\n",
    "       train_regret_winner_11[slice71],\n",
    "       train_regret_winner_12[slice71],\n",
    "       train_regret_winner_13[slice71],\n",
    "       train_regret_winner_14[slice71],\n",
    "       train_regret_winner_15[slice71],\n",
    "       train_regret_winner_16[slice71],\n",
    "       train_regret_winner_17[slice71],\n",
    "       train_regret_winner_18[slice71],\n",
    "       train_regret_winner_19[slice71],\n",
    "       train_regret_winner_20[slice71]]\n",
    "\n",
    "loser71_results = pd.DataFrame(loser71).sort_values(by=[0], ascending=False)\n",
    "winner71_results = pd.DataFrame(winner71).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser71 = np.asarray(loser71_results[4:5][0])[0]\n",
    "median_loser71 = np.asarray(loser71_results[9:10][0])[0]\n",
    "upper_loser71 = np.asarray(loser71_results[14:15][0])[0]\n",
    "\n",
    "lower_winner71 = np.asarray(winner71_results[4:5][0])[0]\n",
    "median_winner71 = np.asarray(winner71_results[9:10][0])[0]\n",
    "upper_winner71 = np.asarray(winner71_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration81 :\n",
    "\n",
    "slice81 = 80\n",
    "\n",
    "loser81 = [train_regret_loser_1[slice81],\n",
    "       train_regret_loser_2[slice81],\n",
    "       train_regret_loser_3[slice81],\n",
    "       train_regret_loser_4[slice81],\n",
    "       train_regret_loser_5[slice81],\n",
    "       train_regret_loser_6[slice81],\n",
    "       train_regret_loser_7[slice81],\n",
    "       train_regret_loser_8[slice81],\n",
    "       train_regret_loser_9[slice81],\n",
    "       train_regret_loser_10[slice81],\n",
    "       train_regret_loser_11[slice81],\n",
    "       train_regret_loser_12[slice81],\n",
    "       train_regret_loser_13[slice81],\n",
    "       train_regret_loser_14[slice81],\n",
    "       train_regret_loser_15[slice81],\n",
    "       train_regret_loser_16[slice81],\n",
    "       train_regret_loser_17[slice81],\n",
    "       train_regret_loser_18[slice81],\n",
    "       train_regret_loser_19[slice81],\n",
    "       train_regret_loser_20[slice81]]\n",
    "\n",
    "winner81 = [train_regret_winner_1[slice81],\n",
    "       train_regret_winner_2[slice81],\n",
    "       train_regret_winner_3[slice81],\n",
    "       train_regret_winner_4[slice81],\n",
    "       train_regret_winner_5[slice81],\n",
    "       train_regret_winner_6[slice81],\n",
    "       train_regret_winner_7[slice81],\n",
    "       train_regret_winner_8[slice81],\n",
    "       train_regret_winner_9[slice81],\n",
    "       train_regret_winner_10[slice81],\n",
    "       train_regret_winner_11[slice81],\n",
    "       train_regret_winner_12[slice81],\n",
    "       train_regret_winner_13[slice81],\n",
    "       train_regret_winner_14[slice81],\n",
    "       train_regret_winner_15[slice81],\n",
    "       train_regret_winner_16[slice81],\n",
    "       train_regret_winner_17[slice81],\n",
    "       train_regret_winner_18[slice81],\n",
    "       train_regret_winner_19[slice81],\n",
    "       train_regret_winner_20[slice81]]\n",
    "\n",
    "loser81_results = pd.DataFrame(loser81).sort_values(by=[0], ascending=False)\n",
    "winner81_results = pd.DataFrame(winner81).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser81 = np.asarray(loser81_results[4:5][0])[0]\n",
    "median_loser81 = np.asarray(loser81_results[9:10][0])[0]\n",
    "upper_loser81 = np.asarray(loser81_results[14:15][0])[0]\n",
    "\n",
    "lower_winner81 = np.asarray(winner81_results[4:5][0])[0]\n",
    "median_winner81 = np.asarray(winner81_results[9:10][0])[0]\n",
    "upper_winner81 = np.asarray(winner81_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration91 :\n",
    "\n",
    "slice91 = 90\n",
    "\n",
    "loser91 = [train_regret_loser_1[slice91],\n",
    "       train_regret_loser_2[slice91],\n",
    "       train_regret_loser_3[slice91],\n",
    "       train_regret_loser_4[slice91],\n",
    "       train_regret_loser_5[slice91],\n",
    "       train_regret_loser_6[slice91],\n",
    "       train_regret_loser_7[slice91],\n",
    "       train_regret_loser_8[slice91],\n",
    "       train_regret_loser_9[slice91],\n",
    "       train_regret_loser_10[slice91],\n",
    "       train_regret_loser_11[slice91],\n",
    "       train_regret_loser_12[slice91],\n",
    "       train_regret_loser_13[slice91],\n",
    "       train_regret_loser_14[slice91],\n",
    "       train_regret_loser_15[slice91],\n",
    "       train_regret_loser_16[slice91],\n",
    "       train_regret_loser_17[slice91],\n",
    "       train_regret_loser_18[slice91],\n",
    "       train_regret_loser_19[slice91],\n",
    "       train_regret_loser_20[slice91]]\n",
    "\n",
    "winner91 = [train_regret_winner_1[slice91],\n",
    "       train_regret_winner_2[slice91],\n",
    "       train_regret_winner_3[slice91],\n",
    "       train_regret_winner_4[slice91],\n",
    "       train_regret_winner_5[slice91],\n",
    "       train_regret_winner_6[slice91],\n",
    "       train_regret_winner_7[slice91],\n",
    "       train_regret_winner_8[slice91],\n",
    "       train_regret_winner_9[slice91],\n",
    "       train_regret_winner_10[slice91],\n",
    "       train_regret_winner_11[slice91],\n",
    "       train_regret_winner_12[slice91],\n",
    "       train_regret_winner_13[slice91],\n",
    "       train_regret_winner_14[slice91],\n",
    "       train_regret_winner_15[slice91],\n",
    "       train_regret_winner_16[slice91],\n",
    "       train_regret_winner_17[slice91],\n",
    "       train_regret_winner_18[slice91],\n",
    "       train_regret_winner_19[slice91],\n",
    "       train_regret_winner_20[slice91]]\n",
    "\n",
    "loser91_results = pd.DataFrame(loser91).sort_values(by=[0], ascending=False)\n",
    "winner91_results = pd.DataFrame(winner91).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser91 = np.asarray(loser91_results[4:5][0])[0]\n",
    "median_loser91 = np.asarray(loser91_results[9:10][0])[0]\n",
    "upper_loser91 = np.asarray(loser91_results[14:15][0])[0]\n",
    "\n",
    "lower_winner91 = np.asarray(winner91_results[4:5][0])[0]\n",
    "median_winner91 = np.asarray(winner91_results[9:10][0])[0]\n",
    "upper_winner91 = np.asarray(winner91_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration101 :\n",
    "\n",
    "slice101 = 100\n",
    "\n",
    "loser101 = [train_regret_loser_1[slice101],\n",
    "       train_regret_loser_2[slice101],\n",
    "       train_regret_loser_3[slice101],\n",
    "       train_regret_loser_4[slice101],\n",
    "       train_regret_loser_5[slice101],\n",
    "       train_regret_loser_6[slice101],\n",
    "       train_regret_loser_7[slice101],\n",
    "       train_regret_loser_8[slice101],\n",
    "       train_regret_loser_9[slice101],\n",
    "       train_regret_loser_10[slice101],\n",
    "       train_regret_loser_11[slice101],\n",
    "       train_regret_loser_12[slice101],\n",
    "       train_regret_loser_13[slice101],\n",
    "       train_regret_loser_14[slice101],\n",
    "       train_regret_loser_15[slice101],\n",
    "       train_regret_loser_16[slice101],\n",
    "       train_regret_loser_17[slice101],\n",
    "       train_regret_loser_18[slice101],\n",
    "       train_regret_loser_19[slice101],\n",
    "       train_regret_loser_20[slice101]]\n",
    "\n",
    "winner101 = [train_regret_winner_1[slice101],\n",
    "       train_regret_winner_2[slice101],\n",
    "       train_regret_winner_3[slice101],\n",
    "       train_regret_winner_4[slice101],\n",
    "       train_regret_winner_5[slice101],\n",
    "       train_regret_winner_6[slice101],\n",
    "       train_regret_winner_7[slice101],\n",
    "       train_regret_winner_8[slice101],\n",
    "       train_regret_winner_9[slice101],\n",
    "       train_regret_winner_10[slice101],\n",
    "       train_regret_winner_11[slice101],\n",
    "       train_regret_winner_12[slice101],\n",
    "       train_regret_winner_13[slice101],\n",
    "       train_regret_winner_14[slice101],\n",
    "       train_regret_winner_15[slice101],\n",
    "       train_regret_winner_16[slice101],\n",
    "       train_regret_winner_17[slice101],\n",
    "       train_regret_winner_18[slice101],\n",
    "       train_regret_winner_19[slice101],\n",
    "       train_regret_winner_20[slice101]]\n",
    "\n",
    "loser101_results = pd.DataFrame(loser101).sort_values(by=[0], ascending=False)\n",
    "winner101_results = pd.DataFrame(winner101).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser101 = np.asarray(loser101_results[4:5][0])[0]\n",
    "median_loser101 = np.asarray(loser101_results[9:10][0])[0]\n",
    "upper_loser101 = np.asarray(loser101_results[14:15][0])[0]\n",
    "\n",
    "lower_winner101 = np.asarray(winner101_results[4:5][0])[0]\n",
    "median_winner101 = np.asarray(winner101_results[9:10][0])[0]\n",
    "upper_winner101 = np.asarray(winner101_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration2 :\n",
    "\n",
    "slice2 = 1\n",
    "\n",
    "loser2 = [train_regret_loser_1[slice2],\n",
    "       train_regret_loser_2[slice2],\n",
    "       train_regret_loser_3[slice2],\n",
    "       train_regret_loser_4[slice2],\n",
    "       train_regret_loser_5[slice2],\n",
    "       train_regret_loser_6[slice2],\n",
    "       train_regret_loser_7[slice2],\n",
    "       train_regret_loser_8[slice2],\n",
    "       train_regret_loser_9[slice2],\n",
    "       train_regret_loser_10[slice2],\n",
    "       train_regret_loser_11[slice2],\n",
    "       train_regret_loser_12[slice2],\n",
    "       train_regret_loser_13[slice2],\n",
    "       train_regret_loser_14[slice2],\n",
    "       train_regret_loser_15[slice2],\n",
    "       train_regret_loser_16[slice2],\n",
    "       train_regret_loser_17[slice2],\n",
    "       train_regret_loser_18[slice2],\n",
    "       train_regret_loser_19[slice2],\n",
    "       train_regret_loser_20[slice2]]\n",
    "\n",
    "winner2 = [train_regret_winner_1[slice2],\n",
    "       train_regret_winner_2[slice2],\n",
    "       train_regret_winner_3[slice2],\n",
    "       train_regret_winner_4[slice2],\n",
    "       train_regret_winner_5[slice2],\n",
    "       train_regret_winner_6[slice2],\n",
    "       train_regret_winner_7[slice2],\n",
    "       train_regret_winner_8[slice2],\n",
    "       train_regret_winner_9[slice2],\n",
    "       train_regret_winner_10[slice2],\n",
    "       train_regret_winner_11[slice2],\n",
    "       train_regret_winner_12[slice2],\n",
    "       train_regret_winner_13[slice2],\n",
    "       train_regret_winner_14[slice2],\n",
    "       train_regret_winner_15[slice2],\n",
    "       train_regret_winner_16[slice2],\n",
    "       train_regret_winner_17[slice2],\n",
    "       train_regret_winner_18[slice2],\n",
    "       train_regret_winner_19[slice2],\n",
    "       train_regret_winner_20[slice2]]\n",
    "\n",
    "loser2_results = pd.DataFrame(loser2).sort_values(by=[0], ascending=False)\n",
    "winner2_results = pd.DataFrame(winner2).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser2 = np.asarray(loser2_results[4:5][0])[0]\n",
    "median_loser2 = np.asarray(loser2_results[9:10][0])[0]\n",
    "upper_loser2 = np.asarray(loser2_results[14:15][0])[0]\n",
    "\n",
    "lower_winner2 = np.asarray(winner2_results[4:5][0])[0]\n",
    "median_winner2 = np.asarray(winner2_results[9:10][0])[0]\n",
    "upper_winner2 = np.asarray(winner2_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration12 :\n",
    "\n",
    "slice12 = 11\n",
    "\n",
    "loser12 = [train_regret_loser_1[slice12],\n",
    "       train_regret_loser_2[slice12],\n",
    "       train_regret_loser_3[slice12],\n",
    "       train_regret_loser_4[slice12],\n",
    "       train_regret_loser_5[slice12],\n",
    "       train_regret_loser_6[slice12],\n",
    "       train_regret_loser_7[slice12],\n",
    "       train_regret_loser_8[slice12],\n",
    "       train_regret_loser_9[slice12],\n",
    "       train_regret_loser_10[slice12],\n",
    "       train_regret_loser_11[slice12],\n",
    "       train_regret_loser_12[slice12],\n",
    "       train_regret_loser_13[slice12],\n",
    "       train_regret_loser_14[slice12],\n",
    "       train_regret_loser_15[slice12],\n",
    "       train_regret_loser_16[slice12],\n",
    "       train_regret_loser_17[slice12],\n",
    "       train_regret_loser_18[slice12],\n",
    "       train_regret_loser_19[slice12],\n",
    "       train_regret_loser_20[slice12]]\n",
    "\n",
    "winner12 = [train_regret_winner_1[slice12],\n",
    "       train_regret_winner_2[slice12],\n",
    "       train_regret_winner_3[slice12],\n",
    "       train_regret_winner_4[slice12],\n",
    "       train_regret_winner_5[slice12],\n",
    "       train_regret_winner_6[slice12],\n",
    "       train_regret_winner_7[slice12],\n",
    "       train_regret_winner_8[slice12],\n",
    "       train_regret_winner_9[slice12],\n",
    "       train_regret_winner_10[slice12],\n",
    "       train_regret_winner_11[slice12],\n",
    "       train_regret_winner_12[slice12],\n",
    "       train_regret_winner_13[slice12],\n",
    "       train_regret_winner_14[slice12],\n",
    "       train_regret_winner_15[slice12],\n",
    "       train_regret_winner_16[slice12],\n",
    "       train_regret_winner_17[slice12],\n",
    "       train_regret_winner_18[slice12],\n",
    "       train_regret_winner_19[slice12],\n",
    "       train_regret_winner_20[slice12]]\n",
    "\n",
    "loser12_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\n",
    "winner12_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser12 = np.asarray(loser12_results[4:5][0])[0]\n",
    "median_loser12 = np.asarray(loser12_results[9:10][0])[0]\n",
    "upper_loser12 = np.asarray(loser12_results[14:15][0])[0]\n",
    "\n",
    "lower_winner12 = np.asarray(winner12_results[4:5][0])[0]\n",
    "median_winner12 = np.asarray(winner12_results[9:10][0])[0]\n",
    "upper_winner12 = np.asarray(winner12_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration22 :\n",
    "\n",
    "slice22 = 21\n",
    "\n",
    "loser22 = [train_regret_loser_1[slice22],\n",
    "       train_regret_loser_2[slice22],\n",
    "       train_regret_loser_3[slice22],\n",
    "       train_regret_loser_4[slice22],\n",
    "       train_regret_loser_5[slice22],\n",
    "       train_regret_loser_6[slice22],\n",
    "       train_regret_loser_7[slice22],\n",
    "       train_regret_loser_8[slice22],\n",
    "       train_regret_loser_9[slice22],\n",
    "       train_regret_loser_10[slice22],\n",
    "       train_regret_loser_11[slice22],\n",
    "       train_regret_loser_12[slice22],\n",
    "       train_regret_loser_13[slice22],\n",
    "       train_regret_loser_14[slice22],\n",
    "       train_regret_loser_15[slice22],\n",
    "       train_regret_loser_16[slice22],\n",
    "       train_regret_loser_17[slice22],\n",
    "       train_regret_loser_18[slice22],\n",
    "       train_regret_loser_19[slice22],\n",
    "       train_regret_loser_20[slice22]]\n",
    "\n",
    "winner22 = [train_regret_winner_1[slice22],\n",
    "       train_regret_winner_2[slice22],\n",
    "       train_regret_winner_3[slice22],\n",
    "       train_regret_winner_4[slice22],\n",
    "       train_regret_winner_5[slice22],\n",
    "       train_regret_winner_6[slice22],\n",
    "       train_regret_winner_7[slice22],\n",
    "       train_regret_winner_8[slice22],\n",
    "       train_regret_winner_9[slice22],\n",
    "       train_regret_winner_10[slice22],\n",
    "       train_regret_winner_11[slice22],\n",
    "       train_regret_winner_12[slice22],\n",
    "       train_regret_winner_13[slice22],\n",
    "       train_regret_winner_14[slice22],\n",
    "       train_regret_winner_15[slice22],\n",
    "       train_regret_winner_16[slice22],\n",
    "       train_regret_winner_17[slice22],\n",
    "       train_regret_winner_18[slice22],\n",
    "       train_regret_winner_19[slice22],\n",
    "       train_regret_winner_20[slice22]]\n",
    "\n",
    "loser22_results = pd.DataFrame(loser22).sort_values(by=[0], ascending=False)\n",
    "winner22_results = pd.DataFrame(winner22).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser22 = np.asarray(loser22_results[4:5][0])[0]\n",
    "median_loser22 = np.asarray(loser22_results[9:10][0])[0]\n",
    "upper_loser22 = np.asarray(loser22_results[14:15][0])[0]\n",
    "\n",
    "lower_winner22 = np.asarray(winner22_results[4:5][0])[0]\n",
    "median_winner22 = np.asarray(winner22_results[9:10][0])[0]\n",
    "upper_winner22 = np.asarray(winner22_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration32 :\n",
    "\n",
    "slice32 = 31\n",
    "\n",
    "loser32 = [train_regret_loser_1[slice32],\n",
    "       train_regret_loser_2[slice32],\n",
    "       train_regret_loser_3[slice32],\n",
    "       train_regret_loser_4[slice32],\n",
    "       train_regret_loser_5[slice32],\n",
    "       train_regret_loser_6[slice32],\n",
    "       train_regret_loser_7[slice32],\n",
    "       train_regret_loser_8[slice32],\n",
    "       train_regret_loser_9[slice32],\n",
    "       train_regret_loser_10[slice32],\n",
    "       train_regret_loser_11[slice32],\n",
    "       train_regret_loser_12[slice32],\n",
    "       train_regret_loser_13[slice32],\n",
    "       train_regret_loser_14[slice32],\n",
    "       train_regret_loser_15[slice32],\n",
    "       train_regret_loser_16[slice32],\n",
    "       train_regret_loser_17[slice32],\n",
    "       train_regret_loser_18[slice32],\n",
    "       train_regret_loser_19[slice32],\n",
    "       train_regret_loser_20[slice32]]\n",
    "\n",
    "winner32 = [train_regret_winner_1[slice32],\n",
    "       train_regret_winner_2[slice32],\n",
    "       train_regret_winner_3[slice32],\n",
    "       train_regret_winner_4[slice32],\n",
    "       train_regret_winner_5[slice32],\n",
    "       train_regret_winner_6[slice32],\n",
    "       train_regret_winner_7[slice32],\n",
    "       train_regret_winner_8[slice32],\n",
    "       train_regret_winner_9[slice32],\n",
    "       train_regret_winner_10[slice32],\n",
    "       train_regret_winner_11[slice32],\n",
    "       train_regret_winner_12[slice32],\n",
    "       train_regret_winner_13[slice32],\n",
    "       train_regret_winner_14[slice32],\n",
    "       train_regret_winner_15[slice32],\n",
    "       train_regret_winner_16[slice32],\n",
    "       train_regret_winner_17[slice32],\n",
    "       train_regret_winner_18[slice32],\n",
    "       train_regret_winner_19[slice32],\n",
    "       train_regret_winner_20[slice32]]\n",
    "\n",
    "loser32_results = pd.DataFrame(loser32).sort_values(by=[0], ascending=False)\n",
    "winner32_results = pd.DataFrame(winner32).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser32 = np.asarray(loser32_results[4:5][0])[0]\n",
    "median_loser32 = np.asarray(loser32_results[9:10][0])[0]\n",
    "upper_loser32 = np.asarray(loser32_results[14:15][0])[0]\n",
    "\n",
    "lower_winner32 = np.asarray(winner32_results[4:5][0])[0]\n",
    "median_winner32 = np.asarray(winner32_results[9:10][0])[0]\n",
    "upper_winner32 = np.asarray(winner32_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration42 :\n",
    "\n",
    "slice42 = 41\n",
    "\n",
    "loser42 = [train_regret_loser_1[slice42],\n",
    "       train_regret_loser_2[slice42],\n",
    "       train_regret_loser_3[slice42],\n",
    "       train_regret_loser_4[slice42],\n",
    "       train_regret_loser_5[slice42],\n",
    "       train_regret_loser_6[slice42],\n",
    "       train_regret_loser_7[slice42],\n",
    "       train_regret_loser_8[slice42],\n",
    "       train_regret_loser_9[slice42],\n",
    "       train_regret_loser_10[slice42],\n",
    "       train_regret_loser_11[slice42],\n",
    "       train_regret_loser_12[slice42],\n",
    "       train_regret_loser_13[slice42],\n",
    "       train_regret_loser_14[slice42],\n",
    "       train_regret_loser_15[slice42],\n",
    "       train_regret_loser_16[slice42],\n",
    "       train_regret_loser_17[slice42],\n",
    "       train_regret_loser_18[slice42],\n",
    "       train_regret_loser_19[slice42],\n",
    "       train_regret_loser_20[slice42]]\n",
    "\n",
    "winner42 = [train_regret_winner_1[slice42],\n",
    "       train_regret_winner_2[slice42],\n",
    "       train_regret_winner_3[slice42],\n",
    "       train_regret_winner_4[slice42],\n",
    "       train_regret_winner_5[slice42],\n",
    "       train_regret_winner_6[slice42],\n",
    "       train_regret_winner_7[slice42],\n",
    "       train_regret_winner_8[slice42],\n",
    "       train_regret_winner_9[slice42],\n",
    "       train_regret_winner_10[slice42],\n",
    "       train_regret_winner_11[slice42],\n",
    "       train_regret_winner_12[slice42],\n",
    "       train_regret_winner_13[slice42],\n",
    "       train_regret_winner_14[slice42],\n",
    "       train_regret_winner_15[slice42],\n",
    "       train_regret_winner_16[slice42],\n",
    "       train_regret_winner_17[slice42],\n",
    "       train_regret_winner_18[slice42],\n",
    "       train_regret_winner_19[slice42],\n",
    "       train_regret_winner_20[slice42]]\n",
    "\n",
    "loser42_results = pd.DataFrame(loser42).sort_values(by=[0], ascending=False)\n",
    "winner42_results = pd.DataFrame(winner42).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser42 = np.asarray(loser42_results[4:5][0])[0]\n",
    "median_loser42 = np.asarray(loser42_results[9:10][0])[0]\n",
    "upper_loser42 = np.asarray(loser42_results[14:15][0])[0]\n",
    "\n",
    "lower_winner42 = np.asarray(winner42_results[4:5][0])[0]\n",
    "median_winner42 = np.asarray(winner42_results[9:10][0])[0]\n",
    "upper_winner42 = np.asarray(winner42_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration52 :\n",
    "\n",
    "slice52 = 51\n",
    "\n",
    "loser52 = [train_regret_loser_1[slice52],\n",
    "       train_regret_loser_2[slice52],\n",
    "       train_regret_loser_3[slice52],\n",
    "       train_regret_loser_4[slice52],\n",
    "       train_regret_loser_5[slice52],\n",
    "       train_regret_loser_6[slice52],\n",
    "       train_regret_loser_7[slice52],\n",
    "       train_regret_loser_8[slice52],\n",
    "       train_regret_loser_9[slice52],\n",
    "       train_regret_loser_10[slice52],\n",
    "       train_regret_loser_11[slice52],\n",
    "       train_regret_loser_12[slice52],\n",
    "       train_regret_loser_13[slice52],\n",
    "       train_regret_loser_14[slice52],\n",
    "       train_regret_loser_15[slice52],\n",
    "       train_regret_loser_16[slice52],\n",
    "       train_regret_loser_17[slice52],\n",
    "       train_regret_loser_18[slice52],\n",
    "       train_regret_loser_19[slice52],\n",
    "       train_regret_loser_20[slice52]]\n",
    "\n",
    "winner52 = [train_regret_winner_1[slice52],\n",
    "       train_regret_winner_2[slice52],\n",
    "       train_regret_winner_3[slice52],\n",
    "       train_regret_winner_4[slice52],\n",
    "       train_regret_winner_5[slice52],\n",
    "       train_regret_winner_6[slice52],\n",
    "       train_regret_winner_7[slice52],\n",
    "       train_regret_winner_8[slice52],\n",
    "       train_regret_winner_9[slice52],\n",
    "       train_regret_winner_10[slice52],\n",
    "       train_regret_winner_11[slice52],\n",
    "       train_regret_winner_12[slice52],\n",
    "       train_regret_winner_13[slice52],\n",
    "       train_regret_winner_14[slice52],\n",
    "       train_regret_winner_15[slice52],\n",
    "       train_regret_winner_16[slice52],\n",
    "       train_regret_winner_17[slice52],\n",
    "       train_regret_winner_18[slice52],\n",
    "       train_regret_winner_19[slice52],\n",
    "       train_regret_winner_20[slice52]]\n",
    "\n",
    "loser52_results = pd.DataFrame(loser52).sort_values(by=[0], ascending=False)\n",
    "winner52_results = pd.DataFrame(winner52).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser52 = np.asarray(loser52_results[4:5][0])[0]\n",
    "median_loser52 = np.asarray(loser52_results[9:10][0])[0]\n",
    "upper_loser52 = np.asarray(loser52_results[14:15][0])[0]\n",
    "\n",
    "lower_winner52 = np.asarray(winner52_results[4:5][0])[0]\n",
    "median_winner52 = np.asarray(winner52_results[9:10][0])[0]\n",
    "upper_winner52 = np.asarray(winner52_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration62 :\n",
    "\n",
    "slice62 = 61\n",
    "\n",
    "loser62 = [train_regret_loser_1[slice62],\n",
    "       train_regret_loser_2[slice62],\n",
    "       train_regret_loser_3[slice62],\n",
    "       train_regret_loser_4[slice62],\n",
    "       train_regret_loser_5[slice62],\n",
    "       train_regret_loser_6[slice62],\n",
    "       train_regret_loser_7[slice62],\n",
    "       train_regret_loser_8[slice62],\n",
    "       train_regret_loser_9[slice62],\n",
    "       train_regret_loser_10[slice62],\n",
    "       train_regret_loser_11[slice62],\n",
    "       train_regret_loser_12[slice62],\n",
    "       train_regret_loser_13[slice62],\n",
    "       train_regret_loser_14[slice62],\n",
    "       train_regret_loser_15[slice62],\n",
    "       train_regret_loser_16[slice62],\n",
    "       train_regret_loser_17[slice62],\n",
    "       train_regret_loser_18[slice62],\n",
    "       train_regret_loser_19[slice62],\n",
    "       train_regret_loser_20[slice62]]\n",
    "\n",
    "winner62 = [train_regret_winner_1[slice62],\n",
    "       train_regret_winner_2[slice62],\n",
    "       train_regret_winner_3[slice62],\n",
    "       train_regret_winner_4[slice62],\n",
    "       train_regret_winner_5[slice62],\n",
    "       train_regret_winner_6[slice62],\n",
    "       train_regret_winner_7[slice62],\n",
    "       train_regret_winner_8[slice62],\n",
    "       train_regret_winner_9[slice62],\n",
    "       train_regret_winner_10[slice62],\n",
    "       train_regret_winner_11[slice62],\n",
    "       train_regret_winner_12[slice62],\n",
    "       train_regret_winner_13[slice62],\n",
    "       train_regret_winner_14[slice62],\n",
    "       train_regret_winner_15[slice62],\n",
    "       train_regret_winner_16[slice62],\n",
    "       train_regret_winner_17[slice62],\n",
    "       train_regret_winner_18[slice62],\n",
    "       train_regret_winner_19[slice62],\n",
    "       train_regret_winner_20[slice62]]\n",
    "\n",
    "loser62_results = pd.DataFrame(loser62).sort_values(by=[0], ascending=False)\n",
    "winner62_results = pd.DataFrame(winner62).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser62 = np.asarray(loser62_results[4:5][0])[0]\n",
    "median_loser62 = np.asarray(loser62_results[9:10][0])[0]\n",
    "upper_loser62 = np.asarray(loser62_results[14:15][0])[0]\n",
    "\n",
    "lower_winner62 = np.asarray(winner62_results[4:5][0])[0]\n",
    "median_winner62 = np.asarray(winner62_results[9:10][0])[0]\n",
    "upper_winner62 = np.asarray(winner62_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration72 :\n",
    "\n",
    "slice72 = 71\n",
    "\n",
    "loser72 = [train_regret_loser_1[slice72],\n",
    "       train_regret_loser_2[slice72],\n",
    "       train_regret_loser_3[slice72],\n",
    "       train_regret_loser_4[slice72],\n",
    "       train_regret_loser_5[slice72],\n",
    "       train_regret_loser_6[slice72],\n",
    "       train_regret_loser_7[slice72],\n",
    "       train_regret_loser_8[slice72],\n",
    "       train_regret_loser_9[slice72],\n",
    "       train_regret_loser_10[slice72],\n",
    "       train_regret_loser_11[slice72],\n",
    "       train_regret_loser_12[slice72],\n",
    "       train_regret_loser_13[slice72],\n",
    "       train_regret_loser_14[slice72],\n",
    "       train_regret_loser_15[slice72],\n",
    "       train_regret_loser_16[slice72],\n",
    "       train_regret_loser_17[slice72],\n",
    "       train_regret_loser_18[slice72],\n",
    "       train_regret_loser_19[slice72],\n",
    "       train_regret_loser_20[slice72]]\n",
    "\n",
    "winner72 = [train_regret_winner_1[slice72],\n",
    "       train_regret_winner_2[slice72],\n",
    "       train_regret_winner_3[slice72],\n",
    "       train_regret_winner_4[slice72],\n",
    "       train_regret_winner_5[slice72],\n",
    "       train_regret_winner_6[slice72],\n",
    "       train_regret_winner_7[slice72],\n",
    "       train_regret_winner_8[slice72],\n",
    "       train_regret_winner_9[slice72],\n",
    "       train_regret_winner_10[slice72],\n",
    "       train_regret_winner_11[slice72],\n",
    "       train_regret_winner_12[slice72],\n",
    "       train_regret_winner_13[slice72],\n",
    "       train_regret_winner_14[slice72],\n",
    "       train_regret_winner_15[slice72],\n",
    "       train_regret_winner_16[slice72],\n",
    "       train_regret_winner_17[slice72],\n",
    "       train_regret_winner_18[slice72],\n",
    "       train_regret_winner_19[slice72],\n",
    "       train_regret_winner_20[slice72]]\n",
    "\n",
    "loser72_results = pd.DataFrame(loser72).sort_values(by=[0], ascending=False)\n",
    "winner72_results = pd.DataFrame(winner72).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser72 = np.asarray(loser72_results[4:5][0])[0]\n",
    "median_loser72 = np.asarray(loser72_results[9:10][0])[0]\n",
    "upper_loser72 = np.asarray(loser72_results[14:15][0])[0]\n",
    "\n",
    "lower_winner72 = np.asarray(winner72_results[4:5][0])[0]\n",
    "median_winner72 = np.asarray(winner72_results[9:10][0])[0]\n",
    "upper_winner72 = np.asarray(winner72_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration82 :\n",
    "\n",
    "slice82 = 81\n",
    "\n",
    "loser82 = [train_regret_loser_1[slice82],\n",
    "       train_regret_loser_2[slice82],\n",
    "       train_regret_loser_3[slice82],\n",
    "       train_regret_loser_4[slice82],\n",
    "       train_regret_loser_5[slice82],\n",
    "       train_regret_loser_6[slice82],\n",
    "       train_regret_loser_7[slice82],\n",
    "       train_regret_loser_8[slice82],\n",
    "       train_regret_loser_9[slice82],\n",
    "       train_regret_loser_10[slice82],\n",
    "       train_regret_loser_11[slice82],\n",
    "       train_regret_loser_12[slice82],\n",
    "       train_regret_loser_13[slice82],\n",
    "       train_regret_loser_14[slice82],\n",
    "       train_regret_loser_15[slice82],\n",
    "       train_regret_loser_16[slice82],\n",
    "       train_regret_loser_17[slice82],\n",
    "       train_regret_loser_18[slice82],\n",
    "       train_regret_loser_19[slice82],\n",
    "       train_regret_loser_20[slice82]]\n",
    "\n",
    "winner82 = [train_regret_winner_1[slice82],\n",
    "       train_regret_winner_2[slice82],\n",
    "       train_regret_winner_3[slice82],\n",
    "       train_regret_winner_4[slice82],\n",
    "       train_regret_winner_5[slice82],\n",
    "       train_regret_winner_6[slice82],\n",
    "       train_regret_winner_7[slice82],\n",
    "       train_regret_winner_8[slice82],\n",
    "       train_regret_winner_9[slice82],\n",
    "       train_regret_winner_10[slice82],\n",
    "       train_regret_winner_11[slice82],\n",
    "       train_regret_winner_12[slice82],\n",
    "       train_regret_winner_13[slice82],\n",
    "       train_regret_winner_14[slice82],\n",
    "       train_regret_winner_15[slice82],\n",
    "       train_regret_winner_16[slice82],\n",
    "       train_regret_winner_17[slice82],\n",
    "       train_regret_winner_18[slice82],\n",
    "       train_regret_winner_19[slice82],\n",
    "       train_regret_winner_20[slice82]]\n",
    "\n",
    "loser82_results = pd.DataFrame(loser82).sort_values(by=[0], ascending=False)\n",
    "winner82_results = pd.DataFrame(winner82).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser82 = np.asarray(loser82_results[4:5][0])[0]\n",
    "median_loser82 = np.asarray(loser82_results[9:10][0])[0]\n",
    "upper_loser82 = np.asarray(loser82_results[14:15][0])[0]\n",
    "\n",
    "lower_winner82 = np.asarray(winner82_results[4:5][0])[0]\n",
    "median_winner82 = np.asarray(winner82_results[9:10][0])[0]\n",
    "upper_winner82 = np.asarray(winner82_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration92 :\n",
    "\n",
    "slice92 = 91\n",
    "\n",
    "loser92 = [train_regret_loser_1[slice92],\n",
    "       train_regret_loser_2[slice92],\n",
    "       train_regret_loser_3[slice92],\n",
    "       train_regret_loser_4[slice92],\n",
    "       train_regret_loser_5[slice92],\n",
    "       train_regret_loser_6[slice92],\n",
    "       train_regret_loser_7[slice92],\n",
    "       train_regret_loser_8[slice92],\n",
    "       train_regret_loser_9[slice92],\n",
    "       train_regret_loser_10[slice92],\n",
    "       train_regret_loser_11[slice92],\n",
    "       train_regret_loser_12[slice92],\n",
    "       train_regret_loser_13[slice92],\n",
    "       train_regret_loser_14[slice92],\n",
    "       train_regret_loser_15[slice92],\n",
    "       train_regret_loser_16[slice92],\n",
    "       train_regret_loser_17[slice92],\n",
    "       train_regret_loser_18[slice92],\n",
    "       train_regret_loser_19[slice92],\n",
    "       train_regret_loser_20[slice92]]\n",
    "\n",
    "winner92 = [train_regret_winner_1[slice92],\n",
    "       train_regret_winner_2[slice92],\n",
    "       train_regret_winner_3[slice92],\n",
    "       train_regret_winner_4[slice92],\n",
    "       train_regret_winner_5[slice92],\n",
    "       train_regret_winner_6[slice92],\n",
    "       train_regret_winner_7[slice92],\n",
    "       train_regret_winner_8[slice92],\n",
    "       train_regret_winner_9[slice92],\n",
    "       train_regret_winner_10[slice92],\n",
    "       train_regret_winner_11[slice92],\n",
    "       train_regret_winner_12[slice92],\n",
    "       train_regret_winner_13[slice92],\n",
    "       train_regret_winner_14[slice92],\n",
    "       train_regret_winner_15[slice92],\n",
    "       train_regret_winner_16[slice92],\n",
    "       train_regret_winner_17[slice92],\n",
    "       train_regret_winner_18[slice92],\n",
    "       train_regret_winner_19[slice92],\n",
    "       train_regret_winner_20[slice92]]\n",
    "\n",
    "loser92_results = pd.DataFrame(loser92).sort_values(by=[0], ascending=False)\n",
    "winner92_results = pd.DataFrame(winner92).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser92 = np.asarray(loser92_results[4:5][0])[0]\n",
    "median_loser92 = np.asarray(loser92_results[9:10][0])[0]\n",
    "upper_loser92 = np.asarray(loser92_results[14:15][0])[0]\n",
    "\n",
    "lower_winner92 = np.asarray(winner92_results[4:5][0])[0]\n",
    "median_winner92 = np.asarray(winner92_results[9:10][0])[0]\n",
    "upper_winner92 = np.asarray(winner92_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration3 :\n",
    "\n",
    "slice3 = 2\n",
    "\n",
    "loser3 = [train_regret_loser_1[slice3],\n",
    "       train_regret_loser_2[slice3],\n",
    "       train_regret_loser_3[slice3],\n",
    "       train_regret_loser_4[slice3],\n",
    "       train_regret_loser_5[slice3],\n",
    "       train_regret_loser_6[slice3],\n",
    "       train_regret_loser_7[slice3],\n",
    "       train_regret_loser_8[slice3],\n",
    "       train_regret_loser_9[slice3],\n",
    "       train_regret_loser_10[slice3],\n",
    "       train_regret_loser_11[slice3],\n",
    "       train_regret_loser_12[slice3],\n",
    "       train_regret_loser_13[slice3],\n",
    "       train_regret_loser_14[slice3],\n",
    "       train_regret_loser_15[slice3],\n",
    "       train_regret_loser_16[slice3],\n",
    "       train_regret_loser_17[slice3],\n",
    "       train_regret_loser_18[slice3],\n",
    "       train_regret_loser_19[slice3],\n",
    "       train_regret_loser_20[slice3]]\n",
    "\n",
    "winner3 = [train_regret_winner_1[slice3],\n",
    "       train_regret_winner_2[slice3],\n",
    "       train_regret_winner_3[slice3],\n",
    "       train_regret_winner_4[slice3],\n",
    "       train_regret_winner_5[slice3],\n",
    "       train_regret_winner_6[slice3],\n",
    "       train_regret_winner_7[slice3],\n",
    "       train_regret_winner_8[slice3],\n",
    "       train_regret_winner_9[slice3],\n",
    "       train_regret_winner_10[slice3],\n",
    "       train_regret_winner_11[slice3],\n",
    "       train_regret_winner_12[slice3],\n",
    "       train_regret_winner_13[slice3],\n",
    "       train_regret_winner_14[slice3],\n",
    "       train_regret_winner_15[slice3],\n",
    "       train_regret_winner_16[slice3],\n",
    "       train_regret_winner_17[slice3],\n",
    "       train_regret_winner_18[slice3],\n",
    "       train_regret_winner_19[slice3],\n",
    "       train_regret_winner_20[slice3]]\n",
    "\n",
    "loser3_results = pd.DataFrame(loser3).sort_values(by=[0], ascending=False)\n",
    "winner3_results = pd.DataFrame(winner3).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser3 = np.asarray(loser3_results[4:5][0])[0]\n",
    "median_loser3 = np.asarray(loser3_results[9:10][0])[0]\n",
    "upper_loser3 = np.asarray(loser3_results[14:15][0])[0]\n",
    "\n",
    "lower_winner3 = np.asarray(winner3_results[4:5][0])[0]\n",
    "median_winner3 = np.asarray(winner3_results[9:10][0])[0]\n",
    "upper_winner3 = np.asarray(winner3_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration13 :\n",
    "\n",
    "slice13 = 12\n",
    "\n",
    "loser13 = [train_regret_loser_1[slice13],\n",
    "       train_regret_loser_2[slice13],\n",
    "       train_regret_loser_3[slice13],\n",
    "       train_regret_loser_4[slice13],\n",
    "       train_regret_loser_5[slice13],\n",
    "       train_regret_loser_6[slice13],\n",
    "       train_regret_loser_7[slice13],\n",
    "       train_regret_loser_8[slice13],\n",
    "       train_regret_loser_9[slice13],\n",
    "       train_regret_loser_10[slice13],\n",
    "       train_regret_loser_11[slice13],\n",
    "       train_regret_loser_12[slice13],\n",
    "       train_regret_loser_13[slice13],\n",
    "       train_regret_loser_14[slice13],\n",
    "       train_regret_loser_15[slice13],\n",
    "       train_regret_loser_16[slice13],\n",
    "       train_regret_loser_17[slice13],\n",
    "       train_regret_loser_18[slice13],\n",
    "       train_regret_loser_19[slice13],\n",
    "       train_regret_loser_20[slice13]]\n",
    "\n",
    "winner13 = [train_regret_winner_1[slice13],\n",
    "       train_regret_winner_2[slice13],\n",
    "       train_regret_winner_3[slice13],\n",
    "       train_regret_winner_4[slice13],\n",
    "       train_regret_winner_5[slice13],\n",
    "       train_regret_winner_6[slice13],\n",
    "       train_regret_winner_7[slice13],\n",
    "       train_regret_winner_8[slice13],\n",
    "       train_regret_winner_9[slice13],\n",
    "       train_regret_winner_10[slice13],\n",
    "       train_regret_winner_11[slice13],\n",
    "       train_regret_winner_12[slice13],\n",
    "       train_regret_winner_13[slice13],\n",
    "       train_regret_winner_14[slice13],\n",
    "       train_regret_winner_15[slice13],\n",
    "       train_regret_winner_16[slice13],\n",
    "       train_regret_winner_17[slice13],\n",
    "       train_regret_winner_18[slice13],\n",
    "       train_regret_winner_19[slice13],\n",
    "       train_regret_winner_20[slice13]]\n",
    "\n",
    "loser13_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\n",
    "winner13_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser13 = np.asarray(loser13_results[4:5][0])[0]\n",
    "median_loser13 = np.asarray(loser13_results[9:10][0])[0]\n",
    "upper_loser13 = np.asarray(loser13_results[14:15][0])[0]\n",
    "\n",
    "lower_winner13 = np.asarray(winner13_results[4:5][0])[0]\n",
    "median_winner13 = np.asarray(winner13_results[9:10][0])[0]\n",
    "upper_winner13 = np.asarray(winner13_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration23 :\n",
    "\n",
    "slice23 = 22\n",
    "\n",
    "loser23 = [train_regret_loser_1[slice23],\n",
    "       train_regret_loser_2[slice23],\n",
    "       train_regret_loser_3[slice23],\n",
    "       train_regret_loser_4[slice23],\n",
    "       train_regret_loser_5[slice23],\n",
    "       train_regret_loser_6[slice23],\n",
    "       train_regret_loser_7[slice23],\n",
    "       train_regret_loser_8[slice23],\n",
    "       train_regret_loser_9[slice23],\n",
    "       train_regret_loser_10[slice23],\n",
    "       train_regret_loser_11[slice23],\n",
    "       train_regret_loser_12[slice23],\n",
    "       train_regret_loser_13[slice23],\n",
    "       train_regret_loser_14[slice23],\n",
    "       train_regret_loser_15[slice23],\n",
    "       train_regret_loser_16[slice23],\n",
    "       train_regret_loser_17[slice23],\n",
    "       train_regret_loser_18[slice23],\n",
    "       train_regret_loser_19[slice23],\n",
    "       train_regret_loser_20[slice23]]\n",
    "\n",
    "winner23 = [train_regret_winner_1[slice23],\n",
    "       train_regret_winner_2[slice23],\n",
    "       train_regret_winner_3[slice23],\n",
    "       train_regret_winner_4[slice23],\n",
    "       train_regret_winner_5[slice23],\n",
    "       train_regret_winner_6[slice23],\n",
    "       train_regret_winner_7[slice23],\n",
    "       train_regret_winner_8[slice23],\n",
    "       train_regret_winner_9[slice23],\n",
    "       train_regret_winner_10[slice23],\n",
    "       train_regret_winner_11[slice23],\n",
    "       train_regret_winner_12[slice23],\n",
    "       train_regret_winner_13[slice23],\n",
    "       train_regret_winner_14[slice23],\n",
    "       train_regret_winner_15[slice23],\n",
    "       train_regret_winner_16[slice23],\n",
    "       train_regret_winner_17[slice23],\n",
    "       train_regret_winner_18[slice23],\n",
    "       train_regret_winner_19[slice23],\n",
    "       train_regret_winner_20[slice23]]\n",
    "\n",
    "loser23_results = pd.DataFrame(loser23).sort_values(by=[0], ascending=False)\n",
    "winner23_results = pd.DataFrame(winner23).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser23 = np.asarray(loser23_results[4:5][0])[0]\n",
    "median_loser23 = np.asarray(loser23_results[9:10][0])[0]\n",
    "upper_loser23 = np.asarray(loser23_results[14:15][0])[0]\n",
    "\n",
    "lower_winner23 = np.asarray(winner23_results[4:5][0])[0]\n",
    "median_winner23 = np.asarray(winner23_results[9:10][0])[0]\n",
    "upper_winner23 = np.asarray(winner23_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration33 :\n",
    "\n",
    "slice33 = 32\n",
    "\n",
    "loser33 = [train_regret_loser_1[slice33],\n",
    "       train_regret_loser_2[slice33],\n",
    "       train_regret_loser_3[slice33],\n",
    "       train_regret_loser_4[slice33],\n",
    "       train_regret_loser_5[slice33],\n",
    "       train_regret_loser_6[slice33],\n",
    "       train_regret_loser_7[slice33],\n",
    "       train_regret_loser_8[slice33],\n",
    "       train_regret_loser_9[slice33],\n",
    "       train_regret_loser_10[slice33],\n",
    "       train_regret_loser_11[slice33],\n",
    "       train_regret_loser_12[slice33],\n",
    "       train_regret_loser_13[slice33],\n",
    "       train_regret_loser_14[slice33],\n",
    "       train_regret_loser_15[slice33],\n",
    "       train_regret_loser_16[slice33],\n",
    "       train_regret_loser_17[slice33],\n",
    "       train_regret_loser_18[slice33],\n",
    "       train_regret_loser_19[slice33],\n",
    "       train_regret_loser_20[slice33]]\n",
    "\n",
    "winner33 = [train_regret_winner_1[slice33],\n",
    "       train_regret_winner_2[slice33],\n",
    "       train_regret_winner_3[slice33],\n",
    "       train_regret_winner_4[slice33],\n",
    "       train_regret_winner_5[slice33],\n",
    "       train_regret_winner_6[slice33],\n",
    "       train_regret_winner_7[slice33],\n",
    "       train_regret_winner_8[slice33],\n",
    "       train_regret_winner_9[slice33],\n",
    "       train_regret_winner_10[slice33],\n",
    "       train_regret_winner_11[slice33],\n",
    "       train_regret_winner_12[slice33],\n",
    "       train_regret_winner_13[slice33],\n",
    "       train_regret_winner_14[slice33],\n",
    "       train_regret_winner_15[slice33],\n",
    "       train_regret_winner_16[slice33],\n",
    "       train_regret_winner_17[slice33],\n",
    "       train_regret_winner_18[slice33],\n",
    "       train_regret_winner_19[slice33],\n",
    "       train_regret_winner_20[slice33]]\n",
    "\n",
    "loser33_results = pd.DataFrame(loser33).sort_values(by=[0], ascending=False)\n",
    "winner33_results = pd.DataFrame(winner33).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser33 = np.asarray(loser33_results[4:5][0])[0]\n",
    "median_loser33 = np.asarray(loser33_results[9:10][0])[0]\n",
    "upper_loser33 = np.asarray(loser33_results[14:15][0])[0]\n",
    "\n",
    "lower_winner33 = np.asarray(winner33_results[4:5][0])[0]\n",
    "median_winner33 = np.asarray(winner33_results[9:10][0])[0]\n",
    "upper_winner33 = np.asarray(winner33_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration43 :\n",
    "\n",
    "slice43 = 42\n",
    "\n",
    "loser43 = [train_regret_loser_1[slice43],\n",
    "       train_regret_loser_2[slice43],\n",
    "       train_regret_loser_3[slice43],\n",
    "       train_regret_loser_4[slice43],\n",
    "       train_regret_loser_5[slice43],\n",
    "       train_regret_loser_6[slice43],\n",
    "       train_regret_loser_7[slice43],\n",
    "       train_regret_loser_8[slice43],\n",
    "       train_regret_loser_9[slice43],\n",
    "       train_regret_loser_10[slice43],\n",
    "       train_regret_loser_11[slice43],\n",
    "       train_regret_loser_12[slice43],\n",
    "       train_regret_loser_13[slice43],\n",
    "       train_regret_loser_14[slice43],\n",
    "       train_regret_loser_15[slice43],\n",
    "       train_regret_loser_16[slice43],\n",
    "       train_regret_loser_17[slice43],\n",
    "       train_regret_loser_18[slice43],\n",
    "       train_regret_loser_19[slice43],\n",
    "       train_regret_loser_20[slice43]]\n",
    "\n",
    "winner43 = [train_regret_winner_1[slice43],\n",
    "       train_regret_winner_2[slice43],\n",
    "       train_regret_winner_3[slice43],\n",
    "       train_regret_winner_4[slice43],\n",
    "       train_regret_winner_5[slice43],\n",
    "       train_regret_winner_6[slice43],\n",
    "       train_regret_winner_7[slice43],\n",
    "       train_regret_winner_8[slice43],\n",
    "       train_regret_winner_9[slice43],\n",
    "       train_regret_winner_10[slice43],\n",
    "       train_regret_winner_11[slice43],\n",
    "       train_regret_winner_12[slice43],\n",
    "       train_regret_winner_13[slice43],\n",
    "       train_regret_winner_14[slice43],\n",
    "       train_regret_winner_15[slice43],\n",
    "       train_regret_winner_16[slice43],\n",
    "       train_regret_winner_17[slice43],\n",
    "       train_regret_winner_18[slice43],\n",
    "       train_regret_winner_19[slice43],\n",
    "       train_regret_winner_20[slice43]]\n",
    "\n",
    "loser43_results = pd.DataFrame(loser43).sort_values(by=[0], ascending=False)\n",
    "winner43_results = pd.DataFrame(winner43).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser43 = np.asarray(loser43_results[4:5][0])[0]\n",
    "median_loser43 = np.asarray(loser43_results[9:10][0])[0]\n",
    "upper_loser43 = np.asarray(loser43_results[14:15][0])[0]\n",
    "\n",
    "lower_winner43 = np.asarray(winner43_results[4:5][0])[0]\n",
    "median_winner43 = np.asarray(winner43_results[9:10][0])[0]\n",
    "upper_winner43 = np.asarray(winner43_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration53 :\n",
    "\n",
    "slice53 = 52\n",
    "\n",
    "loser53 = [train_regret_loser_1[slice53],\n",
    "       train_regret_loser_2[slice53],\n",
    "       train_regret_loser_3[slice53],\n",
    "       train_regret_loser_4[slice53],\n",
    "       train_regret_loser_5[slice53],\n",
    "       train_regret_loser_6[slice53],\n",
    "       train_regret_loser_7[slice53],\n",
    "       train_regret_loser_8[slice53],\n",
    "       train_regret_loser_9[slice53],\n",
    "       train_regret_loser_10[slice53],\n",
    "       train_regret_loser_11[slice53],\n",
    "       train_regret_loser_12[slice53],\n",
    "       train_regret_loser_13[slice53],\n",
    "       train_regret_loser_14[slice53],\n",
    "       train_regret_loser_15[slice53],\n",
    "       train_regret_loser_16[slice53],\n",
    "       train_regret_loser_17[slice53],\n",
    "       train_regret_loser_18[slice53],\n",
    "       train_regret_loser_19[slice53],\n",
    "       train_regret_loser_20[slice53]]\n",
    "\n",
    "winner53 = [train_regret_winner_1[slice53],\n",
    "       train_regret_winner_2[slice53],\n",
    "       train_regret_winner_3[slice53],\n",
    "       train_regret_winner_4[slice53],\n",
    "       train_regret_winner_5[slice53],\n",
    "       train_regret_winner_6[slice53],\n",
    "       train_regret_winner_7[slice53],\n",
    "       train_regret_winner_8[slice53],\n",
    "       train_regret_winner_9[slice53],\n",
    "       train_regret_winner_10[slice53],\n",
    "       train_regret_winner_11[slice53],\n",
    "       train_regret_winner_12[slice53],\n",
    "       train_regret_winner_13[slice53],\n",
    "       train_regret_winner_14[slice53],\n",
    "       train_regret_winner_15[slice53],\n",
    "       train_regret_winner_16[slice53],\n",
    "       train_regret_winner_17[slice53],\n",
    "       train_regret_winner_18[slice53],\n",
    "       train_regret_winner_19[slice53],\n",
    "       train_regret_winner_20[slice53]]\n",
    "\n",
    "loser53_results = pd.DataFrame(loser53).sort_values(by=[0], ascending=False)\n",
    "winner53_results = pd.DataFrame(winner53).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser53 = np.asarray(loser53_results[4:5][0])[0]\n",
    "median_loser53 = np.asarray(loser53_results[9:10][0])[0]\n",
    "upper_loser53 = np.asarray(loser53_results[14:15][0])[0]\n",
    "\n",
    "lower_winner53 = np.asarray(winner53_results[4:5][0])[0]\n",
    "median_winner53 = np.asarray(winner53_results[9:10][0])[0]\n",
    "upper_winner53 = np.asarray(winner53_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration63 :\n",
    "\n",
    "slice63 = 62\n",
    "\n",
    "loser63 = [train_regret_loser_1[slice63],\n",
    "       train_regret_loser_2[slice63],\n",
    "       train_regret_loser_3[slice63],\n",
    "       train_regret_loser_4[slice63],\n",
    "       train_regret_loser_5[slice63],\n",
    "       train_regret_loser_6[slice63],\n",
    "       train_regret_loser_7[slice63],\n",
    "       train_regret_loser_8[slice63],\n",
    "       train_regret_loser_9[slice63],\n",
    "       train_regret_loser_10[slice63],\n",
    "       train_regret_loser_11[slice63],\n",
    "       train_regret_loser_12[slice63],\n",
    "       train_regret_loser_13[slice63],\n",
    "       train_regret_loser_14[slice63],\n",
    "       train_regret_loser_15[slice63],\n",
    "       train_regret_loser_16[slice63],\n",
    "       train_regret_loser_17[slice63],\n",
    "       train_regret_loser_18[slice63],\n",
    "       train_regret_loser_19[slice63],\n",
    "       train_regret_loser_20[slice63]]\n",
    "\n",
    "winner63 = [train_regret_winner_1[slice63],\n",
    "       train_regret_winner_2[slice63],\n",
    "       train_regret_winner_3[slice63],\n",
    "       train_regret_winner_4[slice63],\n",
    "       train_regret_winner_5[slice63],\n",
    "       train_regret_winner_6[slice63],\n",
    "       train_regret_winner_7[slice63],\n",
    "       train_regret_winner_8[slice63],\n",
    "       train_regret_winner_9[slice63],\n",
    "       train_regret_winner_10[slice63],\n",
    "       train_regret_winner_11[slice63],\n",
    "       train_regret_winner_12[slice63],\n",
    "       train_regret_winner_13[slice63],\n",
    "       train_regret_winner_14[slice63],\n",
    "       train_regret_winner_15[slice63],\n",
    "       train_regret_winner_16[slice63],\n",
    "       train_regret_winner_17[slice63],\n",
    "       train_regret_winner_18[slice63],\n",
    "       train_regret_winner_19[slice63],\n",
    "       train_regret_winner_20[slice63]]\n",
    "\n",
    "loser63_results = pd.DataFrame(loser63).sort_values(by=[0], ascending=False)\n",
    "winner63_results = pd.DataFrame(winner63).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser63 = np.asarray(loser63_results[4:5][0])[0]\n",
    "median_loser63 = np.asarray(loser63_results[9:10][0])[0]\n",
    "upper_loser63 = np.asarray(loser63_results[14:15][0])[0]\n",
    "\n",
    "lower_winner63 = np.asarray(winner63_results[4:5][0])[0]\n",
    "median_winner63 = np.asarray(winner63_results[9:10][0])[0]\n",
    "upper_winner63 = np.asarray(winner63_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration73 :\n",
    "\n",
    "slice73 = 72\n",
    "\n",
    "loser73 = [train_regret_loser_1[slice73],\n",
    "       train_regret_loser_2[slice73],\n",
    "       train_regret_loser_3[slice73],\n",
    "       train_regret_loser_4[slice73],\n",
    "       train_regret_loser_5[slice73],\n",
    "       train_regret_loser_6[slice73],\n",
    "       train_regret_loser_7[slice73],\n",
    "       train_regret_loser_8[slice73],\n",
    "       train_regret_loser_9[slice73],\n",
    "       train_regret_loser_10[slice73],\n",
    "       train_regret_loser_11[slice73],\n",
    "       train_regret_loser_12[slice73],\n",
    "       train_regret_loser_13[slice73],\n",
    "       train_regret_loser_14[slice73],\n",
    "       train_regret_loser_15[slice73],\n",
    "       train_regret_loser_16[slice73],\n",
    "       train_regret_loser_17[slice73],\n",
    "       train_regret_loser_18[slice73],\n",
    "       train_regret_loser_19[slice73],\n",
    "       train_regret_loser_20[slice73]]\n",
    "\n",
    "winner73 = [train_regret_winner_1[slice73],\n",
    "       train_regret_winner_2[slice73],\n",
    "       train_regret_winner_3[slice73],\n",
    "       train_regret_winner_4[slice73],\n",
    "       train_regret_winner_5[slice73],\n",
    "       train_regret_winner_6[slice73],\n",
    "       train_regret_winner_7[slice73],\n",
    "       train_regret_winner_8[slice73],\n",
    "       train_regret_winner_9[slice73],\n",
    "       train_regret_winner_10[slice73],\n",
    "       train_regret_winner_11[slice73],\n",
    "       train_regret_winner_12[slice73],\n",
    "       train_regret_winner_13[slice73],\n",
    "       train_regret_winner_14[slice73],\n",
    "       train_regret_winner_15[slice73],\n",
    "       train_regret_winner_16[slice73],\n",
    "       train_regret_winner_17[slice73],\n",
    "       train_regret_winner_18[slice73],\n",
    "       train_regret_winner_19[slice73],\n",
    "       train_regret_winner_20[slice73]]\n",
    "\n",
    "loser73_results = pd.DataFrame(loser73).sort_values(by=[0], ascending=False)\n",
    "winner73_results = pd.DataFrame(winner73).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser73 = np.asarray(loser73_results[4:5][0])[0]\n",
    "median_loser73 = np.asarray(loser73_results[9:10][0])[0]\n",
    "upper_loser73 = np.asarray(loser73_results[14:15][0])[0]\n",
    "\n",
    "lower_winner73 = np.asarray(winner73_results[4:5][0])[0]\n",
    "median_winner73 = np.asarray(winner73_results[9:10][0])[0]\n",
    "upper_winner73 = np.asarray(winner73_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration83 :\n",
    "\n",
    "slice83 = 82\n",
    "\n",
    "loser83 = [train_regret_loser_1[slice83],\n",
    "       train_regret_loser_2[slice83],\n",
    "       train_regret_loser_3[slice83],\n",
    "       train_regret_loser_4[slice83],\n",
    "       train_regret_loser_5[slice83],\n",
    "       train_regret_loser_6[slice83],\n",
    "       train_regret_loser_7[slice83],\n",
    "       train_regret_loser_8[slice83],\n",
    "       train_regret_loser_9[slice83],\n",
    "       train_regret_loser_10[slice83],\n",
    "       train_regret_loser_11[slice83],\n",
    "       train_regret_loser_12[slice83],\n",
    "       train_regret_loser_13[slice83],\n",
    "       train_regret_loser_14[slice83],\n",
    "       train_regret_loser_15[slice83],\n",
    "       train_regret_loser_16[slice83],\n",
    "       train_regret_loser_17[slice83],\n",
    "       train_regret_loser_18[slice83],\n",
    "       train_regret_loser_19[slice83],\n",
    "       train_regret_loser_20[slice83]]\n",
    "\n",
    "winner83 = [train_regret_winner_1[slice83],\n",
    "       train_regret_winner_2[slice83],\n",
    "       train_regret_winner_3[slice83],\n",
    "       train_regret_winner_4[slice83],\n",
    "       train_regret_winner_5[slice83],\n",
    "       train_regret_winner_6[slice83],\n",
    "       train_regret_winner_7[slice83],\n",
    "       train_regret_winner_8[slice83],\n",
    "       train_regret_winner_9[slice83],\n",
    "       train_regret_winner_10[slice83],\n",
    "       train_regret_winner_11[slice83],\n",
    "       train_regret_winner_12[slice83],\n",
    "       train_regret_winner_13[slice83],\n",
    "       train_regret_winner_14[slice83],\n",
    "       train_regret_winner_15[slice83],\n",
    "       train_regret_winner_16[slice83],\n",
    "       train_regret_winner_17[slice83],\n",
    "       train_regret_winner_18[slice83],\n",
    "       train_regret_winner_19[slice83],\n",
    "       train_regret_winner_20[slice83]]\n",
    "\n",
    "loser83_results = pd.DataFrame(loser83).sort_values(by=[0], ascending=False)\n",
    "winner83_results = pd.DataFrame(winner83).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser83 = np.asarray(loser83_results[4:5][0])[0]\n",
    "median_loser83 = np.asarray(loser83_results[9:10][0])[0]\n",
    "upper_loser83 = np.asarray(loser83_results[14:15][0])[0]\n",
    "\n",
    "lower_winner83 = np.asarray(winner83_results[4:5][0])[0]\n",
    "median_winner83 = np.asarray(winner83_results[9:10][0])[0]\n",
    "upper_winner83 = np.asarray(winner83_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration93 :\n",
    "\n",
    "slice93 = 92\n",
    "\n",
    "loser93 = [train_regret_loser_1[slice93],\n",
    "       train_regret_loser_2[slice93],\n",
    "       train_regret_loser_3[slice93],\n",
    "       train_regret_loser_4[slice93],\n",
    "       train_regret_loser_5[slice93],\n",
    "       train_regret_loser_6[slice93],\n",
    "       train_regret_loser_7[slice93],\n",
    "       train_regret_loser_8[slice93],\n",
    "       train_regret_loser_9[slice93],\n",
    "       train_regret_loser_10[slice93],\n",
    "       train_regret_loser_11[slice93],\n",
    "       train_regret_loser_12[slice93],\n",
    "       train_regret_loser_13[slice93],\n",
    "       train_regret_loser_14[slice93],\n",
    "       train_regret_loser_15[slice93],\n",
    "       train_regret_loser_16[slice93],\n",
    "       train_regret_loser_17[slice93],\n",
    "       train_regret_loser_18[slice93],\n",
    "       train_regret_loser_19[slice93],\n",
    "       train_regret_loser_20[slice93]]\n",
    "\n",
    "winner93 = [train_regret_winner_1[slice93],\n",
    "       train_regret_winner_2[slice93],\n",
    "       train_regret_winner_3[slice93],\n",
    "       train_regret_winner_4[slice93],\n",
    "       train_regret_winner_5[slice93],\n",
    "       train_regret_winner_6[slice93],\n",
    "       train_regret_winner_7[slice93],\n",
    "       train_regret_winner_8[slice93],\n",
    "       train_regret_winner_9[slice93],\n",
    "       train_regret_winner_10[slice93],\n",
    "       train_regret_winner_11[slice93],\n",
    "       train_regret_winner_12[slice93],\n",
    "       train_regret_winner_13[slice93],\n",
    "       train_regret_winner_14[slice93],\n",
    "       train_regret_winner_15[slice93],\n",
    "       train_regret_winner_16[slice93],\n",
    "       train_regret_winner_17[slice93],\n",
    "       train_regret_winner_18[slice93],\n",
    "       train_regret_winner_19[slice93],\n",
    "       train_regret_winner_20[slice93]]\n",
    "\n",
    "loser93_results = pd.DataFrame(loser93).sort_values(by=[0], ascending=False)\n",
    "winner93_results = pd.DataFrame(winner93).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser93 = np.asarray(loser93_results[4:5][0])[0]\n",
    "median_loser93 = np.asarray(loser93_results[9:10][0])[0]\n",
    "upper_loser93 = np.asarray(loser93_results[14:15][0])[0]\n",
    "\n",
    "lower_winner93 = np.asarray(winner93_results[4:5][0])[0]\n",
    "median_winner93 = np.asarray(winner93_results[9:10][0])[0]\n",
    "upper_winner93 = np.asarray(winner93_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration4 :\n",
    "\n",
    "slice4 = 3\n",
    "\n",
    "loser4 = [train_regret_loser_1[slice4],\n",
    "       train_regret_loser_2[slice4],\n",
    "       train_regret_loser_3[slice4],\n",
    "       train_regret_loser_4[slice4],\n",
    "       train_regret_loser_5[slice4],\n",
    "       train_regret_loser_6[slice4],\n",
    "       train_regret_loser_7[slice4],\n",
    "       train_regret_loser_8[slice4],\n",
    "       train_regret_loser_9[slice4],\n",
    "       train_regret_loser_10[slice4],\n",
    "       train_regret_loser_11[slice4],\n",
    "       train_regret_loser_12[slice4],\n",
    "       train_regret_loser_13[slice4],\n",
    "       train_regret_loser_14[slice4],\n",
    "       train_regret_loser_15[slice4],\n",
    "       train_regret_loser_16[slice4],\n",
    "       train_regret_loser_17[slice4],\n",
    "       train_regret_loser_18[slice4],\n",
    "       train_regret_loser_19[slice4],\n",
    "       train_regret_loser_20[slice4]]\n",
    "\n",
    "winner4 = [train_regret_winner_1[slice4],\n",
    "       train_regret_winner_2[slice4],\n",
    "       train_regret_winner_3[slice4],\n",
    "       train_regret_winner_4[slice4],\n",
    "       train_regret_winner_5[slice4],\n",
    "       train_regret_winner_6[slice4],\n",
    "       train_regret_winner_7[slice4],\n",
    "       train_regret_winner_8[slice4],\n",
    "       train_regret_winner_9[slice4],\n",
    "       train_regret_winner_10[slice4],\n",
    "       train_regret_winner_11[slice4],\n",
    "       train_regret_winner_12[slice4],\n",
    "       train_regret_winner_13[slice4],\n",
    "       train_regret_winner_14[slice4],\n",
    "       train_regret_winner_15[slice4],\n",
    "       train_regret_winner_16[slice4],\n",
    "       train_regret_winner_17[slice4],\n",
    "       train_regret_winner_18[slice4],\n",
    "       train_regret_winner_19[slice4],\n",
    "       train_regret_winner_20[slice4]]\n",
    "\n",
    "loser4_results = pd.DataFrame(loser4).sort_values(by=[0], ascending=False)\n",
    "winner4_results = pd.DataFrame(winner4).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser4 = np.asarray(loser4_results[4:5][0])[0]\n",
    "median_loser4 = np.asarray(loser4_results[9:10][0])[0]\n",
    "upper_loser4 = np.asarray(loser4_results[14:15][0])[0]\n",
    "\n",
    "lower_winner4 = np.asarray(winner4_results[4:5][0])[0]\n",
    "median_winner4 = np.asarray(winner4_results[9:10][0])[0]\n",
    "upper_winner4 = np.asarray(winner4_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration14 :\n",
    "\n",
    "slice14 = 13\n",
    "\n",
    "loser14 = [train_regret_loser_1[slice14],\n",
    "       train_regret_loser_2[slice14],\n",
    "       train_regret_loser_3[slice14],\n",
    "       train_regret_loser_4[slice14],\n",
    "       train_regret_loser_5[slice14],\n",
    "       train_regret_loser_6[slice14],\n",
    "       train_regret_loser_7[slice14],\n",
    "       train_regret_loser_8[slice14],\n",
    "       train_regret_loser_9[slice14],\n",
    "       train_regret_loser_10[slice14],\n",
    "       train_regret_loser_11[slice14],\n",
    "       train_regret_loser_12[slice14],\n",
    "       train_regret_loser_13[slice14],\n",
    "       train_regret_loser_14[slice14],\n",
    "       train_regret_loser_15[slice14],\n",
    "       train_regret_loser_16[slice14],\n",
    "       train_regret_loser_17[slice14],\n",
    "       train_regret_loser_18[slice14],\n",
    "       train_regret_loser_19[slice14],\n",
    "       train_regret_loser_20[slice14]]\n",
    "\n",
    "winner14 = [train_regret_winner_1[slice14],\n",
    "       train_regret_winner_2[slice14],\n",
    "       train_regret_winner_3[slice14],\n",
    "       train_regret_winner_4[slice14],\n",
    "       train_regret_winner_5[slice14],\n",
    "       train_regret_winner_6[slice14],\n",
    "       train_regret_winner_7[slice14],\n",
    "       train_regret_winner_8[slice14],\n",
    "       train_regret_winner_9[slice14],\n",
    "       train_regret_winner_10[slice14],\n",
    "       train_regret_winner_11[slice14],\n",
    "       train_regret_winner_12[slice14],\n",
    "       train_regret_winner_13[slice14],\n",
    "       train_regret_winner_14[slice14],\n",
    "       train_regret_winner_15[slice14],\n",
    "       train_regret_winner_16[slice14],\n",
    "       train_regret_winner_17[slice14],\n",
    "       train_regret_winner_18[slice14],\n",
    "       train_regret_winner_19[slice14],\n",
    "       train_regret_winner_20[slice14]]\n",
    "\n",
    "loser14_results = pd.DataFrame(loser14).sort_values(by=[0], ascending=False)\n",
    "winner14_results = pd.DataFrame(winner14).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser14 = np.asarray(loser14_results[4:5][0])[0]\n",
    "median_loser14 = np.asarray(loser14_results[9:10][0])[0]\n",
    "upper_loser14 = np.asarray(loser14_results[14:15][0])[0]\n",
    "\n",
    "lower_winner14 = np.asarray(winner14_results[4:5][0])[0]\n",
    "median_winner14 = np.asarray(winner14_results[9:10][0])[0]\n",
    "upper_winner14 = np.asarray(winner14_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration24 :\n",
    "\n",
    "slice24 = 23\n",
    "\n",
    "loser24 = [train_regret_loser_1[slice24],\n",
    "       train_regret_loser_2[slice24],\n",
    "       train_regret_loser_3[slice24],\n",
    "       train_regret_loser_4[slice24],\n",
    "       train_regret_loser_5[slice24],\n",
    "       train_regret_loser_6[slice24],\n",
    "       train_regret_loser_7[slice24],\n",
    "       train_regret_loser_8[slice24],\n",
    "       train_regret_loser_9[slice24],\n",
    "       train_regret_loser_10[slice24],\n",
    "       train_regret_loser_11[slice24],\n",
    "       train_regret_loser_12[slice24],\n",
    "       train_regret_loser_13[slice24],\n",
    "       train_regret_loser_14[slice24],\n",
    "       train_regret_loser_15[slice24],\n",
    "       train_regret_loser_16[slice24],\n",
    "       train_regret_loser_17[slice24],\n",
    "       train_regret_loser_18[slice24],\n",
    "       train_regret_loser_19[slice24],\n",
    "       train_regret_loser_20[slice24]]\n",
    "\n",
    "winner24 = [train_regret_winner_1[slice24],\n",
    "       train_regret_winner_2[slice24],\n",
    "       train_regret_winner_3[slice24],\n",
    "       train_regret_winner_4[slice24],\n",
    "       train_regret_winner_5[slice24],\n",
    "       train_regret_winner_6[slice24],\n",
    "       train_regret_winner_7[slice24],\n",
    "       train_regret_winner_8[slice24],\n",
    "       train_regret_winner_9[slice24],\n",
    "       train_regret_winner_10[slice24],\n",
    "       train_regret_winner_11[slice24],\n",
    "       train_regret_winner_12[slice24],\n",
    "       train_regret_winner_13[slice24],\n",
    "       train_regret_winner_14[slice24],\n",
    "       train_regret_winner_15[slice24],\n",
    "       train_regret_winner_16[slice24],\n",
    "       train_regret_winner_17[slice24],\n",
    "       train_regret_winner_18[slice24],\n",
    "       train_regret_winner_19[slice24],\n",
    "       train_regret_winner_20[slice24]]\n",
    "\n",
    "loser24_results = pd.DataFrame(loser24).sort_values(by=[0], ascending=False)\n",
    "winner24_results = pd.DataFrame(winner24).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser24 = np.asarray(loser24_results[4:5][0])[0]\n",
    "median_loser24 = np.asarray(loser24_results[9:10][0])[0]\n",
    "upper_loser24 = np.asarray(loser24_results[14:15][0])[0]\n",
    "\n",
    "lower_winner24 = np.asarray(winner24_results[4:5][0])[0]\n",
    "median_winner24 = np.asarray(winner24_results[9:10][0])[0]\n",
    "upper_winner24 = np.asarray(winner24_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration34 :\n",
    "\n",
    "slice34 = 33\n",
    "\n",
    "loser34 = [train_regret_loser_1[slice34],\n",
    "       train_regret_loser_2[slice34],\n",
    "       train_regret_loser_3[slice34],\n",
    "       train_regret_loser_4[slice34],\n",
    "       train_regret_loser_5[slice34],\n",
    "       train_regret_loser_6[slice34],\n",
    "       train_regret_loser_7[slice34],\n",
    "       train_regret_loser_8[slice34],\n",
    "       train_regret_loser_9[slice34],\n",
    "       train_regret_loser_10[slice34],\n",
    "       train_regret_loser_11[slice34],\n",
    "       train_regret_loser_12[slice34],\n",
    "       train_regret_loser_13[slice34],\n",
    "       train_regret_loser_14[slice34],\n",
    "       train_regret_loser_15[slice34],\n",
    "       train_regret_loser_16[slice34],\n",
    "       train_regret_loser_17[slice34],\n",
    "       train_regret_loser_18[slice34],\n",
    "       train_regret_loser_19[slice34],\n",
    "       train_regret_loser_20[slice34]]\n",
    "\n",
    "winner34 = [train_regret_winner_1[slice34],\n",
    "       train_regret_winner_2[slice34],\n",
    "       train_regret_winner_3[slice34],\n",
    "       train_regret_winner_4[slice34],\n",
    "       train_regret_winner_5[slice34],\n",
    "       train_regret_winner_6[slice34],\n",
    "       train_regret_winner_7[slice34],\n",
    "       train_regret_winner_8[slice34],\n",
    "       train_regret_winner_9[slice34],\n",
    "       train_regret_winner_10[slice34],\n",
    "       train_regret_winner_11[slice34],\n",
    "       train_regret_winner_12[slice34],\n",
    "       train_regret_winner_13[slice34],\n",
    "       train_regret_winner_14[slice34],\n",
    "       train_regret_winner_15[slice34],\n",
    "       train_regret_winner_16[slice34],\n",
    "       train_regret_winner_17[slice34],\n",
    "       train_regret_winner_18[slice34],\n",
    "       train_regret_winner_19[slice34],\n",
    "       train_regret_winner_20[slice34]]\n",
    "\n",
    "loser34_results = pd.DataFrame(loser34).sort_values(by=[0], ascending=False)\n",
    "winner34_results = pd.DataFrame(winner34).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser34 = np.asarray(loser34_results[4:5][0])[0]\n",
    "median_loser34 = np.asarray(loser34_results[9:10][0])[0]\n",
    "upper_loser34 = np.asarray(loser34_results[14:15][0])[0]\n",
    "\n",
    "lower_winner34 = np.asarray(winner34_results[4:5][0])[0]\n",
    "median_winner34 = np.asarray(winner34_results[9:10][0])[0]\n",
    "upper_winner34 = np.asarray(winner34_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration44 :\n",
    "\n",
    "slice44 = 43\n",
    "\n",
    "loser44 = [train_regret_loser_1[slice44],\n",
    "       train_regret_loser_2[slice44],\n",
    "       train_regret_loser_3[slice44],\n",
    "       train_regret_loser_4[slice44],\n",
    "       train_regret_loser_5[slice44],\n",
    "       train_regret_loser_6[slice44],\n",
    "       train_regret_loser_7[slice44],\n",
    "       train_regret_loser_8[slice44],\n",
    "       train_regret_loser_9[slice44],\n",
    "       train_regret_loser_10[slice44],\n",
    "       train_regret_loser_11[slice44],\n",
    "       train_regret_loser_12[slice44],\n",
    "       train_regret_loser_13[slice44],\n",
    "       train_regret_loser_14[slice44],\n",
    "       train_regret_loser_15[slice44],\n",
    "       train_regret_loser_16[slice44],\n",
    "       train_regret_loser_17[slice44],\n",
    "       train_regret_loser_18[slice44],\n",
    "       train_regret_loser_19[slice44],\n",
    "       train_regret_loser_20[slice44]]\n",
    "\n",
    "winner44 = [train_regret_winner_1[slice44],\n",
    "       train_regret_winner_2[slice44],\n",
    "       train_regret_winner_3[slice44],\n",
    "       train_regret_winner_4[slice44],\n",
    "       train_regret_winner_5[slice44],\n",
    "       train_regret_winner_6[slice44],\n",
    "       train_regret_winner_7[slice44],\n",
    "       train_regret_winner_8[slice44],\n",
    "       train_regret_winner_9[slice44],\n",
    "       train_regret_winner_10[slice44],\n",
    "       train_regret_winner_11[slice44],\n",
    "       train_regret_winner_12[slice44],\n",
    "       train_regret_winner_13[slice44],\n",
    "       train_regret_winner_14[slice44],\n",
    "       train_regret_winner_15[slice44],\n",
    "       train_regret_winner_16[slice44],\n",
    "       train_regret_winner_17[slice44],\n",
    "       train_regret_winner_18[slice44],\n",
    "       train_regret_winner_19[slice44],\n",
    "       train_regret_winner_20[slice44]]\n",
    "\n",
    "loser44_results = pd.DataFrame(loser44).sort_values(by=[0], ascending=False)\n",
    "winner44_results = pd.DataFrame(winner44).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser44 = np.asarray(loser44_results[4:5][0])[0]\n",
    "median_loser44 = np.asarray(loser44_results[9:10][0])[0]\n",
    "upper_loser44 = np.asarray(loser44_results[14:15][0])[0]\n",
    "\n",
    "lower_winner44 = np.asarray(winner44_results[4:5][0])[0]\n",
    "median_winner44 = np.asarray(winner44_results[9:10][0])[0]\n",
    "upper_winner44 = np.asarray(winner44_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration54 :\n",
    "\n",
    "slice54 = 53\n",
    "\n",
    "loser54 = [train_regret_loser_1[slice54],\n",
    "       train_regret_loser_2[slice54],\n",
    "       train_regret_loser_3[slice54],\n",
    "       train_regret_loser_4[slice54],\n",
    "       train_regret_loser_5[slice54],\n",
    "       train_regret_loser_6[slice54],\n",
    "       train_regret_loser_7[slice54],\n",
    "       train_regret_loser_8[slice54],\n",
    "       train_regret_loser_9[slice54],\n",
    "       train_regret_loser_10[slice54],\n",
    "       train_regret_loser_11[slice54],\n",
    "       train_regret_loser_12[slice54],\n",
    "       train_regret_loser_13[slice54],\n",
    "       train_regret_loser_14[slice54],\n",
    "       train_regret_loser_15[slice54],\n",
    "       train_regret_loser_16[slice54],\n",
    "       train_regret_loser_17[slice54],\n",
    "       train_regret_loser_18[slice54],\n",
    "       train_regret_loser_19[slice54],\n",
    "       train_regret_loser_20[slice54]]\n",
    "\n",
    "winner54 = [train_regret_winner_1[slice54],\n",
    "       train_regret_winner_2[slice54],\n",
    "       train_regret_winner_3[slice54],\n",
    "       train_regret_winner_4[slice54],\n",
    "       train_regret_winner_5[slice54],\n",
    "       train_regret_winner_6[slice54],\n",
    "       train_regret_winner_7[slice54],\n",
    "       train_regret_winner_8[slice54],\n",
    "       train_regret_winner_9[slice54],\n",
    "       train_regret_winner_10[slice54],\n",
    "       train_regret_winner_11[slice54],\n",
    "       train_regret_winner_12[slice54],\n",
    "       train_regret_winner_13[slice54],\n",
    "       train_regret_winner_14[slice54],\n",
    "       train_regret_winner_15[slice54],\n",
    "       train_regret_winner_16[slice54],\n",
    "       train_regret_winner_17[slice54],\n",
    "       train_regret_winner_18[slice54],\n",
    "       train_regret_winner_19[slice54],\n",
    "       train_regret_winner_20[slice54]]\n",
    "\n",
    "loser54_results = pd.DataFrame(loser54).sort_values(by=[0], ascending=False)\n",
    "winner54_results = pd.DataFrame(winner54).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser54 = np.asarray(loser54_results[4:5][0])[0]\n",
    "median_loser54 = np.asarray(loser54_results[9:10][0])[0]\n",
    "upper_loser54 = np.asarray(loser54_results[14:15][0])[0]\n",
    "\n",
    "lower_winner54 = np.asarray(winner54_results[4:5][0])[0]\n",
    "median_winner54 = np.asarray(winner54_results[9:10][0])[0]\n",
    "upper_winner54 = np.asarray(winner54_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration64 :\n",
    "\n",
    "slice64 = 63\n",
    "\n",
    "loser64 = [train_regret_loser_1[slice64],\n",
    "       train_regret_loser_2[slice64],\n",
    "       train_regret_loser_3[slice64],\n",
    "       train_regret_loser_4[slice64],\n",
    "       train_regret_loser_5[slice64],\n",
    "       train_regret_loser_6[slice64],\n",
    "       train_regret_loser_7[slice64],\n",
    "       train_regret_loser_8[slice64],\n",
    "       train_regret_loser_9[slice64],\n",
    "       train_regret_loser_10[slice64],\n",
    "       train_regret_loser_11[slice64],\n",
    "       train_regret_loser_12[slice64],\n",
    "       train_regret_loser_13[slice64],\n",
    "       train_regret_loser_14[slice64],\n",
    "       train_regret_loser_15[slice64],\n",
    "       train_regret_loser_16[slice64],\n",
    "       train_regret_loser_17[slice64],\n",
    "       train_regret_loser_18[slice64],\n",
    "       train_regret_loser_19[slice64],\n",
    "       train_regret_loser_20[slice64]]\n",
    "\n",
    "winner64 = [train_regret_winner_1[slice64],\n",
    "       train_regret_winner_2[slice64],\n",
    "       train_regret_winner_3[slice64],\n",
    "       train_regret_winner_4[slice64],\n",
    "       train_regret_winner_5[slice64],\n",
    "       train_regret_winner_6[slice64],\n",
    "       train_regret_winner_7[slice64],\n",
    "       train_regret_winner_8[slice64],\n",
    "       train_regret_winner_9[slice64],\n",
    "       train_regret_winner_10[slice64],\n",
    "       train_regret_winner_11[slice64],\n",
    "       train_regret_winner_12[slice64],\n",
    "       train_regret_winner_13[slice64],\n",
    "       train_regret_winner_14[slice64],\n",
    "       train_regret_winner_15[slice64],\n",
    "       train_regret_winner_16[slice64],\n",
    "       train_regret_winner_17[slice64],\n",
    "       train_regret_winner_18[slice64],\n",
    "       train_regret_winner_19[slice64],\n",
    "       train_regret_winner_20[slice64]]\n",
    "\n",
    "loser64_results = pd.DataFrame(loser64).sort_values(by=[0], ascending=False)\n",
    "winner64_results = pd.DataFrame(winner64).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser64 = np.asarray(loser64_results[4:5][0])[0]\n",
    "median_loser64 = np.asarray(loser64_results[9:10][0])[0]\n",
    "upper_loser64 = np.asarray(loser64_results[14:15][0])[0]\n",
    "\n",
    "lower_winner64 = np.asarray(winner64_results[4:5][0])[0]\n",
    "median_winner64 = np.asarray(winner64_results[9:10][0])[0]\n",
    "upper_winner64 = np.asarray(winner64_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration74 :\n",
    "\n",
    "slice74 = 73\n",
    "\n",
    "loser74 = [train_regret_loser_1[slice74],\n",
    "       train_regret_loser_2[slice74],\n",
    "       train_regret_loser_3[slice74],\n",
    "       train_regret_loser_4[slice74],\n",
    "       train_regret_loser_5[slice74],\n",
    "       train_regret_loser_6[slice74],\n",
    "       train_regret_loser_7[slice74],\n",
    "       train_regret_loser_8[slice74],\n",
    "       train_regret_loser_9[slice74],\n",
    "       train_regret_loser_10[slice74],\n",
    "       train_regret_loser_11[slice74],\n",
    "       train_regret_loser_12[slice74],\n",
    "       train_regret_loser_13[slice74],\n",
    "       train_regret_loser_14[slice74],\n",
    "       train_regret_loser_15[slice74],\n",
    "       train_regret_loser_16[slice74],\n",
    "       train_regret_loser_17[slice74],\n",
    "       train_regret_loser_18[slice74],\n",
    "       train_regret_loser_19[slice74],\n",
    "       train_regret_loser_20[slice74]]\n",
    "\n",
    "winner74 = [train_regret_winner_1[slice74],\n",
    "       train_regret_winner_2[slice74],\n",
    "       train_regret_winner_3[slice74],\n",
    "       train_regret_winner_4[slice74],\n",
    "       train_regret_winner_5[slice74],\n",
    "       train_regret_winner_6[slice74],\n",
    "       train_regret_winner_7[slice74],\n",
    "       train_regret_winner_8[slice74],\n",
    "       train_regret_winner_9[slice74],\n",
    "       train_regret_winner_10[slice74],\n",
    "       train_regret_winner_11[slice74],\n",
    "       train_regret_winner_12[slice74],\n",
    "       train_regret_winner_13[slice74],\n",
    "       train_regret_winner_14[slice74],\n",
    "       train_regret_winner_15[slice74],\n",
    "       train_regret_winner_16[slice74],\n",
    "       train_regret_winner_17[slice74],\n",
    "       train_regret_winner_18[slice74],\n",
    "       train_regret_winner_19[slice74],\n",
    "       train_regret_winner_20[slice74]]\n",
    "\n",
    "loser74_results = pd.DataFrame(loser74).sort_values(by=[0], ascending=False)\n",
    "winner74_results = pd.DataFrame(winner74).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser74 = np.asarray(loser74_results[4:5][0])[0]\n",
    "median_loser74 = np.asarray(loser74_results[9:10][0])[0]\n",
    "upper_loser74 = np.asarray(loser74_results[14:15][0])[0]\n",
    "\n",
    "lower_winner74 = np.asarray(winner74_results[4:5][0])[0]\n",
    "median_winner74 = np.asarray(winner74_results[9:10][0])[0]\n",
    "upper_winner74 = np.asarray(winner74_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration84 :\n",
    "\n",
    "slice84 = 83\n",
    "\n",
    "loser84 = [train_regret_loser_1[slice84],\n",
    "       train_regret_loser_2[slice84],\n",
    "       train_regret_loser_3[slice84],\n",
    "       train_regret_loser_4[slice84],\n",
    "       train_regret_loser_5[slice84],\n",
    "       train_regret_loser_6[slice84],\n",
    "       train_regret_loser_7[slice84],\n",
    "       train_regret_loser_8[slice84],\n",
    "       train_regret_loser_9[slice84],\n",
    "       train_regret_loser_10[slice84],\n",
    "       train_regret_loser_11[slice84],\n",
    "       train_regret_loser_12[slice84],\n",
    "       train_regret_loser_13[slice84],\n",
    "       train_regret_loser_14[slice84],\n",
    "       train_regret_loser_15[slice84],\n",
    "       train_regret_loser_16[slice84],\n",
    "       train_regret_loser_17[slice84],\n",
    "       train_regret_loser_18[slice84],\n",
    "       train_regret_loser_19[slice84],\n",
    "       train_regret_loser_20[slice84]]\n",
    "\n",
    "winner84 = [train_regret_winner_1[slice84],\n",
    "       train_regret_winner_2[slice84],\n",
    "       train_regret_winner_3[slice84],\n",
    "       train_regret_winner_4[slice84],\n",
    "       train_regret_winner_5[slice84],\n",
    "       train_regret_winner_6[slice84],\n",
    "       train_regret_winner_7[slice84],\n",
    "       train_regret_winner_8[slice84],\n",
    "       train_regret_winner_9[slice84],\n",
    "       train_regret_winner_10[slice84],\n",
    "       train_regret_winner_11[slice84],\n",
    "       train_regret_winner_12[slice84],\n",
    "       train_regret_winner_13[slice84],\n",
    "       train_regret_winner_14[slice84],\n",
    "       train_regret_winner_15[slice84],\n",
    "       train_regret_winner_16[slice84],\n",
    "       train_regret_winner_17[slice84],\n",
    "       train_regret_winner_18[slice84],\n",
    "       train_regret_winner_19[slice84],\n",
    "       train_regret_winner_20[slice84]]\n",
    "\n",
    "loser84_results = pd.DataFrame(loser84).sort_values(by=[0], ascending=False)\n",
    "winner84_results = pd.DataFrame(winner84).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser84 = np.asarray(loser84_results[4:5][0])[0]\n",
    "median_loser84 = np.asarray(loser84_results[9:10][0])[0]\n",
    "upper_loser84 = np.asarray(loser84_results[14:15][0])[0]\n",
    "\n",
    "lower_winner84 = np.asarray(winner84_results[4:5][0])[0]\n",
    "median_winner84 = np.asarray(winner84_results[9:10][0])[0]\n",
    "upper_winner84 = np.asarray(winner84_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration94 :\n",
    "\n",
    "slice94 = 93\n",
    "\n",
    "loser94 = [train_regret_loser_1[slice94],\n",
    "       train_regret_loser_2[slice94],\n",
    "       train_regret_loser_3[slice94],\n",
    "       train_regret_loser_4[slice94],\n",
    "       train_regret_loser_5[slice94],\n",
    "       train_regret_loser_6[slice94],\n",
    "       train_regret_loser_7[slice94],\n",
    "       train_regret_loser_8[slice94],\n",
    "       train_regret_loser_9[slice94],\n",
    "       train_regret_loser_10[slice94],\n",
    "       train_regret_loser_11[slice94],\n",
    "       train_regret_loser_12[slice94],\n",
    "       train_regret_loser_13[slice94],\n",
    "       train_regret_loser_14[slice94],\n",
    "       train_regret_loser_15[slice94],\n",
    "       train_regret_loser_16[slice94],\n",
    "       train_regret_loser_17[slice94],\n",
    "       train_regret_loser_18[slice94],\n",
    "       train_regret_loser_19[slice94],\n",
    "       train_regret_loser_20[slice94]]\n",
    "\n",
    "winner94 = [train_regret_winner_1[slice94],\n",
    "       train_regret_winner_2[slice94],\n",
    "       train_regret_winner_3[slice94],\n",
    "       train_regret_winner_4[slice94],\n",
    "       train_regret_winner_5[slice94],\n",
    "       train_regret_winner_6[slice94],\n",
    "       train_regret_winner_7[slice94],\n",
    "       train_regret_winner_8[slice94],\n",
    "       train_regret_winner_9[slice94],\n",
    "       train_regret_winner_10[slice94],\n",
    "       train_regret_winner_11[slice94],\n",
    "       train_regret_winner_12[slice94],\n",
    "       train_regret_winner_13[slice94],\n",
    "       train_regret_winner_14[slice94],\n",
    "       train_regret_winner_15[slice94],\n",
    "       train_regret_winner_16[slice94],\n",
    "       train_regret_winner_17[slice94],\n",
    "       train_regret_winner_18[slice94],\n",
    "       train_regret_winner_19[slice94],\n",
    "       train_regret_winner_20[slice94]]\n",
    "\n",
    "loser94_results = pd.DataFrame(loser94).sort_values(by=[0], ascending=False)\n",
    "winner94_results = pd.DataFrame(winner94).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser94 = np.asarray(loser94_results[4:5][0])[0]\n",
    "median_loser94 = np.asarray(loser94_results[9:10][0])[0]\n",
    "upper_loser94 = np.asarray(loser94_results[14:15][0])[0]\n",
    "\n",
    "lower_winner94 = np.asarray(winner94_results[4:5][0])[0]\n",
    "median_winner94 = np.asarray(winner94_results[9:10][0])[0]\n",
    "upper_winner94 = np.asarray(winner94_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration5 :\n",
    "\n",
    "slice5 = 4\n",
    "\n",
    "loser5 = [train_regret_loser_1[slice5],\n",
    "       train_regret_loser_2[slice5],\n",
    "       train_regret_loser_3[slice5],\n",
    "       train_regret_loser_4[slice5],\n",
    "       train_regret_loser_5[slice5],\n",
    "       train_regret_loser_6[slice5],\n",
    "       train_regret_loser_7[slice5],\n",
    "       train_regret_loser_8[slice5],\n",
    "       train_regret_loser_9[slice5],\n",
    "       train_regret_loser_10[slice5],\n",
    "       train_regret_loser_11[slice5],\n",
    "       train_regret_loser_12[slice5],\n",
    "       train_regret_loser_13[slice5],\n",
    "       train_regret_loser_14[slice5],\n",
    "       train_regret_loser_15[slice5],\n",
    "       train_regret_loser_16[slice5],\n",
    "       train_regret_loser_17[slice5],\n",
    "       train_regret_loser_18[slice5],\n",
    "       train_regret_loser_19[slice5],\n",
    "       train_regret_loser_20[slice5]]\n",
    "\n",
    "winner5 = [train_regret_winner_1[slice5],\n",
    "       train_regret_winner_2[slice5],\n",
    "       train_regret_winner_3[slice5],\n",
    "       train_regret_winner_4[slice5],\n",
    "       train_regret_winner_5[slice5],\n",
    "       train_regret_winner_6[slice5],\n",
    "       train_regret_winner_7[slice5],\n",
    "       train_regret_winner_8[slice5],\n",
    "       train_regret_winner_9[slice5],\n",
    "       train_regret_winner_10[slice5],\n",
    "       train_regret_winner_11[slice5],\n",
    "       train_regret_winner_12[slice5],\n",
    "       train_regret_winner_13[slice5],\n",
    "       train_regret_winner_14[slice5],\n",
    "       train_regret_winner_15[slice5],\n",
    "       train_regret_winner_16[slice5],\n",
    "       train_regret_winner_17[slice5],\n",
    "       train_regret_winner_18[slice5],\n",
    "       train_regret_winner_19[slice5],\n",
    "       train_regret_winner_20[slice5]]\n",
    "\n",
    "loser5_results = pd.DataFrame(loser5).sort_values(by=[0], ascending=False)\n",
    "winner5_results = pd.DataFrame(winner5).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser5 = np.asarray(loser5_results[4:5][0])[0]\n",
    "median_loser5 = np.asarray(loser5_results[9:10][0])[0]\n",
    "upper_loser5 = np.asarray(loser5_results[14:15][0])[0]\n",
    "\n",
    "lower_winner5 = np.asarray(winner5_results[4:5][0])[0]\n",
    "median_winner5 = np.asarray(winner5_results[9:10][0])[0]\n",
    "upper_winner5 = np.asarray(winner5_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration15 :\n",
    "\n",
    "slice15 = 14\n",
    "\n",
    "loser15 = [train_regret_loser_1[slice15],\n",
    "       train_regret_loser_2[slice15],\n",
    "       train_regret_loser_3[slice15],\n",
    "       train_regret_loser_4[slice15],\n",
    "       train_regret_loser_5[slice15],\n",
    "       train_regret_loser_6[slice15],\n",
    "       train_regret_loser_7[slice15],\n",
    "       train_regret_loser_8[slice15],\n",
    "       train_regret_loser_9[slice15],\n",
    "       train_regret_loser_10[slice15],\n",
    "       train_regret_loser_11[slice15],\n",
    "       train_regret_loser_12[slice15],\n",
    "       train_regret_loser_13[slice15],\n",
    "       train_regret_loser_14[slice15],\n",
    "       train_regret_loser_15[slice15],\n",
    "       train_regret_loser_16[slice15],\n",
    "       train_regret_loser_17[slice15],\n",
    "       train_regret_loser_18[slice15],\n",
    "       train_regret_loser_19[slice15],\n",
    "       train_regret_loser_20[slice15]]\n",
    "\n",
    "winner15 = [train_regret_winner_1[slice15],\n",
    "       train_regret_winner_2[slice15],\n",
    "       train_regret_winner_3[slice15],\n",
    "       train_regret_winner_4[slice15],\n",
    "       train_regret_winner_5[slice15],\n",
    "       train_regret_winner_6[slice15],\n",
    "       train_regret_winner_7[slice15],\n",
    "       train_regret_winner_8[slice15],\n",
    "       train_regret_winner_9[slice15],\n",
    "       train_regret_winner_10[slice15],\n",
    "       train_regret_winner_11[slice15],\n",
    "       train_regret_winner_12[slice15],\n",
    "       train_regret_winner_13[slice15],\n",
    "       train_regret_winner_14[slice15],\n",
    "       train_regret_winner_15[slice15],\n",
    "       train_regret_winner_16[slice15],\n",
    "       train_regret_winner_17[slice15],\n",
    "       train_regret_winner_18[slice15],\n",
    "       train_regret_winner_19[slice15],\n",
    "       train_regret_winner_20[slice15]]\n",
    "\n",
    "loser15_results = pd.DataFrame(loser15).sort_values(by=[0], ascending=False)\n",
    "winner15_results = pd.DataFrame(winner15).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser15 = np.asarray(loser15_results[4:5][0])[0]\n",
    "median_loser15 = np.asarray(loser15_results[9:10][0])[0]\n",
    "upper_loser15 = np.asarray(loser15_results[14:15][0])[0]\n",
    "\n",
    "lower_winner15 = np.asarray(winner15_results[4:5][0])[0]\n",
    "median_winner15 = np.asarray(winner15_results[9:10][0])[0]\n",
    "upper_winner15 = np.asarray(winner15_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration25 :\n",
    "\n",
    "slice25 = 24\n",
    "\n",
    "loser25 = [train_regret_loser_1[slice25],\n",
    "       train_regret_loser_2[slice25],\n",
    "       train_regret_loser_3[slice25],\n",
    "       train_regret_loser_4[slice25],\n",
    "       train_regret_loser_5[slice25],\n",
    "       train_regret_loser_6[slice25],\n",
    "       train_regret_loser_7[slice25],\n",
    "       train_regret_loser_8[slice25],\n",
    "       train_regret_loser_9[slice25],\n",
    "       train_regret_loser_10[slice25],\n",
    "       train_regret_loser_11[slice25],\n",
    "       train_regret_loser_12[slice25],\n",
    "       train_regret_loser_13[slice25],\n",
    "       train_regret_loser_14[slice25],\n",
    "       train_regret_loser_15[slice25],\n",
    "       train_regret_loser_16[slice25],\n",
    "       train_regret_loser_17[slice25],\n",
    "       train_regret_loser_18[slice25],\n",
    "       train_regret_loser_19[slice25],\n",
    "       train_regret_loser_20[slice25]]\n",
    "\n",
    "winner25 = [train_regret_winner_1[slice25],\n",
    "       train_regret_winner_2[slice25],\n",
    "       train_regret_winner_3[slice25],\n",
    "       train_regret_winner_4[slice25],\n",
    "       train_regret_winner_5[slice25],\n",
    "       train_regret_winner_6[slice25],\n",
    "       train_regret_winner_7[slice25],\n",
    "       train_regret_winner_8[slice25],\n",
    "       train_regret_winner_9[slice25],\n",
    "       train_regret_winner_10[slice25],\n",
    "       train_regret_winner_11[slice25],\n",
    "       train_regret_winner_12[slice25],\n",
    "       train_regret_winner_13[slice25],\n",
    "       train_regret_winner_14[slice25],\n",
    "       train_regret_winner_15[slice25],\n",
    "       train_regret_winner_16[slice25],\n",
    "       train_regret_winner_17[slice25],\n",
    "       train_regret_winner_18[slice25],\n",
    "       train_regret_winner_19[slice25],\n",
    "       train_regret_winner_20[slice25]]\n",
    "\n",
    "loser25_results = pd.DataFrame(loser25).sort_values(by=[0], ascending=False)\n",
    "winner25_results = pd.DataFrame(winner25).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser25 = np.asarray(loser25_results[4:5][0])[0]\n",
    "median_loser25 = np.asarray(loser25_results[9:10][0])[0]\n",
    "upper_loser25 = np.asarray(loser25_results[14:15][0])[0]\n",
    "\n",
    "lower_winner25 = np.asarray(winner25_results[4:5][0])[0]\n",
    "median_winner25 = np.asarray(winner25_results[9:10][0])[0]\n",
    "upper_winner25= np.asarray(winner25_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration35 :\n",
    "\n",
    "slice35 = 34\n",
    "\n",
    "loser35 = [train_regret_loser_1[slice35],\n",
    "       train_regret_loser_2[slice35],\n",
    "       train_regret_loser_3[slice35],\n",
    "       train_regret_loser_4[slice35],\n",
    "       train_regret_loser_5[slice35],\n",
    "       train_regret_loser_6[slice35],\n",
    "       train_regret_loser_7[slice35],\n",
    "       train_regret_loser_8[slice35],\n",
    "       train_regret_loser_9[slice35],\n",
    "       train_regret_loser_10[slice35],\n",
    "       train_regret_loser_11[slice35],\n",
    "       train_regret_loser_12[slice35],\n",
    "       train_regret_loser_13[slice35],\n",
    "       train_regret_loser_14[slice35],\n",
    "       train_regret_loser_15[slice35],\n",
    "       train_regret_loser_16[slice35],\n",
    "       train_regret_loser_17[slice35],\n",
    "       train_regret_loser_18[slice35],\n",
    "       train_regret_loser_19[slice35],\n",
    "       train_regret_loser_20[slice35]]\n",
    "\n",
    "winner35 = [train_regret_winner_1[slice35],\n",
    "       train_regret_winner_2[slice35],\n",
    "       train_regret_winner_3[slice35],\n",
    "       train_regret_winner_4[slice35],\n",
    "       train_regret_winner_5[slice35],\n",
    "       train_regret_winner_6[slice35],\n",
    "       train_regret_winner_7[slice35],\n",
    "       train_regret_winner_8[slice35],\n",
    "       train_regret_winner_9[slice35],\n",
    "       train_regret_winner_10[slice35],\n",
    "       train_regret_winner_11[slice35],\n",
    "       train_regret_winner_12[slice35],\n",
    "       train_regret_winner_13[slice35],\n",
    "       train_regret_winner_14[slice35],\n",
    "       train_regret_winner_15[slice35],\n",
    "       train_regret_winner_16[slice35],\n",
    "       train_regret_winner_17[slice35],\n",
    "       train_regret_winner_18[slice35],\n",
    "       train_regret_winner_19[slice35],\n",
    "       train_regret_winner_20[slice35]]\n",
    "\n",
    "loser35_results = pd.DataFrame(loser35).sort_values(by=[0], ascending=False)\n",
    "winner35_results = pd.DataFrame(winner35).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser35 = np.asarray(loser35_results[4:5][0])[0]\n",
    "median_loser35 = np.asarray(loser35_results[9:10][0])[0]\n",
    "upper_loser35 = np.asarray(loser35_results[14:15][0])[0]\n",
    "\n",
    "lower_winner35 = np.asarray(winner35_results[4:5][0])[0]\n",
    "median_winner35 = np.asarray(winner35_results[9:10][0])[0]\n",
    "upper_winner35 = np.asarray(winner35_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration45 :\n",
    "\n",
    "slice45 = 44\n",
    "\n",
    "loser45 = [train_regret_loser_1[slice45],\n",
    "       train_regret_loser_2[slice45],\n",
    "       train_regret_loser_3[slice45],\n",
    "       train_regret_loser_4[slice45],\n",
    "       train_regret_loser_5[slice45],\n",
    "       train_regret_loser_6[slice45],\n",
    "       train_regret_loser_7[slice45],\n",
    "       train_regret_loser_8[slice45],\n",
    "       train_regret_loser_9[slice45],\n",
    "       train_regret_loser_10[slice45],\n",
    "       train_regret_loser_11[slice45],\n",
    "       train_regret_loser_12[slice45],\n",
    "       train_regret_loser_13[slice45],\n",
    "       train_regret_loser_14[slice45],\n",
    "       train_regret_loser_15[slice45],\n",
    "       train_regret_loser_16[slice45],\n",
    "       train_regret_loser_17[slice45],\n",
    "       train_regret_loser_18[slice45],\n",
    "       train_regret_loser_19[slice45],\n",
    "       train_regret_loser_20[slice45]]\n",
    "\n",
    "winner45 = [train_regret_winner_1[slice45],\n",
    "       train_regret_winner_2[slice45],\n",
    "       train_regret_winner_3[slice45],\n",
    "       train_regret_winner_4[slice45],\n",
    "       train_regret_winner_5[slice45],\n",
    "       train_regret_winner_6[slice45],\n",
    "       train_regret_winner_7[slice45],\n",
    "       train_regret_winner_8[slice45],\n",
    "       train_regret_winner_9[slice45],\n",
    "       train_regret_winner_10[slice45],\n",
    "       train_regret_winner_11[slice45],\n",
    "       train_regret_winner_12[slice45],\n",
    "       train_regret_winner_13[slice45],\n",
    "       train_regret_winner_14[slice45],\n",
    "       train_regret_winner_15[slice45],\n",
    "       train_regret_winner_16[slice45],\n",
    "       train_regret_winner_17[slice45],\n",
    "       train_regret_winner_18[slice45],\n",
    "       train_regret_winner_19[slice45],\n",
    "       train_regret_winner_20[slice45]]\n",
    "\n",
    "loser45_results = pd.DataFrame(loser45).sort_values(by=[0], ascending=False)\n",
    "winner45_results = pd.DataFrame(winner45).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser45 = np.asarray(loser45_results[4:5][0])[0]\n",
    "median_loser45 = np.asarray(loser45_results[9:10][0])[0]\n",
    "upper_loser45 = np.asarray(loser45_results[14:15][0])[0]\n",
    "\n",
    "lower_winner45 = np.asarray(winner45_results[4:5][0])[0]\n",
    "median_winner45 = np.asarray(winner45_results[9:10][0])[0]\n",
    "upper_winner45 = np.asarray(winner45_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration55 :\n",
    "\n",
    "slice55 = 54\n",
    "\n",
    "loser55 = [train_regret_loser_1[slice55],\n",
    "       train_regret_loser_2[slice55],\n",
    "       train_regret_loser_3[slice55],\n",
    "       train_regret_loser_4[slice55],\n",
    "       train_regret_loser_5[slice55],\n",
    "       train_regret_loser_6[slice55],\n",
    "       train_regret_loser_7[slice55],\n",
    "       train_regret_loser_8[slice55],\n",
    "       train_regret_loser_9[slice55],\n",
    "       train_regret_loser_10[slice55],\n",
    "       train_regret_loser_11[slice55],\n",
    "       train_regret_loser_12[slice55],\n",
    "       train_regret_loser_13[slice55],\n",
    "       train_regret_loser_14[slice55],\n",
    "       train_regret_loser_15[slice55],\n",
    "       train_regret_loser_16[slice55],\n",
    "       train_regret_loser_17[slice55],\n",
    "       train_regret_loser_18[slice55],\n",
    "       train_regret_loser_19[slice55],\n",
    "       train_regret_loser_20[slice55]]\n",
    "\n",
    "winner55 = [train_regret_winner_1[slice55],\n",
    "       train_regret_winner_2[slice55],\n",
    "       train_regret_winner_3[slice55],\n",
    "       train_regret_winner_4[slice55],\n",
    "       train_regret_winner_5[slice55],\n",
    "       train_regret_winner_6[slice55],\n",
    "       train_regret_winner_7[slice55],\n",
    "       train_regret_winner_8[slice55],\n",
    "       train_regret_winner_9[slice55],\n",
    "       train_regret_winner_10[slice55],\n",
    "       train_regret_winner_11[slice55],\n",
    "       train_regret_winner_12[slice55],\n",
    "       train_regret_winner_13[slice55],\n",
    "       train_regret_winner_14[slice55],\n",
    "       train_regret_winner_15[slice55],\n",
    "       train_regret_winner_16[slice55],\n",
    "       train_regret_winner_17[slice55],\n",
    "       train_regret_winner_18[slice55],\n",
    "       train_regret_winner_19[slice55],\n",
    "       train_regret_winner_20[slice55]]\n",
    "\n",
    "loser55_results = pd.DataFrame(loser55).sort_values(by=[0], ascending=False)\n",
    "winner55_results = pd.DataFrame(winner55).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser55 = np.asarray(loser55_results[4:5][0])[0]\n",
    "median_loser55 = np.asarray(loser55_results[9:10][0])[0]\n",
    "upper_loser55 = np.asarray(loser55_results[14:15][0])[0]\n",
    "\n",
    "lower_winner55 = np.asarray(winner55_results[4:5][0])[0]\n",
    "median_winner55 = np.asarray(winner55_results[9:10][0])[0]\n",
    "upper_winner55 = np.asarray(winner55_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration65 :\n",
    "\n",
    "slice65 = 64\n",
    "\n",
    "loser65 = [train_regret_loser_1[slice65],\n",
    "       train_regret_loser_2[slice65],\n",
    "       train_regret_loser_3[slice65],\n",
    "       train_regret_loser_4[slice65],\n",
    "       train_regret_loser_5[slice65],\n",
    "       train_regret_loser_6[slice65],\n",
    "       train_regret_loser_7[slice65],\n",
    "       train_regret_loser_8[slice65],\n",
    "       train_regret_loser_9[slice65],\n",
    "       train_regret_loser_10[slice65],\n",
    "       train_regret_loser_11[slice65],\n",
    "       train_regret_loser_12[slice65],\n",
    "       train_regret_loser_13[slice65],\n",
    "       train_regret_loser_14[slice65],\n",
    "       train_regret_loser_15[slice65],\n",
    "       train_regret_loser_16[slice65],\n",
    "       train_regret_loser_17[slice65],\n",
    "       train_regret_loser_18[slice65],\n",
    "       train_regret_loser_19[slice65],\n",
    "       train_regret_loser_20[slice65]]\n",
    "\n",
    "winner65 = [train_regret_winner_1[slice65],\n",
    "       train_regret_winner_2[slice65],\n",
    "       train_regret_winner_3[slice65],\n",
    "       train_regret_winner_4[slice65],\n",
    "       train_regret_winner_5[slice65],\n",
    "       train_regret_winner_6[slice65],\n",
    "       train_regret_winner_7[slice65],\n",
    "       train_regret_winner_8[slice65],\n",
    "       train_regret_winner_9[slice65],\n",
    "       train_regret_winner_10[slice65],\n",
    "       train_regret_winner_11[slice65],\n",
    "       train_regret_winner_12[slice65],\n",
    "       train_regret_winner_13[slice65],\n",
    "       train_regret_winner_14[slice65],\n",
    "       train_regret_winner_15[slice65],\n",
    "       train_regret_winner_16[slice65],\n",
    "       train_regret_winner_17[slice65],\n",
    "       train_regret_winner_18[slice65],\n",
    "       train_regret_winner_19[slice65],\n",
    "       train_regret_winner_20[slice65]]\n",
    "\n",
    "loser65_results = pd.DataFrame(loser65).sort_values(by=[0], ascending=False)\n",
    "winner65_results = pd.DataFrame(winner65).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser65 = np.asarray(loser65_results[4:5][0])[0]\n",
    "median_loser65 = np.asarray(loser65_results[9:10][0])[0]\n",
    "upper_loser65 = np.asarray(loser65_results[14:15][0])[0]\n",
    "\n",
    "lower_winner65 = np.asarray(winner65_results[4:5][0])[0]\n",
    "median_winner65 = np.asarray(winner65_results[9:10][0])[0]\n",
    "upper_winner65 = np.asarray(winner65_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration75 :\n",
    "\n",
    "slice75 = 74\n",
    "\n",
    "loser75 = [train_regret_loser_1[slice75],\n",
    "       train_regret_loser_2[slice75],\n",
    "       train_regret_loser_3[slice75],\n",
    "       train_regret_loser_4[slice75],\n",
    "       train_regret_loser_5[slice75],\n",
    "       train_regret_loser_6[slice75],\n",
    "       train_regret_loser_7[slice75],\n",
    "       train_regret_loser_8[slice75],\n",
    "       train_regret_loser_9[slice75],\n",
    "       train_regret_loser_10[slice75],\n",
    "       train_regret_loser_11[slice75],\n",
    "       train_regret_loser_12[slice75],\n",
    "       train_regret_loser_13[slice75],\n",
    "       train_regret_loser_14[slice75],\n",
    "       train_regret_loser_15[slice75],\n",
    "       train_regret_loser_16[slice75],\n",
    "       train_regret_loser_17[slice75],\n",
    "       train_regret_loser_18[slice75],\n",
    "       train_regret_loser_19[slice75],\n",
    "       train_regret_loser_20[slice75]]\n",
    "\n",
    "winner75 = [train_regret_winner_1[slice75],\n",
    "       train_regret_winner_2[slice75],\n",
    "       train_regret_winner_3[slice75],\n",
    "       train_regret_winner_4[slice75],\n",
    "       train_regret_winner_5[slice75],\n",
    "       train_regret_winner_6[slice75],\n",
    "       train_regret_winner_7[slice75],\n",
    "       train_regret_winner_8[slice75],\n",
    "       train_regret_winner_9[slice75],\n",
    "       train_regret_winner_10[slice75],\n",
    "       train_regret_winner_11[slice75],\n",
    "       train_regret_winner_12[slice75],\n",
    "       train_regret_winner_13[slice75],\n",
    "       train_regret_winner_14[slice75],\n",
    "       train_regret_winner_15[slice75],\n",
    "       train_regret_winner_16[slice75],\n",
    "       train_regret_winner_17[slice75],\n",
    "       train_regret_winner_18[slice75],\n",
    "       train_regret_winner_19[slice75],\n",
    "       train_regret_winner_20[slice75]]\n",
    "\n",
    "loser75_results = pd.DataFrame(loser75).sort_values(by=[0], ascending=False)\n",
    "winner75_results = pd.DataFrame(winner75).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser75 = np.asarray(loser75_results[4:5][0])[0]\n",
    "median_loser75 = np.asarray(loser75_results[9:10][0])[0]\n",
    "upper_loser75 = np.asarray(loser75_results[14:15][0])[0]\n",
    "\n",
    "lower_winner75 = np.asarray(winner75_results[4:5][0])[0]\n",
    "median_winner75 = np.asarray(winner75_results[9:10][0])[0]\n",
    "upper_winner75 = np.asarray(winner75_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration85 :\n",
    "\n",
    "slice85 = 84\n",
    "\n",
    "loser85 = [train_regret_loser_1[slice85],\n",
    "       train_regret_loser_2[slice85],\n",
    "       train_regret_loser_3[slice85],\n",
    "       train_regret_loser_4[slice85],\n",
    "       train_regret_loser_5[slice85],\n",
    "       train_regret_loser_6[slice85],\n",
    "       train_regret_loser_7[slice85],\n",
    "       train_regret_loser_8[slice85],\n",
    "       train_regret_loser_9[slice85],\n",
    "       train_regret_loser_10[slice85],\n",
    "       train_regret_loser_11[slice85],\n",
    "       train_regret_loser_12[slice85],\n",
    "       train_regret_loser_13[slice85],\n",
    "       train_regret_loser_14[slice85],\n",
    "       train_regret_loser_15[slice85],\n",
    "       train_regret_loser_16[slice85],\n",
    "       train_regret_loser_17[slice85],\n",
    "       train_regret_loser_18[slice85],\n",
    "       train_regret_loser_19[slice85],\n",
    "       train_regret_loser_20[slice85]]\n",
    "\n",
    "winner85 = [train_regret_winner_1[slice85],\n",
    "       train_regret_winner_2[slice85],\n",
    "       train_regret_winner_3[slice85],\n",
    "       train_regret_winner_4[slice85],\n",
    "       train_regret_winner_5[slice85],\n",
    "       train_regret_winner_6[slice85],\n",
    "       train_regret_winner_7[slice85],\n",
    "       train_regret_winner_8[slice85],\n",
    "       train_regret_winner_9[slice85],\n",
    "       train_regret_winner_10[slice85],\n",
    "       train_regret_winner_11[slice85],\n",
    "       train_regret_winner_12[slice85],\n",
    "       train_regret_winner_13[slice85],\n",
    "       train_regret_winner_14[slice85],\n",
    "       train_regret_winner_15[slice85],\n",
    "       train_regret_winner_16[slice85],\n",
    "       train_regret_winner_17[slice85],\n",
    "       train_regret_winner_18[slice85],\n",
    "       train_regret_winner_19[slice85],\n",
    "       train_regret_winner_20[slice85]]\n",
    "\n",
    "loser85_results = pd.DataFrame(loser85).sort_values(by=[0], ascending=False)\n",
    "winner85_results = pd.DataFrame(winner85).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser85 = np.asarray(loser85_results[4:5][0])[0]\n",
    "median_loser85 = np.asarray(loser85_results[9:10][0])[0]\n",
    "upper_loser85 = np.asarray(loser85_results[14:15][0])[0]\n",
    "\n",
    "lower_winner85 = np.asarray(winner85_results[4:5][0])[0]\n",
    "median_winner85 = np.asarray(winner85_results[9:10][0])[0]\n",
    "upper_winner85 = np.asarray(winner85_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration95 :\n",
    "\n",
    "slice95 = 94\n",
    "\n",
    "loser95 = [train_regret_loser_1[slice95],\n",
    "       train_regret_loser_2[slice95],\n",
    "       train_regret_loser_3[slice95],\n",
    "       train_regret_loser_4[slice95],\n",
    "       train_regret_loser_5[slice95],\n",
    "       train_regret_loser_6[slice95],\n",
    "       train_regret_loser_7[slice95],\n",
    "       train_regret_loser_8[slice95],\n",
    "       train_regret_loser_9[slice95],\n",
    "       train_regret_loser_10[slice95],\n",
    "       train_regret_loser_11[slice95],\n",
    "       train_regret_loser_12[slice95],\n",
    "       train_regret_loser_13[slice95],\n",
    "       train_regret_loser_14[slice95],\n",
    "       train_regret_loser_15[slice95],\n",
    "       train_regret_loser_16[slice95],\n",
    "       train_regret_loser_17[slice95],\n",
    "       train_regret_loser_18[slice95],\n",
    "       train_regret_loser_19[slice95],\n",
    "       train_regret_loser_20[slice95]]\n",
    "\n",
    "winner95 = [train_regret_winner_1[slice95],\n",
    "       train_regret_winner_2[slice95],\n",
    "       train_regret_winner_3[slice95],\n",
    "       train_regret_winner_4[slice95],\n",
    "       train_regret_winner_5[slice95],\n",
    "       train_regret_winner_6[slice95],\n",
    "       train_regret_winner_7[slice95],\n",
    "       train_regret_winner_8[slice95],\n",
    "       train_regret_winner_9[slice95],\n",
    "       train_regret_winner_10[slice95],\n",
    "       train_regret_winner_11[slice95],\n",
    "       train_regret_winner_12[slice95],\n",
    "       train_regret_winner_13[slice95],\n",
    "       train_regret_winner_14[slice95],\n",
    "       train_regret_winner_15[slice95],\n",
    "       train_regret_winner_16[slice95],\n",
    "       train_regret_winner_17[slice95],\n",
    "       train_regret_winner_18[slice95],\n",
    "       train_regret_winner_19[slice95],\n",
    "       train_regret_winner_20[slice95]]\n",
    "\n",
    "loser95_results = pd.DataFrame(loser95).sort_values(by=[0], ascending=False)\n",
    "winner95_results = pd.DataFrame(winner95).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser95 = np.asarray(loser95_results[4:5][0])[0]\n",
    "median_loser95 = np.asarray(loser95_results[9:10][0])[0]\n",
    "upper_loser95 = np.asarray(loser95_results[14:15][0])[0]\n",
    "\n",
    "lower_winner95 = np.asarray(winner95_results[4:5][0])[0]\n",
    "median_winner95 = np.asarray(winner95_results[9:10][0])[0]\n",
    "upper_winner95 = np.asarray(winner95_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration6 :\n",
    "\n",
    "slice6 = 5\n",
    "\n",
    "loser6 = [train_regret_loser_1[slice6],\n",
    "       train_regret_loser_2[slice6],\n",
    "       train_regret_loser_3[slice6],\n",
    "       train_regret_loser_4[slice6],\n",
    "       train_regret_loser_5[slice6],\n",
    "       train_regret_loser_6[slice6],\n",
    "       train_regret_loser_7[slice6],\n",
    "       train_regret_loser_8[slice6],\n",
    "       train_regret_loser_9[slice6],\n",
    "       train_regret_loser_10[slice6],\n",
    "       train_regret_loser_11[slice6],\n",
    "       train_regret_loser_12[slice6],\n",
    "       train_regret_loser_13[slice6],\n",
    "       train_regret_loser_14[slice6],\n",
    "       train_regret_loser_15[slice6],\n",
    "       train_regret_loser_16[slice6],\n",
    "       train_regret_loser_17[slice6],\n",
    "       train_regret_loser_18[slice6],\n",
    "       train_regret_loser_19[slice6],\n",
    "       train_regret_loser_20[slice6]]\n",
    "\n",
    "winner6 = [train_regret_winner_1[slice6],\n",
    "       train_regret_winner_2[slice6],\n",
    "       train_regret_winner_3[slice6],\n",
    "       train_regret_winner_4[slice6],\n",
    "       train_regret_winner_5[slice6],\n",
    "       train_regret_winner_6[slice6],\n",
    "       train_regret_winner_7[slice6],\n",
    "       train_regret_winner_8[slice6],\n",
    "       train_regret_winner_9[slice6],\n",
    "       train_regret_winner_10[slice6],\n",
    "       train_regret_winner_11[slice6],\n",
    "       train_regret_winner_12[slice6],\n",
    "       train_regret_winner_13[slice6],\n",
    "       train_regret_winner_14[slice6],\n",
    "       train_regret_winner_15[slice6],\n",
    "       train_regret_winner_16[slice6],\n",
    "       train_regret_winner_17[slice6],\n",
    "       train_regret_winner_18[slice6],\n",
    "       train_regret_winner_19[slice6],\n",
    "       train_regret_winner_20[slice6]]\n",
    "\n",
    "loser6_results = pd.DataFrame(loser6).sort_values(by=[0], ascending=False)\n",
    "winner6_results = pd.DataFrame(winner6).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser6 = np.asarray(loser6_results[4:5][0])[0]\n",
    "median_loser6 = np.asarray(loser6_results[9:10][0])[0]\n",
    "upper_loser6 = np.asarray(loser6_results[14:15][0])[0]\n",
    "\n",
    "lower_winner6 = np.asarray(winner6_results[4:5][0])[0]\n",
    "median_winner6 = np.asarray(winner6_results[9:10][0])[0]\n",
    "upper_winner6 = np.asarray(winner6_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration16 :\n",
    "\n",
    "slice16 = 15\n",
    "\n",
    "loser16 = [train_regret_loser_1[slice16],\n",
    "       train_regret_loser_2[slice16],\n",
    "       train_regret_loser_3[slice16],\n",
    "       train_regret_loser_4[slice16],\n",
    "       train_regret_loser_5[slice16],\n",
    "       train_regret_loser_6[slice16],\n",
    "       train_regret_loser_7[slice16],\n",
    "       train_regret_loser_8[slice16],\n",
    "       train_regret_loser_9[slice16],\n",
    "       train_regret_loser_10[slice16],\n",
    "       train_regret_loser_11[slice16],\n",
    "       train_regret_loser_12[slice16],\n",
    "       train_regret_loser_13[slice16],\n",
    "       train_regret_loser_14[slice16],\n",
    "       train_regret_loser_15[slice16],\n",
    "       train_regret_loser_16[slice16],\n",
    "       train_regret_loser_17[slice16],\n",
    "       train_regret_loser_18[slice16],\n",
    "       train_regret_loser_19[slice16],\n",
    "       train_regret_loser_20[slice16]]\n",
    "\n",
    "winner16 = [train_regret_winner_1[slice16],\n",
    "       train_regret_winner_2[slice16],\n",
    "       train_regret_winner_3[slice16],\n",
    "       train_regret_winner_4[slice16],\n",
    "       train_regret_winner_5[slice16],\n",
    "       train_regret_winner_6[slice16],\n",
    "       train_regret_winner_7[slice16],\n",
    "       train_regret_winner_8[slice16],\n",
    "       train_regret_winner_9[slice16],\n",
    "       train_regret_winner_10[slice16],\n",
    "       train_regret_winner_11[slice16],\n",
    "       train_regret_winner_12[slice16],\n",
    "       train_regret_winner_13[slice16],\n",
    "       train_regret_winner_14[slice16],\n",
    "       train_regret_winner_15[slice16],\n",
    "       train_regret_winner_16[slice16],\n",
    "       train_regret_winner_17[slice16],\n",
    "       train_regret_winner_18[slice16],\n",
    "       train_regret_winner_19[slice16],\n",
    "       train_regret_winner_20[slice16]]\n",
    "\n",
    "loser16_results = pd.DataFrame(loser16).sort_values(by=[0], ascending=False)\n",
    "winner16_results = pd.DataFrame(winner16).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser16 = np.asarray(loser16_results[4:5][0])[0]\n",
    "median_loser16 = np.asarray(loser16_results[9:10][0])[0]\n",
    "upper_loser16 = np.asarray(loser16_results[14:15][0])[0]\n",
    "\n",
    "lower_winner16 = np.asarray(winner16_results[4:5][0])[0]\n",
    "median_winner16 = np.asarray(winner16_results[9:10][0])[0]\n",
    "upper_winner16 = np.asarray(winner16_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration26 :\n",
    "\n",
    "slice26 = 25\n",
    "\n",
    "loser26 = [train_regret_loser_1[slice26],\n",
    "       train_regret_loser_2[slice26],\n",
    "       train_regret_loser_3[slice26],\n",
    "       train_regret_loser_4[slice26],\n",
    "       train_regret_loser_5[slice26],\n",
    "       train_regret_loser_6[slice26],\n",
    "       train_regret_loser_7[slice26],\n",
    "       train_regret_loser_8[slice26],\n",
    "       train_regret_loser_9[slice26],\n",
    "       train_regret_loser_10[slice26],\n",
    "       train_regret_loser_11[slice26],\n",
    "       train_regret_loser_12[slice26],\n",
    "       train_regret_loser_13[slice26],\n",
    "       train_regret_loser_14[slice26],\n",
    "       train_regret_loser_15[slice26],\n",
    "       train_regret_loser_16[slice26],\n",
    "       train_regret_loser_17[slice26],\n",
    "       train_regret_loser_18[slice26],\n",
    "       train_regret_loser_19[slice26],\n",
    "       train_regret_loser_20[slice26]]\n",
    "\n",
    "winner26 = [train_regret_winner_1[slice26],\n",
    "       train_regret_winner_2[slice26],\n",
    "       train_regret_winner_3[slice26],\n",
    "       train_regret_winner_4[slice26],\n",
    "       train_regret_winner_5[slice26],\n",
    "       train_regret_winner_6[slice26],\n",
    "       train_regret_winner_7[slice26],\n",
    "       train_regret_winner_8[slice26],\n",
    "       train_regret_winner_9[slice26],\n",
    "       train_regret_winner_10[slice26],\n",
    "       train_regret_winner_11[slice26],\n",
    "       train_regret_winner_12[slice26],\n",
    "       train_regret_winner_13[slice26],\n",
    "       train_regret_winner_14[slice26],\n",
    "       train_regret_winner_15[slice26],\n",
    "       train_regret_winner_16[slice26],\n",
    "       train_regret_winner_17[slice26],\n",
    "       train_regret_winner_18[slice26],\n",
    "       train_regret_winner_19[slice26],\n",
    "       train_regret_winner_20[slice26]]\n",
    "\n",
    "loser26_results = pd.DataFrame(loser26).sort_values(by=[0], ascending=False)\n",
    "winner26_results = pd.DataFrame(winner26).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser26 = np.asarray(loser26_results[4:5][0])[0]\n",
    "median_loser26 = np.asarray(loser26_results[9:10][0])[0]\n",
    "upper_loser26 = np.asarray(loser26_results[14:15][0])[0]\n",
    "\n",
    "lower_winner26 = np.asarray(winner26_results[4:5][0])[0]\n",
    "median_winner26 = np.asarray(winner26_results[9:10][0])[0]\n",
    "upper_winner26 = np.asarray(winner26_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration36 :\n",
    "\n",
    "slice36 = 35\n",
    "\n",
    "loser36 = [train_regret_loser_1[slice36],\n",
    "       train_regret_loser_2[slice36],\n",
    "       train_regret_loser_3[slice36],\n",
    "       train_regret_loser_4[slice36],\n",
    "       train_regret_loser_5[slice36],\n",
    "       train_regret_loser_6[slice36],\n",
    "       train_regret_loser_7[slice36],\n",
    "       train_regret_loser_8[slice36],\n",
    "       train_regret_loser_9[slice36],\n",
    "       train_regret_loser_10[slice36],\n",
    "       train_regret_loser_11[slice36],\n",
    "       train_regret_loser_12[slice36],\n",
    "       train_regret_loser_13[slice36],\n",
    "       train_regret_loser_14[slice36],\n",
    "       train_regret_loser_15[slice36],\n",
    "       train_regret_loser_16[slice36],\n",
    "       train_regret_loser_17[slice36],\n",
    "       train_regret_loser_18[slice36],\n",
    "       train_regret_loser_19[slice36],\n",
    "       train_regret_loser_20[slice36]]\n",
    "\n",
    "winner36 = [train_regret_winner_1[slice36],\n",
    "       train_regret_winner_2[slice36],\n",
    "       train_regret_winner_3[slice36],\n",
    "       train_regret_winner_4[slice36],\n",
    "       train_regret_winner_5[slice36],\n",
    "       train_regret_winner_6[slice36],\n",
    "       train_regret_winner_7[slice36],\n",
    "       train_regret_winner_8[slice36],\n",
    "       train_regret_winner_9[slice36],\n",
    "       train_regret_winner_10[slice36],\n",
    "       train_regret_winner_11[slice36],\n",
    "       train_regret_winner_12[slice36],\n",
    "       train_regret_winner_13[slice36],\n",
    "       train_regret_winner_14[slice36],\n",
    "       train_regret_winner_15[slice36],\n",
    "       train_regret_winner_16[slice36],\n",
    "       train_regret_winner_17[slice36],\n",
    "       train_regret_winner_18[slice36],\n",
    "       train_regret_winner_19[slice36],\n",
    "       train_regret_winner_20[slice36]]\n",
    "\n",
    "loser36_results = pd.DataFrame(loser36).sort_values(by=[0], ascending=False)\n",
    "winner36_results = pd.DataFrame(winner36).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser36 = np.asarray(loser36_results[4:5][0])[0]\n",
    "median_loser36 = np.asarray(loser36_results[9:10][0])[0]\n",
    "upper_loser36 = np.asarray(loser36_results[14:15][0])[0]\n",
    "\n",
    "lower_winner36 = np.asarray(winner36_results[4:5][0])[0]\n",
    "median_winner36 = np.asarray(winner36_results[9:10][0])[0]\n",
    "upper_winner36 = np.asarray(winner36_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration46 :\n",
    "\n",
    "slice46 = 45\n",
    "\n",
    "loser46 = [train_regret_loser_1[slice46],\n",
    "       train_regret_loser_2[slice46],\n",
    "       train_regret_loser_3[slice46],\n",
    "       train_regret_loser_4[slice46],\n",
    "       train_regret_loser_5[slice46],\n",
    "       train_regret_loser_6[slice46],\n",
    "       train_regret_loser_7[slice46],\n",
    "       train_regret_loser_8[slice46],\n",
    "       train_regret_loser_9[slice46],\n",
    "       train_regret_loser_10[slice46],\n",
    "       train_regret_loser_11[slice46],\n",
    "       train_regret_loser_12[slice46],\n",
    "       train_regret_loser_13[slice46],\n",
    "       train_regret_loser_14[slice46],\n",
    "       train_regret_loser_15[slice46],\n",
    "       train_regret_loser_16[slice46],\n",
    "       train_regret_loser_17[slice46],\n",
    "       train_regret_loser_18[slice46],\n",
    "       train_regret_loser_19[slice46],\n",
    "       train_regret_loser_20[slice46]]\n",
    "\n",
    "winner46 = [train_regret_winner_1[slice46],\n",
    "       train_regret_winner_2[slice46],\n",
    "       train_regret_winner_3[slice46],\n",
    "       train_regret_winner_4[slice46],\n",
    "       train_regret_winner_5[slice46],\n",
    "       train_regret_winner_6[slice46],\n",
    "       train_regret_winner_7[slice46],\n",
    "       train_regret_winner_8[slice46],\n",
    "       train_regret_winner_9[slice46],\n",
    "       train_regret_winner_10[slice46],\n",
    "       train_regret_winner_11[slice46],\n",
    "       train_regret_winner_12[slice46],\n",
    "       train_regret_winner_13[slice46],\n",
    "       train_regret_winner_14[slice46],\n",
    "       train_regret_winner_15[slice46],\n",
    "       train_regret_winner_16[slice46],\n",
    "       train_regret_winner_17[slice46],\n",
    "       train_regret_winner_18[slice46],\n",
    "       train_regret_winner_19[slice46],\n",
    "       train_regret_winner_20[slice46]]\n",
    "\n",
    "loser46_results = pd.DataFrame(loser46).sort_values(by=[0], ascending=False)\n",
    "winner46_results = pd.DataFrame(winner46).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser46 = np.asarray(loser46_results[4:5][0])[0]\n",
    "median_loser46 = np.asarray(loser46_results[9:10][0])[0]\n",
    "upper_loser46 = np.asarray(loser46_results[14:15][0])[0]\n",
    "\n",
    "lower_winner46 = np.asarray(winner46_results[4:5][0])[0]\n",
    "median_winner46 = np.asarray(winner46_results[9:10][0])[0]\n",
    "upper_winner46 = np.asarray(winner46_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration56 :\n",
    "\n",
    "slice56 = 55\n",
    "\n",
    "loser56 = [train_regret_loser_1[slice56],\n",
    "       train_regret_loser_2[slice56],\n",
    "       train_regret_loser_3[slice56],\n",
    "       train_regret_loser_4[slice56],\n",
    "       train_regret_loser_5[slice56],\n",
    "       train_regret_loser_6[slice56],\n",
    "       train_regret_loser_7[slice56],\n",
    "       train_regret_loser_8[slice56],\n",
    "       train_regret_loser_9[slice56],\n",
    "       train_regret_loser_10[slice56],\n",
    "       train_regret_loser_11[slice56],\n",
    "       train_regret_loser_12[slice56],\n",
    "       train_regret_loser_13[slice56],\n",
    "       train_regret_loser_14[slice56],\n",
    "       train_regret_loser_15[slice56],\n",
    "       train_regret_loser_16[slice56],\n",
    "       train_regret_loser_17[slice56],\n",
    "       train_regret_loser_18[slice56],\n",
    "       train_regret_loser_19[slice56],\n",
    "       train_regret_loser_20[slice56]]\n",
    "\n",
    "winner56 = [train_regret_winner_1[slice56],\n",
    "       train_regret_winner_2[slice56],\n",
    "       train_regret_winner_3[slice56],\n",
    "       train_regret_winner_4[slice56],\n",
    "       train_regret_winner_5[slice56],\n",
    "       train_regret_winner_6[slice56],\n",
    "       train_regret_winner_7[slice56],\n",
    "       train_regret_winner_8[slice56],\n",
    "       train_regret_winner_9[slice56],\n",
    "       train_regret_winner_10[slice56],\n",
    "       train_regret_winner_11[slice56],\n",
    "       train_regret_winner_12[slice56],\n",
    "       train_regret_winner_13[slice56],\n",
    "       train_regret_winner_14[slice56],\n",
    "       train_regret_winner_15[slice56],\n",
    "       train_regret_winner_16[slice56],\n",
    "       train_regret_winner_17[slice56],\n",
    "       train_regret_winner_18[slice56],\n",
    "       train_regret_winner_19[slice56],\n",
    "       train_regret_winner_20[slice56]]\n",
    "\n",
    "loser56_results = pd.DataFrame(loser56).sort_values(by=[0], ascending=False)\n",
    "winner56_results = pd.DataFrame(winner56).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser56 = np.asarray(loser56_results[4:5][0])[0]\n",
    "median_loser56 = np.asarray(loser56_results[9:10][0])[0]\n",
    "upper_loser56 = np.asarray(loser56_results[14:15][0])[0]\n",
    "\n",
    "lower_winner56 = np.asarray(winner56_results[4:5][0])[0]\n",
    "median_winner56 = np.asarray(winner56_results[9:10][0])[0]\n",
    "upper_winner56 = np.asarray(winner56_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration66 :\n",
    "\n",
    "slice66 = 65\n",
    "\n",
    "loser66 = [train_regret_loser_1[slice66],\n",
    "       train_regret_loser_2[slice66],\n",
    "       train_regret_loser_3[slice66],\n",
    "       train_regret_loser_4[slice66],\n",
    "       train_regret_loser_5[slice66],\n",
    "       train_regret_loser_6[slice66],\n",
    "       train_regret_loser_7[slice66],\n",
    "       train_regret_loser_8[slice66],\n",
    "       train_regret_loser_9[slice66],\n",
    "       train_regret_loser_10[slice66],\n",
    "       train_regret_loser_11[slice66],\n",
    "       train_regret_loser_12[slice66],\n",
    "       train_regret_loser_13[slice66],\n",
    "       train_regret_loser_14[slice66],\n",
    "       train_regret_loser_15[slice66],\n",
    "       train_regret_loser_16[slice66],\n",
    "       train_regret_loser_17[slice66],\n",
    "       train_regret_loser_18[slice66],\n",
    "       train_regret_loser_19[slice66],\n",
    "       train_regret_loser_20[slice66]]\n",
    "\n",
    "winner66 = [train_regret_winner_1[slice66],\n",
    "       train_regret_winner_2[slice66],\n",
    "       train_regret_winner_3[slice66],\n",
    "       train_regret_winner_4[slice66],\n",
    "       train_regret_winner_5[slice66],\n",
    "       train_regret_winner_6[slice66],\n",
    "       train_regret_winner_7[slice66],\n",
    "       train_regret_winner_8[slice66],\n",
    "       train_regret_winner_9[slice66],\n",
    "       train_regret_winner_10[slice66],\n",
    "       train_regret_winner_11[slice66],\n",
    "       train_regret_winner_12[slice66],\n",
    "       train_regret_winner_13[slice66],\n",
    "       train_regret_winner_14[slice66],\n",
    "       train_regret_winner_15[slice66],\n",
    "       train_regret_winner_16[slice66],\n",
    "       train_regret_winner_17[slice66],\n",
    "       train_regret_winner_18[slice66],\n",
    "       train_regret_winner_19[slice66],\n",
    "       train_regret_winner_20[slice66]]\n",
    "\n",
    "loser66_results = pd.DataFrame(loser66).sort_values(by=[0], ascending=False)\n",
    "winner66_results = pd.DataFrame(winner66).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser66 = np.asarray(loser66_results[4:5][0])[0]\n",
    "median_loser66 = np.asarray(loser66_results[9:10][0])[0]\n",
    "upper_loser66 = np.asarray(loser66_results[14:15][0])[0]\n",
    "\n",
    "lower_winner66 = np.asarray(winner66_results[4:5][0])[0]\n",
    "median_winner66 = np.asarray(winner66_results[9:10][0])[0]\n",
    "upper_winner66 = np.asarray(winner66_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration76 :\n",
    "\n",
    "slice76 = 75\n",
    "\n",
    "loser76 = [train_regret_loser_1[slice76],\n",
    "       train_regret_loser_2[slice76],\n",
    "       train_regret_loser_3[slice76],\n",
    "       train_regret_loser_4[slice76],\n",
    "       train_regret_loser_5[slice76],\n",
    "       train_regret_loser_6[slice76],\n",
    "       train_regret_loser_7[slice76],\n",
    "       train_regret_loser_8[slice76],\n",
    "       train_regret_loser_9[slice76],\n",
    "       train_regret_loser_10[slice76],\n",
    "       train_regret_loser_11[slice76],\n",
    "       train_regret_loser_12[slice76],\n",
    "       train_regret_loser_13[slice76],\n",
    "       train_regret_loser_14[slice76],\n",
    "       train_regret_loser_15[slice76],\n",
    "       train_regret_loser_16[slice76],\n",
    "       train_regret_loser_17[slice76],\n",
    "       train_regret_loser_18[slice76],\n",
    "       train_regret_loser_19[slice76],\n",
    "       train_regret_loser_20[slice76]]\n",
    "\n",
    "winner76 = [train_regret_winner_1[slice76],\n",
    "       train_regret_winner_2[slice76],\n",
    "       train_regret_winner_3[slice76],\n",
    "       train_regret_winner_4[slice76],\n",
    "       train_regret_winner_5[slice76],\n",
    "       train_regret_winner_6[slice76],\n",
    "       train_regret_winner_7[slice76],\n",
    "       train_regret_winner_8[slice76],\n",
    "       train_regret_winner_9[slice76],\n",
    "       train_regret_winner_10[slice76],\n",
    "       train_regret_winner_11[slice76],\n",
    "       train_regret_winner_12[slice76],\n",
    "       train_regret_winner_13[slice76],\n",
    "       train_regret_winner_14[slice76],\n",
    "       train_regret_winner_15[slice76],\n",
    "       train_regret_winner_16[slice76],\n",
    "       train_regret_winner_17[slice76],\n",
    "       train_regret_winner_18[slice76],\n",
    "       train_regret_winner_19[slice76],\n",
    "       train_regret_winner_20[slice76]]\n",
    "\n",
    "loser76_results = pd.DataFrame(loser76).sort_values(by=[0], ascending=False)\n",
    "winner76_results = pd.DataFrame(winner76).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser76 = np.asarray(loser76_results[4:5][0])[0]\n",
    "median_loser76 = np.asarray(loser76_results[9:10][0])[0]\n",
    "upper_loser76 = np.asarray(loser76_results[14:15][0])[0]\n",
    "\n",
    "lower_winner76 = np.asarray(winner76_results[4:5][0])[0]\n",
    "median_winner76 = np.asarray(winner76_results[9:10][0])[0]\n",
    "upper_winner76 = np.asarray(winner76_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration86 :\n",
    "\n",
    "slice86 = 85\n",
    "\n",
    "loser86 = [train_regret_loser_1[slice86],\n",
    "       train_regret_loser_2[slice86],\n",
    "       train_regret_loser_3[slice86],\n",
    "       train_regret_loser_4[slice86],\n",
    "       train_regret_loser_5[slice86],\n",
    "       train_regret_loser_6[slice86],\n",
    "       train_regret_loser_7[slice86],\n",
    "       train_regret_loser_8[slice86],\n",
    "       train_regret_loser_9[slice86],\n",
    "       train_regret_loser_10[slice86],\n",
    "       train_regret_loser_11[slice86],\n",
    "       train_regret_loser_12[slice86],\n",
    "       train_regret_loser_13[slice86],\n",
    "       train_regret_loser_14[slice86],\n",
    "       train_regret_loser_15[slice86],\n",
    "       train_regret_loser_16[slice86],\n",
    "       train_regret_loser_17[slice86],\n",
    "       train_regret_loser_18[slice86],\n",
    "       train_regret_loser_19[slice86],\n",
    "       train_regret_loser_20[slice86]]\n",
    "\n",
    "winner86 = [train_regret_winner_1[slice86],\n",
    "       train_regret_winner_2[slice86],\n",
    "       train_regret_winner_3[slice86],\n",
    "       train_regret_winner_4[slice86],\n",
    "       train_regret_winner_5[slice86],\n",
    "       train_regret_winner_6[slice86],\n",
    "       train_regret_winner_7[slice86],\n",
    "       train_regret_winner_8[slice86],\n",
    "       train_regret_winner_9[slice86],\n",
    "       train_regret_winner_10[slice86],\n",
    "       train_regret_winner_11[slice86],\n",
    "       train_regret_winner_12[slice86],\n",
    "       train_regret_winner_13[slice86],\n",
    "       train_regret_winner_14[slice86],\n",
    "       train_regret_winner_15[slice86],\n",
    "       train_regret_winner_16[slice86],\n",
    "       train_regret_winner_17[slice86],\n",
    "       train_regret_winner_18[slice86],\n",
    "       train_regret_winner_19[slice86],\n",
    "       train_regret_winner_20[slice86]]\n",
    "\n",
    "loser86_results = pd.DataFrame(loser86).sort_values(by=[0], ascending=False)\n",
    "winner86_results = pd.DataFrame(winner86).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser86 = np.asarray(loser86_results[4:5][0])[0]\n",
    "median_loser86 = np.asarray(loser86_results[9:10][0])[0]\n",
    "upper_loser86 = np.asarray(loser86_results[14:15][0])[0]\n",
    "\n",
    "lower_winner86 = np.asarray(winner86_results[4:5][0])[0]\n",
    "median_winner86 = np.asarray(winner86_results[9:10][0])[0]\n",
    "upper_winner86 = np.asarray(winner86_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration96 :\n",
    "\n",
    "slice96 = 95\n",
    "\n",
    "loser96 = [train_regret_loser_1[slice96],\n",
    "       train_regret_loser_2[slice96],\n",
    "       train_regret_loser_3[slice96],\n",
    "       train_regret_loser_4[slice96],\n",
    "       train_regret_loser_5[slice96],\n",
    "       train_regret_loser_6[slice96],\n",
    "       train_regret_loser_7[slice96],\n",
    "       train_regret_loser_8[slice96],\n",
    "       train_regret_loser_9[slice96],\n",
    "       train_regret_loser_10[slice96],\n",
    "       train_regret_loser_11[slice96],\n",
    "       train_regret_loser_12[slice96],\n",
    "       train_regret_loser_13[slice96],\n",
    "       train_regret_loser_14[slice96],\n",
    "       train_regret_loser_15[slice96],\n",
    "       train_regret_loser_16[slice96],\n",
    "       train_regret_loser_17[slice96],\n",
    "       train_regret_loser_18[slice96],\n",
    "       train_regret_loser_19[slice96],\n",
    "       train_regret_loser_20[slice96]]\n",
    "\n",
    "winner96 = [train_regret_winner_1[slice96],\n",
    "       train_regret_winner_2[slice96],\n",
    "       train_regret_winner_3[slice96],\n",
    "       train_regret_winner_4[slice96],\n",
    "       train_regret_winner_5[slice96],\n",
    "       train_regret_winner_6[slice96],\n",
    "       train_regret_winner_7[slice96],\n",
    "       train_regret_winner_8[slice96],\n",
    "       train_regret_winner_9[slice96],\n",
    "       train_regret_winner_10[slice96],\n",
    "       train_regret_winner_11[slice96],\n",
    "       train_regret_winner_12[slice96],\n",
    "       train_regret_winner_13[slice96],\n",
    "       train_regret_winner_14[slice96],\n",
    "       train_regret_winner_15[slice96],\n",
    "       train_regret_winner_16[slice96],\n",
    "       train_regret_winner_17[slice96],\n",
    "       train_regret_winner_18[slice96],\n",
    "       train_regret_winner_19[slice96],\n",
    "       train_regret_winner_20[slice96]]\n",
    "\n",
    "loser96_results = pd.DataFrame(loser96).sort_values(by=[0], ascending=False)\n",
    "winner96_results = pd.DataFrame(winner96).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser96 = np.asarray(loser96_results[4:5][0])[0]\n",
    "median_loser96 = np.asarray(loser96_results[9:10][0])[0]\n",
    "upper_loser96 = np.asarray(loser96_results[14:15][0])[0]\n",
    "\n",
    "lower_winner96 = np.asarray(winner96_results[4:5][0])[0]\n",
    "median_winner96 = np.asarray(winner96_results[9:10][0])[0]\n",
    "upper_winner96 = np.asarray(winner96_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration7 :\n",
    "\n",
    "slice7 = 6\n",
    "\n",
    "loser7 = [train_regret_loser_1[slice7],\n",
    "       train_regret_loser_2[slice7],\n",
    "       train_regret_loser_3[slice7],\n",
    "       train_regret_loser_4[slice7],\n",
    "       train_regret_loser_5[slice7],\n",
    "       train_regret_loser_6[slice7],\n",
    "       train_regret_loser_7[slice7],\n",
    "       train_regret_loser_8[slice7],\n",
    "       train_regret_loser_9[slice7],\n",
    "       train_regret_loser_10[slice7],\n",
    "       train_regret_loser_11[slice7],\n",
    "       train_regret_loser_12[slice7],\n",
    "       train_regret_loser_13[slice7],\n",
    "       train_regret_loser_14[slice7],\n",
    "       train_regret_loser_15[slice7],\n",
    "       train_regret_loser_16[slice7],\n",
    "       train_regret_loser_17[slice7],\n",
    "       train_regret_loser_18[slice7],\n",
    "       train_regret_loser_19[slice7],\n",
    "       train_regret_loser_20[slice7]]\n",
    "\n",
    "winner7 = [train_regret_winner_1[slice7],\n",
    "       train_regret_winner_2[slice7],\n",
    "       train_regret_winner_3[slice7],\n",
    "       train_regret_winner_4[slice7],\n",
    "       train_regret_winner_5[slice7],\n",
    "       train_regret_winner_6[slice7],\n",
    "       train_regret_winner_7[slice7],\n",
    "       train_regret_winner_8[slice7],\n",
    "       train_regret_winner_9[slice7],\n",
    "       train_regret_winner_10[slice7],\n",
    "       train_regret_winner_11[slice7],\n",
    "       train_regret_winner_12[slice7],\n",
    "       train_regret_winner_13[slice7],\n",
    "       train_regret_winner_14[slice7],\n",
    "       train_regret_winner_15[slice7],\n",
    "       train_regret_winner_16[slice7],\n",
    "       train_regret_winner_17[slice7],\n",
    "       train_regret_winner_18[slice7],\n",
    "       train_regret_winner_19[slice7],\n",
    "       train_regret_winner_20[slice7]]\n",
    "\n",
    "loser7_results = pd.DataFrame(loser7).sort_values(by=[0], ascending=False)\n",
    "winner7_results = pd.DataFrame(winner7).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser7 = np.asarray(loser7_results[4:5][0])[0]\n",
    "median_loser7 = np.asarray(loser7_results[9:10][0])[0]\n",
    "upper_loser7 = np.asarray(loser7_results[14:15][0])[0]\n",
    "\n",
    "lower_winner7 = np.asarray(winner7_results[4:5][0])[0]\n",
    "median_winner7 = np.asarray(winner7_results[9:10][0])[0]\n",
    "upper_winner7 = np.asarray(winner7_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration17 :\n",
    "\n",
    "slice17 = 16\n",
    "\n",
    "loser17 = [train_regret_loser_1[slice17],\n",
    "       train_regret_loser_2[slice17],\n",
    "       train_regret_loser_3[slice17],\n",
    "       train_regret_loser_4[slice17],\n",
    "       train_regret_loser_5[slice17],\n",
    "       train_regret_loser_6[slice17],\n",
    "       train_regret_loser_7[slice17],\n",
    "       train_regret_loser_8[slice17],\n",
    "       train_regret_loser_9[slice17],\n",
    "       train_regret_loser_10[slice17],\n",
    "       train_regret_loser_11[slice17],\n",
    "       train_regret_loser_12[slice17],\n",
    "       train_regret_loser_13[slice17],\n",
    "       train_regret_loser_14[slice17],\n",
    "       train_regret_loser_15[slice17],\n",
    "       train_regret_loser_16[slice17],\n",
    "       train_regret_loser_17[slice17],\n",
    "       train_regret_loser_18[slice17],\n",
    "       train_regret_loser_19[slice17],\n",
    "       train_regret_loser_20[slice17]]\n",
    "\n",
    "winner17 = [train_regret_winner_1[slice17],\n",
    "       train_regret_winner_2[slice17],\n",
    "       train_regret_winner_3[slice17],\n",
    "       train_regret_winner_4[slice17],\n",
    "       train_regret_winner_5[slice17],\n",
    "       train_regret_winner_6[slice17],\n",
    "       train_regret_winner_7[slice17],\n",
    "       train_regret_winner_8[slice17],\n",
    "       train_regret_winner_9[slice17],\n",
    "       train_regret_winner_10[slice17],\n",
    "       train_regret_winner_11[slice17],\n",
    "       train_regret_winner_12[slice17],\n",
    "       train_regret_winner_13[slice17],\n",
    "       train_regret_winner_14[slice17],\n",
    "       train_regret_winner_15[slice17],\n",
    "       train_regret_winner_16[slice17],\n",
    "       train_regret_winner_17[slice17],\n",
    "       train_regret_winner_18[slice17],\n",
    "       train_regret_winner_19[slice17],\n",
    "       train_regret_winner_20[slice17]]\n",
    "\n",
    "loser17_results = pd.DataFrame(loser17).sort_values(by=[0], ascending=False)\n",
    "winner17_results = pd.DataFrame(winner17).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser17 = np.asarray(loser17_results[4:5][0])[0]\n",
    "median_loser17 = np.asarray(loser17_results[9:10][0])[0]\n",
    "upper_loser17 = np.asarray(loser17_results[14:15][0])[0]\n",
    "\n",
    "lower_winner17 = np.asarray(winner17_results[4:5][0])[0]\n",
    "median_winner17 = np.asarray(winner17_results[9:10][0])[0]\n",
    "upper_winner17 = np.asarray(winner17_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration27 :\n",
    "\n",
    "slice27 = 26\n",
    "\n",
    "loser27 = [train_regret_loser_1[slice27],\n",
    "       train_regret_loser_2[slice27],\n",
    "       train_regret_loser_3[slice27],\n",
    "       train_regret_loser_4[slice27],\n",
    "       train_regret_loser_5[slice27],\n",
    "       train_regret_loser_6[slice27],\n",
    "       train_regret_loser_7[slice27],\n",
    "       train_regret_loser_8[slice27],\n",
    "       train_regret_loser_9[slice27],\n",
    "       train_regret_loser_10[slice27],\n",
    "       train_regret_loser_11[slice27],\n",
    "       train_regret_loser_12[slice27],\n",
    "       train_regret_loser_13[slice27],\n",
    "       train_regret_loser_14[slice27],\n",
    "       train_regret_loser_15[slice27],\n",
    "       train_regret_loser_16[slice27],\n",
    "       train_regret_loser_17[slice27],\n",
    "       train_regret_loser_18[slice27],\n",
    "       train_regret_loser_19[slice27],\n",
    "       train_regret_loser_20[slice27]]\n",
    "\n",
    "winner27 = [train_regret_winner_1[slice27],\n",
    "       train_regret_winner_2[slice27],\n",
    "       train_regret_winner_3[slice27],\n",
    "       train_regret_winner_4[slice27],\n",
    "       train_regret_winner_5[slice27],\n",
    "       train_regret_winner_6[slice27],\n",
    "       train_regret_winner_7[slice27],\n",
    "       train_regret_winner_8[slice27],\n",
    "       train_regret_winner_9[slice27],\n",
    "       train_regret_winner_10[slice27],\n",
    "       train_regret_winner_11[slice27],\n",
    "       train_regret_winner_12[slice27],\n",
    "       train_regret_winner_13[slice27],\n",
    "       train_regret_winner_14[slice27],\n",
    "       train_regret_winner_15[slice27],\n",
    "       train_regret_winner_16[slice27],\n",
    "       train_regret_winner_17[slice27],\n",
    "       train_regret_winner_18[slice27],\n",
    "       train_regret_winner_19[slice27],\n",
    "       train_regret_winner_20[slice27]]\n",
    "\n",
    "loser27_results = pd.DataFrame(loser27).sort_values(by=[0], ascending=False)\n",
    "winner27_results = pd.DataFrame(winner27).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser27 = np.asarray(loser27_results[4:5][0])[0]\n",
    "median_loser27 = np.asarray(loser27_results[9:10][0])[0]\n",
    "upper_loser27 = np.asarray(loser27_results[14:15][0])[0]\n",
    "\n",
    "lower_winner27 = np.asarray(winner27_results[4:5][0])[0]\n",
    "median_winner27 = np.asarray(winner27_results[9:10][0])[0]\n",
    "upper_winner27 = np.asarray(winner27_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration37 :\n",
    "\n",
    "slice37 = 36\n",
    "\n",
    "loser37 = [train_regret_loser_1[slice37],\n",
    "       train_regret_loser_2[slice37],\n",
    "       train_regret_loser_3[slice37],\n",
    "       train_regret_loser_4[slice37],\n",
    "       train_regret_loser_5[slice37],\n",
    "       train_regret_loser_6[slice37],\n",
    "       train_regret_loser_7[slice37],\n",
    "       train_regret_loser_8[slice37],\n",
    "       train_regret_loser_9[slice37],\n",
    "       train_regret_loser_10[slice37],\n",
    "       train_regret_loser_11[slice37],\n",
    "       train_regret_loser_12[slice37],\n",
    "       train_regret_loser_13[slice37],\n",
    "       train_regret_loser_14[slice37],\n",
    "       train_regret_loser_15[slice37],\n",
    "       train_regret_loser_16[slice37],\n",
    "       train_regret_loser_17[slice37],\n",
    "       train_regret_loser_18[slice37],\n",
    "       train_regret_loser_19[slice37],\n",
    "       train_regret_loser_20[slice37]]\n",
    "\n",
    "winner37 = [train_regret_winner_1[slice37],\n",
    "       train_regret_winner_2[slice37],\n",
    "       train_regret_winner_3[slice37],\n",
    "       train_regret_winner_4[slice37],\n",
    "       train_regret_winner_5[slice37],\n",
    "       train_regret_winner_6[slice37],\n",
    "       train_regret_winner_7[slice37],\n",
    "       train_regret_winner_8[slice37],\n",
    "       train_regret_winner_9[slice37],\n",
    "       train_regret_winner_10[slice37],\n",
    "       train_regret_winner_11[slice37],\n",
    "       train_regret_winner_12[slice37],\n",
    "       train_regret_winner_13[slice37],\n",
    "       train_regret_winner_14[slice37],\n",
    "       train_regret_winner_15[slice37],\n",
    "       train_regret_winner_16[slice37],\n",
    "       train_regret_winner_17[slice37],\n",
    "       train_regret_winner_18[slice37],\n",
    "       train_regret_winner_19[slice37],\n",
    "       train_regret_winner_20[slice37]]\n",
    "\n",
    "loser37_results = pd.DataFrame(loser37).sort_values(by=[0], ascending=False)\n",
    "winner37_results = pd.DataFrame(winner37).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser37 = np.asarray(loser37_results[4:5][0])[0]\n",
    "median_loser37 = np.asarray(loser37_results[9:10][0])[0]\n",
    "upper_loser37 = np.asarray(loser37_results[14:15][0])[0]\n",
    "\n",
    "lower_winner37 = np.asarray(winner37_results[4:5][0])[0]\n",
    "median_winner37 = np.asarray(winner37_results[9:10][0])[0]\n",
    "upper_winner37 = np.asarray(winner37_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration47 :\n",
    "\n",
    "slice47 = 46\n",
    "\n",
    "loser47 = [train_regret_loser_1[slice47],\n",
    "       train_regret_loser_2[slice47],\n",
    "       train_regret_loser_3[slice47],\n",
    "       train_regret_loser_4[slice47],\n",
    "       train_regret_loser_5[slice47],\n",
    "       train_regret_loser_6[slice47],\n",
    "       train_regret_loser_7[slice47],\n",
    "       train_regret_loser_8[slice47],\n",
    "       train_regret_loser_9[slice47],\n",
    "       train_regret_loser_10[slice47],\n",
    "       train_regret_loser_11[slice47],\n",
    "       train_regret_loser_12[slice47],\n",
    "       train_regret_loser_13[slice47],\n",
    "       train_regret_loser_14[slice47],\n",
    "       train_regret_loser_15[slice47],\n",
    "       train_regret_loser_16[slice47],\n",
    "       train_regret_loser_17[slice47],\n",
    "       train_regret_loser_18[slice47],\n",
    "       train_regret_loser_19[slice47],\n",
    "       train_regret_loser_20[slice47]]\n",
    "\n",
    "winner47 = [train_regret_winner_1[slice47],\n",
    "       train_regret_winner_2[slice47],\n",
    "       train_regret_winner_3[slice47],\n",
    "       train_regret_winner_4[slice47],\n",
    "       train_regret_winner_5[slice47],\n",
    "       train_regret_winner_6[slice47],\n",
    "       train_regret_winner_7[slice47],\n",
    "       train_regret_winner_8[slice47],\n",
    "       train_regret_winner_9[slice47],\n",
    "       train_regret_winner_10[slice47],\n",
    "       train_regret_winner_11[slice47],\n",
    "       train_regret_winner_12[slice47],\n",
    "       train_regret_winner_13[slice47],\n",
    "       train_regret_winner_14[slice47],\n",
    "       train_regret_winner_15[slice47],\n",
    "       train_regret_winner_16[slice47],\n",
    "       train_regret_winner_17[slice47],\n",
    "       train_regret_winner_18[slice47],\n",
    "       train_regret_winner_19[slice47],\n",
    "       train_regret_winner_20[slice47]]\n",
    "\n",
    "loser47_results = pd.DataFrame(loser47).sort_values(by=[0], ascending=False)\n",
    "winner47_results = pd.DataFrame(winner47).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser47 = np.asarray(loser47_results[4:5][0])[0]\n",
    "median_loser47 = np.asarray(loser47_results[9:10][0])[0]\n",
    "upper_loser47 = np.asarray(loser47_results[14:15][0])[0]\n",
    "\n",
    "lower_winner47 = np.asarray(winner47_results[4:5][0])[0]\n",
    "median_winner47 = np.asarray(winner47_results[9:10][0])[0]\n",
    "upper_winner47 = np.asarray(winner47_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration57 :\n",
    "\n",
    "slice57 = 56\n",
    "\n",
    "loser57 = [train_regret_loser_1[slice57],\n",
    "       train_regret_loser_2[slice57],\n",
    "       train_regret_loser_3[slice57],\n",
    "       train_regret_loser_4[slice57],\n",
    "       train_regret_loser_5[slice57],\n",
    "       train_regret_loser_6[slice57],\n",
    "       train_regret_loser_7[slice57],\n",
    "       train_regret_loser_8[slice57],\n",
    "       train_regret_loser_9[slice57],\n",
    "       train_regret_loser_10[slice57],\n",
    "       train_regret_loser_11[slice57],\n",
    "       train_regret_loser_12[slice57],\n",
    "       train_regret_loser_13[slice57],\n",
    "       train_regret_loser_14[slice57],\n",
    "       train_regret_loser_15[slice57],\n",
    "       train_regret_loser_16[slice57],\n",
    "       train_regret_loser_17[slice57],\n",
    "       train_regret_loser_18[slice57],\n",
    "       train_regret_loser_19[slice57],\n",
    "       train_regret_loser_20[slice57]]\n",
    "\n",
    "winner57 = [train_regret_winner_1[slice57],\n",
    "       train_regret_winner_2[slice57],\n",
    "       train_regret_winner_3[slice57],\n",
    "       train_regret_winner_4[slice57],\n",
    "       train_regret_winner_5[slice57],\n",
    "       train_regret_winner_6[slice57],\n",
    "       train_regret_winner_7[slice57],\n",
    "       train_regret_winner_8[slice57],\n",
    "       train_regret_winner_9[slice57],\n",
    "       train_regret_winner_10[slice57],\n",
    "       train_regret_winner_11[slice57],\n",
    "       train_regret_winner_12[slice57],\n",
    "       train_regret_winner_13[slice57],\n",
    "       train_regret_winner_14[slice57],\n",
    "       train_regret_winner_15[slice57],\n",
    "       train_regret_winner_16[slice57],\n",
    "       train_regret_winner_17[slice57],\n",
    "       train_regret_winner_18[slice57],\n",
    "       train_regret_winner_19[slice57],\n",
    "       train_regret_winner_20[slice57]]\n",
    "\n",
    "loser57_results = pd.DataFrame(loser57).sort_values(by=[0], ascending=False)\n",
    "winner57_results = pd.DataFrame(winner57).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser57 = np.asarray(loser57_results[4:5][0])[0]\n",
    "median_loser57 = np.asarray(loser57_results[9:10][0])[0]\n",
    "upper_loser57 = np.asarray(loser57_results[14:15][0])[0]\n",
    "\n",
    "lower_winner57 = np.asarray(winner57_results[4:5][0])[0]\n",
    "median_winner57 = np.asarray(winner57_results[9:10][0])[0]\n",
    "upper_winner57 = np.asarray(winner57_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration67 :\n",
    "\n",
    "slice67 = 66\n",
    "\n",
    "loser67 = [train_regret_loser_1[slice67],\n",
    "       train_regret_loser_2[slice67],\n",
    "       train_regret_loser_3[slice67],\n",
    "       train_regret_loser_4[slice67],\n",
    "       train_regret_loser_5[slice67],\n",
    "       train_regret_loser_6[slice67],\n",
    "       train_regret_loser_7[slice67],\n",
    "       train_regret_loser_8[slice67],\n",
    "       train_regret_loser_9[slice67],\n",
    "       train_regret_loser_10[slice67],\n",
    "       train_regret_loser_11[slice67],\n",
    "       train_regret_loser_12[slice67],\n",
    "       train_regret_loser_13[slice67],\n",
    "       train_regret_loser_14[slice67],\n",
    "       train_regret_loser_15[slice67],\n",
    "       train_regret_loser_16[slice67],\n",
    "       train_regret_loser_17[slice67],\n",
    "       train_regret_loser_18[slice67],\n",
    "       train_regret_loser_19[slice67],\n",
    "       train_regret_loser_20[slice67]]\n",
    "\n",
    "winner67 = [train_regret_winner_1[slice67],\n",
    "       train_regret_winner_2[slice67],\n",
    "       train_regret_winner_3[slice67],\n",
    "       train_regret_winner_4[slice67],\n",
    "       train_regret_winner_5[slice67],\n",
    "       train_regret_winner_6[slice67],\n",
    "       train_regret_winner_7[slice67],\n",
    "       train_regret_winner_8[slice67],\n",
    "       train_regret_winner_9[slice67],\n",
    "       train_regret_winner_10[slice67],\n",
    "       train_regret_winner_11[slice67],\n",
    "       train_regret_winner_12[slice67],\n",
    "       train_regret_winner_13[slice67],\n",
    "       train_regret_winner_14[slice67],\n",
    "       train_regret_winner_15[slice67],\n",
    "       train_regret_winner_16[slice67],\n",
    "       train_regret_winner_17[slice67],\n",
    "       train_regret_winner_18[slice67],\n",
    "       train_regret_winner_19[slice67],\n",
    "       train_regret_winner_20[slice67]]\n",
    "\n",
    "loser67_results = pd.DataFrame(loser67).sort_values(by=[0], ascending=False)\n",
    "winner67_results = pd.DataFrame(winner67).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser67 = np.asarray(loser67_results[4:5][0])[0]\n",
    "median_loser67 = np.asarray(loser67_results[9:10][0])[0]\n",
    "upper_loser67 = np.asarray(loser67_results[14:15][0])[0]\n",
    "\n",
    "lower_winner67 = np.asarray(winner67_results[4:5][0])[0]\n",
    "median_winner67 = np.asarray(winner67_results[9:10][0])[0]\n",
    "upper_winner67 = np.asarray(winner67_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration77 :\n",
    "\n",
    "slice77 = 76\n",
    "\n",
    "loser77 = [train_regret_loser_1[slice77],\n",
    "       train_regret_loser_2[slice77],\n",
    "       train_regret_loser_3[slice77],\n",
    "       train_regret_loser_4[slice77],\n",
    "       train_regret_loser_5[slice77],\n",
    "       train_regret_loser_6[slice77],\n",
    "       train_regret_loser_7[slice77],\n",
    "       train_regret_loser_8[slice77],\n",
    "       train_regret_loser_9[slice77],\n",
    "       train_regret_loser_10[slice77],\n",
    "       train_regret_loser_11[slice77],\n",
    "       train_regret_loser_12[slice77],\n",
    "       train_regret_loser_13[slice77],\n",
    "       train_regret_loser_14[slice77],\n",
    "       train_regret_loser_15[slice77],\n",
    "       train_regret_loser_16[slice77],\n",
    "       train_regret_loser_17[slice77],\n",
    "       train_regret_loser_18[slice77],\n",
    "       train_regret_loser_19[slice77],\n",
    "       train_regret_loser_20[slice77]]\n",
    "\n",
    "winner77 = [train_regret_winner_1[slice77],\n",
    "       train_regret_winner_2[slice77],\n",
    "       train_regret_winner_3[slice77],\n",
    "       train_regret_winner_4[slice77],\n",
    "       train_regret_winner_5[slice77],\n",
    "       train_regret_winner_6[slice77],\n",
    "       train_regret_winner_7[slice77],\n",
    "       train_regret_winner_8[slice77],\n",
    "       train_regret_winner_9[slice77],\n",
    "       train_regret_winner_10[slice77],\n",
    "       train_regret_winner_11[slice77],\n",
    "       train_regret_winner_12[slice77],\n",
    "       train_regret_winner_13[slice77],\n",
    "       train_regret_winner_14[slice77],\n",
    "       train_regret_winner_15[slice77],\n",
    "       train_regret_winner_16[slice77],\n",
    "       train_regret_winner_17[slice77],\n",
    "       train_regret_winner_18[slice77],\n",
    "       train_regret_winner_19[slice77],\n",
    "       train_regret_winner_20[slice77]]\n",
    "\n",
    "loser77_results = pd.DataFrame(loser77).sort_values(by=[0], ascending=False)\n",
    "winner77_results = pd.DataFrame(winner77).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser77 = np.asarray(loser77_results[4:5][0])[0]\n",
    "median_loser77 = np.asarray(loser77_results[9:10][0])[0]\n",
    "upper_loser77 = np.asarray(loser77_results[14:15][0])[0]\n",
    "\n",
    "lower_winner77 = np.asarray(winner77_results[4:5][0])[0]\n",
    "median_winner77 = np.asarray(winner77_results[9:10][0])[0]\n",
    "upper_winner77 = np.asarray(winner77_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration87 :\n",
    "\n",
    "slice87 = 86\n",
    "\n",
    "loser87 = [train_regret_loser_1[slice87],\n",
    "       train_regret_loser_2[slice87],\n",
    "       train_regret_loser_3[slice87],\n",
    "       train_regret_loser_4[slice87],\n",
    "       train_regret_loser_5[slice87],\n",
    "       train_regret_loser_6[slice87],\n",
    "       train_regret_loser_7[slice87],\n",
    "       train_regret_loser_8[slice87],\n",
    "       train_regret_loser_9[slice87],\n",
    "       train_regret_loser_10[slice87],\n",
    "       train_regret_loser_11[slice87],\n",
    "       train_regret_loser_12[slice87],\n",
    "       train_regret_loser_13[slice87],\n",
    "       train_regret_loser_14[slice87],\n",
    "       train_regret_loser_15[slice87],\n",
    "       train_regret_loser_16[slice87],\n",
    "       train_regret_loser_17[slice87],\n",
    "       train_regret_loser_18[slice87],\n",
    "       train_regret_loser_19[slice87],\n",
    "       train_regret_loser_20[slice87]]\n",
    "\n",
    "winner87 = [train_regret_winner_1[slice87],\n",
    "       train_regret_winner_2[slice87],\n",
    "       train_regret_winner_3[slice87],\n",
    "       train_regret_winner_4[slice87],\n",
    "       train_regret_winner_5[slice87],\n",
    "       train_regret_winner_6[slice87],\n",
    "       train_regret_winner_7[slice87],\n",
    "       train_regret_winner_8[slice87],\n",
    "       train_regret_winner_9[slice87],\n",
    "       train_regret_winner_10[slice87],\n",
    "       train_regret_winner_11[slice87],\n",
    "       train_regret_winner_12[slice87],\n",
    "       train_regret_winner_13[slice87],\n",
    "       train_regret_winner_14[slice87],\n",
    "       train_regret_winner_15[slice87],\n",
    "       train_regret_winner_16[slice87],\n",
    "       train_regret_winner_17[slice87],\n",
    "       train_regret_winner_18[slice87],\n",
    "       train_regret_winner_19[slice87],\n",
    "       train_regret_winner_20[slice87]]\n",
    "\n",
    "loser87_results = pd.DataFrame(loser87).sort_values(by=[0], ascending=False)\n",
    "winner87_results = pd.DataFrame(winner87).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser87 = np.asarray(loser87_results[4:5][0])[0]\n",
    "median_loser87 = np.asarray(loser87_results[9:10][0])[0]\n",
    "upper_loser87 = np.asarray(loser87_results[14:15][0])[0]\n",
    "\n",
    "lower_winner87 = np.asarray(winner87_results[4:5][0])[0]\n",
    "median_winner87 = np.asarray(winner87_results[9:10][0])[0]\n",
    "upper_winner87 = np.asarray(winner87_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration97 :\n",
    "\n",
    "slice97 = 96\n",
    "\n",
    "loser97 = [train_regret_loser_1[slice97],\n",
    "       train_regret_loser_2[slice97],\n",
    "       train_regret_loser_3[slice97],\n",
    "       train_regret_loser_4[slice97],\n",
    "       train_regret_loser_5[slice97],\n",
    "       train_regret_loser_6[slice97],\n",
    "       train_regret_loser_7[slice97],\n",
    "       train_regret_loser_8[slice97],\n",
    "       train_regret_loser_9[slice97],\n",
    "       train_regret_loser_10[slice97],\n",
    "       train_regret_loser_11[slice97],\n",
    "       train_regret_loser_12[slice97],\n",
    "       train_regret_loser_13[slice97],\n",
    "       train_regret_loser_14[slice97],\n",
    "       train_regret_loser_15[slice97],\n",
    "       train_regret_loser_16[slice97],\n",
    "       train_regret_loser_17[slice97],\n",
    "       train_regret_loser_18[slice97],\n",
    "       train_regret_loser_19[slice97],\n",
    "       train_regret_loser_20[slice97]]\n",
    "\n",
    "winner97 = [train_regret_winner_1[slice97],\n",
    "       train_regret_winner_2[slice97],\n",
    "       train_regret_winner_3[slice97],\n",
    "       train_regret_winner_4[slice97],\n",
    "       train_regret_winner_5[slice97],\n",
    "       train_regret_winner_6[slice97],\n",
    "       train_regret_winner_7[slice97],\n",
    "       train_regret_winner_8[slice97],\n",
    "       train_regret_winner_9[slice97],\n",
    "       train_regret_winner_10[slice97],\n",
    "       train_regret_winner_11[slice97],\n",
    "       train_regret_winner_12[slice97],\n",
    "       train_regret_winner_13[slice97],\n",
    "       train_regret_winner_14[slice97],\n",
    "       train_regret_winner_15[slice97],\n",
    "       train_regret_winner_16[slice97],\n",
    "       train_regret_winner_17[slice97],\n",
    "       train_regret_winner_18[slice97],\n",
    "       train_regret_winner_19[slice97],\n",
    "       train_regret_winner_20[slice97]]\n",
    "\n",
    "loser97_results = pd.DataFrame(loser97).sort_values(by=[0], ascending=False)\n",
    "winner97_results = pd.DataFrame(winner97).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser97 = np.asarray(loser97_results[4:5][0])[0]\n",
    "median_loser97 = np.asarray(loser97_results[9:10][0])[0]\n",
    "upper_loser97 = np.asarray(loser97_results[14:15][0])[0]\n",
    "\n",
    "lower_winner97 = np.asarray(winner97_results[4:5][0])[0]\n",
    "median_winner97 = np.asarray(winner97_results[9:10][0])[0]\n",
    "upper_winner97 = np.asarray(winner97_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration8 :\n",
    "\n",
    "slice8 = 7\n",
    "\n",
    "loser8 = [train_regret_loser_1[slice8],\n",
    "       train_regret_loser_2[slice8],\n",
    "       train_regret_loser_3[slice8],\n",
    "       train_regret_loser_4[slice8],\n",
    "       train_regret_loser_5[slice8],\n",
    "       train_regret_loser_6[slice8],\n",
    "       train_regret_loser_7[slice8],\n",
    "       train_regret_loser_8[slice8],\n",
    "       train_regret_loser_9[slice8],\n",
    "       train_regret_loser_10[slice8],\n",
    "       train_regret_loser_11[slice8],\n",
    "       train_regret_loser_12[slice8],\n",
    "       train_regret_loser_13[slice8],\n",
    "       train_regret_loser_14[slice8],\n",
    "       train_regret_loser_15[slice8],\n",
    "       train_regret_loser_16[slice8],\n",
    "       train_regret_loser_17[slice8],\n",
    "       train_regret_loser_18[slice8],\n",
    "       train_regret_loser_19[slice8],\n",
    "       train_regret_loser_20[slice8]]\n",
    "\n",
    "winner8 = [train_regret_winner_1[slice8],\n",
    "       train_regret_winner_2[slice8],\n",
    "       train_regret_winner_3[slice8],\n",
    "       train_regret_winner_4[slice8],\n",
    "       train_regret_winner_5[slice8],\n",
    "       train_regret_winner_6[slice8],\n",
    "       train_regret_winner_7[slice8],\n",
    "       train_regret_winner_8[slice8],\n",
    "       train_regret_winner_9[slice8],\n",
    "       train_regret_winner_10[slice8],\n",
    "       train_regret_winner_11[slice8],\n",
    "       train_regret_winner_12[slice8],\n",
    "       train_regret_winner_13[slice8],\n",
    "       train_regret_winner_14[slice8],\n",
    "       train_regret_winner_15[slice8],\n",
    "       train_regret_winner_16[slice8],\n",
    "       train_regret_winner_17[slice8],\n",
    "       train_regret_winner_18[slice8],\n",
    "       train_regret_winner_19[slice8],\n",
    "       train_regret_winner_20[slice8]]\n",
    "\n",
    "loser8_results = pd.DataFrame(loser8).sort_values(by=[0], ascending=False)\n",
    "winner8_results = pd.DataFrame(winner8).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser8 = np.asarray(loser8_results[4:5][0])[0]\n",
    "median_loser8 = np.asarray(loser8_results[9:10][0])[0]\n",
    "upper_loser8 = np.asarray(loser8_results[14:15][0])[0]\n",
    "\n",
    "lower_winner8 = np.asarray(winner8_results[4:5][0])[0]\n",
    "median_winner8 = np.asarray(winner8_results[9:10][0])[0]\n",
    "upper_winner8 = np.asarray(winner8_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration18 :\n",
    "\n",
    "slice18 = 17\n",
    "\n",
    "loser18 = [train_regret_loser_1[slice18],\n",
    "       train_regret_loser_2[slice18],\n",
    "       train_regret_loser_3[slice18],\n",
    "       train_regret_loser_4[slice18],\n",
    "       train_regret_loser_5[slice18],\n",
    "       train_regret_loser_6[slice18],\n",
    "       train_regret_loser_7[slice18],\n",
    "       train_regret_loser_8[slice18],\n",
    "       train_regret_loser_9[slice18],\n",
    "       train_regret_loser_10[slice18],\n",
    "       train_regret_loser_11[slice18],\n",
    "       train_regret_loser_12[slice18],\n",
    "       train_regret_loser_13[slice18],\n",
    "       train_regret_loser_14[slice18],\n",
    "       train_regret_loser_15[slice18],\n",
    "       train_regret_loser_16[slice18],\n",
    "       train_regret_loser_17[slice18],\n",
    "       train_regret_loser_18[slice18],\n",
    "       train_regret_loser_19[slice18],\n",
    "       train_regret_loser_20[slice18]]\n",
    "\n",
    "winner18 = [train_regret_winner_1[slice18],\n",
    "       train_regret_winner_2[slice18],\n",
    "       train_regret_winner_3[slice18],\n",
    "       train_regret_winner_4[slice18],\n",
    "       train_regret_winner_5[slice18],\n",
    "       train_regret_winner_6[slice18],\n",
    "       train_regret_winner_7[slice18],\n",
    "       train_regret_winner_8[slice18],\n",
    "       train_regret_winner_9[slice18],\n",
    "       train_regret_winner_10[slice18],\n",
    "       train_regret_winner_11[slice18],\n",
    "       train_regret_winner_12[slice18],\n",
    "       train_regret_winner_13[slice18],\n",
    "       train_regret_winner_14[slice18],\n",
    "       train_regret_winner_15[slice18],\n",
    "       train_regret_winner_16[slice18],\n",
    "       train_regret_winner_17[slice18],\n",
    "       train_regret_winner_18[slice18],\n",
    "       train_regret_winner_19[slice18],\n",
    "       train_regret_winner_20[slice18]]\n",
    "\n",
    "loser18_results = pd.DataFrame(loser18).sort_values(by=[0], ascending=False)\n",
    "winner18_results = pd.DataFrame(winner18).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser18 = np.asarray(loser18_results[4:5][0])[0]\n",
    "median_loser18 = np.asarray(loser18_results[9:10][0])[0]\n",
    "upper_loser18 = np.asarray(loser18_results[14:15][0])[0]\n",
    "\n",
    "lower_winner18 = np.asarray(winner18_results[4:5][0])[0]\n",
    "median_winner18 = np.asarray(winner18_results[9:10][0])[0]\n",
    "upper_winner18 = np.asarray(winner18_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration28 :\n",
    "\n",
    "slice28 = 27\n",
    "\n",
    "loser28 = [train_regret_loser_1[slice28],\n",
    "       train_regret_loser_2[slice28],\n",
    "       train_regret_loser_3[slice28],\n",
    "       train_regret_loser_4[slice28],\n",
    "       train_regret_loser_5[slice28],\n",
    "       train_regret_loser_6[slice28],\n",
    "       train_regret_loser_7[slice28],\n",
    "       train_regret_loser_8[slice28],\n",
    "       train_regret_loser_9[slice28],\n",
    "       train_regret_loser_10[slice28],\n",
    "       train_regret_loser_11[slice28],\n",
    "       train_regret_loser_12[slice28],\n",
    "       train_regret_loser_13[slice28],\n",
    "       train_regret_loser_14[slice28],\n",
    "       train_regret_loser_15[slice28],\n",
    "       train_regret_loser_16[slice28],\n",
    "       train_regret_loser_17[slice28],\n",
    "       train_regret_loser_18[slice28],\n",
    "       train_regret_loser_19[slice28],\n",
    "       train_regret_loser_20[slice28]]\n",
    "\n",
    "winner28 = [train_regret_winner_1[slice28],\n",
    "       train_regret_winner_2[slice28],\n",
    "       train_regret_winner_3[slice28],\n",
    "       train_regret_winner_4[slice28],\n",
    "       train_regret_winner_5[slice28],\n",
    "       train_regret_winner_6[slice28],\n",
    "       train_regret_winner_7[slice28],\n",
    "       train_regret_winner_8[slice28],\n",
    "       train_regret_winner_9[slice28],\n",
    "       train_regret_winner_10[slice28],\n",
    "       train_regret_winner_11[slice28],\n",
    "       train_regret_winner_12[slice28],\n",
    "       train_regret_winner_13[slice28],\n",
    "       train_regret_winner_14[slice28],\n",
    "       train_regret_winner_15[slice28],\n",
    "       train_regret_winner_16[slice28],\n",
    "       train_regret_winner_17[slice28],\n",
    "       train_regret_winner_18[slice28],\n",
    "       train_regret_winner_19[slice28],\n",
    "       train_regret_winner_20[slice28]]\n",
    "\n",
    "loser28_results = pd.DataFrame(loser28).sort_values(by=[0], ascending=False)\n",
    "winner28_results = pd.DataFrame(winner28).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser28 = np.asarray(loser28_results[4:5][0])[0]\n",
    "median_loser28 = np.asarray(loser28_results[9:10][0])[0]\n",
    "upper_loser28 = np.asarray(loser28_results[14:15][0])[0]\n",
    "\n",
    "lower_winner28 = np.asarray(winner28_results[4:5][0])[0]\n",
    "median_winner28 = np.asarray(winner28_results[9:10][0])[0]\n",
    "upper_winner28 = np.asarray(winner28_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration38 :\n",
    "\n",
    "slice38 = 37\n",
    "\n",
    "loser38 = [train_regret_loser_1[slice38],\n",
    "       train_regret_loser_2[slice38],\n",
    "       train_regret_loser_3[slice38],\n",
    "       train_regret_loser_4[slice38],\n",
    "       train_regret_loser_5[slice38],\n",
    "       train_regret_loser_6[slice38],\n",
    "       train_regret_loser_7[slice38],\n",
    "       train_regret_loser_8[slice38],\n",
    "       train_regret_loser_9[slice38],\n",
    "       train_regret_loser_10[slice38],\n",
    "       train_regret_loser_11[slice38],\n",
    "       train_regret_loser_12[slice38],\n",
    "       train_regret_loser_13[slice38],\n",
    "       train_regret_loser_14[slice38],\n",
    "       train_regret_loser_15[slice38],\n",
    "       train_regret_loser_16[slice38],\n",
    "       train_regret_loser_17[slice38],\n",
    "       train_regret_loser_18[slice38],\n",
    "       train_regret_loser_19[slice38],\n",
    "       train_regret_loser_20[slice38]]\n",
    "\n",
    "winner38 = [train_regret_winner_1[slice38],\n",
    "       train_regret_winner_2[slice38],\n",
    "       train_regret_winner_3[slice38],\n",
    "       train_regret_winner_4[slice38],\n",
    "       train_regret_winner_5[slice38],\n",
    "       train_regret_winner_6[slice38],\n",
    "       train_regret_winner_7[slice38],\n",
    "       train_regret_winner_8[slice38],\n",
    "       train_regret_winner_9[slice38],\n",
    "       train_regret_winner_10[slice38],\n",
    "       train_regret_winner_11[slice38],\n",
    "       train_regret_winner_12[slice38],\n",
    "       train_regret_winner_13[slice38],\n",
    "       train_regret_winner_14[slice38],\n",
    "       train_regret_winner_15[slice38],\n",
    "       train_regret_winner_16[slice38],\n",
    "       train_regret_winner_17[slice38],\n",
    "       train_regret_winner_18[slice38],\n",
    "       train_regret_winner_19[slice38],\n",
    "       train_regret_winner_20[slice38]]\n",
    "\n",
    "loser38_results = pd.DataFrame(loser38).sort_values(by=[0], ascending=False)\n",
    "winner38_results = pd.DataFrame(winner38).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser38 = np.asarray(loser38_results[4:5][0])[0]\n",
    "median_loser38 = np.asarray(loser38_results[9:10][0])[0]\n",
    "upper_loser38 = np.asarray(loser38_results[14:15][0])[0]\n",
    "\n",
    "lower_winner38 = np.asarray(winner38_results[4:5][0])[0]\n",
    "median_winner38 = np.asarray(winner38_results[9:10][0])[0]\n",
    "upper_winner38 = np.asarray(winner38_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration48 :\n",
    "\n",
    "slice48 = 47\n",
    "\n",
    "loser48 = [train_regret_loser_1[slice48],\n",
    "       train_regret_loser_2[slice48],\n",
    "       train_regret_loser_3[slice48],\n",
    "       train_regret_loser_4[slice48],\n",
    "       train_regret_loser_5[slice48],\n",
    "       train_regret_loser_6[slice48],\n",
    "       train_regret_loser_7[slice48],\n",
    "       train_regret_loser_8[slice48],\n",
    "       train_regret_loser_9[slice48],\n",
    "       train_regret_loser_10[slice48],\n",
    "       train_regret_loser_11[slice48],\n",
    "       train_regret_loser_12[slice48],\n",
    "       train_regret_loser_13[slice48],\n",
    "       train_regret_loser_14[slice48],\n",
    "       train_regret_loser_15[slice48],\n",
    "       train_regret_loser_16[slice48],\n",
    "       train_regret_loser_17[slice48],\n",
    "       train_regret_loser_18[slice48],\n",
    "       train_regret_loser_19[slice48],\n",
    "       train_regret_loser_20[slice48]]\n",
    "\n",
    "winner48 = [train_regret_winner_1[slice48],\n",
    "       train_regret_winner_2[slice48],\n",
    "       train_regret_winner_3[slice48],\n",
    "       train_regret_winner_4[slice48],\n",
    "       train_regret_winner_5[slice48],\n",
    "       train_regret_winner_6[slice48],\n",
    "       train_regret_winner_7[slice48],\n",
    "       train_regret_winner_8[slice48],\n",
    "       train_regret_winner_9[slice48],\n",
    "       train_regret_winner_10[slice48],\n",
    "       train_regret_winner_11[slice48],\n",
    "       train_regret_winner_12[slice48],\n",
    "       train_regret_winner_13[slice48],\n",
    "       train_regret_winner_14[slice48],\n",
    "       train_regret_winner_15[slice48],\n",
    "       train_regret_winner_16[slice48],\n",
    "       train_regret_winner_17[slice48],\n",
    "       train_regret_winner_18[slice48],\n",
    "       train_regret_winner_19[slice48],\n",
    "       train_regret_winner_20[slice48]]\n",
    "\n",
    "loser48_results = pd.DataFrame(loser48).sort_values(by=[0], ascending=False)\n",
    "winner48_results = pd.DataFrame(winner48).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser48 = np.asarray(loser48_results[4:5][0])[0]\n",
    "median_loser48 = np.asarray(loser48_results[9:10][0])[0]\n",
    "upper_loser48 = np.asarray(loser48_results[14:15][0])[0]\n",
    "\n",
    "lower_winner48 = np.asarray(winner48_results[4:5][0])[0]\n",
    "median_winner48 = np.asarray(winner48_results[9:10][0])[0]\n",
    "upper_winner48 = np.asarray(winner48_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration58 :\n",
    "\n",
    "slice58 = 57\n",
    "\n",
    "loser58 = [train_regret_loser_1[slice58],\n",
    "       train_regret_loser_2[slice58],\n",
    "       train_regret_loser_3[slice58],\n",
    "       train_regret_loser_4[slice58],\n",
    "       train_regret_loser_5[slice58],\n",
    "       train_regret_loser_6[slice58],\n",
    "       train_regret_loser_7[slice58],\n",
    "       train_regret_loser_8[slice58],\n",
    "       train_regret_loser_9[slice58],\n",
    "       train_regret_loser_10[slice58],\n",
    "       train_regret_loser_11[slice58],\n",
    "       train_regret_loser_12[slice58],\n",
    "       train_regret_loser_13[slice58],\n",
    "       train_regret_loser_14[slice58],\n",
    "       train_regret_loser_15[slice58],\n",
    "       train_regret_loser_16[slice58],\n",
    "       train_regret_loser_17[slice58],\n",
    "       train_regret_loser_18[slice58],\n",
    "       train_regret_loser_19[slice58],\n",
    "       train_regret_loser_20[slice58]]\n",
    "\n",
    "winner58 = [train_regret_winner_1[slice58],\n",
    "       train_regret_winner_2[slice58],\n",
    "       train_regret_winner_3[slice58],\n",
    "       train_regret_winner_4[slice58],\n",
    "       train_regret_winner_5[slice58],\n",
    "       train_regret_winner_6[slice58],\n",
    "       train_regret_winner_7[slice58],\n",
    "       train_regret_winner_8[slice58],\n",
    "       train_regret_winner_9[slice58],\n",
    "       train_regret_winner_10[slice58],\n",
    "       train_regret_winner_11[slice58],\n",
    "       train_regret_winner_12[slice58],\n",
    "       train_regret_winner_13[slice58],\n",
    "       train_regret_winner_14[slice58],\n",
    "       train_regret_winner_15[slice58],\n",
    "       train_regret_winner_16[slice58],\n",
    "       train_regret_winner_17[slice58],\n",
    "       train_regret_winner_18[slice58],\n",
    "       train_regret_winner_19[slice58],\n",
    "       train_regret_winner_20[slice58]]\n",
    "\n",
    "loser58_results = pd.DataFrame(loser58).sort_values(by=[0], ascending=False)\n",
    "winner58_results = pd.DataFrame(winner58).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser58 = np.asarray(loser58_results[4:5][0])[0]\n",
    "median_loser58 = np.asarray(loser58_results[9:10][0])[0]\n",
    "upper_loser58 = np.asarray(loser58_results[14:15][0])[0]\n",
    "\n",
    "lower_winner58 = np.asarray(winner58_results[4:5][0])[0]\n",
    "median_winner58 = np.asarray(winner58_results[9:10][0])[0]\n",
    "upper_winner58 = np.asarray(winner58_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration68 :\n",
    "\n",
    "slice68 = 67\n",
    "\n",
    "loser68 = [train_regret_loser_1[slice68],\n",
    "       train_regret_loser_2[slice68],\n",
    "       train_regret_loser_3[slice68],\n",
    "       train_regret_loser_4[slice68],\n",
    "       train_regret_loser_5[slice68],\n",
    "       train_regret_loser_6[slice68],\n",
    "       train_regret_loser_7[slice68],\n",
    "       train_regret_loser_8[slice68],\n",
    "       train_regret_loser_9[slice68],\n",
    "       train_regret_loser_10[slice68],\n",
    "       train_regret_loser_11[slice68],\n",
    "       train_regret_loser_12[slice68],\n",
    "       train_regret_loser_13[slice68],\n",
    "       train_regret_loser_14[slice68],\n",
    "       train_regret_loser_15[slice68],\n",
    "       train_regret_loser_16[slice68],\n",
    "       train_regret_loser_17[slice68],\n",
    "       train_regret_loser_18[slice68],\n",
    "       train_regret_loser_19[slice68],\n",
    "       train_regret_loser_20[slice68]]\n",
    "\n",
    "winner68 = [train_regret_winner_1[slice68],\n",
    "       train_regret_winner_2[slice68],\n",
    "       train_regret_winner_3[slice68],\n",
    "       train_regret_winner_4[slice68],\n",
    "       train_regret_winner_5[slice68],\n",
    "       train_regret_winner_6[slice68],\n",
    "       train_regret_winner_7[slice68],\n",
    "       train_regret_winner_8[slice68],\n",
    "       train_regret_winner_9[slice68],\n",
    "       train_regret_winner_10[slice68],\n",
    "       train_regret_winner_11[slice68],\n",
    "       train_regret_winner_12[slice68],\n",
    "       train_regret_winner_13[slice68],\n",
    "       train_regret_winner_14[slice68],\n",
    "       train_regret_winner_15[slice68],\n",
    "       train_regret_winner_16[slice68],\n",
    "       train_regret_winner_17[slice68],\n",
    "       train_regret_winner_18[slice68],\n",
    "       train_regret_winner_19[slice68],\n",
    "       train_regret_winner_20[slice68]]\n",
    "\n",
    "loser68_results = pd.DataFrame(loser68).sort_values(by=[0], ascending=False)\n",
    "winner68_results = pd.DataFrame(winner68).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser68 = np.asarray(loser68_results[4:5][0])[0]\n",
    "median_loser68 = np.asarray(loser68_results[9:10][0])[0]\n",
    "upper_loser68 = np.asarray(loser68_results[14:15][0])[0]\n",
    "\n",
    "lower_winner68 = np.asarray(winner68_results[4:5][0])[0]\n",
    "median_winner68 = np.asarray(winner68_results[9:10][0])[0]\n",
    "upper_winner68 = np.asarray(winner68_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration78 :\n",
    "\n",
    "slice78 = 77\n",
    "\n",
    "loser78 = [train_regret_loser_1[slice78],\n",
    "       train_regret_loser_2[slice78],\n",
    "       train_regret_loser_3[slice78],\n",
    "       train_regret_loser_4[slice78],\n",
    "       train_regret_loser_5[slice78],\n",
    "       train_regret_loser_6[slice78],\n",
    "       train_regret_loser_7[slice78],\n",
    "       train_regret_loser_8[slice78],\n",
    "       train_regret_loser_9[slice78],\n",
    "       train_regret_loser_10[slice78],\n",
    "       train_regret_loser_11[slice78],\n",
    "       train_regret_loser_12[slice78],\n",
    "       train_regret_loser_13[slice78],\n",
    "       train_regret_loser_14[slice78],\n",
    "       train_regret_loser_15[slice78],\n",
    "       train_regret_loser_16[slice78],\n",
    "       train_regret_loser_17[slice78],\n",
    "       train_regret_loser_18[slice78],\n",
    "       train_regret_loser_19[slice78],\n",
    "       train_regret_loser_20[slice78]]\n",
    "\n",
    "winner78 = [train_regret_winner_1[slice78],\n",
    "       train_regret_winner_2[slice78],\n",
    "       train_regret_winner_3[slice78],\n",
    "       train_regret_winner_4[slice78],\n",
    "       train_regret_winner_5[slice78],\n",
    "       train_regret_winner_6[slice78],\n",
    "       train_regret_winner_7[slice78],\n",
    "       train_regret_winner_8[slice78],\n",
    "       train_regret_winner_9[slice78],\n",
    "       train_regret_winner_10[slice78],\n",
    "       train_regret_winner_11[slice78],\n",
    "       train_regret_winner_12[slice78],\n",
    "       train_regret_winner_13[slice78],\n",
    "       train_regret_winner_14[slice78],\n",
    "       train_regret_winner_15[slice78],\n",
    "       train_regret_winner_16[slice78],\n",
    "       train_regret_winner_17[slice78],\n",
    "       train_regret_winner_18[slice78],\n",
    "       train_regret_winner_19[slice78],\n",
    "       train_regret_winner_20[slice78]]\n",
    "\n",
    "loser78_results = pd.DataFrame(loser78).sort_values(by=[0], ascending=False)\n",
    "winner78_results = pd.DataFrame(winner78).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser78 = np.asarray(loser78_results[4:5][0])[0]\n",
    "median_loser78 = np.asarray(loser78_results[9:10][0])[0]\n",
    "upper_loser78 = np.asarray(loser78_results[14:15][0])[0]\n",
    "\n",
    "lower_winner78 = np.asarray(winner78_results[4:5][0])[0]\n",
    "median_winner78 = np.asarray(winner78_results[9:10][0])[0]\n",
    "upper_winner78 = np.asarray(winner78_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration88 :\n",
    "\n",
    "slice88 = 87\n",
    "\n",
    "loser88 = [train_regret_loser_1[slice88],\n",
    "       train_regret_loser_2[slice88],\n",
    "       train_regret_loser_3[slice88],\n",
    "       train_regret_loser_4[slice88],\n",
    "       train_regret_loser_5[slice88],\n",
    "       train_regret_loser_6[slice88],\n",
    "       train_regret_loser_7[slice88],\n",
    "       train_regret_loser_8[slice88],\n",
    "       train_regret_loser_9[slice88],\n",
    "       train_regret_loser_10[slice88],\n",
    "       train_regret_loser_11[slice88],\n",
    "       train_regret_loser_12[slice88],\n",
    "       train_regret_loser_13[slice88],\n",
    "       train_regret_loser_14[slice88],\n",
    "       train_regret_loser_15[slice88],\n",
    "       train_regret_loser_16[slice88],\n",
    "       train_regret_loser_17[slice88],\n",
    "       train_regret_loser_18[slice88],\n",
    "       train_regret_loser_19[slice88],\n",
    "       train_regret_loser_20[slice88]]\n",
    "\n",
    "winner88 = [train_regret_winner_1[slice88],\n",
    "       train_regret_winner_2[slice88],\n",
    "       train_regret_winner_3[slice88],\n",
    "       train_regret_winner_4[slice88],\n",
    "       train_regret_winner_5[slice88],\n",
    "       train_regret_winner_6[slice88],\n",
    "       train_regret_winner_7[slice88],\n",
    "       train_regret_winner_8[slice88],\n",
    "       train_regret_winner_9[slice88],\n",
    "       train_regret_winner_10[slice88],\n",
    "       train_regret_winner_11[slice88],\n",
    "       train_regret_winner_12[slice88],\n",
    "       train_regret_winner_13[slice88],\n",
    "       train_regret_winner_14[slice88],\n",
    "       train_regret_winner_15[slice88],\n",
    "       train_regret_winner_16[slice88],\n",
    "       train_regret_winner_17[slice88],\n",
    "       train_regret_winner_18[slice88],\n",
    "       train_regret_winner_19[slice88],\n",
    "       train_regret_winner_20[slice88]]\n",
    "\n",
    "loser88_results = pd.DataFrame(loser88).sort_values(by=[0], ascending=False)\n",
    "winner88_results = pd.DataFrame(winner88).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser88 = np.asarray(loser88_results[4:5][0])[0]\n",
    "median_loser88 = np.asarray(loser88_results[9:10][0])[0]\n",
    "upper_loser88 = np.asarray(loser88_results[14:15][0])[0]\n",
    "\n",
    "lower_winner88 = np.asarray(winner88_results[4:5][0])[0]\n",
    "median_winner88 = np.asarray(winner88_results[9:10][0])[0]\n",
    "upper_winner88 = np.asarray(winner88_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration98 :\n",
    "\n",
    "slice98 = 97\n",
    "\n",
    "loser98 = [train_regret_loser_1[slice98],\n",
    "       train_regret_loser_2[slice98],\n",
    "       train_regret_loser_3[slice98],\n",
    "       train_regret_loser_4[slice98],\n",
    "       train_regret_loser_5[slice98],\n",
    "       train_regret_loser_6[slice98],\n",
    "       train_regret_loser_7[slice98],\n",
    "       train_regret_loser_8[slice98],\n",
    "       train_regret_loser_9[slice98],\n",
    "       train_regret_loser_10[slice98],\n",
    "       train_regret_loser_11[slice98],\n",
    "       train_regret_loser_12[slice98],\n",
    "       train_regret_loser_13[slice98],\n",
    "       train_regret_loser_14[slice98],\n",
    "       train_regret_loser_15[slice98],\n",
    "       train_regret_loser_16[slice98],\n",
    "       train_regret_loser_17[slice98],\n",
    "       train_regret_loser_18[slice98],\n",
    "       train_regret_loser_19[slice98],\n",
    "       train_regret_loser_20[slice98]]\n",
    "\n",
    "winner98 = [train_regret_winner_1[slice98],\n",
    "       train_regret_winner_2[slice98],\n",
    "       train_regret_winner_3[slice98],\n",
    "       train_regret_winner_4[slice98],\n",
    "       train_regret_winner_5[slice98],\n",
    "       train_regret_winner_6[slice98],\n",
    "       train_regret_winner_7[slice98],\n",
    "       train_regret_winner_8[slice98],\n",
    "       train_regret_winner_9[slice98],\n",
    "       train_regret_winner_10[slice98],\n",
    "       train_regret_winner_11[slice98],\n",
    "       train_regret_winner_12[slice98],\n",
    "       train_regret_winner_13[slice98],\n",
    "       train_regret_winner_14[slice98],\n",
    "       train_regret_winner_15[slice98],\n",
    "       train_regret_winner_16[slice98],\n",
    "       train_regret_winner_17[slice98],\n",
    "       train_regret_winner_18[slice98],\n",
    "       train_regret_winner_19[slice98],\n",
    "       train_regret_winner_20[slice98]]\n",
    "\n",
    "loser98_results = pd.DataFrame(loser98).sort_values(by=[0], ascending=False)\n",
    "winner98_results = pd.DataFrame(winner98).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser98 = np.asarray(loser98_results[4:5][0])[0]\n",
    "median_loser98 = np.asarray(loser98_results[9:10][0])[0]\n",
    "upper_loser98 = np.asarray(loser98_results[14:15][0])[0]\n",
    "\n",
    "lower_winner98 = np.asarray(winner98_results[4:5][0])[0]\n",
    "median_winner98 = np.asarray(winner98_results[9:10][0])[0]\n",
    "upper_winner98 = np.asarray(winner98_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration9 :\n",
    "\n",
    "slice9 = 8\n",
    "\n",
    "loser9 = [train_regret_loser_1[slice9],\n",
    "       train_regret_loser_2[slice9],\n",
    "       train_regret_loser_3[slice9],\n",
    "       train_regret_loser_4[slice9],\n",
    "       train_regret_loser_5[slice9],\n",
    "       train_regret_loser_6[slice9],\n",
    "       train_regret_loser_7[slice9],\n",
    "       train_regret_loser_8[slice9],\n",
    "       train_regret_loser_9[slice9],\n",
    "       train_regret_loser_10[slice9],\n",
    "       train_regret_loser_11[slice9],\n",
    "       train_regret_loser_12[slice9],\n",
    "       train_regret_loser_13[slice9],\n",
    "       train_regret_loser_14[slice9],\n",
    "       train_regret_loser_15[slice9],\n",
    "       train_regret_loser_16[slice9],\n",
    "       train_regret_loser_17[slice9],\n",
    "       train_regret_loser_18[slice9],\n",
    "       train_regret_loser_19[slice9],\n",
    "       train_regret_loser_20[slice9]]\n",
    "\n",
    "winner9 = [train_regret_winner_1[slice9],\n",
    "       train_regret_winner_2[slice9],\n",
    "       train_regret_winner_3[slice9],\n",
    "       train_regret_winner_4[slice9],\n",
    "       train_regret_winner_5[slice9],\n",
    "       train_regret_winner_6[slice9],\n",
    "       train_regret_winner_7[slice9],\n",
    "       train_regret_winner_8[slice9],\n",
    "       train_regret_winner_9[slice9],\n",
    "       train_regret_winner_10[slice9],\n",
    "       train_regret_winner_11[slice9],\n",
    "       train_regret_winner_12[slice9],\n",
    "       train_regret_winner_13[slice9],\n",
    "       train_regret_winner_14[slice9],\n",
    "       train_regret_winner_15[slice9],\n",
    "       train_regret_winner_16[slice9],\n",
    "       train_regret_winner_17[slice9],\n",
    "       train_regret_winner_18[slice9],\n",
    "       train_regret_winner_19[slice9],\n",
    "       train_regret_winner_20[slice9]]\n",
    "\n",
    "loser9_results = pd.DataFrame(loser9).sort_values(by=[0], ascending=False)\n",
    "winner9_results = pd.DataFrame(winner9).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser9 = np.asarray(loser9_results[4:5][0])[0]\n",
    "median_loser9 = np.asarray(loser9_results[9:10][0])[0]\n",
    "upper_loser9 = np.asarray(loser9_results[14:15][0])[0]\n",
    "\n",
    "lower_winner9 = np.asarray(winner9_results[4:5][0])[0]\n",
    "median_winner9 = np.asarray(winner9_results[9:10][0])[0]\n",
    "upper_winner9 = np.asarray(winner9_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration19 :\n",
    "\n",
    "slice19 = 18\n",
    "\n",
    "loser19 = [train_regret_loser_1[slice19],\n",
    "       train_regret_loser_2[slice19],\n",
    "       train_regret_loser_3[slice19],\n",
    "       train_regret_loser_4[slice19],\n",
    "       train_regret_loser_5[slice19],\n",
    "       train_regret_loser_6[slice19],\n",
    "       train_regret_loser_7[slice19],\n",
    "       train_regret_loser_8[slice19],\n",
    "       train_regret_loser_9[slice19],\n",
    "       train_regret_loser_10[slice19],\n",
    "       train_regret_loser_11[slice19],\n",
    "       train_regret_loser_12[slice19],\n",
    "       train_regret_loser_13[slice19],\n",
    "       train_regret_loser_14[slice19],\n",
    "       train_regret_loser_15[slice19],\n",
    "       train_regret_loser_16[slice19],\n",
    "       train_regret_loser_17[slice19],\n",
    "       train_regret_loser_18[slice19],\n",
    "       train_regret_loser_19[slice19],\n",
    "       train_regret_loser_20[slice19]]\n",
    "\n",
    "winner19 = [train_regret_winner_1[slice19],\n",
    "       train_regret_winner_2[slice19],\n",
    "       train_regret_winner_3[slice19],\n",
    "       train_regret_winner_4[slice19],\n",
    "       train_regret_winner_5[slice19],\n",
    "       train_regret_winner_6[slice19],\n",
    "       train_regret_winner_7[slice19],\n",
    "       train_regret_winner_8[slice19],\n",
    "       train_regret_winner_9[slice19],\n",
    "       train_regret_winner_10[slice19],\n",
    "       train_regret_winner_11[slice19],\n",
    "       train_regret_winner_12[slice19],\n",
    "       train_regret_winner_13[slice19],\n",
    "       train_regret_winner_14[slice19],\n",
    "       train_regret_winner_15[slice19],\n",
    "       train_regret_winner_16[slice19],\n",
    "       train_regret_winner_17[slice19],\n",
    "       train_regret_winner_18[slice19],\n",
    "       train_regret_winner_19[slice19],\n",
    "       train_regret_winner_20[slice19]]\n",
    "\n",
    "loser19_results = pd.DataFrame(loser19).sort_values(by=[0], ascending=False)\n",
    "winner19_results = pd.DataFrame(winner19).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser19 = np.asarray(loser19_results[4:5][0])[0]\n",
    "median_loser19 = np.asarray(loser19_results[9:10][0])[0]\n",
    "upper_loser19 = np.asarray(loser19_results[14:15][0])[0]\n",
    "\n",
    "lower_winner19 = np.asarray(winner19_results[4:5][0])[0]\n",
    "median_winner19 = np.asarray(winner19_results[9:10][0])[0]\n",
    "upper_winner19 = np.asarray(winner19_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration29 :\n",
    "\n",
    "slice29 = 28\n",
    "\n",
    "loser29 = [train_regret_loser_1[slice29],\n",
    "       train_regret_loser_2[slice29],\n",
    "       train_regret_loser_3[slice29],\n",
    "       train_regret_loser_4[slice29],\n",
    "       train_regret_loser_5[slice29],\n",
    "       train_regret_loser_6[slice29],\n",
    "       train_regret_loser_7[slice29],\n",
    "       train_regret_loser_8[slice29],\n",
    "       train_regret_loser_9[slice29],\n",
    "       train_regret_loser_10[slice29],\n",
    "       train_regret_loser_11[slice29],\n",
    "       train_regret_loser_12[slice29],\n",
    "       train_regret_loser_13[slice29],\n",
    "       train_regret_loser_14[slice29],\n",
    "       train_regret_loser_15[slice29],\n",
    "       train_regret_loser_16[slice29],\n",
    "       train_regret_loser_17[slice29],\n",
    "       train_regret_loser_18[slice29],\n",
    "       train_regret_loser_19[slice29],\n",
    "       train_regret_loser_20[slice29]]\n",
    "\n",
    "winner29 = [train_regret_winner_1[slice29],\n",
    "       train_regret_winner_2[slice29],\n",
    "       train_regret_winner_3[slice29],\n",
    "       train_regret_winner_4[slice29],\n",
    "       train_regret_winner_5[slice29],\n",
    "       train_regret_winner_6[slice29],\n",
    "       train_regret_winner_7[slice29],\n",
    "       train_regret_winner_8[slice29],\n",
    "       train_regret_winner_9[slice29],\n",
    "       train_regret_winner_10[slice29],\n",
    "       train_regret_winner_11[slice29],\n",
    "       train_regret_winner_12[slice29],\n",
    "       train_regret_winner_13[slice29],\n",
    "       train_regret_winner_14[slice29],\n",
    "       train_regret_winner_15[slice29],\n",
    "       train_regret_winner_16[slice29],\n",
    "       train_regret_winner_17[slice29],\n",
    "       train_regret_winner_18[slice29],\n",
    "       train_regret_winner_19[slice29],\n",
    "       train_regret_winner_20[slice29]]\n",
    "\n",
    "loser29_results = pd.DataFrame(loser29).sort_values(by=[0], ascending=False)\n",
    "winner29_results = pd.DataFrame(winner29).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser29 = np.asarray(loser29_results[4:5][0])[0]\n",
    "median_loser29 = np.asarray(loser29_results[9:10][0])[0]\n",
    "upper_loser29 = np.asarray(loser29_results[14:15][0])[0]\n",
    "\n",
    "lower_winner29 = np.asarray(winner29_results[4:5][0])[0]\n",
    "median_winner29 = np.asarray(winner29_results[9:10][0])[0]\n",
    "upper_winner29 = np.asarray(winner29_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration39 :\n",
    "\n",
    "slice39 = 38\n",
    "\n",
    "loser39 = [train_regret_loser_1[slice39],\n",
    "       train_regret_loser_2[slice39],\n",
    "       train_regret_loser_3[slice39],\n",
    "       train_regret_loser_4[slice39],\n",
    "       train_regret_loser_5[slice39],\n",
    "       train_regret_loser_6[slice39],\n",
    "       train_regret_loser_7[slice39],\n",
    "       train_regret_loser_8[slice39],\n",
    "       train_regret_loser_9[slice39],\n",
    "       train_regret_loser_10[slice39],\n",
    "       train_regret_loser_11[slice39],\n",
    "       train_regret_loser_12[slice39],\n",
    "       train_regret_loser_13[slice39],\n",
    "       train_regret_loser_14[slice39],\n",
    "       train_regret_loser_15[slice39],\n",
    "       train_regret_loser_16[slice39],\n",
    "       train_regret_loser_17[slice39],\n",
    "       train_regret_loser_18[slice39],\n",
    "       train_regret_loser_19[slice39],\n",
    "       train_regret_loser_20[slice39]]\n",
    "\n",
    "winner39 = [train_regret_winner_1[slice39],\n",
    "       train_regret_winner_2[slice39],\n",
    "       train_regret_winner_3[slice39],\n",
    "       train_regret_winner_4[slice39],\n",
    "       train_regret_winner_5[slice39],\n",
    "       train_regret_winner_6[slice39],\n",
    "       train_regret_winner_7[slice39],\n",
    "       train_regret_winner_8[slice39],\n",
    "       train_regret_winner_9[slice39],\n",
    "       train_regret_winner_10[slice39],\n",
    "       train_regret_winner_11[slice39],\n",
    "       train_regret_winner_12[slice39],\n",
    "       train_regret_winner_13[slice39],\n",
    "       train_regret_winner_14[slice39],\n",
    "       train_regret_winner_15[slice39],\n",
    "       train_regret_winner_16[slice39],\n",
    "       train_regret_winner_17[slice39],\n",
    "       train_regret_winner_18[slice39],\n",
    "       train_regret_winner_19[slice39],\n",
    "       train_regret_winner_20[slice39]]\n",
    "\n",
    "loser39_results = pd.DataFrame(loser39).sort_values(by=[0], ascending=False)\n",
    "winner39_results = pd.DataFrame(winner39).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser39 = np.asarray(loser39_results[4:5][0])[0]\n",
    "median_loser39 = np.asarray(loser39_results[9:10][0])[0]\n",
    "upper_loser39 = np.asarray(loser39_results[14:15][0])[0]\n",
    "\n",
    "lower_winner39 = np.asarray(winner39_results[4:5][0])[0]\n",
    "median_winner39 = np.asarray(winner39_results[9:10][0])[0]\n",
    "upper_winner39 = np.asarray(winner39_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration49 :\n",
    "\n",
    "slice49 = 48\n",
    "\n",
    "loser49 = [train_regret_loser_1[slice49],\n",
    "       train_regret_loser_2[slice49],\n",
    "       train_regret_loser_3[slice49],\n",
    "       train_regret_loser_4[slice49],\n",
    "       train_regret_loser_5[slice49],\n",
    "       train_regret_loser_6[slice49],\n",
    "       train_regret_loser_7[slice49],\n",
    "       train_regret_loser_8[slice49],\n",
    "       train_regret_loser_9[slice49],\n",
    "       train_regret_loser_10[slice49],\n",
    "       train_regret_loser_11[slice49],\n",
    "       train_regret_loser_12[slice49],\n",
    "       train_regret_loser_13[slice49],\n",
    "       train_regret_loser_14[slice49],\n",
    "       train_regret_loser_15[slice49],\n",
    "       train_regret_loser_16[slice49],\n",
    "       train_regret_loser_17[slice49],\n",
    "       train_regret_loser_18[slice49],\n",
    "       train_regret_loser_19[slice49],\n",
    "       train_regret_loser_20[slice49]]\n",
    "\n",
    "winner49 = [train_regret_winner_1[slice49],\n",
    "       train_regret_winner_2[slice49],\n",
    "       train_regret_winner_3[slice49],\n",
    "       train_regret_winner_4[slice49],\n",
    "       train_regret_winner_5[slice49],\n",
    "       train_regret_winner_6[slice49],\n",
    "       train_regret_winner_7[slice49],\n",
    "       train_regret_winner_8[slice49],\n",
    "       train_regret_winner_9[slice49],\n",
    "       train_regret_winner_10[slice49],\n",
    "       train_regret_winner_11[slice49],\n",
    "       train_regret_winner_12[slice49],\n",
    "       train_regret_winner_13[slice49],\n",
    "       train_regret_winner_14[slice49],\n",
    "       train_regret_winner_15[slice49],\n",
    "       train_regret_winner_16[slice49],\n",
    "       train_regret_winner_17[slice49],\n",
    "       train_regret_winner_18[slice49],\n",
    "       train_regret_winner_19[slice49],\n",
    "       train_regret_winner_20[slice49]]\n",
    "\n",
    "loser49_results = pd.DataFrame(loser49).sort_values(by=[0], ascending=False)\n",
    "winner49_results = pd.DataFrame(winner49).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser49 = np.asarray(loser49_results[4:5][0])[0]\n",
    "median_loser49 = np.asarray(loser49_results[9:10][0])[0]\n",
    "upper_loser49 = np.asarray(loser49_results[14:15][0])[0]\n",
    "\n",
    "lower_winner49 = np.asarray(winner49_results[4:5][0])[0]\n",
    "median_winner49 = np.asarray(winner49_results[9:10][0])[0]\n",
    "upper_winner49 = np.asarray(winner49_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration59 :\n",
    "\n",
    "slice59 = 58\n",
    "\n",
    "loser59 = [train_regret_loser_1[slice59],\n",
    "       train_regret_loser_2[slice59],\n",
    "       train_regret_loser_3[slice59],\n",
    "       train_regret_loser_4[slice59],\n",
    "       train_regret_loser_5[slice59],\n",
    "       train_regret_loser_6[slice59],\n",
    "       train_regret_loser_7[slice59],\n",
    "       train_regret_loser_8[slice59],\n",
    "       train_regret_loser_9[slice59],\n",
    "       train_regret_loser_10[slice59],\n",
    "       train_regret_loser_11[slice59],\n",
    "       train_regret_loser_12[slice59],\n",
    "       train_regret_loser_13[slice59],\n",
    "       train_regret_loser_14[slice59],\n",
    "       train_regret_loser_15[slice59],\n",
    "       train_regret_loser_16[slice59],\n",
    "       train_regret_loser_17[slice59],\n",
    "       train_regret_loser_18[slice59],\n",
    "       train_regret_loser_19[slice59],\n",
    "       train_regret_loser_20[slice59]]\n",
    "\n",
    "winner59 = [train_regret_winner_1[slice59],\n",
    "       train_regret_winner_2[slice59],\n",
    "       train_regret_winner_3[slice59],\n",
    "       train_regret_winner_4[slice59],\n",
    "       train_regret_winner_5[slice59],\n",
    "       train_regret_winner_6[slice59],\n",
    "       train_regret_winner_7[slice59],\n",
    "       train_regret_winner_8[slice59],\n",
    "       train_regret_winner_9[slice59],\n",
    "       train_regret_winner_10[slice59],\n",
    "       train_regret_winner_11[slice59],\n",
    "       train_regret_winner_12[slice59],\n",
    "       train_regret_winner_13[slice59],\n",
    "       train_regret_winner_14[slice59],\n",
    "       train_regret_winner_15[slice59],\n",
    "       train_regret_winner_16[slice59],\n",
    "       train_regret_winner_17[slice59],\n",
    "       train_regret_winner_18[slice59],\n",
    "       train_regret_winner_19[slice59],\n",
    "       train_regret_winner_20[slice59]]\n",
    "\n",
    "loser59_results = pd.DataFrame(loser59).sort_values(by=[0], ascending=False)\n",
    "winner59_results = pd.DataFrame(winner59).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser59 = np.asarray(loser59_results[4:5][0])[0]\n",
    "median_loser59 = np.asarray(loser59_results[9:10][0])[0]\n",
    "upper_loser59 = np.asarray(loser59_results[14:15][0])[0]\n",
    "\n",
    "lower_winner59 = np.asarray(winner59_results[4:5][0])[0]\n",
    "median_winner59 = np.asarray(winner59_results[9:10][0])[0]\n",
    "upper_winner59 = np.asarray(winner59_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration69 :\n",
    "\n",
    "slice69 = 68\n",
    "\n",
    "loser69 = [train_regret_loser_1[slice69],\n",
    "       train_regret_loser_2[slice69],\n",
    "       train_regret_loser_3[slice69],\n",
    "       train_regret_loser_4[slice69],\n",
    "       train_regret_loser_5[slice69],\n",
    "       train_regret_loser_6[slice69],\n",
    "       train_regret_loser_7[slice69],\n",
    "       train_regret_loser_8[slice69],\n",
    "       train_regret_loser_9[slice69],\n",
    "       train_regret_loser_10[slice69],\n",
    "       train_regret_loser_11[slice69],\n",
    "       train_regret_loser_12[slice69],\n",
    "       train_regret_loser_13[slice69],\n",
    "       train_regret_loser_14[slice69],\n",
    "       train_regret_loser_15[slice69],\n",
    "       train_regret_loser_16[slice69],\n",
    "       train_regret_loser_17[slice69],\n",
    "       train_regret_loser_18[slice69],\n",
    "       train_regret_loser_19[slice69],\n",
    "       train_regret_loser_20[slice69]]\n",
    "\n",
    "winner69 = [train_regret_winner_1[slice69],\n",
    "       train_regret_winner_2[slice69],\n",
    "       train_regret_winner_3[slice69],\n",
    "       train_regret_winner_4[slice69],\n",
    "       train_regret_winner_5[slice69],\n",
    "       train_regret_winner_6[slice69],\n",
    "       train_regret_winner_7[slice69],\n",
    "       train_regret_winner_8[slice69],\n",
    "       train_regret_winner_9[slice69],\n",
    "       train_regret_winner_10[slice69],\n",
    "       train_regret_winner_11[slice69],\n",
    "       train_regret_winner_12[slice69],\n",
    "       train_regret_winner_13[slice69],\n",
    "       train_regret_winner_14[slice69],\n",
    "       train_regret_winner_15[slice69],\n",
    "       train_regret_winner_16[slice69],\n",
    "       train_regret_winner_17[slice69],\n",
    "       train_regret_winner_18[slice69],\n",
    "       train_regret_winner_19[slice69],\n",
    "       train_regret_winner_20[slice69]]\n",
    "\n",
    "loser69_results = pd.DataFrame(loser69).sort_values(by=[0], ascending=False)\n",
    "winner69_results = pd.DataFrame(winner69).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser69 = np.asarray(loser69_results[4:5][0])[0]\n",
    "median_loser69 = np.asarray(loser69_results[9:10][0])[0]\n",
    "upper_loser69 = np.asarray(loser69_results[14:15][0])[0]\n",
    "\n",
    "lower_winner69 = np.asarray(winner69_results[4:5][0])[0]\n",
    "median_winner69 = np.asarray(winner69_results[9:10][0])[0]\n",
    "upper_winner69 = np.asarray(winner69_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration79 :\n",
    "\n",
    "slice79 = 78\n",
    "\n",
    "loser79 = [train_regret_loser_1[slice79],\n",
    "       train_regret_loser_2[slice79],\n",
    "       train_regret_loser_3[slice79],\n",
    "       train_regret_loser_4[slice79],\n",
    "       train_regret_loser_5[slice79],\n",
    "       train_regret_loser_6[slice79],\n",
    "       train_regret_loser_7[slice79],\n",
    "       train_regret_loser_8[slice79],\n",
    "       train_regret_loser_9[slice79],\n",
    "       train_regret_loser_10[slice79],\n",
    "       train_regret_loser_11[slice79],\n",
    "       train_regret_loser_12[slice79],\n",
    "       train_regret_loser_13[slice79],\n",
    "       train_regret_loser_14[slice79],\n",
    "       train_regret_loser_15[slice79],\n",
    "       train_regret_loser_16[slice79],\n",
    "       train_regret_loser_17[slice79],\n",
    "       train_regret_loser_18[slice79],\n",
    "       train_regret_loser_19[slice79],\n",
    "       train_regret_loser_20[slice79]]\n",
    "\n",
    "winner79 = [train_regret_winner_1[slice79],\n",
    "       train_regret_winner_2[slice79],\n",
    "       train_regret_winner_3[slice79],\n",
    "       train_regret_winner_4[slice79],\n",
    "       train_regret_winner_5[slice79],\n",
    "       train_regret_winner_6[slice79],\n",
    "       train_regret_winner_7[slice79],\n",
    "       train_regret_winner_8[slice79],\n",
    "       train_regret_winner_9[slice79],\n",
    "       train_regret_winner_10[slice79],\n",
    "       train_regret_winner_11[slice79],\n",
    "       train_regret_winner_12[slice79],\n",
    "       train_regret_winner_13[slice79],\n",
    "       train_regret_winner_14[slice79],\n",
    "       train_regret_winner_15[slice79],\n",
    "       train_regret_winner_16[slice79],\n",
    "       train_regret_winner_17[slice79],\n",
    "       train_regret_winner_18[slice79],\n",
    "       train_regret_winner_19[slice79],\n",
    "       train_regret_winner_20[slice79]]\n",
    "\n",
    "loser79_results = pd.DataFrame(loser79).sort_values(by=[0], ascending=False)\n",
    "winner79_results = pd.DataFrame(winner79).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser79 = np.asarray(loser79_results[4:5][0])[0]\n",
    "median_loser79 = np.asarray(loser79_results[9:10][0])[0]\n",
    "upper_loser79 = np.asarray(loser79_results[14:15][0])[0]\n",
    "\n",
    "lower_winner79 = np.asarray(winner79_results[4:5][0])[0]\n",
    "median_winner79 = np.asarray(winner79_results[9:10][0])[0]\n",
    "upper_winner79 = np.asarray(winner79_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration89 :\n",
    "\n",
    "slice89 = 88\n",
    "\n",
    "loser89 = [train_regret_loser_1[slice89],\n",
    "       train_regret_loser_2[slice89],\n",
    "       train_regret_loser_3[slice89],\n",
    "       train_regret_loser_4[slice89],\n",
    "       train_regret_loser_5[slice89],\n",
    "       train_regret_loser_6[slice89],\n",
    "       train_regret_loser_7[slice89],\n",
    "       train_regret_loser_8[slice89],\n",
    "       train_regret_loser_9[slice89],\n",
    "       train_regret_loser_10[slice89],\n",
    "       train_regret_loser_11[slice89],\n",
    "       train_regret_loser_12[slice89],\n",
    "       train_regret_loser_13[slice89],\n",
    "       train_regret_loser_14[slice89],\n",
    "       train_regret_loser_15[slice89],\n",
    "       train_regret_loser_16[slice89],\n",
    "       train_regret_loser_17[slice89],\n",
    "       train_regret_loser_18[slice89],\n",
    "       train_regret_loser_19[slice89],\n",
    "       train_regret_loser_20[slice89]]\n",
    "\n",
    "winner89 = [train_regret_winner_1[slice89],\n",
    "       train_regret_winner_2[slice89],\n",
    "       train_regret_winner_3[slice89],\n",
    "       train_regret_winner_4[slice89],\n",
    "       train_regret_winner_5[slice89],\n",
    "       train_regret_winner_6[slice89],\n",
    "       train_regret_winner_7[slice89],\n",
    "       train_regret_winner_8[slice89],\n",
    "       train_regret_winner_9[slice89],\n",
    "       train_regret_winner_10[slice89],\n",
    "       train_regret_winner_11[slice89],\n",
    "       train_regret_winner_12[slice89],\n",
    "       train_regret_winner_13[slice89],\n",
    "       train_regret_winner_14[slice89],\n",
    "       train_regret_winner_15[slice89],\n",
    "       train_regret_winner_16[slice89],\n",
    "       train_regret_winner_17[slice89],\n",
    "       train_regret_winner_18[slice89],\n",
    "       train_regret_winner_19[slice89],\n",
    "       train_regret_winner_20[slice89]]\n",
    "\n",
    "loser89_results = pd.DataFrame(loser89).sort_values(by=[0], ascending=False)\n",
    "winner89_results = pd.DataFrame(winner89).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser89 = np.asarray(loser89_results[4:5][0])[0]\n",
    "median_loser89 = np.asarray(loser89_results[9:10][0])[0]\n",
    "upper_loser89 = np.asarray(loser89_results[14:15][0])[0]\n",
    "\n",
    "lower_winner89 = np.asarray(winner89_results[4:5][0])[0]\n",
    "median_winner89 = np.asarray(winner89_results[9:10][0])[0]\n",
    "upper_winner89 = np.asarray(winner89_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.263532134808551, -5.391416894122287)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iteration99 :\n",
    "\n",
    "slice99 = 98\n",
    "\n",
    "loser99 = [train_regret_loser_1[slice99],\n",
    "       train_regret_loser_2[slice99],\n",
    "       train_regret_loser_3[slice99],\n",
    "       train_regret_loser_4[slice99],\n",
    "       train_regret_loser_5[slice99],\n",
    "       train_regret_loser_6[slice99],\n",
    "       train_regret_loser_7[slice99],\n",
    "       train_regret_loser_8[slice99],\n",
    "       train_regret_loser_9[slice99],\n",
    "       train_regret_loser_10[slice99],\n",
    "       train_regret_loser_11[slice99],\n",
    "       train_regret_loser_12[slice99],\n",
    "       train_regret_loser_13[slice99],\n",
    "       train_regret_loser_14[slice99],\n",
    "       train_regret_loser_15[slice99],\n",
    "       train_regret_loser_16[slice99],\n",
    "       train_regret_loser_17[slice99],\n",
    "       train_regret_loser_18[slice99],\n",
    "       train_regret_loser_19[slice99],\n",
    "       train_regret_loser_20[slice99]]\n",
    "\n",
    "winner99 = [train_regret_winner_1[slice99],\n",
    "       train_regret_winner_2[slice99],\n",
    "       train_regret_winner_3[slice99],\n",
    "       train_regret_winner_4[slice99],\n",
    "       train_regret_winner_5[slice99],\n",
    "       train_regret_winner_6[slice99],\n",
    "       train_regret_winner_7[slice99],\n",
    "       train_regret_winner_8[slice99],\n",
    "       train_regret_winner_9[slice99],\n",
    "       train_regret_winner_10[slice99],\n",
    "       train_regret_winner_11[slice99],\n",
    "       train_regret_winner_12[slice99],\n",
    "       train_regret_winner_13[slice99],\n",
    "       train_regret_winner_14[slice99],\n",
    "       train_regret_winner_15[slice99],\n",
    "       train_regret_winner_16[slice99],\n",
    "       train_regret_winner_17[slice99],\n",
    "       train_regret_winner_18[slice99],\n",
    "       train_regret_winner_19[slice99],\n",
    "       train_regret_winner_20[slice99]]\n",
    "\n",
    "loser99_results = pd.DataFrame(loser99).sort_values(by=[0], ascending=False)\n",
    "winner99_results = pd.DataFrame(winner99).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser99 = np.asarray(loser99_results[4:5][0])[0]\n",
    "median_loser99 = np.asarray(loser99_results[9:10][0])[0]\n",
    "upper_loser99 = np.asarray(loser99_results[14:15][0])[0]\n",
    "\n",
    "lower_winner99 = np.asarray(winner99_results[4:5][0])[0]\n",
    "median_winner99 = np.asarray(winner99_results[9:10][0])[0]\n",
    "upper_winner99 = np.asarray(winner99_results[14:15][0])[0]\n",
    "\n",
    "lower_loser99, lower_winner99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration10 :\n",
    "\n",
    "slice10 = 9\n",
    "\n",
    "loser10 = [train_regret_loser_1[slice10],\n",
    "       train_regret_loser_2[slice10],\n",
    "       train_regret_loser_3[slice10],\n",
    "       train_regret_loser_4[slice10],\n",
    "       train_regret_loser_5[slice10],\n",
    "       train_regret_loser_6[slice10],\n",
    "       train_regret_loser_7[slice10],\n",
    "       train_regret_loser_8[slice10],\n",
    "       train_regret_loser_9[slice10],\n",
    "       train_regret_loser_10[slice10],\n",
    "       train_regret_loser_11[slice10],\n",
    "       train_regret_loser_12[slice10],\n",
    "       train_regret_loser_13[slice10],\n",
    "       train_regret_loser_14[slice10],\n",
    "       train_regret_loser_15[slice10],\n",
    "       train_regret_loser_16[slice10],\n",
    "       train_regret_loser_17[slice10],\n",
    "       train_regret_loser_18[slice10],\n",
    "       train_regret_loser_19[slice10],\n",
    "       train_regret_loser_20[slice10]]\n",
    "\n",
    "winner10 = [train_regret_winner_1[slice10],\n",
    "       train_regret_winner_2[slice10],\n",
    "       train_regret_winner_3[slice10],\n",
    "       train_regret_winner_4[slice10],\n",
    "       train_regret_winner_5[slice10],\n",
    "       train_regret_winner_6[slice10],\n",
    "       train_regret_winner_7[slice10],\n",
    "       train_regret_winner_8[slice10],\n",
    "       train_regret_winner_9[slice10],\n",
    "       train_regret_winner_10[slice10],\n",
    "       train_regret_winner_11[slice10],\n",
    "       train_regret_winner_12[slice10],\n",
    "       train_regret_winner_13[slice10],\n",
    "       train_regret_winner_14[slice10],\n",
    "       train_regret_winner_15[slice10],\n",
    "       train_regret_winner_16[slice10],\n",
    "       train_regret_winner_17[slice10],\n",
    "       train_regret_winner_18[slice10],\n",
    "       train_regret_winner_19[slice10],\n",
    "       train_regret_winner_20[slice10]]\n",
    "\n",
    "loser10_results = pd.DataFrame(loser10).sort_values(by=[0], ascending=False)\n",
    "winner10_results = pd.DataFrame(winner10).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser10 = np.asarray(loser10_results[4:5][0])[0]\n",
    "median_loser10 = np.asarray(loser10_results[9:10][0])[0]\n",
    "upper_loser10 = np.asarray(loser10_results[14:15][0])[0]\n",
    "\n",
    "lower_winner10 = np.asarray(winner10_results[4:5][0])[0]\n",
    "median_winner10 = np.asarray(winner10_results[9:10][0])[0]\n",
    "upper_winner10 = np.asarray(winner10_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration20 :\n",
    "\n",
    "slice20 = 19\n",
    "\n",
    "loser20 = [train_regret_loser_1[slice20],\n",
    "       train_regret_loser_2[slice20],\n",
    "       train_regret_loser_3[slice20],\n",
    "       train_regret_loser_4[slice20],\n",
    "       train_regret_loser_5[slice20],\n",
    "       train_regret_loser_6[slice20],\n",
    "       train_regret_loser_7[slice20],\n",
    "       train_regret_loser_8[slice20],\n",
    "       train_regret_loser_9[slice20],\n",
    "       train_regret_loser_10[slice20],\n",
    "       train_regret_loser_11[slice20],\n",
    "       train_regret_loser_12[slice20],\n",
    "       train_regret_loser_13[slice20],\n",
    "       train_regret_loser_14[slice20],\n",
    "       train_regret_loser_15[slice20],\n",
    "       train_regret_loser_16[slice20],\n",
    "       train_regret_loser_17[slice20],\n",
    "       train_regret_loser_18[slice20],\n",
    "       train_regret_loser_19[slice20],\n",
    "       train_regret_loser_20[slice20]]\n",
    "\n",
    "winner20 = [train_regret_winner_1[slice20],\n",
    "       train_regret_winner_2[slice20],\n",
    "       train_regret_winner_3[slice20],\n",
    "       train_regret_winner_4[slice20],\n",
    "       train_regret_winner_5[slice20],\n",
    "       train_regret_winner_6[slice20],\n",
    "       train_regret_winner_7[slice20],\n",
    "       train_regret_winner_8[slice20],\n",
    "       train_regret_winner_9[slice20],\n",
    "       train_regret_winner_10[slice20],\n",
    "       train_regret_winner_11[slice20],\n",
    "       train_regret_winner_12[slice20],\n",
    "       train_regret_winner_13[slice20],\n",
    "       train_regret_winner_14[slice20],\n",
    "       train_regret_winner_15[slice20],\n",
    "       train_regret_winner_16[slice20],\n",
    "       train_regret_winner_17[slice20],\n",
    "       train_regret_winner_18[slice20],\n",
    "       train_regret_winner_19[slice20],\n",
    "       train_regret_winner_20[slice20]]\n",
    "\n",
    "loser20_results = pd.DataFrame(loser20).sort_values(by=[0], ascending=False)\n",
    "winner20_results = pd.DataFrame(winner20).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser20 = np.asarray(loser20_results[4:5][0])[0]\n",
    "median_loser20 = np.asarray(loser20_results[9:10][0])[0]\n",
    "upper_loser20 = np.asarray(loser20_results[14:15][0])[0]\n",
    "\n",
    "lower_winner20 = np.asarray(winner20_results[4:5][0])[0]\n",
    "median_winner20 = np.asarray(winner20_results[9:10][0])[0]\n",
    "upper_winner20 = np.asarray(winner20_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration30 :\n",
    "\n",
    "slice30 = 29\n",
    "\n",
    "loser30 = [train_regret_loser_1[slice30],\n",
    "       train_regret_loser_2[slice30],\n",
    "       train_regret_loser_3[slice30],\n",
    "       train_regret_loser_4[slice30],\n",
    "       train_regret_loser_5[slice30],\n",
    "       train_regret_loser_6[slice30],\n",
    "       train_regret_loser_7[slice30],\n",
    "       train_regret_loser_8[slice30],\n",
    "       train_regret_loser_9[slice30],\n",
    "       train_regret_loser_10[slice30],\n",
    "       train_regret_loser_11[slice30],\n",
    "       train_regret_loser_12[slice30],\n",
    "       train_regret_loser_13[slice30],\n",
    "       train_regret_loser_14[slice30],\n",
    "       train_regret_loser_15[slice30],\n",
    "       train_regret_loser_16[slice30],\n",
    "       train_regret_loser_17[slice30],\n",
    "       train_regret_loser_18[slice30],\n",
    "       train_regret_loser_19[slice30],\n",
    "       train_regret_loser_20[slice30]]\n",
    "\n",
    "winner30 = [train_regret_winner_1[slice30],\n",
    "       train_regret_winner_2[slice30],\n",
    "       train_regret_winner_3[slice30],\n",
    "       train_regret_winner_4[slice30],\n",
    "       train_regret_winner_5[slice30],\n",
    "       train_regret_winner_6[slice30],\n",
    "       train_regret_winner_7[slice30],\n",
    "       train_regret_winner_8[slice30],\n",
    "       train_regret_winner_9[slice30],\n",
    "       train_regret_winner_10[slice30],\n",
    "       train_regret_winner_11[slice30],\n",
    "       train_regret_winner_12[slice30],\n",
    "       train_regret_winner_13[slice30],\n",
    "       train_regret_winner_14[slice30],\n",
    "       train_regret_winner_15[slice30],\n",
    "       train_regret_winner_16[slice30],\n",
    "       train_regret_winner_17[slice30],\n",
    "       train_regret_winner_18[slice30],\n",
    "       train_regret_winner_19[slice30],\n",
    "       train_regret_winner_20[slice30]]\n",
    "\n",
    "loser30_results = pd.DataFrame(loser30).sort_values(by=[0], ascending=False)\n",
    "winner30_results = pd.DataFrame(winner30).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser30 = np.asarray(loser30_results[4:5][0])[0]\n",
    "median_loser30 = np.asarray(loser30_results[9:10][0])[0]\n",
    "upper_loser30 = np.asarray(loser30_results[14:15][0])[0]\n",
    "\n",
    "lower_winner30 = np.asarray(winner30_results[4:5][0])[0]\n",
    "median_winner30 = np.asarray(winner30_results[9:10][0])[0]\n",
    "upper_winner30 = np.asarray(winner30_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration40 :\n",
    "\n",
    "slice40 = 39\n",
    "\n",
    "loser40 = [train_regret_loser_1[slice40],\n",
    "       train_regret_loser_2[slice40],\n",
    "       train_regret_loser_3[slice40],\n",
    "       train_regret_loser_4[slice40],\n",
    "       train_regret_loser_5[slice40],\n",
    "       train_regret_loser_6[slice40],\n",
    "       train_regret_loser_7[slice40],\n",
    "       train_regret_loser_8[slice40],\n",
    "       train_regret_loser_9[slice40],\n",
    "       train_regret_loser_10[slice40],\n",
    "       train_regret_loser_11[slice40],\n",
    "       train_regret_loser_12[slice40],\n",
    "       train_regret_loser_13[slice40],\n",
    "       train_regret_loser_14[slice40],\n",
    "       train_regret_loser_15[slice40],\n",
    "       train_regret_loser_16[slice40],\n",
    "       train_regret_loser_17[slice40],\n",
    "       train_regret_loser_18[slice40],\n",
    "       train_regret_loser_19[slice40],\n",
    "       train_regret_loser_20[slice40]]\n",
    "\n",
    "winner40 = [train_regret_winner_1[slice40],\n",
    "       train_regret_winner_2[slice40],\n",
    "       train_regret_winner_3[slice40],\n",
    "       train_regret_winner_4[slice40],\n",
    "       train_regret_winner_5[slice40],\n",
    "       train_regret_winner_6[slice40],\n",
    "       train_regret_winner_7[slice40],\n",
    "       train_regret_winner_8[slice40],\n",
    "       train_regret_winner_9[slice40],\n",
    "       train_regret_winner_10[slice40],\n",
    "       train_regret_winner_11[slice40],\n",
    "       train_regret_winner_12[slice40],\n",
    "       train_regret_winner_13[slice40],\n",
    "       train_regret_winner_14[slice40],\n",
    "       train_regret_winner_15[slice40],\n",
    "       train_regret_winner_16[slice40],\n",
    "       train_regret_winner_17[slice40],\n",
    "       train_regret_winner_18[slice40],\n",
    "       train_regret_winner_19[slice40],\n",
    "       train_regret_winner_20[slice40]]\n",
    "\n",
    "loser40_results = pd.DataFrame(loser40).sort_values(by=[0], ascending=False)\n",
    "winner40_results = pd.DataFrame(winner40).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser40 = np.asarray(loser40_results[4:5][0])[0]\n",
    "median_loser40 = np.asarray(loser40_results[9:10][0])[0]\n",
    "upper_loser40 = np.asarray(loser40_results[14:15][0])[0]\n",
    "\n",
    "lower_winner40 = np.asarray(winner40_results[4:5][0])[0]\n",
    "median_winner40 = np.asarray(winner40_results[9:10][0])[0]\n",
    "upper_winner40 = np.asarray(winner40_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration50 :\n",
    "\n",
    "slice50 = 49\n",
    "\n",
    "loser50 = [train_regret_loser_1[slice50],\n",
    "       train_regret_loser_2[slice50],\n",
    "       train_regret_loser_3[slice50],\n",
    "       train_regret_loser_4[slice50],\n",
    "       train_regret_loser_5[slice50],\n",
    "       train_regret_loser_6[slice50],\n",
    "       train_regret_loser_7[slice50],\n",
    "       train_regret_loser_8[slice50],\n",
    "       train_regret_loser_9[slice50],\n",
    "       train_regret_loser_10[slice50],\n",
    "       train_regret_loser_11[slice50],\n",
    "       train_regret_loser_12[slice50],\n",
    "       train_regret_loser_13[slice50],\n",
    "       train_regret_loser_14[slice50],\n",
    "       train_regret_loser_15[slice50],\n",
    "       train_regret_loser_16[slice50],\n",
    "       train_regret_loser_17[slice50],\n",
    "       train_regret_loser_18[slice50],\n",
    "       train_regret_loser_19[slice50],\n",
    "       train_regret_loser_20[slice50]]\n",
    "\n",
    "winner50 = [train_regret_winner_1[slice50],\n",
    "       train_regret_winner_2[slice50],\n",
    "       train_regret_winner_3[slice50],\n",
    "       train_regret_winner_4[slice50],\n",
    "       train_regret_winner_5[slice50],\n",
    "       train_regret_winner_6[slice50],\n",
    "       train_regret_winner_7[slice50],\n",
    "       train_regret_winner_8[slice50],\n",
    "       train_regret_winner_9[slice50],\n",
    "       train_regret_winner_10[slice50],\n",
    "       train_regret_winner_11[slice50],\n",
    "       train_regret_winner_12[slice50],\n",
    "       train_regret_winner_13[slice50],\n",
    "       train_regret_winner_14[slice50],\n",
    "       train_regret_winner_15[slice50],\n",
    "       train_regret_winner_16[slice50],\n",
    "       train_regret_winner_17[slice50],\n",
    "       train_regret_winner_18[slice50],\n",
    "       train_regret_winner_19[slice50],\n",
    "       train_regret_winner_20[slice50]]\n",
    "\n",
    "loser50_results = pd.DataFrame(loser50).sort_values(by=[0], ascending=False)\n",
    "winner50_results = pd.DataFrame(winner50).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser50 = np.asarray(loser50_results[4:5][0])[0]\n",
    "median_loser50 = np.asarray(loser50_results[9:10][0])[0]\n",
    "upper_loser50 = np.asarray(loser50_results[14:15][0])[0]\n",
    "\n",
    "lower_winner50 = np.asarray(winner50_results[4:5][0])[0]\n",
    "median_winner50 = np.asarray(winner50_results[9:10][0])[0]\n",
    "upper_winner50 = np.asarray(winner50_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration60 :\n",
    "\n",
    "slice60 = 59\n",
    "\n",
    "loser60 = [train_regret_loser_1[slice60],\n",
    "       train_regret_loser_2[slice60],\n",
    "       train_regret_loser_3[slice60],\n",
    "       train_regret_loser_4[slice60],\n",
    "       train_regret_loser_5[slice60],\n",
    "       train_regret_loser_6[slice60],\n",
    "       train_regret_loser_7[slice60],\n",
    "       train_regret_loser_8[slice60],\n",
    "       train_regret_loser_9[slice60],\n",
    "       train_regret_loser_10[slice60],\n",
    "       train_regret_loser_11[slice60],\n",
    "       train_regret_loser_12[slice60],\n",
    "       train_regret_loser_13[slice60],\n",
    "       train_regret_loser_14[slice60],\n",
    "       train_regret_loser_15[slice60],\n",
    "       train_regret_loser_16[slice60],\n",
    "       train_regret_loser_17[slice60],\n",
    "       train_regret_loser_18[slice60],\n",
    "       train_regret_loser_19[slice60],\n",
    "       train_regret_loser_20[slice60]]\n",
    "\n",
    "winner60 = [train_regret_winner_1[slice60],\n",
    "       train_regret_winner_2[slice60],\n",
    "       train_regret_winner_3[slice60],\n",
    "       train_regret_winner_4[slice60],\n",
    "       train_regret_winner_5[slice60],\n",
    "       train_regret_winner_6[slice60],\n",
    "       train_regret_winner_7[slice60],\n",
    "       train_regret_winner_8[slice60],\n",
    "       train_regret_winner_9[slice60],\n",
    "       train_regret_winner_10[slice60],\n",
    "       train_regret_winner_11[slice60],\n",
    "       train_regret_winner_12[slice60],\n",
    "       train_regret_winner_13[slice60],\n",
    "       train_regret_winner_14[slice60],\n",
    "       train_regret_winner_15[slice60],\n",
    "       train_regret_winner_16[slice60],\n",
    "       train_regret_winner_17[slice60],\n",
    "       train_regret_winner_18[slice60],\n",
    "       train_regret_winner_19[slice60],\n",
    "       train_regret_winner_20[slice60]]\n",
    "\n",
    "loser60_results = pd.DataFrame(loser60).sort_values(by=[0], ascending=False)\n",
    "winner60_results = pd.DataFrame(winner60).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser60 = np.asarray(loser60_results[4:5][0])[0]\n",
    "median_loser60 = np.asarray(loser60_results[9:10][0])[0]\n",
    "upper_loser60 = np.asarray(loser60_results[14:15][0])[0]\n",
    "\n",
    "lower_winner60 = np.asarray(winner60_results[4:5][0])[0]\n",
    "median_winner60 = np.asarray(winner60_results[9:10][0])[0]\n",
    "upper_winner60 = np.asarray(winner60_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration70 :\n",
    "\n",
    "slice70 = 69\n",
    "\n",
    "loser70 = [train_regret_loser_1[slice70],\n",
    "       train_regret_loser_2[slice70],\n",
    "       train_regret_loser_3[slice70],\n",
    "       train_regret_loser_4[slice70],\n",
    "       train_regret_loser_5[slice70],\n",
    "       train_regret_loser_6[slice70],\n",
    "       train_regret_loser_7[slice70],\n",
    "       train_regret_loser_8[slice70],\n",
    "       train_regret_loser_9[slice70],\n",
    "       train_regret_loser_10[slice70],\n",
    "       train_regret_loser_11[slice70],\n",
    "       train_regret_loser_12[slice70],\n",
    "       train_regret_loser_13[slice70],\n",
    "       train_regret_loser_14[slice70],\n",
    "       train_regret_loser_15[slice70],\n",
    "       train_regret_loser_16[slice70],\n",
    "       train_regret_loser_17[slice70],\n",
    "       train_regret_loser_18[slice70],\n",
    "       train_regret_loser_19[slice70],\n",
    "       train_regret_loser_20[slice70]]\n",
    "\n",
    "winner70 = [train_regret_winner_1[slice70],\n",
    "       train_regret_winner_2[slice70],\n",
    "       train_regret_winner_3[slice70],\n",
    "       train_regret_winner_4[slice70],\n",
    "       train_regret_winner_5[slice70],\n",
    "       train_regret_winner_6[slice70],\n",
    "       train_regret_winner_7[slice70],\n",
    "       train_regret_winner_8[slice70],\n",
    "       train_regret_winner_9[slice70],\n",
    "       train_regret_winner_10[slice70],\n",
    "       train_regret_winner_11[slice70],\n",
    "       train_regret_winner_12[slice70],\n",
    "       train_regret_winner_13[slice70],\n",
    "       train_regret_winner_14[slice70],\n",
    "       train_regret_winner_15[slice70],\n",
    "       train_regret_winner_16[slice70],\n",
    "       train_regret_winner_17[slice70],\n",
    "       train_regret_winner_18[slice70],\n",
    "       train_regret_winner_19[slice70],\n",
    "       train_regret_winner_20[slice70]]\n",
    "\n",
    "loser70_results = pd.DataFrame(loser70).sort_values(by=[0], ascending=False)\n",
    "winner70_results = pd.DataFrame(winner70).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser70 = np.asarray(loser70_results[4:5][0])[0]\n",
    "median_loser70 = np.asarray(loser70_results[9:10][0])[0]\n",
    "upper_loser70 = np.asarray(loser70_results[14:15][0])[0]\n",
    "\n",
    "lower_winner70 = np.asarray(winner70_results[4:5][0])[0]\n",
    "median_winner70 = np.asarray(winner70_results[9:10][0])[0]\n",
    "upper_winner70 = np.asarray(winner70_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration80 :\n",
    "\n",
    "slice80 = 79\n",
    "\n",
    "loser80 = [train_regret_loser_1[slice80],\n",
    "       train_regret_loser_2[slice80],\n",
    "       train_regret_loser_3[slice80],\n",
    "       train_regret_loser_4[slice80],\n",
    "       train_regret_loser_5[slice80],\n",
    "       train_regret_loser_6[slice80],\n",
    "       train_regret_loser_7[slice80],\n",
    "       train_regret_loser_8[slice80],\n",
    "       train_regret_loser_9[slice80],\n",
    "       train_regret_loser_10[slice80],\n",
    "       train_regret_loser_11[slice80],\n",
    "       train_regret_loser_12[slice80],\n",
    "       train_regret_loser_13[slice80],\n",
    "       train_regret_loser_14[slice80],\n",
    "       train_regret_loser_15[slice80],\n",
    "       train_regret_loser_16[slice80],\n",
    "       train_regret_loser_17[slice80],\n",
    "       train_regret_loser_18[slice80],\n",
    "       train_regret_loser_19[slice80],\n",
    "       train_regret_loser_20[slice80]]\n",
    "\n",
    "winner80 = [train_regret_winner_1[slice80],\n",
    "       train_regret_winner_2[slice80],\n",
    "       train_regret_winner_3[slice80],\n",
    "       train_regret_winner_4[slice80],\n",
    "       train_regret_winner_5[slice80],\n",
    "       train_regret_winner_6[slice80],\n",
    "       train_regret_winner_7[slice80],\n",
    "       train_regret_winner_8[slice80],\n",
    "       train_regret_winner_9[slice80],\n",
    "       train_regret_winner_10[slice80],\n",
    "       train_regret_winner_11[slice80],\n",
    "       train_regret_winner_12[slice80],\n",
    "       train_regret_winner_13[slice80],\n",
    "       train_regret_winner_14[slice80],\n",
    "       train_regret_winner_15[slice80],\n",
    "       train_regret_winner_16[slice80],\n",
    "       train_regret_winner_17[slice80],\n",
    "       train_regret_winner_18[slice80],\n",
    "       train_regret_winner_19[slice80],\n",
    "       train_regret_winner_20[slice80]]\n",
    "\n",
    "loser80_results = pd.DataFrame(loser80).sort_values(by=[0], ascending=False)\n",
    "winner80_results = pd.DataFrame(winner80).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser80 = np.asarray(loser80_results[4:5][0])[0]\n",
    "median_loser80 = np.asarray(loser80_results[9:10][0])[0]\n",
    "upper_loser80 = np.asarray(loser80_results[14:15][0])[0]\n",
    "\n",
    "lower_winner80 = np.asarray(winner80_results[4:5][0])[0]\n",
    "median_winner80 = np.asarray(winner80_results[9:10][0])[0]\n",
    "upper_winner80 = np.asarray(winner80_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration90 :\n",
    "\n",
    "slice90 = 89\n",
    "\n",
    "loser90 = [train_regret_loser_1[slice90],\n",
    "       train_regret_loser_2[slice90],\n",
    "       train_regret_loser_3[slice90],\n",
    "       train_regret_loser_4[slice90],\n",
    "       train_regret_loser_5[slice90],\n",
    "       train_regret_loser_6[slice90],\n",
    "       train_regret_loser_7[slice90],\n",
    "       train_regret_loser_8[slice90],\n",
    "       train_regret_loser_9[slice90],\n",
    "       train_regret_loser_10[slice90],\n",
    "       train_regret_loser_11[slice90],\n",
    "       train_regret_loser_12[slice90],\n",
    "       train_regret_loser_13[slice90],\n",
    "       train_regret_loser_14[slice90],\n",
    "       train_regret_loser_15[slice90],\n",
    "       train_regret_loser_16[slice90],\n",
    "       train_regret_loser_17[slice90],\n",
    "       train_regret_loser_18[slice90],\n",
    "       train_regret_loser_19[slice90],\n",
    "       train_regret_loser_20[slice90]]\n",
    "\n",
    "winner90 = [train_regret_winner_1[slice90],\n",
    "       train_regret_winner_2[slice90],\n",
    "       train_regret_winner_3[slice90],\n",
    "       train_regret_winner_4[slice90],\n",
    "       train_regret_winner_5[slice90],\n",
    "       train_regret_winner_6[slice90],\n",
    "       train_regret_winner_7[slice90],\n",
    "       train_regret_winner_8[slice90],\n",
    "       train_regret_winner_9[slice90],\n",
    "       train_regret_winner_10[slice90],\n",
    "       train_regret_winner_11[slice90],\n",
    "       train_regret_winner_12[slice90],\n",
    "       train_regret_winner_13[slice90],\n",
    "       train_regret_winner_14[slice90],\n",
    "       train_regret_winner_15[slice90],\n",
    "       train_regret_winner_16[slice90],\n",
    "       train_regret_winner_17[slice90],\n",
    "       train_regret_winner_18[slice90],\n",
    "       train_regret_winner_19[slice90],\n",
    "       train_regret_winner_20[slice90]]\n",
    "\n",
    "loser90_results = pd.DataFrame(loser90).sort_values(by=[0], ascending=False)\n",
    "winner90_results = pd.DataFrame(winner90).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser90 = np.asarray(loser90_results[4:5][0])[0]\n",
    "median_loser90 = np.asarray(loser90_results[9:10][0])[0]\n",
    "upper_loser90 = np.asarray(loser90_results[14:15][0])[0]\n",
    "\n",
    "lower_winner90 = np.asarray(winner90_results[4:5][0])[0]\n",
    "median_winner90 = np.asarray(winner90_results[9:10][0])[0]\n",
    "upper_winner90 = np.asarray(winner90_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration100 :\n",
    "\n",
    "slice100 = 99\n",
    "\n",
    "loser100 = [train_regret_loser_1[slice100],\n",
    "       train_regret_loser_2[slice100],\n",
    "       train_regret_loser_3[slice100],\n",
    "       train_regret_loser_4[slice100],\n",
    "       train_regret_loser_5[slice100],\n",
    "       train_regret_loser_6[slice100],\n",
    "       train_regret_loser_7[slice100],\n",
    "       train_regret_loser_8[slice100],\n",
    "       train_regret_loser_9[slice100],\n",
    "       train_regret_loser_10[slice100],\n",
    "       train_regret_loser_11[slice100],\n",
    "       train_regret_loser_12[slice100],\n",
    "       train_regret_loser_13[slice100],\n",
    "       train_regret_loser_14[slice100],\n",
    "       train_regret_loser_15[slice100],\n",
    "       train_regret_loser_16[slice100],\n",
    "       train_regret_loser_17[slice100],\n",
    "       train_regret_loser_18[slice100],\n",
    "       train_regret_loser_19[slice100],\n",
    "       train_regret_loser_20[slice100]]\n",
    "\n",
    "winner100 = [train_regret_winner_1[slice100],\n",
    "       train_regret_winner_2[slice100],\n",
    "       train_regret_winner_3[slice100],\n",
    "       train_regret_winner_4[slice100],\n",
    "       train_regret_winner_5[slice100],\n",
    "       train_regret_winner_6[slice100],\n",
    "       train_regret_winner_7[slice100],\n",
    "       train_regret_winner_8[slice100],\n",
    "       train_regret_winner_9[slice100],\n",
    "       train_regret_winner_10[slice100],\n",
    "       train_regret_winner_11[slice100],\n",
    "       train_regret_winner_12[slice100],\n",
    "       train_regret_winner_13[slice100],\n",
    "       train_regret_winner_14[slice100],\n",
    "       train_regret_winner_15[slice100],\n",
    "       train_regret_winner_16[slice100],\n",
    "       train_regret_winner_17[slice100],\n",
    "       train_regret_winner_18[slice100],\n",
    "       train_regret_winner_19[slice100],\n",
    "       train_regret_winner_20[slice100]]\n",
    "\n",
    "loser100_results = pd.DataFrame(loser100).sort_values(by=[0], ascending=False)\n",
    "winner100_results = pd.DataFrame(winner100).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser100 = np.asarray(loser100_results[4:5][0])[0]\n",
    "median_loser100 = np.asarray(loser100_results[9:10][0])[0]\n",
    "upper_loser100 = np.asarray(loser100_results[14:15][0])[0]\n",
    "\n",
    "lower_winner100 = np.asarray(winner100_results[4:5][0])[0]\n",
    "median_winner100 = np.asarray(winner100_results[9:10][0])[0]\n",
    "upper_winner100 = np.asarray(winner100_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summarise arrays: 'Loser'\n",
    "\n",
    "lower_loser = [lower_loser1,\n",
    "            lower_loser2,\n",
    "            lower_loser3,\n",
    "            lower_loser4,\n",
    "            lower_loser5,\n",
    "            lower_loser6,\n",
    "            lower_loser7,\n",
    "            lower_loser8,\n",
    "            lower_loser9,\n",
    "            lower_loser10,\n",
    "            lower_loser11,\n",
    "            lower_loser12,\n",
    "            lower_loser13,\n",
    "            lower_loser14,\n",
    "            lower_loser15,\n",
    "            lower_loser16,\n",
    "            lower_loser17,\n",
    "            lower_loser18,\n",
    "            lower_loser19,\n",
    "            lower_loser20,\n",
    "            lower_loser21,\n",
    "            lower_loser22,\n",
    "            lower_loser23,\n",
    "            lower_loser24,\n",
    "            lower_loser25,\n",
    "            lower_loser26,\n",
    "            lower_loser27,\n",
    "            lower_loser28,\n",
    "            lower_loser29,\n",
    "            lower_loser30,\n",
    "            lower_loser31,\n",
    "            lower_loser32,\n",
    "            lower_loser33,\n",
    "            lower_loser34,\n",
    "            lower_loser35,\n",
    "            lower_loser36,\n",
    "            lower_loser37,\n",
    "            lower_loser38,\n",
    "            lower_loser39,\n",
    "            lower_loser40,\n",
    "            lower_loser41,\n",
    "            lower_loser42,\n",
    "            lower_loser43,\n",
    "            lower_loser44,\n",
    "            lower_loser45,\n",
    "            lower_loser46,\n",
    "            lower_loser47,\n",
    "            lower_loser48,\n",
    "            lower_loser49,\n",
    "            lower_loser50,\n",
    "            lower_loser51,\n",
    "            lower_loser52,\n",
    "            lower_loser53,\n",
    "            lower_loser54,\n",
    "            lower_loser55,\n",
    "            lower_loser56,\n",
    "            lower_loser57,\n",
    "            lower_loser58,\n",
    "            lower_loser59,\n",
    "            lower_loser60,\n",
    "            lower_loser61,\n",
    "            lower_loser62,\n",
    "            lower_loser63,\n",
    "            lower_loser64,\n",
    "            lower_loser65,\n",
    "            lower_loser66,\n",
    "            lower_loser67,\n",
    "            lower_loser68,\n",
    "            lower_loser69,\n",
    "            lower_loser70,\n",
    "            lower_loser71,\n",
    "            lower_loser72,\n",
    "            lower_loser73,\n",
    "            lower_loser74,\n",
    "            lower_loser75,\n",
    "            lower_loser76,\n",
    "            lower_loser77,\n",
    "            lower_loser78,\n",
    "            lower_loser79,\n",
    "            lower_loser80,\n",
    "            lower_loser81,\n",
    "            lower_loser82,\n",
    "            lower_loser83,\n",
    "            lower_loser84,\n",
    "            lower_loser85,\n",
    "            lower_loser86,\n",
    "            lower_loser87,\n",
    "            lower_loser88,\n",
    "            lower_loser89,\n",
    "            lower_loser90,\n",
    "            lower_loser91,\n",
    "            lower_loser92,\n",
    "            lower_loser93,\n",
    "            lower_loser94,\n",
    "            lower_loser95,\n",
    "            lower_loser96,\n",
    "            lower_loser97,\n",
    "            lower_loser98,\n",
    "            lower_loser99,\n",
    "            lower_loser100,\n",
    "            lower_loser101]\n",
    "\n",
    "median_loser = [median_loser1,\n",
    "            median_loser2,\n",
    "            median_loser3,\n",
    "            median_loser4,\n",
    "            median_loser5,\n",
    "            median_loser6,\n",
    "            median_loser7,\n",
    "            median_loser8,\n",
    "            median_loser9,\n",
    "            median_loser10,\n",
    "            median_loser11,\n",
    "            median_loser12,\n",
    "            median_loser13,\n",
    "            median_loser14,\n",
    "            median_loser15,\n",
    "            median_loser16,\n",
    "            median_loser17,\n",
    "            median_loser18,\n",
    "            median_loser19,\n",
    "            median_loser20,\n",
    "            median_loser21,\n",
    "            median_loser22,\n",
    "            median_loser23,\n",
    "            median_loser24,\n",
    "            median_loser25,\n",
    "            median_loser26,\n",
    "            median_loser27,\n",
    "            median_loser28,\n",
    "            median_loser29,\n",
    "            median_loser30,\n",
    "            median_loser31,\n",
    "            median_loser32,\n",
    "            median_loser33,\n",
    "            median_loser34,\n",
    "            median_loser35,\n",
    "            median_loser36,\n",
    "            median_loser37,\n",
    "            median_loser38,\n",
    "            median_loser39,\n",
    "            median_loser40,\n",
    "            median_loser41,\n",
    "            median_loser42,\n",
    "            median_loser43,\n",
    "            median_loser44,\n",
    "            median_loser45,\n",
    "            median_loser46,\n",
    "            median_loser47,\n",
    "            median_loser48,\n",
    "            median_loser49,\n",
    "            median_loser50,\n",
    "            median_loser51,\n",
    "            median_loser52,\n",
    "            median_loser53,\n",
    "            median_loser54,\n",
    "            median_loser55,\n",
    "            median_loser56,\n",
    "            median_loser57,\n",
    "            median_loser58,\n",
    "            median_loser59,\n",
    "            median_loser60,\n",
    "            median_loser61,\n",
    "            median_loser62,\n",
    "            median_loser63,\n",
    "            median_loser64,\n",
    "            median_loser65,\n",
    "            median_loser66,\n",
    "            median_loser67,\n",
    "            median_loser68,\n",
    "            median_loser69,\n",
    "            median_loser70,\n",
    "            median_loser71,\n",
    "            median_loser72,\n",
    "            median_loser73,\n",
    "            median_loser74,\n",
    "            median_loser75,\n",
    "            median_loser76,\n",
    "            median_loser77,\n",
    "            median_loser78,\n",
    "            median_loser79,\n",
    "            median_loser80,\n",
    "            median_loser81,\n",
    "            median_loser82,\n",
    "            median_loser83,\n",
    "            median_loser84,\n",
    "            median_loser85,\n",
    "            median_loser86,\n",
    "            median_loser87,\n",
    "            median_loser88,\n",
    "            median_loser89,\n",
    "            median_loser90,\n",
    "            median_loser91,\n",
    "            median_loser92,\n",
    "            median_loser93,\n",
    "            median_loser94,\n",
    "            median_loser95,\n",
    "            median_loser96,\n",
    "            median_loser97,\n",
    "            median_loser98,\n",
    "            median_loser99,\n",
    "            median_loser100,\n",
    "            median_loser101]\n",
    "\n",
    "upper_loser = [upper_loser1,\n",
    "            upper_loser2,\n",
    "            upper_loser3,\n",
    "            upper_loser4,\n",
    "            upper_loser5,\n",
    "            upper_loser6,\n",
    "            upper_loser7,\n",
    "            upper_loser8,\n",
    "            upper_loser9,\n",
    "            upper_loser10,\n",
    "            upper_loser11,\n",
    "            upper_loser12,\n",
    "            upper_loser13,\n",
    "            upper_loser14,\n",
    "            upper_loser15,\n",
    "            upper_loser16,\n",
    "            upper_loser17,\n",
    "            upper_loser18,\n",
    "            upper_loser19,\n",
    "            upper_loser20,\n",
    "            upper_loser21,\n",
    "            upper_loser22,\n",
    "            upper_loser23,\n",
    "            upper_loser24,\n",
    "            upper_loser25,\n",
    "            upper_loser26,\n",
    "            upper_loser27,\n",
    "            upper_loser28,\n",
    "            upper_loser29,\n",
    "            upper_loser30,\n",
    "            upper_loser31,\n",
    "            upper_loser32,\n",
    "            upper_loser33,\n",
    "            upper_loser34,\n",
    "            upper_loser35,\n",
    "            upper_loser36,\n",
    "            upper_loser37,\n",
    "            upper_loser38,\n",
    "            upper_loser39,\n",
    "            upper_loser40,\n",
    "            upper_loser41,\n",
    "            upper_loser42,\n",
    "            upper_loser43,\n",
    "            upper_loser44,\n",
    "            upper_loser45,\n",
    "            upper_loser46,\n",
    "            upper_loser47,\n",
    "            upper_loser48,\n",
    "            upper_loser49,\n",
    "            upper_loser50,\n",
    "            upper_loser51,\n",
    "            upper_loser52,\n",
    "            upper_loser53,\n",
    "            upper_loser54,\n",
    "            upper_loser55,\n",
    "            upper_loser56,\n",
    "            upper_loser57,\n",
    "            upper_loser58,\n",
    "            upper_loser59,\n",
    "            upper_loser60,\n",
    "            upper_loser61,\n",
    "            upper_loser62,\n",
    "            upper_loser63,\n",
    "            upper_loser64,\n",
    "            upper_loser65,\n",
    "            upper_loser66,\n",
    "            upper_loser67,\n",
    "            upper_loser68,\n",
    "            upper_loser69,\n",
    "            upper_loser70,\n",
    "            upper_loser71,\n",
    "            upper_loser72,\n",
    "            upper_loser73,\n",
    "            upper_loser74,\n",
    "            upper_loser75,\n",
    "            upper_loser76,\n",
    "            upper_loser77,\n",
    "            upper_loser78,\n",
    "            upper_loser79,\n",
    "            upper_loser80,\n",
    "            upper_loser81,\n",
    "            upper_loser82,\n",
    "            upper_loser83,\n",
    "            upper_loser84,\n",
    "            upper_loser85,\n",
    "            upper_loser86,\n",
    "            upper_loser87,\n",
    "            upper_loser88,\n",
    "            upper_loser89,\n",
    "            upper_loser90,\n",
    "            upper_loser91,\n",
    "            upper_loser92,\n",
    "            upper_loser93,\n",
    "            upper_loser94,\n",
    "            upper_loser95,\n",
    "            upper_loser96,\n",
    "            upper_loser97,\n",
    "            upper_loser98,\n",
    "            upper_loser99,\n",
    "            upper_loser100,\n",
    "            upper_loser101]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summarise arrays: 'Winner'\n",
    "\n",
    "lower_winner = [lower_winner1,\n",
    "            lower_winner2,\n",
    "            lower_winner3,\n",
    "            lower_winner4,\n",
    "            lower_winner5,\n",
    "            lower_winner6,\n",
    "            lower_winner7,\n",
    "            lower_winner8,\n",
    "            lower_winner9,\n",
    "            lower_winner10,\n",
    "            lower_winner11,\n",
    "            lower_winner12,\n",
    "            lower_winner13,\n",
    "            lower_winner14,\n",
    "            lower_winner15,\n",
    "            lower_winner16,\n",
    "            lower_winner17,\n",
    "            lower_winner18,\n",
    "            lower_winner19,\n",
    "            lower_winner20,\n",
    "            lower_winner21,\n",
    "            lower_winner22,\n",
    "            lower_winner23,\n",
    "            lower_winner24,\n",
    "            lower_winner25,\n",
    "            lower_winner26,\n",
    "            lower_winner27,\n",
    "            lower_winner28,\n",
    "            lower_winner29,\n",
    "            lower_winner30,\n",
    "            lower_winner31,\n",
    "            lower_winner32,\n",
    "            lower_winner33,\n",
    "            lower_winner34,\n",
    "            lower_winner35,\n",
    "            lower_winner36,\n",
    "            lower_winner37,\n",
    "            lower_winner38,\n",
    "            lower_winner39,\n",
    "            lower_winner40,\n",
    "            lower_winner41,\n",
    "            lower_winner42,\n",
    "            lower_winner43,\n",
    "            lower_winner44,\n",
    "            lower_winner45,\n",
    "            lower_winner46,\n",
    "            lower_winner47,\n",
    "            lower_winner48,\n",
    "            lower_winner49,\n",
    "            lower_winner50,\n",
    "            lower_winner51,\n",
    "            lower_winner52,\n",
    "            lower_winner53,\n",
    "            lower_winner54,\n",
    "            lower_winner55,\n",
    "            lower_winner56,\n",
    "            lower_winner57,\n",
    "            lower_winner58,\n",
    "            lower_winner59,\n",
    "            lower_winner60,\n",
    "            lower_winner61,\n",
    "            lower_winner62,\n",
    "            lower_winner63,\n",
    "            lower_winner64,\n",
    "            lower_winner65,\n",
    "            lower_winner66,\n",
    "            lower_winner67,\n",
    "            lower_winner68,\n",
    "            lower_winner69,\n",
    "            lower_winner70,\n",
    "            lower_winner71,\n",
    "            lower_winner72,\n",
    "            lower_winner73,\n",
    "            lower_winner74,\n",
    "            lower_winner75,\n",
    "            lower_winner76,\n",
    "            lower_winner77,\n",
    "            lower_winner78,\n",
    "            lower_winner79,\n",
    "            lower_winner80,\n",
    "            lower_winner81,\n",
    "            lower_winner82,\n",
    "            lower_winner83,\n",
    "            lower_winner84,\n",
    "            lower_winner85,\n",
    "            lower_winner86,\n",
    "            lower_winner87,\n",
    "            lower_winner88,\n",
    "            lower_winner89,\n",
    "            lower_winner90,\n",
    "            lower_winner91,\n",
    "            lower_winner92,\n",
    "            lower_winner93,\n",
    "            lower_winner94,\n",
    "            lower_winner95,\n",
    "            lower_winner96,\n",
    "            lower_winner97,\n",
    "            lower_winner98,\n",
    "            lower_winner99,\n",
    "            lower_winner100,\n",
    "            lower_winner101]\n",
    "\n",
    "median_winner = [median_winner1,\n",
    "            median_winner2,\n",
    "            median_winner3,\n",
    "            median_winner4,\n",
    "            median_winner5,\n",
    "            median_winner6,\n",
    "            median_winner7,\n",
    "            median_winner8,\n",
    "            median_winner9,\n",
    "            median_winner10,\n",
    "            median_winner11,\n",
    "            median_winner12,\n",
    "            median_winner13,\n",
    "            median_winner14,\n",
    "            median_winner15,\n",
    "            median_winner16,\n",
    "            median_winner17,\n",
    "            median_winner18,\n",
    "            median_winner19,\n",
    "            median_winner20,\n",
    "            median_winner21,\n",
    "            median_winner22,\n",
    "            median_winner23,\n",
    "            median_winner24,\n",
    "            median_winner25,\n",
    "            median_winner26,\n",
    "            median_winner27,\n",
    "            median_winner28,\n",
    "            median_winner29,\n",
    "            median_winner30,\n",
    "            median_winner31,\n",
    "            median_winner32,\n",
    "            median_winner33,\n",
    "            median_winner34,\n",
    "            median_winner35,\n",
    "            median_winner36,\n",
    "            median_winner37,\n",
    "            median_winner38,\n",
    "            median_winner39,\n",
    "            median_winner40,\n",
    "            median_winner41,\n",
    "            median_winner42,\n",
    "            median_winner43,\n",
    "            median_winner44,\n",
    "            median_winner45,\n",
    "            median_winner46,\n",
    "            median_winner47,\n",
    "            median_winner48,\n",
    "            median_winner49,\n",
    "            median_winner50,\n",
    "            median_winner51,\n",
    "            median_winner52,\n",
    "            median_winner53,\n",
    "            median_winner54,\n",
    "            median_winner55,\n",
    "            median_winner56,\n",
    "            median_winner57,\n",
    "            median_winner58,\n",
    "            median_winner59,\n",
    "            median_winner60,\n",
    "            median_winner61,\n",
    "            median_winner62,\n",
    "            median_winner63,\n",
    "            median_winner64,\n",
    "            median_winner65,\n",
    "            median_winner66,\n",
    "            median_winner67,\n",
    "            median_winner68,\n",
    "            median_winner69,\n",
    "            median_winner70,\n",
    "            median_winner71,\n",
    "            median_winner72,\n",
    "            median_winner73,\n",
    "            median_winner74,\n",
    "            median_winner75,\n",
    "            median_winner76,\n",
    "            median_winner77,\n",
    "            median_winner78,\n",
    "            median_winner79,\n",
    "            median_winner80,\n",
    "            median_winner81,\n",
    "            median_winner82,\n",
    "            median_winner83,\n",
    "            median_winner84,\n",
    "            median_winner85,\n",
    "            median_winner86,\n",
    "            median_winner87,\n",
    "            median_winner88,\n",
    "            median_winner89,\n",
    "            median_winner90,\n",
    "            median_winner91,\n",
    "            median_winner92,\n",
    "            median_winner93,\n",
    "            median_winner94,\n",
    "            median_winner95,\n",
    "            median_winner96,\n",
    "            median_winner97,\n",
    "            median_winner98,\n",
    "            median_winner99,\n",
    "            median_winner100,\n",
    "            median_winner101]\n",
    "\n",
    "upper_winner = [upper_winner1,\n",
    "            upper_winner2,\n",
    "            upper_winner3,\n",
    "            upper_winner4,\n",
    "            upper_winner5,\n",
    "            upper_winner6,\n",
    "            upper_winner7,\n",
    "            upper_winner8,\n",
    "            upper_winner9,\n",
    "            upper_winner10,\n",
    "            upper_winner11,\n",
    "            upper_winner12,\n",
    "            upper_winner13,\n",
    "            upper_winner14,\n",
    "            upper_winner15,\n",
    "            upper_winner16,\n",
    "            upper_winner17,\n",
    "            upper_winner18,\n",
    "            upper_winner19,\n",
    "            upper_winner20,\n",
    "            upper_winner21,\n",
    "            upper_winner22,\n",
    "            upper_winner23,\n",
    "            upper_winner24,\n",
    "            upper_winner25,\n",
    "            upper_winner26,\n",
    "            upper_winner27,\n",
    "            upper_winner28,\n",
    "            upper_winner29,\n",
    "            upper_winner30,\n",
    "            upper_winner31,\n",
    "            upper_winner32,\n",
    "            upper_winner33,\n",
    "            upper_winner34,\n",
    "            upper_winner35,\n",
    "            upper_winner36,\n",
    "            upper_winner37,\n",
    "            upper_winner38,\n",
    "            upper_winner39,\n",
    "            upper_winner40,\n",
    "            upper_winner41,\n",
    "            upper_winner42,\n",
    "            upper_winner43,\n",
    "            upper_winner44,\n",
    "            upper_winner45,\n",
    "            upper_winner46,\n",
    "            upper_winner47,\n",
    "            upper_winner48,\n",
    "            upper_winner49,\n",
    "            upper_winner50,\n",
    "            upper_winner51,\n",
    "            upper_winner52,\n",
    "            upper_winner53,\n",
    "            upper_winner54,\n",
    "            upper_winner55,\n",
    "            upper_winner56,\n",
    "            upper_winner57,\n",
    "            upper_winner58,\n",
    "            upper_winner59,\n",
    "            upper_winner60,\n",
    "            upper_winner61,\n",
    "            upper_winner62,\n",
    "            upper_winner63,\n",
    "            upper_winner64,\n",
    "            upper_winner65,\n",
    "            upper_winner66,\n",
    "            upper_winner67,\n",
    "            upper_winner68,\n",
    "            upper_winner69,\n",
    "            upper_winner70,\n",
    "            upper_winner71,\n",
    "            upper_winner72,\n",
    "            upper_winner73,\n",
    "            upper_winner74,\n",
    "            upper_winner75,\n",
    "            upper_winner76,\n",
    "            upper_winner77,\n",
    "            upper_winner78,\n",
    "            upper_winner79,\n",
    "            upper_winner80,\n",
    "            upper_winner81,\n",
    "            upper_winner82,\n",
    "            upper_winner83,\n",
    "            upper_winner84,\n",
    "            upper_winner85,\n",
    "            upper_winner86,\n",
    "            upper_winner87,\n",
    "            upper_winner88,\n",
    "            upper_winner89,\n",
    "            upper_winner90,\n",
    "            upper_winner91,\n",
    "            upper_winner92,\n",
    "            upper_winner93,\n",
    "            upper_winner94,\n",
    "            upper_winner95,\n",
    "            upper_winner96,\n",
    "            upper_winner97,\n",
    "            upper_winner98,\n",
    "            upper_winner99,\n",
    "            upper_winner100,\n",
    "            upper_winner101]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Visualise!\n",
    "\n",
    "title = obj_func\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(median_loser, color = 'Red')\n",
    "plt.plot(median_winner, color = 'Blue')\n",
    "\n",
    "xstar = np.arange(0, max_iter+1, step=1)\n",
    "plt.fill_between(xstar, lower_loser, upper_loser, facecolor = 'Red', alpha=0.4, label='GP ERM Regret: IQR')\n",
    "plt.fill_between(xstar, lower_winner, upper_winner, facecolor = 'Blue', alpha=0.4, label='STP ERM Regret: IQR ' r'($\\nu$' ' = {})'.format(df))\n",
    "\n",
    "plt.title(title, weight = 'bold', family = 'Arial')\n",
    "plt.xlabel('(Post-initialization) iteration $\\it{k}$', weight = 'bold', family = 'Arial') # x-axis label\n",
    "plt.ylabel('ln(Regret)', weight = 'bold', family = 'Arial') # y-axis label\n",
    "plt.legend(loc=0) # add plot legend\n",
    "\n",
    "plt.show() #visualise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
