{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Import modules:\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import rc\n",
    "rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
    "rc('text', usetex=False)\n",
    "\n",
    "from collections import OrderedDict\n",
    "from numpy.linalg import slogdet\n",
    "from scipy.linalg import inv\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import gamma\n",
    "from scipy.stats import norm, t\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from pyGPGO.logger import EventLogger\n",
    "from pyGPGO.GPGO import GPGO\n",
    "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\n",
    "from pyGPGO.surrogates.tStudentProcess import tStudentProcess\n",
    "from pyGPGO.surrogates.tStudentProcess import logpdf\n",
    "from pyGPGO.acquisition import Acquisition\n",
    "from pyGPGO.covfunc import squaredExponential, matern32, matern52\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2a. User-defined - inputs:\n",
    "\n",
    "### Objective Function:\n",
    "obj_func = 'XGBoost' # 6-D;\n",
    "\n",
    "### Data inputs:\n",
    "n_test = 50\n",
    "\n",
    "set_seed = 888\n",
    "\n",
    "### Student-t parameter input:\n",
    "df1 = 5 # Degree(s)-of-freedom (DF)\n",
    "df2 = 11 # Degree(s)-of-freedom (DF)\n",
    "\n",
    "### Acquisition / Utility function - MLE/Type II:\n",
    "util_gp = 'RegretMinimized' # Gaussian MLE\n",
    "util_stp = 'tRegretMinimized' # Student-t MLE\n",
    "\n",
    "#util_gp = 'ExpectedImprovement' # Gaussian MLE\n",
    "#util_stp = 'tExpectedImprovement' # Student-t MLE\n",
    "\n",
    "### Objective Classifier:\n",
    "obj_classifier = 'binary:logistic'\n",
    "#obj_classifier = 'binary:hinge'\n",
    "\n",
    "### Probabilistic / Surrogate / Stochastic model - MLE/Type II: \n",
    "#surrogate_model_gp = 'Gaussian Process'\n",
    "surrogate_model_stp = 'Student-t Process'\n",
    "\n",
    "### Covariance Function:\n",
    "cov_func = squaredExponential()\n",
    "#cov_func = matern32()\n",
    "#cov_func = matern52()\n",
    "\n",
    "n_init = 5  # Number of iterations used to initialise Bayesian optimisation; minimum 2\n",
    "\n",
    "### MLE / Type II Empirical Bayes:\n",
    "optimize = False # MLE Boolean\n",
    "usegrads = False # MLE Boolean (pyGPGO not programmed for Student-t MLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2b. User-defined - data:\n",
    "\n",
    "### https://archive.ics.uci.edu/ml/datasets/Skin+Segmentation\n",
    "\n",
    "def ReadData():\n",
    "    #Data in format [B G R Label] from\n",
    "    data = np.genfromtxt('/home/ulsterconorc/Downloads/Skin_NonSkin.txt', dtype=np.int32)\n",
    "\n",
    "    labels = data[:,3]\n",
    "    data = data[:,0:3]\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "data, labels = ReadData()\n",
    "\n",
    "X = data\n",
    "y = labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Objective Function - XGBoost Classification 6-D:\n",
    "\n",
    "if obj_func == 'XGBoost':\n",
    "    \n",
    "    # Constraints:\n",
    "    param_lb_alpha = 0\n",
    "    param_ub_alpha = 10\n",
    "    \n",
    "    param_lb_gamma = 0\n",
    "    param_ub_gamma = 10\n",
    "    \n",
    "    param_lb_max_depth = 5\n",
    "    param_ub_max_depth = 15\n",
    "    \n",
    "    param_lb_min_child_weight = 1\n",
    "    param_ub_min_child_weight = 20\n",
    "    \n",
    "    param_lb_subsample = .5\n",
    "    param_ub_subsample = 1\n",
    "    \n",
    "    param_lb_colsample = .1\n",
    "    param_ub_colsample = 1\n",
    "    \n",
    "    # 6-D inputs' parameter bounds:\n",
    "    param = { 'alpha':  ('cont', (param_lb_alpha, param_ub_alpha)),\n",
    "         'gamma':  ('cont', (param_lb_gamma, param_ub_gamma)),     \n",
    "         'max_depth':  ('int', (param_lb_max_depth, param_ub_max_depth)),\n",
    "         'subsample':  ('cont', (param_lb_subsample, param_ub_subsample)),\n",
    "          'min_child_weight':  ('int', (param_lb_min_child_weight, param_ub_min_child_weight)),\n",
    "            'colsample': ('cont', (param_lb_colsample, param_ub_colsample))\n",
    "        }\n",
    "       \n",
    "    # True y bounds:\n",
    "    y_global_orig = 1\n",
    "    dim = 6\n",
    "    \n",
    "    max_iter = 30  # iterations of Bayesian optimisation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4a. Add new acquisition functions: add CBM & ERM (Nyugen and Osborne, 2019) method .\n",
    "\n",
    "### Inherits from class Acquisition()\n",
    "\n",
    "class Acquisition_new(Acquisition):    \n",
    "    def __init__(self, mode, eps=1e-06, **params):\n",
    "        \"\"\"\n",
    "        Acquisition function class.\n",
    "        Parameters\n",
    "        ----------\n",
    "        mode: str\n",
    "            Defines the behaviour of the acquisition strategy.\n",
    "        eps: float\n",
    "            Small floating value to avoid `np.sqrt` or zero-division warnings.\n",
    "        params: float\n",
    "            Extra parameters needed for certain acquisition functions, e.g. UCB needs\n",
    "            to be supplied with `beta`.\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "        self.eps = eps\n",
    "\n",
    "        mode_dict = {\n",
    "            'ExpectedImprovement': self.ExpectedImprovement,\n",
    "            'tExpectedImprovement': self.tExpectedImprovement,\n",
    "            'RegretMinimized': self.RegretMinimized,\n",
    "            'tRegretMinimized': self.tRegretMinimized\n",
    "        }\n",
    "\n",
    "        self.f = mode_dict[mode]\n",
    "   \n",
    "    def ExpectedImprovement(self, tau, mean, std):\n",
    "        \"\"\"\n",
    "        Expected Improvement acquisition function.\n",
    "        Parameters\n",
    "        ----------\n",
    "        tau: float\n",
    "            Best observed function evaluation.\n",
    "        mean: float\n",
    "            Point mean of the posterior process.\n",
    "        std: float\n",
    "            Point std of the posterior process.\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Expected improvement.\n",
    "        \"\"\"\n",
    "        z = (mean - tau - self.eps) / (std + self.eps)\n",
    "        return (mean - tau) * norm.cdf(z) + std * norm.pdf(z)[0]\n",
    "\n",
    "\n",
    "    def RegretMinimized(self, tau, mean, std):\n",
    "        \"\"\"\n",
    "        Regret Minimized acquisition function.\n",
    "        Parameters\n",
    "        ----------\n",
    "        tau: float\n",
    "            Best observed function evaluation.\n",
    "        mean: float\n",
    "            Point mean of the posterior process.\n",
    "        std: float\n",
    "            Point std of the posterior process.\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Expected improvement.\n",
    "        \"\"\"\n",
    "        \n",
    "        z = (mean - y_global_orig - self.eps) / (std + self.eps)\n",
    "        return z * (std + self.eps) * norm.cdf(z) + std * norm.pdf(z)[0]\n",
    "    \n",
    "    \n",
    "    def tExpectedImprovement(self, tau, mean, std, nu=3.0):\n",
    "        \"\"\"\n",
    "        Expected Improvement acquisition function. Only to be used with `tStudentProcess` surrogate.\n",
    "        Parameters\n",
    "        ----------\n",
    "        tau: float\n",
    "            Best observed function evaluation.\n",
    "        mean: float\n",
    "            Point mean of the posterior process.\n",
    "        std: float\n",
    "            Point std of the posterior process.\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Expected improvement.\n",
    "        \"\"\"\n",
    "        gamma = (mean - tau - self.eps) / (std + self.eps)\n",
    "        return gamma * std * t.cdf(gamma, df=nu) + std * (1 + (gamma ** 2 - 1)/(nu - 1)) * t.pdf(gamma, df=nu)\n",
    "    \n",
    "    \n",
    "    def tRegretMinimized(self, tau, mean, std, nu=3.0):\n",
    "        \"\"\"\n",
    "        Regret Minimized acquisition function. Only to be used with `tStudentProcess` surrogate.\n",
    "        Parameters\n",
    "        ----------\n",
    "        tau: float\n",
    "            Best observed function evaluation.\n",
    "        mean: float\n",
    "            Point mean of the posterior process.\n",
    "        std: float\n",
    "            Point std of the posterior process.\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Expected improvement.\n",
    "        \"\"\"\n",
    "        \n",
    "        gamma = (mean - y_global_orig - self.eps) / (std + self.eps)\n",
    "        return gamma * (std + self.eps) * t.cdf(gamma, df=nu) + std * (nu + gamma ** 2)/(nu - 1) * t.pdf(gamma, df=nu)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4b. Re-define tStudentProcess class with non-zero prior mean function:\n",
    "\n",
    "### [Nyugen and Osborne, 2019] \"Knowing The What But Not The Where in Bayesian Optimization\"\n",
    "\n",
    "### Inherits from class tStudentProcess()\n",
    "\n",
    "class tStudentProcess_prior(tStudentProcess):\n",
    "    def __init__(self, covfunc, nu, optimize=False, mprior=0):\n",
    "        \"\"\"\n",
    "        t-Student Process regressor class.\n",
    "        This class DOES NOT support gradients in ML estimation yet.\n",
    "        Parameters\n",
    "        ----------\n",
    "        covfunc: instance from a class of covfunc module\n",
    "            An instance from a class from the `covfunc` module.\n",
    "        nu: float\n",
    "            (>2.0) Degrees of freedom\n",
    "        Attributes\n",
    "        ----------\n",
    "        covfunc: object\n",
    "            Internal covariance function.\n",
    "        nu: float\n",
    "            Degrees of freedom.\n",
    "        optimize: bool\n",
    "            Whether to optimize covariance function hyperparameters.\n",
    "        \"\"\"\n",
    "        self.covfunc = covfunc\n",
    "        self.nu = nu\n",
    "        self.optimize = optimize\n",
    "        self.mprior = mprior\n",
    "        \n",
    "    def logpdf(x, nu, Sigma):\n",
    "        \"\"\"\n",
    "        Marginal log-likelihood of a Student-t Process\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: array-like\n",
    "            Point to be evaluated\n",
    "        df: float\n",
    "            Degrees of freedom (>2.0)\n",
    "        mu: array-like\n",
    "            Mean of the process.\n",
    "        Sigma: array-like\n",
    "            Covariance matrix of the process.\n",
    "        Returns\n",
    "        -------\n",
    "        logp: float\n",
    "            log-likelihood \n",
    "        \"\"\"\n",
    "        d = len(x)\n",
    "        x = np.atleast_2d(x)\n",
    "        xm = x - self.mprior\n",
    "        V = nu * Sigma\n",
    "        V_inv = np.linalg.inv(V)\n",
    "        _, logdet = slogdet(np.pi * V)\n",
    "\n",
    "        logz = -gamma(nu / 2.0 + d / 2.0) + gamma(nu / 2.0) + 0.5 * logdet\n",
    "        logp = -0.5 * (nu + d) * np.log(1 + np.sum(np.dot(xm, V_inv) * xm, axis=1))\n",
    "\n",
    "        logp = logp - logz\n",
    "\n",
    "        return logp[0]\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits a t-Student Process regressor\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: np.ndarray, shape=(nsamples, nfeatures)\n",
    "            Training instances to fit the GP.\n",
    "        y: np.ndarray, shape=(nsamples,)\n",
    "            Corresponding continuous target values to `X`.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n1 = X.shape[0]\n",
    "\n",
    "        if self.optimize:\n",
    "            self.optHyp(param_key=self.covfunc.parameters, param_bounds=self.covfunc.bounds)\n",
    "\n",
    "        self.K11 = self.covfunc.K(self.X, self.X)\n",
    "        self.beta1 = np.dot(np.dot(self.y.T, inv(self.K11)), self.y)\n",
    "        self.logp = logpdf(self.y, self.nu, mu=self.mprior, Sigma=self.K11)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5a. Cumulative Regret Calculator:\n",
    "\n",
    "def min_max_array(x):\n",
    "    new_list = []\n",
    "    for i, num in enumerate(x):\n",
    "            new_list.append(np.min(x[0:i+1]))\n",
    "    return new_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5b. Set-seeds:\n",
    "\n",
    "run_num_1 = 111\n",
    "run_num_2 = 222\n",
    "run_num_3 = 333\n",
    "run_num_4 = 444\n",
    "run_num_5 = 555\n",
    "run_num_6 = 666\n",
    "run_num_7 = 777\n",
    "run_num_8 = 888\n",
    "run_num_9 = 999\n",
    "run_num_10 = 1000\n",
    "run_num_11 = 1111\n",
    "run_num_12 = 8888\n",
    "run_num_13 = 1333\n",
    "run_num_14 = 1444\n",
    "run_num_15 = 1555\n",
    "run_num_16 = 1666\n",
    "run_num_17 = 1777\n",
    "run_num_18 = 1888\n",
    "run_num_19 = 1999\n",
    "run_num_20 = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.12170176  1.69069754 11.          0.92695323  3.          0.36579277]. \t  0.8942284251873676 \t 0.9866393347679695\n",
      "init   \t [ 1.49162957  0.22478325  6.          0.85613654 15.          0.19363322]. \t  0.8978194270778079 \t 0.9866393347679695\n",
      "init   \t [ 5.27957805  2.81819356 11.          0.81062146 14.          0.74826845]. \t  0.9866393347679695 \t 0.9866393347679695\n",
      "init   \t [3.81060005 6.24236569 7.         0.95038709 5.         0.9861745 ]. \t  0.9833171695772546 \t 0.9866393347679695\n",
      "init   \t [4.77531134 7.87990376 6.         0.78863691 6.         0.47918609]. \t  0.8970080761532108 \t 0.9866393347679695\n",
      "1      \t [4.26395039 7.01189993 6.5300658  0.87437512 5.46993376 0.74792339]. \t  0.9793468982699757 \t 0.9866393347679695\n",
      "2      \t [3.78175245 7.07970621 7.28191963 0.81587231 4.93499351 0.71065767]. \t  0.9827410730465015 \t 0.9866393347679695\n",
      "3      \t [4.43067047 6.67488596 7.459413   0.91754713 5.28506766 0.94602244]. \t  0.9831299358867144 \t 0.9866393347679695\n",
      "4      \t [3.67459855 6.7757184  7.26590645 0.99844901 5.69496553 0.99737451]. \t  0.9829427069670954 \t 0.9866393347679695\n",
      "5      \t [3.95092532 6.50415193 7.22231602 0.59309103 5.43916948 0.35680518]. \t  0.8971232983910578 \t 0.9866393347679695\n",
      "6      \t [3.97600082 6.79775047 7.09496539 0.5        5.2889828  1.        ]. \t  \u001b[92m0.9925251357192764\u001b[0m \t 0.9925251357192764\n",
      "7      \t [4.43350748 7.75341213 7.05998506 0.90800464 5.85676936 0.52597695]. \t  0.8954190077762977 \t 0.9925251357192764\n",
      "8      \t [ 4.64195671  2.26936663 11.00000003  0.85669599 14.3335335   0.75219383]. \t  0.9865337177770837 \t 0.9925251357192764\n",
      "9      \t [ 4.77191329  2.79287621 10.29605906  0.86360357 14.21079029  0.77757771]. \t  0.9865769228233038 \t 0.9925251357192764\n",
      "10     \t [ 4.86975455  2.28575826 10.61911352  0.87318284 13.56235922  0.75565219]. \t  0.9862168623792242 \t 0.9925251357192764\n",
      "11     \t [ 5.22644365  2.24819197 10.52556311  0.63317699 14.22027857  0.35691183]. \t  0.8960239220531044 \t 0.9925251357192764\n",
      "12     \t [ 4.57942732  2.79494378 10.87028877  0.72324395 13.87193328  0.24395638]. \t  0.8948717260184154 \t 0.9925251357192764\n",
      "13     \t [ 4.80633561  2.57236893 10.75064515  0.5        13.98852299  1.        ]. \t  \u001b[92m0.994243821862313\u001b[0m \t 0.994243821862313\n",
      "14     \t [4.85415575 7.22221265 6.56755592 0.93345273 6.48131513 0.82141371]. \t  0.9793468976476816 \t 0.994243821862313\n",
      "15     \t [5.3289585  7.3255982  6.62125441 0.80214477 5.85358846 0.38292294]. \t  0.8977762150480656 \t 0.994243821862313\n",
      "16     \t [4.5869199  7.24730948 6.45090879 1.         6.31263454 0.1       ]. \t  0.9083044496517679 \t 0.994243821862313\n",
      "17     \t [3.51428277 6.3446039  7.93077403 0.8535161  4.93966655 1.        ]. \t  0.9938981645523265 \t 0.994243821862313\n",
      "18     \t [5.08806378 7.88998902 6.63247692 0.5        6.69255781 0.35723501]. \t  0.8982418870898158 \t 0.994243821862313\n",
      "19     \t [4.6556241  7.354207   6.57822234 0.5        6.13332183 0.58173759]. \t  0.8977906187353089 \t 0.994243821862313\n",
      "20     \t [5.34077457 8.10184774 6.58542728 1.         6.28424864 0.96341005]. \t  0.9827986854445849 \t 0.994243821862313\n",
      "21     \t [3.83486046 6.61941423 7.39592854 1.         5.15923622 0.8820747 ]. \t  0.9869129834601581 \t 0.994243821862313\n",
      "22     \t [2.79922213 6.40416352 7.36986786 0.5        4.86810968 0.94716593]. \t  0.9835092017772201 \t 0.994243821862313\n",
      "23     \t [3.4693175  6.27172166 7.5552712  0.5        4.14280493 0.87318087]. \t  0.9835524095891915 \t 0.994243821862313\n",
      "24     \t [3.28119598 5.52943843 7.59284633 0.5        4.79857559 1.        ]. \t  0.992635553985339 \t 0.994243821862313\n",
      "25     \t [3.03189563 6.08474708 8.00753939 0.5        4.61905691 0.34395967]. \t  0.8957550673759953 \t 0.994243821862313\n",
      "26     \t [5.61496783 7.8561805  5.94055072 1.         6.7299732  0.50037666]. \t  0.9086597128310098 \t 0.994243821862313\n",
      "27     \t [2.81685353 5.85301457 7.76533562 1.         4.32641392 1.        ]. \t  0.9940277868127959 \t 0.994243821862313\n",
      "28     \t [3.04447581 5.81308132 6.94190556 0.78950811 4.41690571 0.51503898]. \t  0.896955271114957 \t 0.994243821862313\n",
      "29     \t [4.78177522 8.27897927 6.14023908 1.         6.9815245  0.80830228]. \t  0.9827794821416158 \t 0.994243821862313\n",
      "30     \t [5.01084664 7.91142197 6.38169093 1.         6.5232582  0.53321658]. \t  0.9081796261081552 \t 0.994243821862313\n"
     ]
    }
   ],
   "source": [
    "### 6(a). Bayesian optimization runs (x20): GP run number = 1\n",
    "\n",
    "np.random.seed(run_num_1)\n",
    "surrogate_gp_1 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.15, random_state=run_num_1)\n",
    "\n",
    "def f_syn_polarity1(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    clf = XGBClassifier(reg_alpha=alpha, gamma=gamma,max_depth=int(max_depth),subsample=subsample,\n",
    "                              min_child_weight=min_child_weight,colsample_bytree=colsample,objective=obj_classifier, \n",
    "                             booster='gbtree', n_estimators = 5,\n",
    "                             silent=None, random_state=run_num_1)\n",
    "    score = np.array(cross_val_score(clf, X=X_train1, y=y_train1).mean())\n",
    "    return score\n",
    "\n",
    "gpgo_gp_1 = GPGO(surrogate_gp_1, Acquisition_new(util_gp), f_syn_polarity1, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_1.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.12170176  1.69069754 11.          0.92695323  3.          0.36579277]. \t  0.8942284251873676 \t 0.9866393347679695\n",
      "init   \t [ 1.49162957  0.22478325  6.          0.85613654 15.          0.19363322]. \t  0.8978194270778079 \t 0.9866393347679695\n",
      "init   \t [ 5.27957805  2.81819356 11.          0.81062146 14.          0.74826845]. \t  0.9866393347679695 \t 0.9866393347679695\n",
      "init   \t [3.81060005 6.24236569 7.         0.95038709 5.         0.9861745 ]. \t  0.9833171695772546 \t 0.9866393347679695\n",
      "init   \t [4.77531134 7.87990376 6.         0.78863691 6.         0.47918609]. \t  0.8970080761532108 \t 0.9866393347679695\n",
      "1      \t [4.2734787  7.0280739  6.52018953 0.8727775  5.47981057 0.74291623]. \t  0.9791836695033004 \t 0.9866393347679695\n",
      "2      \t [3.58466967 6.23229731 6.60352003 0.95553562 6.01626193 0.98701028]. \t  0.9791308608004261 \t 0.9866393347679695\n",
      "3      \t [4.27619005 5.89416536 6.2609122  0.90830309 5.45031527 0.78548052]. \t  0.9792844880539043 \t 0.9866393347679695\n",
      "4      \t [3.56029615 6.31078319 6.42494094 0.90342101 5.35973873 0.26821708]. \t  0.896864058848471 \t 0.9866393347679695\n",
      "5      \t [4.21305401 6.15908924 7.08779101 0.89613359 5.76178423 0.50823795]. \t  0.8960959206450537 \t 0.9866393347679695\n",
      "6      \t [3.894936   6.28356661 6.57914402 0.5        5.49398422 0.95969975]. \t  0.9793901040076333 \t 0.9866393347679695\n",
      "7      \t [4.35991005 6.82800264 5.92766895 1.         6.27938426 0.50255727]. \t  0.9097158891702394 \t 0.9866393347679695\n",
      "8      \t [3.97778144 6.35548139 6.52659818 1.         5.55181031 0.85924908]. \t  0.9827074695826221 \t 0.9866393347679695\n",
      "9      \t [3.49931027 5.33279488 6.86276964 0.826144   5.4287785  0.82687019]. \t  0.9794669190863919 \t 0.9866393347679695\n",
      "10     \t [4.85855571 6.98337754 6.08159044 0.50372398 5.55557953 0.11259512]. \t  0.897660989491317 \t 0.9866393347679695\n",
      "11     \t [4.11748738 7.40916175 6.51240543 0.5        6.12925754 0.19676351]. \t  0.8975217731160353 \t 0.9866393347679695\n",
      "12     \t [4.15302456 5.58687899 6.83724674 0.6245606  4.6736999  0.46526982]. \t  0.8968352519579913 \t 0.9866393347679695\n",
      "13     \t [4.9067487  7.18544341 6.48923753 0.5        6.18503176 0.8147823 ]. \t  0.979351697747414 \t 0.9866393347679695\n",
      "14     \t [4.88645716 7.34996759 6.63914554 1.         5.97901204 0.25704299]. \t  0.9083140487449325 \t 0.9866393347679695\n",
      "15     \t [3.76640462 5.6073879  6.28703496 0.57314802 6.22514259 0.42858093]. \t  0.8983523101267995 \t 0.9866393347679695\n",
      "16     \t [3.78856169 5.80778098 6.71490654 0.76898824 5.42605128 0.66193703]. \t  0.8974881544405239 \t 0.9866393347679695\n",
      "17     \t [3.47662841 5.12970892 7.23998327 1.         4.49852857 1.        ]. \t  \u001b[92m0.9940757945170685\u001b[0m \t 0.9940757945170685\n",
      "18     \t [3.97792732 4.61627158 6.70102897 1.         5.01643508 1.        ]. \t  0.9896014360574158 \t 0.9940757945170685\n",
      "19     \t [4.41383106 5.35743426 7.35048339 1.         4.7972386  1.        ]. \t  0.9940277866745083 \t 0.9940757945170685\n",
      "20     \t [4.1964601  5.55732939 6.5891927  1.         4.31640736 1.        ]. \t  0.9896014360574158 \t 0.9940757945170685\n",
      "21     \t [4.44436112 6.05854465 6.48070356 0.5234797  6.46633466 1.        ]. \t  0.9890445418616188 \t 0.9940757945170685\n",
      "22     \t [4.36607207 6.63514983 6.36789172 0.51988982 6.03237349 0.59490904]. \t  0.8966864203790436 \t 0.9940757945170685\n",
      "23     \t [4.03483105 5.17705411 6.65672439 0.97039911 6.37785125 1.        ]. \t  0.9894718145575278 \t 0.9940757945170685\n",
      "24     \t [4.99039853 6.25231884 6.86939857 0.64792552 5.09346421 1.        ]. \t  0.9891645615025908 \t 0.9940757945170685\n",
      "25     \t [3.65579804 5.5239662  5.79375594 0.84775904 6.27385951 1.        ]. \t  0.9842005199235264 \t 0.9940757945170685\n",
      "26     \t [4.98756125 6.10037958 7.10569399 1.         6.11800459 1.        ]. \t  0.994013384370141 \t 0.9940757945170685\n",
      "27     \t [3.93794495 5.14211539 6.91428013 0.51794912 4.74485855 1.        ]. \t  0.9892173706203278 \t 0.9940757945170685\n",
      "28     \t [3.58442946 7.16516323 5.71876451 0.87868175 5.40340572 0.1       ]. \t  0.8974545564390035 \t 0.9940757945170685\n",
      "29     \t [4.04139464 6.63909855 6.12944459 0.5716031  4.48399588 0.67527318]. \t  0.9796781539670326 \t 0.9940757945170685\n",
      "30     \t [3.1715041  6.40534623 5.89737235 0.72122488 5.08026874 1.        ]. \t  0.9841189066810613 \t 0.9940757945170685\n"
     ]
    }
   ],
   "source": [
    "### 6(a). Bayesian optimization runs (x20): STP DF1 run number = 1\n",
    "\n",
    "np.random.seed(run_num_1)\n",
    "surrogate_stp_df1_1 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_1 = GPGO(surrogate_stp_df1_1, Acquisition_new(util_stp), f_syn_polarity1, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_1.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.157481542262337, -5.1287086966603415)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(a). Training Regret Minimisation: run number = 1\n",
    "\n",
    "gp_output_1 = np.append(np.max(gpgo_gp_1.GP.y[0:n_init]),gpgo_gp_1.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_1 = np.append(np.max(gpgo_stp_df1_1.GP.y[0:n_init]),gpgo_stp_df1_1.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_1 = np.log(y_global_orig - gp_output_1)\n",
    "regret_stp_df1_1 = np.log(y_global_orig - stp_df1_output_1)\n",
    "\n",
    "train_regret_gp_1 = min_max_array(regret_gp_1)\n",
    "train_regret_stp_df1_1 = min_max_array(regret_stp_df1_1)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 1\n",
    "min_train_regret_gp_1 = min(train_regret_gp_1)\n",
    "min_train_regret_stp_df1_1 = min(train_regret_stp_df1_1)\n",
    "\n",
    "min_train_regret_gp_1, min_train_regret_stp_df1_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [5.26028426 6.85317185 7.         0.94823061 6.         0.13331976]. \t  0.8151063793757732 \t 0.985290307470739\n",
      "init   \t [ 1.80155663  7.60569419 13.          0.54810697  8.          0.35874142]. \t  0.8167434741257803 \t 0.985290307470739\n",
      "init   \t [ 7.32780005  1.0764748   8.          0.89159454 19.          0.83473675]. \t  0.985290307470739 \t 0.985290307470739\n",
      "init   \t [4.68533467 8.47744556 5.         0.77589728 4.         0.9820415 ]. \t  0.9737587521793104 \t 0.985290307470739\n",
      "init   \t [0.64660223 1.7685673  9.         0.59891157 5.         0.42323959]. \t  0.8158601174182804 \t 0.985290307470739\n",
      "1      \t [4.86093057 7.98138477 5.61082137 0.82852732 4.61081523 0.72283992]. \t  0.9734178926873115 \t 0.985290307470739\n",
      "2      \t [4.1696363  8.00823624 5.01622419 0.96604538 4.47079443 0.66798221]. \t  0.9736387333680638 \t 0.985290307470739\n",
      "3      \t [4.86361472 7.65616658 5.00303193 0.99612356 4.00985553 0.70075134]. \t  0.9736531343586993 \t 0.985290307470739\n",
      "4      \t [4.82231486 8.27077445 5.00220115 0.76125054 4.31448056 0.16474879]. \t  0.8117650106746579 \t 0.985290307470739\n",
      "5      \t [4.3488295  7.93002903 5.5525408  0.60536964 3.87599714 0.72804516]. \t  0.9742484308735199 \t 0.985290307470739\n",
      "6      \t [4.61708739 7.81862292 5.03203001 0.5        4.38515933 1.        ]. \t  0.9838980715318116 \t 0.985290307470739\n",
      "7      \t [4.54483901 8.00505059 5.35981426 1.         4.1767544  1.        ]. \t  0.9842005222744152 \t 0.985290307470739\n",
      "8      \t [4.7244539  6.99643447 6.11534628 0.81911111 5.11851199 0.22602803]. \t  0.8155720789367491 \t 0.985290307470739\n",
      "9      \t [ 7.50820564  0.9078754   7.52144393  0.96863682 19.71820262  0.9459114 ]. \t  0.9832019521103286 \t 0.985290307470739\n",
      "10     \t [ 7.42331803  0.24596417  7.94654788  0.98684338 19.30725223  0.8265061 ]. \t  0.9832499616814833 \t 0.985290307470739\n",
      "11     \t [ 8.07139004  0.73520094  7.69419448  0.9936578  19.06874802  0.96658369]. \t  0.9831827508816731 \t 0.985290307470739\n",
      "12     \t [ 7.82138189  0.86130572  8.34595307  0.97509102 19.60515106  0.91661447]. \t  0.9849734526951736 \t 0.985290307470739\n",
      "13     \t [ 7.74758472  0.77699084  7.84256217  0.64788927 19.39773016  0.3084007 ]. \t  0.8168827009417733 \t 0.985290307470739\n",
      "14     \t [5.55206989 7.42098972 6.67649012 0.55595732 5.24744171 0.47005925]. \t  0.8156680941378628 \t 0.985290307470739\n",
      "15     \t [4.9212928  7.68606164 6.33128569 0.91129937 5.9568544  0.44738962]. \t  0.8161433652036335 \t 0.985290307470739\n",
      "16     \t [5.62635324 7.00009535 5.93040027 1.         5.71998163 0.49254198]. \t  0.8851501160851384 \t 0.985290307470739\n",
      "17     \t [ 7.59349193  0.69569978  7.92741839  0.5        19.33480544  1.        ]. \t  \u001b[92m0.9923667038717197\u001b[0m \t 0.9923667038717197\n",
      "18     \t [5.10899197 6.94372484 6.54670453 1.         5.59879197 1.        ]. \t  0.9892893842856219 \t 0.9923667038717197\n",
      "19     \t [5.13121926 6.82609205 6.3152358  0.5        6.01067728 0.71673259]. \t  0.9795677433067862 \t 0.9923667038717197\n",
      "20     \t [5.09308184 7.24115305 5.82624922 0.5        5.35623818 1.        ]. \t  0.9839172746964934 \t 0.9923667038717197\n",
      "21     \t [ 7.58906155  0.77074787  7.91336674  1.         19.32253602  1.        ]. \t  \u001b[92m0.9929908104576338\u001b[0m \t 0.9929908104576338\n",
      "22     \t [ 8.28151874  0.08990873  7.6851128   0.88592353 20.          0.99649828]. \t  0.9836292252902442 \t 0.9929908104576338\n",
      "23     \t [ 7.75512571  0.6236738   8.7180773   0.71554739 18.44837773  0.72268317]. \t  0.9850502661836251 \t 0.9929908104576338\n",
      "24     \t [ 7.99893507  1.57848219  8.53793221  0.58953883 18.57793344  0.87017015]. \t  0.9849254467886395 \t 0.9929908104576338\n",
      "25     \t [ 7.2408399   1.31938519  9.02800872  0.5223827  18.85423033  0.69929392]. \t  0.985405530054305 \t 0.9929908104576338\n",
      "26     \t [ 7.73581679  0.05472958  6.85182184  0.89269225 19.74510635  0.96155059]. \t  0.9798461880883681 \t 0.9929908104576338\n",
      "27     \t [ 7.20871315  1.32778986  8.43497173  0.5        18.0631566   0.70989181]. \t  0.9848438339610369 \t 0.9929908104576338\n",
      "28     \t [ 7.51562606  1.40378112  8.69697749  1.         18.44262756  0.33885745]. \t  0.8870368382855364 \t 0.9929908104576338\n",
      "29     \t [ 7.41785306  1.23407008  8.71730019  0.82959107 18.46927975  1.        ]. \t  \u001b[92m0.9939509730477692\u001b[0m \t 0.9939509730477692\n",
      "30     \t [ 8.37590458  0.7697777   6.9154575   1.         20.          1.        ]. \t  0.9888621122811506 \t 0.9939509730477692\n"
     ]
    }
   ],
   "source": [
    "### 6(b). Bayesian optimization runs (x20): GP run number = 2\n",
    "\n",
    "np.random.seed(run_num_2)\n",
    "surrogate_gp_2 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.15, random_state=run_num_2)\n",
    "\n",
    "def f_syn_polarity2(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    clf = XGBClassifier(reg_alpha=alpha, gamma=gamma,max_depth=int(max_depth),subsample=subsample,\n",
    "                              min_child_weight=min_child_weight,colsample_bytree=colsample,objective=obj_classifier, \n",
    "                             booster='gbtree', n_estimators = 5,\n",
    "                             silent=None, random_state=run_num_2)\n",
    "    score = np.array(cross_val_score(clf, X=X_train2, y=y_train2).mean())\n",
    "    return score\n",
    "\n",
    "gpgo_gp_2 = GPGO(surrogate_gp_2, Acquisition_new(util_gp), f_syn_polarity2, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_2.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [5.26028426 6.85317185 7.         0.94823061 6.         0.13331976]. \t  0.8151063793757732 \t 0.985290307470739\n",
      "init   \t [ 1.80155663  7.60569419 13.          0.54810697  8.          0.35874142]. \t  0.8167434741257803 \t 0.985290307470739\n",
      "init   \t [ 7.32780005  1.0764748   8.          0.89159454 19.          0.83473675]. \t  0.985290307470739 \t 0.985290307470739\n",
      "init   \t [4.68533467 8.47744556 5.         0.77589728 4.         0.9820415 ]. \t  0.9737587521793104 \t 0.985290307470739\n",
      "init   \t [0.64660223 1.7685673  9.         0.59891157 5.         0.42323959]. \t  0.8158601174182804 \t 0.985290307470739\n",
      "1      \t [4.90951601 7.84413092 5.77979917 0.84309065 4.77980794 0.65111672]. \t  0.8116449891668037 \t 0.985290307470739\n",
      "2      \t [5.69694782 8.24737973 5.00827062 0.81963511 4.33214951 0.99278512]. \t  0.9733458808197558 \t 0.985290307470739\n",
      "3      \t [5.3250049  8.60930356 5.80333236 0.77879818 3.92640928 0.95729029]. \t  0.9737683537616512 \t 0.985290307470739\n",
      "4      \t [5.13449385 8.89620334 5.31699481 0.75864704 4.71887739 0.99642138]. \t  0.9737059418861292 \t 0.985290307470739\n",
      "5      \t [5.23616335 8.54633086 5.22747337 0.52878411 4.22170539 0.30573036]. \t  0.8114481549310796 \t 0.985290307470739\n",
      "6      \t [5.13935146 8.39154759 5.35362071 1.         4.30519701 1.        ]. \t  0.9842005222744152 \t 0.985290307470739\n",
      "7      \t [5.18811888 8.38572294 5.35733664 0.5        4.30546985 1.        ]. \t  0.9839172746964934 \t 0.985290307470739\n",
      "8      \t [5.46097213 9.14461848 5.         0.75990238 3.82099458 1.        ]. \t  0.9840997058672684 \t 0.985290307470739\n",
      "9      \t [5.7743682  7.75464326 6.47639602 0.86195761 5.43711299 0.48564782]. \t  0.8152456205736732 \t 0.985290307470739\n",
      "10     \t [6.23860467 8.94737502 5.65019365 0.78330927 4.50943964 0.97092915]. \t  0.9738067600218705 \t 0.985290307470739\n",
      "11     \t [4.70399132 7.70510854 6.97236136 0.82713372 5.43746054 0.28439456]. \t  0.8150631716329458 \t 0.985290307470739\n",
      "12     \t [5.23705239 6.92519141 6.72809159 0.88620744 4.89105785 0.23314519]. \t  0.8144966884389765 \t 0.985290307470739\n",
      "13     \t [5.00161392 7.17086727 6.1806531  0.5        5.6826307  0.1       ]. \t  0.8173435943875075 \t 0.985290307470739\n",
      "14     \t [5.63380618 8.82418504 5.31167759 0.77099817 4.25115622 1.        ]. \t  0.9841285099228528 \t 0.985290307470739\n",
      "15     \t [4.58101466 9.44897899 5.34489081 0.69508161 3.7149776  0.98133641]. \t  0.9734562996389685 \t 0.985290307470739\n",
      "16     \t [6.48893011 8.0115292  5.87926242 0.78849884 4.47459091 0.91137234]. \t  0.9723473184689166 \t 0.985290307470739\n",
      "17     \t [6.33701859 8.32310449 5.44234835 0.7554814  5.22383998 1.        ]. \t  0.984042095958361 \t 0.985290307470739\n",
      "18     \t [4.92412293 8.93520758 5.04756134 0.75890854 3.07786173 0.97779136]. \t  0.9737107427464434 \t 0.985290307470739\n",
      "19     \t [5.9298772  8.44364625 6.01691539 0.69952513 4.85052639 1.        ]. \t  \u001b[92m0.9889245233269471\u001b[0m \t 0.9889245233269471\n",
      "20     \t [6.35318002 8.42728844 5.64452101 1.         4.81581689 0.45496896]. \t  0.8856446063569486 \t 0.9889245233269471\n",
      "21     \t [4.92173068 8.97860055 5.16770097 0.75048991 3.73552748 0.9572214 ]. \t  0.9720400654831235 \t 0.9889245233269471\n",
      "22     \t [5.36573608 7.77200271 5.26263567 0.80444652 3.24192942 0.97305574]. \t  0.973312275903857 \t 0.9889245233269471\n",
      "23     \t [5.38356255 9.90709754 5.42477009 0.66211799 3.16447136 1.        ]. \t  0.984133309815154 \t 0.9889245233269471\n",
      "24     \t [5.99083157 8.80355599 5.30024866 0.73918118 2.99472405 0.98317153]. \t  0.9735619168372858 \t 0.9889245233269471\n",
      "25     \t [5.92114352 7.9827774  5.90579722 0.60424891 3.552193   0.66468546]. \t  0.8115825804718958 \t 0.9889245233269471\n",
      "26     \t [ 5.09591286 10.          5.74251794  0.56868624  4.25460273  1.        ]. \t  0.984075700805116 \t 0.9889245233269471\n",
      "27     \t [ 4.99477982 10.          5.          0.5         3.8879794   1.        ]. \t  0.9838884698111833 \t 0.9889245233269471\n",
      "28     \t [ 5.12029149 10.          5.38043626  1.          3.82008466  0.59938961]. \t  0.8851741219770163 \t 0.9889245233269471\n",
      "29     \t [4.22095737 9.75359839 5.39988954 0.5962922  4.74955782 0.97959572]. \t  0.9744212602545229 \t 0.9889245233269471\n",
      "30     \t [5.51021334 8.40067545 5.         1.         3.25318619 1.        ]. \t  0.9842005222744152 \t 0.9889245233269471\n"
     ]
    }
   ],
   "source": [
    "### 6(b). Bayesian optimization runs (x20): STP DF1 run number = 2\n",
    "\n",
    "np.random.seed(run_num_2)\n",
    "surrogate_stp_df1_2 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_2 = GPGO(surrogate_stp_df1_2, Acquisition_new(util_stp), f_syn_polarity2, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_2.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.1078578542166415, -4.503021923520911)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(b). Training Regret Minimisation: run number = 2\n",
    "\n",
    "gp_output_2 = np.append(np.max(gpgo_gp_2.GP.y[0:n_init]),gpgo_gp_2.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_2 = np.append(np.max(gpgo_stp_df1_2.GP.y[0:n_init]),gpgo_stp_df1_2.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_2 = np.log(y_global_orig - gp_output_2)\n",
    "regret_stp_df1_2 = np.log(y_global_orig - stp_df1_output_2)\n",
    "\n",
    "train_regret_gp_2 = min_max_array(regret_gp_2)\n",
    "train_regret_stp_df1_2 = min_max_array(regret_stp_df1_2)\n",
    "\n",
    "# GP, STP df1- training regret minimization: run number = 2\n",
    "min_train_regret_gp_2 = min(train_regret_gp_2)\n",
    "min_train_regret_stp_df1_2 = min(train_regret_stp_df1_2)\n",
    "\n",
    "min_train_regret_gp_2, min_train_regret_stp_df1_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.43291087  7.28950728  8.          0.66640751 15.          0.43184964]. \t  0.9065377491537258 \t 0.9901631579395763\n",
      "init   \t [ 0.48303666  1.04530188 11.          0.98399654 13.          0.62888345]. \t  0.9046558247711304 \t 0.9901631579395763\n",
      "init   \t [ 9.6557048   4.68513871 10.          0.6018496   9.          0.12912301]. \t  0.9074066957437422 \t 0.9901631579395763\n",
      "init   \t [ 3.2007756   5.90000752 14.          0.9799891   4.          0.75368544]. \t  0.9901631579395763 \t 0.9901631579395763\n",
      "init   \t [ 4.84228675  1.26861034 13.          0.86540685 17.          0.98837363]. \t  0.9896014694519493 \t 0.9901631579395763\n",
      "1      \t [ 2.83918184  6.44340525 14.27805963  1.          3.44389232  0.77426256]. \t  \u001b[92m0.991656188575034\u001b[0m \t 0.991656188575034\n",
      "2      \t [ 3.30787206  6.72177194 14.29223823  0.99945053  4.14598392  0.75044251]. \t  0.9900527405032052 \t 0.991656188575034\n",
      "3      \t [ 3.68566625  6.38891902 14.07103318  0.99190192  3.47002746  0.90769189]. \t  0.9899615251252349 \t 0.991656188575034\n",
      "4      \t [ 3.41056005  6.12613242 14.77509739  0.99994433  3.73482954  0.64755606]. \t  0.9052127100475884 \t 0.991656188575034\n",
      "5      \t [ 3.32388187  6.42041672 14.00395647  0.99993012  3.67413415  0.13303336]. \t  0.9054575524024697 \t 0.991656188575034\n",
      "6      \t [ 3.24830114  6.37392304 14.16902838  0.5         3.76136511  0.79110621]. \t  0.9891597838036237 \t 0.991656188575034\n",
      "7      \t [ 4.5722076   0.40744115 12.99829553  0.85878863 16.99960657  0.9950444 ]. \t  0.9895678617703231 \t 0.991656188575034\n",
      "8      \t [ 4.87203757  0.78521958 13.74782423  0.82839951 16.99944283  1.        ]. \t  \u001b[92m0.9953096063518676\u001b[0m \t 0.9953096063518676\n",
      "9      \t [ 5.27855003  0.65804715 13.13214342  0.97026608 16.56063939  0.99682215]. \t  0.9898895171988032 \t 0.9953096063518676\n",
      "10     \t [ 4.4774936   0.90881154 13.32711042  0.99459446 16.37357839  0.99999598]. \t  0.9898943172294038 \t 0.9953096063518676\n",
      "11     \t [ 4.81060721  0.80168013 13.23987864  0.69079494 16.74208177  0.35496009]. \t  0.9055631684253842 \t 0.9953096063518676\n",
      "12     \t [ 4.81769883  0.80910351 13.21260108  0.5        16.66241923  1.        ]. \t  0.9942726234280043 \t 0.9953096063518676\n",
      "13     \t [ 4.80566147  0.80503529 13.25084948  1.         16.84192024  0.97615215]. \t  0.9912049111626416 \t 0.9953096063518676\n",
      "14     \t [ 5.30539213  1.64431272 13.76684073  0.82145141 16.34859894  0.89083886]. \t  0.9895390524598708 \t 0.9953096063518676\n",
      "15     \t [ 5.15595034  1.64463093 12.7465573   0.94497801 15.99161499  0.84037871]. \t  0.9897166873337778 \t 0.9953096063518676\n",
      "16     \t [ 5.86393363  1.64156749 12.93697076  0.70877235 16.69482388  0.80342654]. \t  0.9889341471025683 \t 0.9953096063518676\n",
      "17     \t [ 5.04718522  2.24237682 13.03715724  0.63412456 16.68620668  0.68742446]. \t  0.9890733760619609 \t 0.9953096063518676\n",
      "18     \t [ 4.35022919  0.         13.84917606  0.74166062 16.58220094  0.91268667]. \t  0.9896302737841207 \t 0.9953096063518676\n",
      "19     \t [ 3.73938121  0.38229574 13.63038955  0.56930429 17.23273061  0.93753305]. \t  0.9896398749516068 \t 0.9953096063518676\n",
      "20     \t [ 4.46280006  0.         13.70489295  0.5        17.57578487  0.83627834]. \t  0.9888189267315995 \t 0.9953096063518676\n",
      "21     \t [ 5.31457071  1.84269803 13.0825764   1.         16.56794574  1.        ]. \t  0.995141574166588 \t 0.9953096063518676\n",
      "22     \t [ 5.27133769  1.56711855 13.11827016  0.66006048 16.42656028  0.55321167]. \t  0.9055631781053745 \t 0.9953096063518676\n",
      "23     \t [ 5.23951245  1.91448129 12.0772404   0.51913462 16.77219065  1.        ]. \t  0.9944070464105165 \t 0.9953096063518676\n",
      "24     \t [ 5.72290169  1.0508715  12.04445797  0.91716309 16.42205772  1.        ]. \t  0.995151176993501 \t 0.9953096063518676\n",
      "25     \t [4.04563423e+00 9.86218175e-03 1.35401372e+01 7.95554652e-01\n",
      " 1.71338481e+01 2.54580491e-01]. \t  0.9043581689425114 \t 0.9953096063518676\n",
      "26     \t [ 4.27747089  1.93901354 13.96202154  0.55099084 16.57879016  1.        ]. \t  0.9950407585891309 \t 0.9953096063518676\n",
      "27     \t [ 4.17350295  0.82504368 14.26653298  0.5        16.84150365  0.56023678]. \t  0.9057648046968648 \t 0.9953096063518676\n",
      "28     \t [ 4.16085197  2.09850355 12.77765792  0.56877092 16.41692674  1.        ]. \t  0.9950983690511865 \t 0.9953096063518676\n",
      "29     \t [ 4.5256698   2.16038794 13.54918939  0.74953516 15.76673233  1.        ]. \t  0.9951703798816154 \t 0.9953096063518676\n",
      "30     \t [ 5.88008382  0.93838892 12.37958254  0.56505169 17.29961915  1.        ]. \t  0.9941238033959726 \t 0.9953096063518676\n"
     ]
    }
   ],
   "source": [
    "### 6(c). Bayesian optimization runs (x20): GP run number = 3\n",
    "\n",
    "np.random.seed(run_num_3)\n",
    "surrogate_gp_3 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=0.15, random_state=run_num_3)\n",
    "\n",
    "def f_syn_polarity3(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    clf = XGBClassifier(reg_alpha=alpha, gamma=gamma,max_depth=int(max_depth),subsample=subsample,\n",
    "                              min_child_weight=min_child_weight,colsample_bytree=colsample,objective=obj_classifier, \n",
    "                             booster='gbtree', n_estimators = 5,\n",
    "                             silent=None, random_state=run_num_3)\n",
    "    score = np.array(cross_val_score(clf, X=X_train3, y=y_train3).mean())\n",
    "    return score\n",
    "\n",
    "gpgo_gp_3 = GPGO(surrogate_gp_3, Acquisition_new(util_gp), f_syn_polarity3, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_3.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.43291087  7.28950728  8.          0.66640751 15.          0.43184964]. \t  0.9065377491537258 \t 0.9901631579395763\n",
      "init   \t [ 0.48303666  1.04530188 11.          0.98399654 13.          0.62888345]. \t  0.9046558247711304 \t 0.9901631579395763\n",
      "init   \t [ 9.6557048   4.68513871 10.          0.6018496   9.          0.12912301]. \t  0.9074066957437422 \t 0.9901631579395763\n",
      "init   \t [ 3.2007756   5.90000752 14.          0.9799891   4.          0.75368544]. \t  0.9901631579395763 \t 0.9901631579395763\n",
      "init   \t [ 4.84228675  1.26861034 13.          0.86540685 17.          0.98837363]. \t  0.9896014694519493 \t 0.9901631579395763\n",
      "1      \t [ 2.52597163  6.29929221 14.00000036  0.5         4.00000036  0.1       ]. \t  0.9054623625279173 \t 0.9901631579395763\n",
      "2      \t [ 2.89470865  6.0172221  14.96146823  0.74353917  4.00000003  0.47834438]. \t  0.9057167985828637 \t 0.9901631579395763\n",
      "3      \t [ 3.29408862  6.62168792 14.30589426  0.77508907  4.54398873  0.41211648]. \t  0.9055727780282904 \t 0.9901631579395763\n",
      "4      \t [ 3.51930428  6.16079659 14.27676666  0.50004134  3.67894092  0.10005967]. \t  0.9056832052829374 \t 0.9901631579395763\n",
      "5      \t [ 2.95724557  6.67939961 14.30933201  1.          3.59008064  0.62993503]. \t  0.876095783471579 \t 0.9901631579395763\n",
      "6      \t [ 3.03422626  6.06498471 14.28706995  1.          4.09607422  0.1       ]. \t  0.876038173839237 \t 0.9901631579395763\n",
      "7      \t [ 3.03317692  6.22988312 14.26950812  0.5         4.03402813  0.74939801]. \t  0.9892654027305353 \t 0.9901631579395763\n",
      "8      \t [ 3.36305331  6.57977079 13.18182173  0.68632734  3.96587903  0.42013432]. \t  0.905419145934813 \t 0.9901631579395763\n",
      "9      \t [ 4.87047205  0.31948571 13.00000418  0.92187702 16.75099861  0.99836126]. \t  0.9896542787771195 \t 0.9901631579395763\n",
      "10     \t [ 4.57227015  0.95853297 12.96330593  0.8418674  16.19687084  0.80521439]. \t  0.989793499577663 \t 0.9901631579395763\n",
      "11     \t [ 4.12268522  0.76716504 13.01090295  0.92273216 16.90158865  1.        ]. \t  \u001b[92m0.9952952019732125\u001b[0m \t 0.9952952019732125\n",
      "12     \t [ 4.63274123  0.82962555 12.35861783  0.98724528 16.73551214  0.99279584]. \t  0.989932723558775 \t 0.9952952019732125\n",
      "13     \t [ 4.62323406  0.76210467 12.83044441  0.5        16.86278438  0.53118452]. \t  0.9057312030998043 \t 0.9952952019732125\n",
      "14     \t [ 4.64442341  0.84166009 12.94508969  1.         16.69872641  1.        ]. \t  0.9951415743048736 \t 0.9952952019732125\n",
      "15     \t [ 4.19820022  1.57082058 12.62924254  0.5        16.62425326  1.        ]. \t  0.9948391250833617 \t 0.9952952019732125\n",
      "16     \t [ 4.29615906  0.38545601 12.65332937  0.5        16.32974731  1.        ]. \t  0.9949447411062763 \t 0.9952952019732125\n",
      "17     \t [ 5.05784853  1.09202054 12.59725041  0.5        16.41992382  1.        ]. \t  0.9942774241500328 \t 0.9952952019732125\n",
      "18     \t [ 4.34295387  1.26150821 12.51972849  0.5        17.62471647  1.        ]. \t  0.9949495451471587 \t 0.9952952019732125\n",
      "19     \t [ 4.49230983  0.97950366 12.7331559   0.52050803 16.90226541  1.        ]. \t  0.9950359573139601 \t 0.9952952019732125\n",
      "20     \t [ 4.14168385  1.78465164 12.66693644  1.         17.4341125   0.83460763]. \t  0.991296127093754 \t 0.9952952019732125\n",
      "21     \t [ 4.883003    1.93253712 12.29319796  0.63100967 17.19396971  0.89580985]. \t  0.9890685758930746 \t 0.9952952019732125\n",
      "22     \t [ 5.05420346  0.04018173 12.57691962  0.78077636 15.93901144  0.75111748]. \t  0.9895006522150651 \t 0.9952952019732125\n",
      "23     \t [ 4.34031026  0.         12.86159839  1.         16.29998808  0.66784737]. \t  0.991262521071555 \t 0.9952952019732125\n",
      "24     \t [ 4.86672039  1.92408074 12.53645476  0.86367976 16.38621557  0.54206517]. \t  0.904684629379873 \t 0.9952952019732125\n",
      "25     \t [ 4.5003298   1.54914895 12.56813331  0.76725852 17.09955807  0.85886768]. \t  0.9896350776867177 \t 0.9952952019732125\n",
      "26     \t [ 4.41770153  2.05917504 13.05640375  0.5        17.68685771  1.        ]. \t  0.994872731105561 \t 0.9952952019732125\n",
      "27     \t [ 3.55653923  1.47153704 13.07190337  0.60342605 17.68227839  1.        ]. \t  0.9949399425968171 \t 0.9952952019732125\n",
      "28     \t [ 5.03197241  2.32119598 12.91408439  0.5        16.70191462  1.        ]. \t  0.9942678225676902 \t 0.9952952019732125\n",
      "29     \t [ 4.17368202  1.26326165 13.10451269  1.         18.07409303  1.        ]. \t  0.99511277066413 \t 0.9952952019732125\n",
      "30     \t [ 4.72747926  0.28850411 12.47614292  0.89287451 16.33878301  0.7904009 ]. \t  0.9897598930023217 \t 0.9952952019732125\n"
     ]
    }
   ],
   "source": [
    "### 6(c). Bayesian optimization runs (x20): STP DF1 run number = 3\n",
    "\n",
    "np.random.seed(run_num_3)\n",
    "surrogate_stp_df1_3 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_3 = GPGO(surrogate_stp_df1_3, Acquisition_new(util_stp), f_syn_polarity3, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_3.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.362238766542459, -5.359172434222741)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(c). Training Regret Minimisation: run number = 3\n",
    "\n",
    "gp_output_3 = np.append(np.max(gpgo_gp_3.GP.y[0:n_init]),gpgo_gp_3.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_3 = np.append(np.max(gpgo_stp_df1_3.GP.y[0:n_init]),gpgo_stp_df1_3.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_3 = np.log(y_global_orig - gp_output_3)\n",
    "regret_stp_df1_3 = np.log(y_global_orig - stp_df1_output_3)\n",
    "\n",
    "train_regret_gp_3 = min_max_array(regret_gp_3)\n",
    "train_regret_stp_df1_3 = min_max_array(regret_stp_df1_3)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 3\n",
    "min_train_regret_gp_3 = min(train_regret_gp_3)\n",
    "min_train_regret_stp_df1_3 = min(train_regret_stp_df1_3)\n",
    "\n",
    "min_train_regret_gp_3, min_train_regret_stp_df1_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.39361047  8.21294965  8.          0.78209507 13.          0.13522592]. \t  0.8347415658712235 \t 0.9926499150795474\n",
      "init   \t [ 0.65672388  2.7650064   7.          0.85261754 13.          0.71765519]. \t  0.9917329695663795 \t 0.9926499150795474\n",
      "init   \t [ 7.14863772  5.28551197 11.          0.74670457  4.          0.21785319]. \t  0.8357065202641044 \t 0.9926499150795474\n",
      "init   \t [3.48953944 9.17796441 8.         0.54726456 8.         0.57654823]. \t  0.834251863599193 \t 0.9926499150795474\n",
      "init   \t [ 3.03907726  6.22312661 14.          0.97978402  3.          0.96911775]. \t  0.9926499150795474 \t 0.9926499150795474\n",
      "1      \t [ 3.73269969  6.05945236 13.49455955  0.9399098   3.16473106  0.84137088]. \t  0.9925682997627786 \t 0.9926499150795474\n",
      "2      \t [ 3.11946485  6.26532907 13.53703995  0.941204    3.75068206  1.        ]. \t  \u001b[92m0.9969322797140739\u001b[0m \t 0.9969322797140739\n",
      "3      \t [ 3.63473624  6.23056588 14.22444083  1.          3.60821681  0.92045576]. \t  0.9921986707174074 \t 0.9969322797140739\n",
      "4      \t [ 3.50048337  6.85975081 13.73021537  0.92641258  3.27792413  0.93785996]. \t  0.9924914877954644 \t 0.9969322797140739\n",
      "5      \t [ 3.28872256  6.35612941 13.80726931  0.99847285  3.40748254  0.27306411]. \t  0.8375836475200962 \t 0.9969322797140739\n",
      "6      \t [ 3.41500434  6.28547426 13.84249885  0.5         3.35438378  1.        ]. \t  0.9961305426793156 \t 0.9969322797140739\n",
      "7      \t [ 0.70627665  2.74894188  7.78679216  0.84982642 12.60660438  0.53468571]. \t  0.835298471890518 \t 0.9969322797140739\n",
      "8      \t [ 0.87699794  2.92127631  7.57994842  0.90613932 13.62249148  0.6429588 ]. \t  0.8348327942480379 \t 0.9969322797140739\n",
      "9      \t [ 0.          2.55556789  7.41927537  0.6345807  13.26055034  0.52891699]. \t  0.8349768041546217 \t 0.9969322797140739\n",
      "10     \t [ 0.37284647  3.51150752  7.27631285  0.6972639  13.00940448  0.47029096]. \t  0.835792945014893 \t 0.9969322797140739\n",
      "11     \t [ 0.83187747  2.73440031  7.17268513  0.5        13.1137065   0.1       ]. \t  0.8346359520608782 \t 0.9969322797140739\n",
      "12     \t [ 0.66507503  2.91921111  7.4363304   0.5        13.05304361  1.        ]. \t  0.9928131823587623 \t 0.9969322797140739\n",
      "13     \t [ 3.42587156  6.31682955 13.77569019  1.          3.34644622  1.        ]. \t  \u001b[92m0.9969898901761297\u001b[0m \t 0.9969898901761297\n",
      "14     \t [ 2.70066772  6.86109046 14.42026913  0.77075867  3.77058436  0.99559847]. \t  0.9927267274617183 \t 0.9969898901761297\n",
      "15     \t [ 3.45190611  6.9875031  13.96720214  0.6615835   4.39930377  0.87530686]. \t  0.9920402120424989 \t 0.9969898901761297\n",
      "16     \t [ 4.23238806  6.42270915 13.294969    0.66558205  4.00917382  0.68281197]. \t  0.9919393946673293 \t 0.9969898901761297\n",
      "17     \t [ 3.42099762  7.0186651  12.94317043  0.5         3.97218289  0.7631661 ]. \t  0.9922898559491221 \t 0.9969898901761297\n",
      "18     \t [ 3.90173244  6.36068665 12.56356081  0.5350935   3.26196456  0.6375305 ]. \t  0.8358265316771115 \t 0.9969898901761297\n",
      "19     \t [ 2.50481567  7.11169082 13.51087548  0.52581691  3.55327699  0.92824947]. \t  0.992270652646151 \t 0.9969898901761297\n",
      "20     \t [ 2.88240698  7.39572684 13.56261523  1.          4.09366707  1.        ]. \t  0.9969322797140739 \t 0.9969898901761297\n",
      "21     \t [ 3.10790845  6.93472914 13.69677417  0.63948057  3.87255569  0.82399804]. \t  0.9922802533987803 \t 0.9969898901761297\n",
      "22     \t [ 2.17194588  7.07843534 14.06971757  1.          2.88982266  1.        ]. \t  \u001b[92m0.9971243109460725\u001b[0m \t 0.9971243109460725\n",
      "23     \t [ 1.88423486  6.468808   13.85434419  1.          3.49350908  1.        ]. \t  \u001b[92m0.9973067387980157\u001b[0m \t 0.9973067387980157\n",
      "24     \t [ 2.40483096  6.70162859 13.04378663  1.          2.95099943  1.        ]. \t  0.9971099089182722 \t 0.9973067387980157\n",
      "25     \t [ 2.16457426  6.37678175 13.75750631  0.5         2.79559941  1.        ]. \t  0.9965482101975126 \t 0.9973067387980157\n",
      "26     \t [ 4.05204508  7.37862299 13.29581678  1.          4.23167554  1.        ]. \t  0.9967306453785914 \t 0.9973067387980157\n",
      "27     \t [ 3.58833303  6.8046346  13.06732632  1.          4.74809682  1.        ]. \t  0.9968122586210765 \t 0.9973067387980157\n",
      "28     \t [ 3.66919509  6.84657095 12.78920915  1.          3.8763166   1.        ]. \t  0.9968842709726466 \t 0.9973067387980157\n",
      "29     \t [ 2.30630879  6.79908491 12.69775842  0.98955251  4.17455261  1.        ]. \t  0.9970859044784159 \t 0.9973067387980157\n",
      "30     \t [ 4.40619703  6.78604577 14.22666215  1.          4.29493148  1.        ]. \t  0.9967306444105923 \t 0.9973067387980157\n"
     ]
    }
   ],
   "source": [
    "### 6(d). Bayesian optimization runs (x20): GP run number = 4\n",
    "\n",
    "np.random.seed(run_num_4)\n",
    "surrogate_gp_4 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=0.15, random_state=run_num_4)\n",
    "\n",
    "def f_syn_polarity4(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    clf = XGBClassifier(reg_alpha=alpha, gamma=gamma,max_depth=int(max_depth),subsample=subsample,\n",
    "                              min_child_weight=min_child_weight,colsample_bytree=colsample,objective=obj_classifier, \n",
    "                             booster='gbtree', n_estimators = 5,\n",
    "                             silent=None, random_state=run_num_4)\n",
    "    score = np.array(cross_val_score(clf, X=X_train4, y=y_train4).mean())\n",
    "    return score\n",
    "\n",
    "gpgo_gp_4 = GPGO(surrogate_gp_4, Acquisition_new(util_gp), f_syn_polarity4, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_4.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.39361047  8.21294965  8.          0.78209507 13.          0.13522592]. \t  0.8347415658712235 \t 0.9926499150795474\n",
      "init   \t [ 0.65672388  2.7650064   7.          0.85261754 13.          0.71765519]. \t  0.9917329695663795 \t 0.9926499150795474\n",
      "init   \t [ 7.14863772  5.28551197 11.          0.74670457  4.          0.21785319]. \t  0.8357065202641044 \t 0.9926499150795474\n",
      "init   \t [3.48953944 9.17796441 8.         0.54726456 8.         0.57654823]. \t  0.834251863599193 \t 0.9926499150795474\n",
      "init   \t [ 3.03907726  6.22312661 14.          0.97978402  3.          0.96911775]. \t  0.9926499150795474 \t 0.9926499150795474\n",
      "1      \t [ 3.89816267  6.02498016 13.37367365  0.93104749  3.20726584  0.81173586]. \t  0.9925106901304365 \t 0.9926499150795474\n",
      "2      \t [ 3.15634102  6.27063233 13.44400788  0.93412892  3.88151037  1.        ]. \t  \u001b[92m0.9968698685299898\u001b[0m \t 0.9968698685299898\n",
      "3      \t [ 3.41840637  6.88246362 13.36243828  0.9998242   3.1438087   0.99297827]. \t  0.992352261740069 \t 0.9968698685299898\n",
      "4      \t [ 3.72546945  6.54588381 14.07627112  0.93980711  3.57370157  0.8367711 ]. \t  0.99256349904075 \t 0.9968698685299898\n",
      "5      \t [ 3.29260866  6.39405117 13.57525876  0.50707606  3.3145322   0.45172605]. \t  0.8352744361981216 \t 0.9968698685299898\n",
      "6      \t [ 1.27170167  3.15391949  7.31400352  0.85477381 13.62799229  0.71897504]. \t  0.9918049765248123 \t 0.9968698685299898\n",
      "7      \t [ 1.15663975  3.03729108  7.7221983   0.82730421 12.79520848  0.60093404]. \t  0.8355097013089213 \t 0.9968698685299898\n",
      "8      \t [ 3.45882533  6.30827585 13.63572988  1.          3.34147049  1.        ]. \t  \u001b[92m0.9969898901761297\u001b[0m \t 0.9969898901761297\n",
      "9      \t [ 2.61452281  7.00320019 14.11446447  1.          3.62734444  0.93074514]. \t  0.9922802853427483 \t 0.9969898901761297\n",
      "10     \t [ 4.1781882   6.59225    13.00397404  1.          3.90913165  0.55350304]. \t  0.83618661445437 \t 0.9969898901761297\n",
      "11     \t [ 0.95829126  3.53896647  6.86137929  0.84022622 13.10961677  0.37278129]. \t  0.8340886582719157 \t 0.9969898901761297\n",
      "12     \t [ 1.47313395  2.60063681  6.88921653  0.86731934 13.17237938  0.44518361]. \t  0.8343238991827399 \t 0.9969898901761297\n",
      "13     \t [ 3.10054313  7.08813553 14.40894843  1.          2.72974893  0.7511043 ]. \t  0.9921842678598938 \t 0.9969898901761297\n",
      "14     \t [ 0.70666068  2.81425345  7.3979239   0.88484123 13.53686182  0.16508374]. \t  0.8354136818900687 \t 0.9969898901761297\n",
      "15     \t [ 4.18124547  6.64174052 13.75435465  1.          2.63947841  0.4247513 ]. \t  0.8362154196162551 \t 0.9969898901761297\n",
      "16     \t [ 3.10441581  6.78664852 13.98087848  1.          3.31038309  0.60617008]. \t  0.8365226713575081 \t 0.9969898901761297\n",
      "17     \t [ 3.05741126  6.7817936  14.36212923  0.5         3.02496829  1.        ]. \t  0.9961833506216299 \t 0.9969898901761297\n",
      "18     \t [ 4.21136481  6.5753949  13.46696105  0.5         3.38030123  1.        ]. \t  0.9960249241672604 \t 0.9969898901761297\n",
      "19     \t [ 2.32563849  6.89619987 14.16217474  0.97325421  2.6018908   1.        ]. \t  \u001b[92m0.9971819212698425\u001b[0m \t 0.9971819212698425\n",
      "20     \t [ 3.23163633  6.70248254 14.00031419  0.84133214  2.13847521  1.        ]. \t  0.9969034748287601 \t 0.9971819212698425\n",
      "21     \t [ 3.34628454  6.92168035 13.58294541  0.51587179  4.27760626  1.        ]. \t  0.9958376943487184 \t 0.9971819212698425\n",
      "22     \t [ 2.85255801  6.52722713 14.76601359  1.          2.4524305   1.        ]. \t  0.9970955071670432 \t 0.9971819212698425\n",
      "23     \t [ 3.89886535  6.03069973 13.63327665  0.5         4.18379943  0.86391891]. \t  0.992270651678152 \t 0.9971819212698425\n",
      "24     \t [ 2.9637035   6.82173446 14.2517849   1.          2.70106741  1.        ]. \t  0.9969514839850442 \t 0.9971819212698425\n",
      "25     \t [ 2.08398142  6.2802     14.45104575  0.72154037  3.39577164  1.        ]. \t  0.9969946892387314 \t 0.9971819212698425\n",
      "26     \t [ 2.96770081  6.27144923 14.30841204  0.56149241  4.14494302  1.        ]. \t  0.9962025548926 \t 0.9971819212698425\n",
      "27     \t [ 2.21247681  6.71245628 13.66506453  0.5         3.69289407  1.        ]. \t  0.9962265535244623 \t 0.9971819212698425\n",
      "28     \t [ 2.63948857  6.26911845 14.36451618  0.59743951  2.27435524  0.61033256]. \t  0.8362394129932657 \t 0.9971819212698425\n",
      "29     \t [ 3.8301636   5.86646801 14.39130547  0.58827586  2.80715389  0.94857752]. \t  0.9920258095998419 \t 0.9971819212698425\n",
      "30     \t [ 0.96583744  2.97026674  7.17117327  0.5        13.29998912  0.79717971]. \t  0.9915073234619051 \t 0.9971819212698425\n"
     ]
    }
   ],
   "source": [
    "### 6(d). Bayesian optimization runs (x20): STP DF1 run number = 4\n",
    "\n",
    "np.random.seed(run_num_4)\n",
    "surrogate_stp_df1_4 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_4 = GPGO(surrogate_stp_df1_4, Acquisition_new(util_stp), f_syn_polarity4, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_4.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.917002476917337, -5.871699927585947)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(d). Training Regret Minimisation: run number = 4\n",
    "\n",
    "gp_output_4 = np.append(np.max(gpgo_gp_4.GP.y[0:n_init]),gpgo_gp_4.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_4 = np.append(np.max(gpgo_stp_df1_4.GP.y[0:n_init]),gpgo_stp_df1_4.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_4 = np.log(y_global_orig - gp_output_4)\n",
    "regret_stp_df1_4 = np.log(y_global_orig - stp_df1_output_4)\n",
    "\n",
    "train_regret_gp_4 = min_max_array(regret_gp_4)\n",
    "train_regret_stp_df1_4 = min_max_array(regret_stp_df1_4)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 4\n",
    "min_train_regret_gp_4 = min(train_regret_gp_4)\n",
    "min_train_regret_stp_df1_4 = min(train_regret_stp_df1_4)\n",
    "\n",
    "min_train_regret_gp_4, min_train_regret_stp_df1_4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 7.17834087  0.47855133  9.          0.99615185 10.          0.90306432]. \t  0.9946519027322408 \t 0.9952231976422058\n",
      "init   \t [ 6.77503421  3.3973338  11.          0.81021773 17.          0.81898565]. \t  0.9950935800834317 \t 0.9952231976422058\n",
      "init   \t [0.22674351 6.47067187 9.         0.77223434 4.         0.60667042]. \t  0.7947747619154123 \t 0.9952231976422058\n",
      "init   \t [ 2.06334907  5.19016261  9.          0.80805327 18.          0.67462476]. \t  0.9952231976422058 \t 0.9952231976422058\n",
      "init   \t [ 7.19469073  7.30014235  6.          0.71569795 12.          0.66615557]. \t  0.788961048818825 \t 0.9952231976422058\n",
      "1      \t [ 2.46652882  4.47984721  8.64346559  0.85798734 17.99984352  0.59924168]. \t  0.7934929837920489 \t 0.9952231976422058\n",
      "2      \t [ 2.31036503  4.99682792  9.20489112  0.79903804 17.20152528  0.52688047]. \t  0.7952596500517135 \t 0.9952231976422058\n",
      "3      \t [ 2.48116955  4.88728985  9.64365037  0.79988781 18.1993675   0.39627932]. \t  0.7953940762147939 \t 0.9952231976422058\n",
      "4      \t [ 2.89314434  5.41310099  8.95760941  0.81669782 17.92989311  0.90105744]. \t  0.9943206450299901 \t 0.9952231976422058\n",
      "5      \t [ 2.623493    5.42035817  8.78108978  0.62746379 17.94978429  0.10002606]. \t  0.791999940849175 \t 0.9952231976422058\n",
      "6      \t [ 2.49872927  5.01898206  9.12618237  0.5        17.90424583  1.        ]. \t  0.993629319070466 \t 0.9952231976422058\n",
      "7      \t [ 6.04183617  3.73599428 10.64761873  0.80923575 17.16798288  0.81643819]. \t  0.9950119618626659 \t 0.9952231976422058\n",
      "8      \t [ 6.04434177  3.08820852 10.9695897   0.83985643 16.7595703   0.51224056]. \t  0.7952020653107749 \t 0.9952231976422058\n",
      "9      \t [ 6.56246716  3.32263461 10.18199071  0.66307249 16.90268459  0.99809851]. \t  0.9947959225953861 \t 0.9952231976422058\n",
      "10     \t [ 6.39855415  3.11817531 10.65514196  0.58282127 17.5915476   0.99999681]. \t  0.9947047072174159 \t 0.9952231976422058\n",
      "11     \t [ 6.61747553  3.40669569 10.38908067  1.         17.40068997  0.40403686]. \t  0.8445784698835851 \t 0.9952231976422058\n",
      "12     \t [ 6.49000943  3.54582695 10.65772291  0.5        17.12844164  0.74995838]. \t  0.9942246242282816 \t 0.9952231976422058\n",
      "13     \t [ 6.38723706  3.3123748  10.64760153  1.         17.14011423  1.        ]. \t  0.9948679299686756 \t 0.9952231976422058\n",
      "14     \t [ 6.82578886  0.38677363  9.00029435  0.95230618 10.79366627  0.70535184]. \t  0.994829531659868 \t 0.9952231976422058\n",
      "15     \t [ 7.54584504  0.          9.25267801  1.         10.58403185  0.78064688]. \t  0.9948631322889296 \t 0.9952231976422058\n",
      "16     \t [6.79750575e+00 9.15910668e-03 9.55482491e+00 1.00000000e+00\n",
      " 1.03049094e+01 9.90763019e-01]. \t  0.9949255431964427 \t 0.9952231976422058\n",
      "17     \t [ 7.20845686  0.68297553  9.66031212  0.99942208 10.48646876  0.67428928]. \t  0.994680707202698 \t 0.9952231976422058\n",
      "18     \t [ 7.16543571  0.36873589  9.30695528  0.5        10.49110573  1.        ]. \t  0.9941286063305705 \t 0.9952231976422058\n",
      "19     \t [ 7.03284574  0.14651214  9.29815756  0.74691177 10.2262456   0.24733004]. \t  0.7935985646904274 \t 0.9952231976422058\n",
      "20     \t [ 7.14025594  0.37693827  9.27677629  1.         10.51921808  1.        ]. \t  0.9947095080777301 \t 0.9952231976422058\n",
      "21     \t [7.70453472 0.15409444 9.89665016 0.82817333 9.6908004  1.        ]. \t  0.9945030712225065 \t 0.9952231976422058\n",
      "22     \t [6.89072932 0.74738953 9.92670913 0.70013613 9.4864311  1.        ]. \t  0.9947287104127022 \t 0.9952231976422058\n",
      "23     \t [ 6.15136834  0.96193852  9.32550934  0.70607689 10.19226297  0.83081235]. \t  0.9947287095829886 \t 0.9952231976422058\n",
      "24     \t [ 6.36143063  0.67923527 10.26329378  0.5        10.34209302  0.88856821]. \t  0.9943158405742508 \t 0.9952231976422058\n",
      "25     \t [ 6.05267045  0.60167409  9.61602163  0.60495294 11.14698714  0.63250427]. \t  0.7955861375930476 \t 0.9952231976422058\n",
      "26     \t [ 7.28048252  0.16046509 10.52588862  0.53921686 10.21105646  0.94571004]. \t  0.9943638459968245 \t 0.9952231976422058\n",
      "27     \t [ 5.60226024  3.2669763   9.83467318  0.5        17.42030234  0.95342391]. \t  0.9941190069607969 \t 0.9952231976422058\n",
      "28     \t [ 6.94633146  0.4593029   9.88152947  0.63670979 10.05780323  0.91505373]. \t  0.9944886672587082 \t 0.9952231976422058\n",
      "29     \t [ 7.0439432   0.904641   10.88069304  1.          9.82239139  0.8996863 ]. \t  0.9951655804041569 \t 0.9952231976422058\n",
      "30     \t [ 6.32690933  1.4747237  10.20054013  1.          9.89430659  0.73212868]. \t  \u001b[92m0.9953960215609513\u001b[0m \t 0.9953960215609513\n"
     ]
    }
   ],
   "source": [
    "### 6(e). Bayesian optimization runs (x20): GP run number = 5\n",
    "\n",
    "np.random.seed(run_num_5)\n",
    "surrogate_gp_5 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "X_train5, X_test5, y_train5, y_test5 = train_test_split(X, y, test_size=0.15, random_state=run_num_5)\n",
    "\n",
    "def f_syn_polarity5(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    clf = XGBClassifier(reg_alpha=alpha, gamma=gamma,max_depth=int(max_depth),subsample=subsample,\n",
    "                              min_child_weight=min_child_weight,colsample_bytree=colsample,objective=obj_classifier, \n",
    "                             booster='gbtree', n_estimators = 5,\n",
    "                             silent=None, random_state=run_num_5)\n",
    "    score = np.array(cross_val_score(clf, X=X_train5, y=y_train5).mean())\n",
    "    return score\n",
    "\n",
    "\n",
    "gpgo_gp_5 = GPGO(surrogate_gp_5, Acquisition_new(util_gp), f_syn_polarity5, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_5.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 7.17834087  0.47855133  9.          0.99615185 10.          0.90306432]. \t  0.9946519027322408 \t 0.9952231976422058\n",
      "init   \t [ 6.77503421  3.3973338  11.          0.81021773 17.          0.81898565]. \t  0.9950935800834317 \t 0.9952231976422058\n",
      "init   \t [0.22674351 6.47067187 9.         0.77223434 4.         0.60667042]. \t  0.7947747619154123 \t 0.9952231976422058\n",
      "init   \t [ 2.06334907  5.19016261  9.          0.80805327 18.          0.67462476]. \t  0.9952231976422058 \t 0.9952231976422058\n",
      "init   \t [ 7.19469073  7.30014235  6.          0.71569795 12.          0.66615557]. \t  0.788961048818825 \t 0.9952231976422058\n",
      "1      \t [ 2.56527142  4.30553457  8.55581752  0.87024749 17.99985683  0.58072488]. \t  0.793483381933135 \t 0.9952231976422058\n",
      "2      \t [ 2.4083359   4.86359044  9.18778705  0.80487503 17.06994923  0.49196723]. \t  0.7950244080346048 \t 0.9952231976422058\n",
      "3      \t [ 2.99657294  5.18882545  9.24474937  0.78782127 18.08993812  0.28946542]. \t  0.7948323838551706 \t 0.9952231976422058\n",
      "4      \t [ 2.27193297  4.49861261  9.69560467  0.81988543 18.05265038  0.8078588 ]. \t  \u001b[92m0.9953480195955174\u001b[0m \t 0.9953480195955174\n",
      "5      \t [ 2.05381896  4.62519149  9.30358491  0.5        17.99844981  0.1       ]. \t  0.7954277002141176 \t 0.9953480195955174\n",
      "6      \t [ 2.54141065  4.83065625  9.18605432  0.5        17.86797232  1.        ]. \t  0.9936341199307802 \t 0.9953480195955174\n",
      "7      \t [ 5.90829112  3.67669239 10.66441809  0.81302343 17.19332285  0.77284029]. \t  0.9950743709724664 \t 0.9953480195955174\n",
      "8      \t [ 6.673683    4.07066516 10.67156834  0.77799237 17.48981861  0.6087407 ]. \t  0.7945683290704704 \t 0.9953480195955174\n",
      "9      \t [ 6.2920406   3.92827373 10.94805107  0.67002136 16.48454169  0.68660702]. \t  0.994647097861645 \t 0.9953480195955174\n",
      "10     \t [ 6.45247333  3.45877951 10.27339388  0.74001789 16.70350237  0.65903129]. \t  0.7973816316934316 \t 0.9953480195955174\n",
      "11     \t [ 6.21649563  3.54664535 11.16923     0.95028241 16.9727761   0.24982259]. \t  0.7942130987340454 \t 0.9953480195955174\n",
      "12     \t [ 6.28105328  3.691306   11.02651419  0.5        17.00635154  1.        ]. \t  0.99386936097989 \t 0.9953480195955174\n",
      "13     \t [ 6.354536    3.83050385 10.87511115  1.         16.86510054  1.        ]. \t  0.9949063365746178 \t 0.9953480195955174\n",
      "14     \t [ 2.38751284  4.80157677  9.24981109  1.         17.9019835   0.73876844]. \t  0.995324007964811 \t 0.9953480195955174\n",
      "15     \t [ 2.14031493  5.33600189  9.91435718  0.50427758 17.83595651  0.77256433]. \t  0.9948295264050161 \t 0.9953480195955174\n",
      "16     \t [ 2.21516693  5.08802586  9.59675643  0.50782208 18.62606378  0.86742952]. \t  0.994992755240841 \t 0.9953480195955174\n",
      "17     \t [ 2.22906144  4.99216063  9.48948816  0.5585584  18.0552058   0.75205481]. \t  0.9947671168803588 \t 0.9953480195955174\n",
      "18     \t [ 2.41934111  5.19409057 10.49016806  0.87624637 18.42512476  1.        ]. \t  0.9950503632137563 \t 0.9953480195955174\n",
      "19     \t [ 2.24164922  5.94153578  9.77390925  0.92767315 18.36313448  1.        ]. \t  0.9941814160705974 \t 0.9953480195955174\n",
      "20     \t [ 1.62586622  5.3665974  10.1719238   0.96141576 18.41720031  1.        ]. \t  0.9951319778390971 \t 0.9953480195955174\n",
      "21     \t [ 2.0255221   5.58663747 10.17142957  0.8942604  18.49939656  0.32547546]. \t  0.7952836690115553 \t 0.9953480195955174\n",
      "22     \t [ 2.20535226  5.28003061  9.90322616  0.89612332 18.23462851  1.        ]. \t  0.9943302413574809 \t 0.9953480195955174\n",
      "23     \t [ 1.95557869  5.79836228 10.4350689   0.5        18.42421091  1.        ]. \t  0.9941334022126039 \t 0.9953480195955174\n",
      "24     \t [ 1.86882696  4.64855572 10.62804459  0.5        18.63551255  0.99270829]. \t  0.9950071610023518 \t 0.9953480195955174\n",
      "25     \t [ 1.86654607  4.97249831 10.91625409  0.64724218 17.79694048  0.91577475]. \t  \u001b[92m0.9953768197791216\u001b[0m \t 0.9953768197791216\n",
      "26     \t [ 2.53305815  5.90587107  8.76922735  0.57232234 18.41399086  1.        ]. \t  0.9937781432510651 \t 0.9953768197791216\n",
      "27     \t [ 1.73803156  6.05510785  9.25559171  0.50447493 18.61656001  1.        ]. \t  0.9936245190398655 \t 0.9953768197791216\n",
      "28     \t [ 2.15066209  6.20611352  9.17976268  0.51408579 17.78213668  1.        ]. \t  0.9938309534059488 \t 0.9953768197791216\n",
      "29     \t [ 1.980456    5.07068928 10.53535241  0.57984233 18.26133959  0.96888386]. \t  0.9951655873184357 \t 0.9953768197791216\n",
      "30     \t [ 2.11584832  4.26567301 10.85996839  1.         18.33520952  0.78376229]. \t  \u001b[92m0.995957711846291\u001b[0m \t 0.995957711846291\n"
     ]
    }
   ],
   "source": [
    "### 6(e). Bayesian optimization runs (x20): STP DF1 run number = 5\n",
    "\n",
    "np.random.seed(run_num_5)\n",
    "surrogate_stp_df1_5 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_5 = GPGO(surrogate_stp_df1_3, Acquisition_new(util_stp), f_syn_polarity5, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_5.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.38083447122454, -5.510944372658239)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(e). Training Regret Minimisation: run number = 5\n",
    "\n",
    "gp_output_5 = np.append(np.max(gpgo_gp_5.GP.y[0:n_init]),gpgo_gp_5.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_5 = np.append(np.max(gpgo_stp_df1_5.GP.y[0:n_init]),gpgo_stp_df1_5.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_5 = np.log(y_global_orig - gp_output_5)\n",
    "regret_stp_df1_5 = np.log(y_global_orig - stp_df1_output_5)\n",
    "\n",
    "train_regret_gp_5 = min_max_array(regret_gp_5)\n",
    "train_regret_stp_df1_5 = min_max_array(regret_stp_df1_5)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 5\n",
    "min_train_regret_gp_5 = min(train_regret_gp_5)\n",
    "min_train_regret_stp_df1_5 = min(train_regret_stp_df1_5)\n",
    "\n",
    "min_train_regret_gp_5, min_train_regret_stp_df1_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 7.00437122  8.44186643 11.          0.86392903  5.          0.40237152]. \t  0.8417892131861526 \t 0.9902303948753781\n",
      "init   \t [ 2.44342852  2.59875544  6.          0.75403315 15.          0.79035779]. \t  0.9902303948753781 \t 0.9902303948753781\n",
      "init   \t [ 8.19956557  3.97476246 10.          0.57775603 12.          0.10459795]. \t  0.8423221084044542 \t 0.9902303948753781\n",
      "init   \t [1.12857654 1.10953672 5.         0.92389099 6.         0.93150723]. \t  0.984248594694138 \t 0.9902303948753781\n",
      "init   \t [2.94894531 5.24380613 5.         0.62956592 7.         0.34881625]. \t  0.8346696302681474 \t 0.9902303948753781\n",
      "1      \t [ 2.07484791  2.26706386  5.37466561  0.73884936 14.58344115  0.84152118]. \t  0.9840949574840945 \t 0.9902303948753781\n",
      "2      \t [ 2.18736441  3.13572509  5.39376544  0.73528334 14.73962994  0.71832142]. \t  0.9831876056709915 \t 0.9902303948753781\n",
      "3      \t [ 2.60768516  2.70095971  5.74322957  0.7280191  14.18358483  0.66957962]. \t  0.9833172290377595 \t 0.9902303948753781\n",
      "4      \t [ 2.83941023  2.56572913  5.23237007  0.68094181 14.85939756  0.70666456]. \t  0.9834372523433262 \t 0.9902303948753781\n",
      "5      \t [ 2.31384745  2.56408142  5.58101026  0.50835031 14.73636679  0.11813013]. \t  0.8365083170382314 \t 0.9902303948753781\n",
      "6      \t [ 2.44755312  2.69954507  5.57410636  0.5        14.64539141  1.        ]. \t  0.9841525123553487 \t 0.9902303948753781\n",
      "7      \t [ 2.45105708  2.6473824   5.52736569  1.         14.67891264  0.73125704]. \t  0.9819537317451604 \t 0.9902303948753781\n",
      "8      \t [ 2.9067053   1.64387875  5.89458685  0.57861397 14.55521119  0.72935449]. \t  0.9843494059847423 \t 0.9902303948753781\n",
      "9      \t [ 2.55346849  1.60645293  5.54028129  0.53195309 15.4456558   0.8529416 ]. \t  0.9837637000584148 \t 0.9902303948753781\n",
      "10     \t [ 2.05533798  1.48355885  6.23356355  0.57444487 14.94394276  0.88560841]. \t  0.9899663539192353 \t 0.9902303948753781\n",
      "11     \t [ 2.83030762  1.68670467  6.4824849   0.5        15.40676931  0.74350838]. \t  0.9889821858519654 \t 0.9902303948753781\n",
      "12     \t [ 2.53702981  1.56659039  6.0905602   1.         15.19207577  0.51055233]. \t  0.8865855833289743 \t 0.9902303948753781\n",
      "13     \t [ 2.5374413   1.90076126  5.96558411  0.5        15.00367608  0.81412148]. \t  0.9840277483436931 \t 0.9902303948753781\n",
      "14     \t [ 2.75775848  0.71994133  6.20850115  0.50348948 15.24749474  1.        ]. \t  0.9888237075405052 \t 0.9902303948753781\n",
      "15     \t [ 2.18266374  1.1283473   6.42190208  0.56403164 15.99711366  1.        ]. \t  0.9889101231644458 \t 0.9902303948753781\n",
      "16     \t [ 2.41815526  1.05829688  7.12206096  0.75673356 15.29977841  1.        ]. \t  \u001b[92m0.9932164454983045\u001b[0m \t 0.9932164454983045\n",
      "17     \t [ 2.61785499  1.28287228  6.40791133  0.86975622 15.44719748  1.        ]. \t  0.989006137190161 \t 0.9932164454983045\n",
      "18     \t [ 2.31135327  0.85814096  6.69519471  0.5        15.45607228  0.43336622]. \t  0.8414099788247281 \t 0.9932164454983045\n",
      "19     \t [ 1.93012228  1.96586546  7.14291967  0.57429322 15.68003664  0.97151014]. \t  0.9922419200841931 \t 0.9932164454983045\n",
      "20     \t [ 2.52713516  0.99739761  5.04724684  0.58483694 14.59553805  0.80852659]. \t  0.9838357142076974 \t 0.9932164454983045\n",
      "21     \t [ 3.40737917  1.16681191  5.09665226  0.5        14.99854473  0.5831028 ]. \t  0.8346840358913727 \t 0.9932164454983045\n",
      "22     \t [ 2.41021832  2.15820645  7.28870586  0.72269927 14.74683795  0.77676932]. \t  0.9925347653725073 \t 0.9932164454983045\n",
      "23     \t [ 2.76890687  1.90808034  7.60086158  0.5        15.57253223  1.        ]. \t  0.9921602754508824 \t 0.9932164454983045\n",
      "24     \t [ 2.51559388  2.11732704  7.32239079  1.         15.56066146  0.45683617]. \t  0.8879153841606081 \t 0.9932164454983045\n",
      "25     \t [ 2.05888311  0.59507517  5.38037938  0.56443871 15.50615613  1.        ]. \t  0.9840660963165514 \t 0.9932164454983045\n",
      "26     \t [ 2.409423    1.69495992  7.10179707  0.5        15.31799907  0.96211939]. \t  0.9918914688972471 \t 0.9932164454983045\n",
      "27     \t [ 2.56825048  3.01046288  7.32528644  0.55697751 15.32569626  0.95782907]. \t  0.9925155669095315 \t 0.9932164454983045\n",
      "28     \t [ 3.37714695  2.61589693  7.03694816  0.66028465 14.85705473  0.69938536]. \t  0.9917474299506924 \t 0.9932164454983045\n",
      "29     \t [ 1.67284252  0.5596433   6.35239506  0.94413974 15.26079547  1.        ]. \t  0.9892077747062117 \t 0.9932164454983045\n",
      "30     \t [ 2.54176633  3.17793327  6.8955115   0.62207014 14.50774714  0.45407776]. \t  0.8420484802476684 \t 0.9932164454983045\n"
     ]
    }
   ],
   "source": [
    "### 6(f). Bayesian optimization runs (x20): GP run number = 6\n",
    "\n",
    "np.random.seed(run_num_6)\n",
    "surrogate_gp_6 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "X_train6, X_test6, y_train6, y_test6 = train_test_split(X, y, test_size=0.15, random_state=run_num_6)\n",
    "\n",
    "def f_syn_polarity6(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    clf = XGBClassifier(reg_alpha=alpha, gamma=gamma,max_depth=int(max_depth),subsample=subsample,\n",
    "                              min_child_weight=min_child_weight,colsample_bytree=colsample,objective=obj_classifier, \n",
    "                             booster='gbtree', n_estimators = 5,\n",
    "                             silent=None, random_state=run_num_6)\n",
    "    score = np.array(cross_val_score(clf, X=X_train6, y=y_train6).mean())\n",
    "    return score\n",
    "\n",
    "gpgo_gp_6 = GPGO(surrogate_gp_6, Acquisition_new(util_gp), f_syn_polarity6, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_6.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 7.00437122  8.44186643 11.          0.86392903  5.          0.40237152]. \t  0.8417892131861526 \t 0.9902303948753781\n",
      "init   \t [ 2.44342852  2.59875544  6.          0.75403315 15.          0.79035779]. \t  0.9902303948753781 \t 0.9902303948753781\n",
      "init   \t [ 8.19956557  3.97476246 10.          0.57775603 12.          0.10459795]. \t  0.8423221084044542 \t 0.9902303948753781\n",
      "init   \t [1.12857654 1.10953672 5.         0.92389099 6.         0.93150723]. \t  0.984248594694138 \t 0.9902303948753781\n",
      "init   \t [2.94894531 5.24380613 5.         0.62956592 7.         0.34881625]. \t  0.8346696302681474 \t 0.9902303948753781\n",
      "1      \t [ 1.88295477  1.89414643  6.28043772  0.68711924 14.4387337   0.98011809]. \t  0.9901775891456329 \t 0.9902303948753781\n",
      "2      \t [ 2.07843536  1.95180503  5.42270936  0.90672359 14.66062332  0.49282597]. \t  0.8339686987159993 \t 0.9902303948753781\n",
      "3      \t [ 2.84846895  1.99012602  6.20287372  0.87662836 14.32777113  0.76138687]. \t  0.9901199773007215 \t 0.9902303948753781\n",
      "4      \t [ 2.32604837  2.53544246  5.83098391  0.50102276 14.10449501  1.        ]. \t  0.9841669149362913 \t 0.9902303948753781\n",
      "5      \t [ 2.15754115  2.49549099  6.3072168   1.         14.29958649  0.40311371]. \t  0.885985474268562 \t 0.9902303948753781\n",
      "6      \t [ 2.3972131   2.11237962  6.09483179  0.5        14.5942433   0.63916694]. \t  0.8411507334740476 \t 0.9902303948753781\n",
      "7      \t [ 2.31703172  2.30109774  5.94996917  1.         14.464299    1.        ]. \t  0.9840660970079792 \t 0.9902303948753781\n",
      "8      \t [ 2.23807278  1.66056831  6.32652784  0.81199998 13.39910974  0.99257517]. \t  0.9901103758566644 \t 0.9902303948753781\n",
      "9      \t [ 2.66008657  2.32346019  6.89992557  0.71820156 13.75938106  1.        ]. \t  0.9889869333340475 \t 0.9902303948753781\n",
      "10     \t [ 3.22617064  3.15557031  6.31267983  0.79235608 14.50365704  0.91647076]. \t  0.9899519444240141 \t 0.9902303948753781\n",
      "11     \t [ 3.21213588  2.49658633  6.15167704  0.87504136 13.46847015  0.8623368 ]. \t  \u001b[92m0.9903168084250352\u001b[0m \t 0.9903168084250352\n",
      "12     \t [ 3.05728635  2.997426    5.44155215  0.96041813 14.66562057  0.46017503]. \t  0.8348472582277758 \t 0.9903168084250352\n",
      "13     \t [ 2.36975621  3.671872    5.92683032  0.62760558 14.94904431  0.90894928]. \t  0.9840805590517192 \t 0.9903168084250352\n",
      "14     \t [ 2.8588052   3.43423206  6.17992227  1.         15.54822635  0.68327333]. \t  0.9857751774204205 \t 0.9903168084250352\n",
      "15     \t [ 2.62249181  2.17674473  6.30534122  0.80749078 13.77996181  1.        ]. \t  0.9889245250539606 \t 0.9903168084250352\n",
      "16     \t [ 3.5315416   2.27097229  6.82927148  1.         13.79852884  0.43206022]. \t  0.8861150958376176 \t 0.9903168084250352\n",
      "17     \t [ 2.07411053  1.29743802  6.97778798  1.         13.88640919  0.75826779]. \t  0.9859864138913883 \t 0.9903168084250352\n",
      "18     \t [ 1.48814773  1.27209059  6.09947141  0.93854615 13.82742683  0.77434576]. \t  0.9901007708171822 \t 0.9903168084250352\n",
      "19     \t [ 2.42349097  3.2660791   6.68319152  0.99337752 15.01024339  1.        ]. \t  0.9890445460086723 \t 0.9903168084250352\n",
      "20     \t [ 1.48439922  1.71503503  6.79377265  0.56315029 13.60536346  1.        ]. \t  0.9890157409850729 \t 0.9903168084250352\n",
      "21     \t [ 2.61788505  3.19715791  6.06408104  1.         14.90405564  0.89386899]. \t  0.9857319700924498 \t 0.9903168084250352\n",
      "22     \t [ 2.67266107  3.40078383  6.49445395  0.5        15.13489763  0.62118107]. \t  0.8415587989950453 \t 0.9903168084250352\n",
      "23     \t [ 2.01201519  3.43387785  6.15988372  0.84266916 15.81515575  1.        ]. \t  0.9889965350546759 \t 0.9903168084250352\n",
      "24     \t [ 3.78106491  2.53867248  5.99671682  0.60371547 14.0661055   0.82579865]. \t  0.9836628836512441 \t 0.9903168084250352\n",
      "25     \t [ 1.78167712  1.74117771  6.53313539  1.         13.77055358  0.80322431]. \t  0.9861112362595567 \t 0.9903168084250352\n",
      "26     \t [ 1.85825878  1.13717004  6.52820602  0.54758067 13.67754256  1.        ]. \t  0.9889629320747594 \t 0.9903168084250352\n",
      "27     \t [ 1.46244452  3.01323507  5.9522687   0.59238816 14.79077939  1.        ]. \t  0.9841333075312363 \t 0.9903168084250352\n",
      "28     \t [ 1.28900711  2.0033843   5.61179107  0.5        13.8442375   1.        ]. \t  0.9842245240154911 \t 0.9903168084250352\n",
      "29     \t [ 3.33315476  2.62211068  6.25357586  0.92082552 14.09474695  0.68279454]. \t  0.9898367250210444 \t 0.9903168084250352\n",
      "30     \t [ 2.92412682  2.07479036  7.27236072  1.         14.67706495  1.        ]. \t  \u001b[92m0.9934228802792444\u001b[0m \t 0.9934228802792444\n"
     ]
    }
   ],
   "source": [
    "### 6(f). Bayesian optimization runs (x20): STP DF1 run number = 6\n",
    "\n",
    "np.random.seed(run_num_6)\n",
    "surrogate_stp_df1_6 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_6 = GPGO(surrogate_stp_df1_6, Acquisition_new(util_stp), f_syn_polarity6, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_6.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.993254051620601, -5.024158361885674)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(f). Training Regret Minimisation: run number = 6\n",
    "\n",
    "gp_output_6 = np.append(np.max(gpgo_gp_6.GP.y[0:n_init]),gpgo_gp_6.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_6 = np.append(np.max(gpgo_stp_df1_6.GP.y[0:n_init]),gpgo_stp_df1_6.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_gp_6 = np.log(y_global_orig - gp_output_6)\n",
    "regret_stp_df1_6 = np.log(y_global_orig - stp_df1_output_6)\n",
    "\n",
    "train_regret_gp_6 = min_max_array(regret_gp_6)\n",
    "train_regret_stp_df1_6 = min_max_array(regret_stp_df1_6)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 6\n",
    "min_train_regret_gp_6 = min(train_regret_gp_6)\n",
    "min_train_regret_stp_df1_6 = min(train_regret_stp_df1_6)\n",
    "\n",
    "min_train_regret_gp_6, min_train_regret_stp_df1_6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.52663735  3.0235661  12.          0.81812715  8.          0.65052854]. \t  0.8560769562555897 \t 0.9892990896497961\n",
      "init   \t [ 7.5633323   8.82772753  5.          0.54668663 19.          0.56839702]. \t  0.8563649518687818 \t 0.9892990896497961\n",
      "init   \t [ 3.86398258  9.70445521 12.          0.8132366  19.          0.9367083 ]. \t  0.9892990896497961 \t 0.9892990896497961\n",
      "init   \t [8.12320793 1.19710634 7.         0.61146405 8.         0.3000627 ]. \t  0.8537726141070033 \t 0.9892990896497961\n",
      "init   \t [ 5.5087939   2.19588947  7.          0.94140422 14.          0.14184967]. \t  0.8533213263231928 \t 0.9892990896497961\n",
      "1      \t [ 3.13025235  9.32150908 12.24262115  0.81419842 18.75733823  0.82462765]. \t  \u001b[92m0.9895871385029347\u001b[0m \t 0.9895871385029347\n",
      "2      \t [ 3.88821205  9.19710197 12.24260801  0.71936385 18.31700479  0.87485881]. \t  0.9887565987954314 \t 0.9895871385029347\n",
      "3      \t [ 3.5897047   9.89804462 12.65272789  0.75167846 18.50727539  0.91022615]. \t  0.9892030728583695 \t 0.9895871385029347\n",
      "4      \t [ 3.79659217  9.24281524 12.72853748  0.71330581 19.04382903  0.79667122]. \t  0.9886557843242589 \t 0.9895871385029347\n",
      "5      \t [ 3.70981902  9.55015485 12.33293558  0.99682488 18.68324033  0.24140459]. \t  0.8538734099096233 \t 0.9895871385029347\n",
      "6      \t [ 3.66815958  9.40943406 12.43576618  1.         18.71527836  1.        ]. \t  \u001b[92m0.9952279968430929\u001b[0m \t 0.9952279968430929\n",
      "7      \t [ 3.61485399  9.50705639 12.31173739  0.5        18.7446864   0.82633103]. \t  0.9889822353582014 \t 0.9952279968430929\n",
      "8      \t [ 4.76333521  9.80411847 12.63591943  0.68119541 18.65222356  0.86743208]. \t  0.988732602237853 \t 0.9952279968430929\n",
      "9      \t [ 4.83569759  9.0177821  12.11473944  0.71983877 19.04856767  0.75643399]. \t  0.9885357665501152 \t 0.9952279968430929\n",
      "10     \t [ 7.33944596  8.9376126   5.24880741  0.59822833 18.00335396  0.60636513]. \t  0.8561441140920425 \t 0.9952279968430929\n",
      "11     \t [ 3.75281844  8.53864456 11.74579628  0.83926832 19.19556831  0.67237518]. \t  0.9890206477721377 \t 0.9952279968430929\n",
      "12     \t [ 3.14634036  8.18393679 12.36991186  0.74728863 18.70026065  0.50102844]. \t  0.8557985311104859 \t 0.9952279968430929\n",
      "13     \t [ 4.34100013  8.27678226 12.44587292  0.67150381 18.78053872  0.46538138]. \t  0.8525676131029893 \t 0.9952279968430929\n",
      "14     \t [ 4.86455424  9.80356385 11.68104827  0.88337424 18.38657127  0.85280297]. \t  0.9889246312572822 \t 0.9952279968430929\n",
      "15     \t [ 4.47235417  8.85875529 11.39694682  0.93813407 18.53677285  0.52194321]. \t  0.8542574779050435 \t 0.9952279968430929\n",
      "16     \t [ 6.83812347  8.43406377  5.5772349   0.60686366 18.70670384  0.63044023]. \t  0.854487859737326 \t 0.9952279968430929\n",
      "17     \t [ 7.36403756  9.28722483  5.87400741  0.60203451 18.76008349  0.72558139]. \t  0.9822802642293066 \t 0.9952279968430929\n",
      "18     \t [ 6.78565713  9.38829986  5.1975541   0.57621296 18.73381871  0.80065984]. \t  0.9824530949240455 \t 0.9952279968430929\n",
      "19     \t [ 7.03568074  9.22490399  5.47580405  0.5        18.69377144  0.1       ]. \t  0.8467202943990596 \t 0.9952279968430929\n",
      "20     \t [ 7.17988441  9.0797643   5.3903903   1.         18.69504029  0.7984045 ]. \t  0.9868985715430861 \t 0.9952279968430929\n",
      "21     \t [ 5.09977604  9.15363297 12.10280944  0.5        18.10268007  1.        ]. \t  0.9936725301321475 \t 0.9952279968430929\n",
      "22     \t [ 4.86979924  9.43007393 12.10702072  0.5        18.33266595  0.35254876]. \t  0.8519819277812125 \t 0.9952279968430929\n",
      "23     \t [ 4.77378655  9.27387744 12.16096853  1.         18.436247    1.        ]. \t  0.9951031746132101 \t 0.9952279968430929\n",
      "24     \t [ 4.66463435  9.39628451 11.87925548  0.5        18.63318276  1.        ]. \t  0.9937013341877479 \t 0.9952279968430929\n",
      "25     \t [ 4.66820366 10.         12.33338281  0.62402829 17.62953514  1.        ]. \t  0.9950455615237287 \t 0.9952279968430929\n",
      "26     \t [ 5.67769935 10.         12.26750953  0.66969002 17.92970233  1.        ]. \t  0.9949831525522136 \t 0.9952279968430929\n",
      "27     \t [ 5.29164568  9.58104644 13.08225957  0.5        17.65516149  1.        ]. \t  0.9937925488742901 \t 0.9952279968430929\n",
      "28     \t [ 3.99001337  8.65865491 12.33604921  1.         19.74403546  0.18592097]. \t  0.8856589990346438 \t 0.9952279968430929\n",
      "29     \t [ 5.47557019  9.49204221 12.40046225  0.97082533 17.21585153  1.        ]. \t  0.9950599668703827 \t 0.9952279968430929\n",
      "30     \t [ 5.18878738  9.83327666 12.64254035  1.         17.79813543  1.        ]. \t  0.9951175766410104 \t 0.9952279968430929\n"
     ]
    }
   ],
   "source": [
    "### 6(g). Bayesian optimization runs (x20): GP run number = 7\n",
    "\n",
    "np.random.seed(run_num_7)\n",
    "surrogate_gp_7 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "X_train7, X_test7, y_train7, y_test7 = train_test_split(X, y, test_size=0.15, random_state=run_num_7)\n",
    "\n",
    "def f_syn_polarity7(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    clf = XGBClassifier(reg_alpha=alpha, gamma=gamma,max_depth=int(max_depth),subsample=subsample,\n",
    "                              min_child_weight=min_child_weight,colsample_bytree=colsample,objective=obj_classifier, \n",
    "                             booster='gbtree', n_estimators = 5,\n",
    "                             silent=None, random_state=run_num_7)\n",
    "    score = np.array(cross_val_score(clf, X=X_train7, y=y_train7).mean())\n",
    "    return score\n",
    "\n",
    "gpgo_gp_7 = GPGO(surrogate_gp_7, Acquisition_new(util_gp), f_syn_polarity7, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_7.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.52663735  3.0235661  12.          0.81812715  8.          0.65052854]. \t  0.8560769562555897 \t 0.9892990896497961\n",
      "init   \t [ 7.5633323   8.82772753  5.          0.54668663 19.          0.56839702]. \t  0.8563649518687818 \t 0.9892990896497961\n",
      "init   \t [ 3.86398258  9.70445521 12.          0.8132366  19.          0.9367083 ]. \t  0.9892990896497961 \t 0.9892990896497961\n",
      "init   \t [8.12320793 1.19710634 7.         0.61146405 8.         0.3000627 ]. \t  0.8537726141070033 \t 0.9892990896497961\n",
      "init   \t [ 5.5087939   2.19588947  7.          0.94140422 14.          0.14184967]. \t  0.8533213263231928 \t 0.9892990896497961\n",
      "1      \t [ 2.96119506  9.23321774 12.29853838  0.81439348 18.70143946  0.79881784]. \t  \u001b[92m0.9896063419441914\u001b[0m \t 0.9896063419441914\n",
      "2      \t [ 3.7030148   9.04206623 11.75712403  0.80998932 18.47504277  0.3868791 ]. \t  0.8529996992432557 \t 0.9896063419441914\n",
      "3      \t [ 3.73825922  9.49179546 12.58906372  0.82112166 18.21231068  0.98939016]. \t  0.98946231820905 \t 0.9896063419441914\n",
      "4      \t [ 3.74132223  9.19198566 12.71164703  0.81539372 19.0084717   0.53543201]. \t  0.8550832074870988 \t 0.9896063419441914\n",
      "5      \t [ 3.39131553  9.90746041 12.25403369  0.5139508  18.51996472  0.4463037 ]. \t  0.8527356401717024 \t 0.9896063419441914\n",
      "6      \t [ 3.57663982  9.21791097 12.16698978  0.5        18.6535954   1.        ]. \t  \u001b[92m0.993749339057179\u001b[0m \t 0.993749339057179\n",
      "7      \t [ 3.49448613  9.48176579 12.17849369  1.         18.60145665  0.89111805]. \t  \u001b[92m0.9959673215874827\u001b[0m \t 0.9959673215874827\n",
      "8      \t [ 4.7648027   9.69700258 12.20895127  0.58789383 18.43438128  0.69654721]. \t  0.988713399211453 \t 0.9959673215874827\n",
      "9      \t [ 4.40702387 10.         12.78352926  0.5        18.78204148  1.        ]. \t  0.9935621095152082 \t 0.9959673215874827\n",
      "10     \t [ 7.28332744  7.93329584  5.57142909  0.60027793 18.71100334  0.46261051]. \t  0.8561201097904716 \t 0.9959673215874827\n",
      "11     \t [ 7.73185764  8.10265649  5.40479433  0.54680194 19.68080387  0.42595429]. \t  0.8563697527290959 \t 0.9959673215874827\n",
      "12     \t [ 7.98334158  8.54673408  5.92186332  0.5678895  18.96968425  0.75204377]. \t  0.9822466566859661 \t 0.9959673215874827\n",
      "13     \t [ 7.22713338  8.53877047  5.75724205  0.50001107 19.2981535   0.99996813]. \t  0.9823666765343934 \t 0.9959673215874827\n",
      "14     \t [ 7.76854438  8.18323223  5.36640071  0.5        19.04603504  1.        ]. \t  0.9839700803548382 \t 0.9959673215874827\n",
      "15     \t [ 7.57192817  8.44655489  5.60219325  1.         19.15193077  0.74117767]. \t  0.986783350895546 \t 0.9959673215874827\n",
      "16     \t [ 7.56758363  8.45783773  5.64001073  0.5        19.11351337  0.61189523]. \t  0.8467587012815728 \t 0.9959673215874827\n",
      "17     \t [ 7.87287658  7.88383403  6.34784679  0.85632747 19.22198035  1.        ]. \t  0.9889869319511918 \t 0.9959673215874827\n",
      "18     \t [ 7.88904815  8.31890851  6.2085314   0.95994266 18.31207802  1.        ]. \t  0.9890349401394767 \t 0.9959673215874827\n",
      "19     \t [ 6.84110536  8.02426201  5.          0.80521887 19.54577264  1.        ]. \t  0.98394127685238 \t 0.9959673215874827\n",
      "20     \t [ 7.49643192  8.74528366  5.          0.7760293  19.97717347  1.        ]. \t  0.9839604790490665 \t 0.9959673215874827\n",
      "21     \t [ 8.58822214  8.47067449  6.20702715  0.99896604 18.93336429  1.        ]. \t  0.9889485250686784 \t 0.9959673215874827\n",
      "22     \t [ 4.16589168  9.66133584 12.35857696  0.62013521 18.6112391   0.81856667]. \t  0.9891838730125379 \t 0.9959673215874827\n",
      "23     \t [ 5.08293136 10.         12.82611099  0.98263717 18.2436802   1.        ]. \t  0.9951751866882091 \t 0.9959673215874827\n",
      "24     \t [ 5.16298222 10.         12.34027787  0.89153245 19.00985858  1.        ]. \t  0.9951847879939809 \t 0.9959673215874827\n",
      "25     \t [ 5.29250484  9.54958175 12.81974084  0.5        18.62541262  1.        ]. \t  0.9936437238639778 \t 0.9959673215874827\n",
      "26     \t [ 7.99200947  8.75090815  6.63173763  0.83816804 18.9717844   1.        ]. \t  0.9889677283716495 \t 0.9959673215874827\n",
      "27     \t [ 5.18494644 10.         12.49321919  0.5        18.48463258  1.        ]. \t  0.9935525075180087 \t 0.9959673215874827\n",
      "28     \t [ 7.35078606  8.01069425  5.58212451  0.85471801 19.97982391  1.        ]. \t  0.9840180886814087 \t 0.9959673215874827\n",
      "29     \t [ 7.99000613  8.41234899  6.08393061  0.83366714 18.96437009  1.        ]. \t  0.9889773306454201 \t 0.9959673215874827\n",
      "30     \t [ 6.9443461   7.53552372  5.92975372  0.97591709 19.01469345  1.        ]. \t  0.9840372901866674 \t 0.9959673215874827\n"
     ]
    }
   ],
   "source": [
    "### 6(g). Bayesian optimization runs (x20): STP DF1 run number = 7\n",
    "\n",
    "np.random.seed(run_num_7)\n",
    "surrogate_stp_df1_7 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_7 = GPGO(surrogate_stp_df1_7, Acquisition_new(util_stp), f_syn_polarity7, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_7.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.344989113198684, -5.513324505293958)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(g). Training Regret Minimisation: run number = 7\n",
    "\n",
    "gp_output_7 = np.append(np.max(gpgo_gp_7.GP.y[0:n_init]),gpgo_gp_7.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_7 = np.append(np.max(gpgo_stp_df1_7.GP.y[0:n_init]),gpgo_stp_df1_7.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_7 = np.log(y_global_orig - gp_output_7)\n",
    "regret_stp_df1_7 = np.log(y_global_orig - stp_df1_output_7)\n",
    "\n",
    "train_regret_gp_7 = min_max_array(regret_gp_7)\n",
    "train_regret_stp_df1_7 = min_max_array(regret_stp_df1_7)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 7\n",
    "min_train_regret_gp_7 = min(train_regret_gp_7)\n",
    "min_train_regret_stp_df1_7 = min(train_regret_stp_df1_7)\n",
    "\n",
    "min_train_regret_gp_7, min_train_regret_stp_df1_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.59560606  1.64569495 11.          0.89021195 15.          0.4857008 ]. \t  0.8866767704430721 \t 0.9915505688196039\n",
      "init   \t [ 0.57460093  9.25007434  5.          0.56647642 17.          0.7533211 ]. \t  0.9812288212003665 \t 0.9915505688196039\n",
      "init   \t [1.90674396 4.32019547 9.         0.93708225 3.         0.91775905]. \t  0.9915505688196039 \t 0.9915505688196039\n",
      "init   \t [ 0.36770475  8.43368681 14.          0.6826844  15.          0.6291966 ]. \t  0.8877377519986741 \t 0.9915505688196039\n",
      "init   \t [ 5.33840324  0.449182   14.          0.52045439  2.          0.48604556]. \t  0.8862014953669619 \t 0.9915505688196039\n",
      "1      \t [1.76658979 4.32144428 9.8843894  0.97991169 2.99999998 1.        ]. \t  \u001b[92m0.995847296208214\u001b[0m \t 0.995847296208214\n",
      "2      \t [1.89709294 4.03023869 9.4791362  0.87197605 3.67049712 0.76666055]. \t  0.9915505691653229 \t 0.995847296208214\n",
      "3      \t [2.06515989 3.60289567 9.4784739  1.         2.96420198 1.        ]. \t  0.9957560793782383 \t 0.995847296208214\n",
      "4      \t [2.54805228 4.27997428 9.55989904 0.87909585 3.19403295 1.        ]. \t  0.9954296253020769 \t 0.995847296208214\n",
      "5      \t [2.0804687  4.0947699  9.55530593 0.67072883 2.94690094 0.36385686]. \t  0.8876657372270795 \t 0.995847296208214\n",
      "6      \t [1.94792258 4.07157707 9.46528472 0.5        3.16864955 1.        ]. \t  0.9951031706727654 \t 0.995847296208214\n",
      "7      \t [ 1.14672051  9.14145178  5.30757329  0.68125132 17.61514162  0.6654209 ]. \t  0.8891779872063323 \t 0.995847296208214\n",
      "8      \t [ 1.01427682  9.60844175  5.6816717   0.61303086 16.83444112  0.84187808]. \t  0.9807727437574348 \t 0.995847296208214\n",
      "9      \t [ 0.61658939  9.95209633  5.30535456  0.58249917 17.46026298  0.85172058]. \t  0.9811088011445316 \t 0.995847296208214\n",
      "10     \t [2.04031042 4.12388954 9.47413141 1.         3.18884558 0.95406973]. \t  0.9921314692529978 \t 0.995847296208214\n",
      "11     \t [ 2.29216362  3.68085788 10.50515484  0.70522076  3.43176692  0.9060956 ]. \t  0.9909984746543956 \t 0.995847296208214\n",
      "12     \t [ 0.36418742  9.29895661  5.80437334  0.53043911 17.31206275  0.65705526]. \t  0.8880402050921662 \t 0.995847296208214\n",
      "13     \t [ 0.79343092  9.69387195  5.30946577  0.86918258 17.09483041  0.15320164]. \t  0.8891731755595881 \t 0.995847296208214\n",
      "14     \t [ 2.5169017   3.90488095 10.35485439  0.77614637  2.41767341  1.        ]. \t  \u001b[92m0.9964089897431618\u001b[0m \t 0.9964089897431618\n",
      "15     \t [ 2.48003921  4.66612612 10.61800752  0.58200701  3.07723288  1.        ]. \t  0.9961305435787046 \t 0.9964089897431618\n",
      "16     \t [ 1.70156367  4.11559256 10.87645173  0.6810322   2.74181337  1.        ]. \t  0.9963705832755113 \t 0.9964089897431618\n",
      "17     \t [2.23233868 3.74538543 8.37276114 0.69398038 3.47579839 0.67629992]. \t  0.9907680354336291 \t 0.9964089897431618\n",
      "18     \t [1.2744698  3.66979973 8.53741311 0.82007962 3.31977944 0.60902025]. \t  0.8873584863155998 \t 0.9964089897431618\n",
      "19     \t [1.78159178 4.58283257 8.41803907 0.59459806 3.75879858 0.50518985]. \t  0.8877617493858668 \t 0.9964089897431618\n",
      "20     \t [ 0.68701036  9.47874769  5.37787593  1.         17.18816328  1.        ]. \t  0.9844885706435074 \t 0.9964089897431618\n",
      "21     \t [ 2.22354319  4.18317446 10.60700266  1.          2.94022958  1.        ]. \t  \u001b[92m0.9966634321591988\u001b[0m \t 0.9966634321591988\n",
      "22     \t [ 1.95440668  4.77086612 10.27635197  0.50963185  2.24777716  1.        ]. \t  0.995952914720263 \t 0.9966634321591988\n",
      "23     \t [2.82869542 3.24365486 9.49728175 0.59099732 3.62540235 0.77312625]. \t  0.9908400467480346 \t 0.9966634321591988\n",
      "24     \t [ 2.12626126  4.21483552 10.34842545  0.5         2.79821846  1.        ]. \t  0.9958953046039243 \t 0.9966634321591988\n",
      "25     \t [ 1.43192289  5.19090719 10.69121614  0.80360998  2.85244709  0.93227155]. \t  0.9916273816166177 \t 0.9966634321591988\n",
      "26     \t [ 1.10813324  4.58218523 10.46560093  1.          2.13825666  1.        ]. \t  \u001b[92m0.9969658840081027\u001b[0m \t 0.9969658840081027\n",
      "27     \t [ 3.37142458  3.75769998 10.24776145  0.69982338  3.19540679  0.92928202]. \t  0.9909168605130613 \t 0.9969658840081027\n",
      "28     \t [ 1.70729019  4.72250892 10.74164196  0.94354378  2.35700455  0.42337809]. \t  0.8863935264606271 \t 0.9969658840081027\n",
      "29     \t [ 2.88892637  2.95289036 10.1148902   0.91847369  2.8847181   0.98315638]. \t  0.9913153281162291 \t 0.9969658840081027\n",
      "30     \t [0.92653913 4.86935701 9.82428398 0.52913901 2.70274647 0.74894355]. \t  0.9913681355745152 \t 0.9969658840081027\n"
     ]
    }
   ],
   "source": [
    "### 6(h). Bayesian optimization runs (x20): GP run number = 8\n",
    "\n",
    "np.random.seed(run_num_8)\n",
    "surrogate_gp_8 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "X_train8, X_test8, y_train8, y_test8 = train_test_split(X, y, test_size=0.15, random_state=run_num_8)\n",
    "\n",
    "def f_syn_polarity8(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    clf = XGBClassifier(reg_alpha=alpha, gamma=gamma,max_depth=int(max_depth),subsample=subsample,\n",
    "                              min_child_weight=min_child_weight,colsample_bytree=colsample,objective=obj_classifier, \n",
    "                             booster='gbtree', n_estimators = 5,\n",
    "                             silent=None, random_state=run_num_8)\n",
    "    score = np.array(cross_val_score(clf, X=X_train8, y=y_train8).mean())\n",
    "    return score\n",
    "\n",
    "gpgo_gp_8 = GPGO(surrogate_gp_8, Acquisition_new(util_gp), f_syn_polarity8, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_8.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.59560606  1.64569495 11.          0.89021195 15.          0.4857008 ]. \t  0.8866767704430721 \t 0.9915505688196039\n",
      "init   \t [ 0.57460093  9.25007434  5.          0.56647642 17.          0.7533211 ]. \t  0.9812288212003665 \t 0.9915505688196039\n",
      "init   \t [1.90674396 4.32019547 9.         0.93708225 3.         0.91775905]. \t  0.9915505688196039 \t 0.9915505688196039\n",
      "init   \t [ 0.36770475  8.43368681 14.          0.6826844  15.          0.6291966 ]. \t  0.8877377519986741 \t 0.9915505688196039\n",
      "init   \t [ 5.33840324  0.449182   14.          0.52045439  2.          0.48604556]. \t  0.8862014953669619 \t 0.9915505688196039\n",
      "1      \t [1.84479639 4.30796743 9.78796116 1.         2.20977516 1.        ]. \t  \u001b[92m0.9957704817517494\u001b[0m \t 0.9957704817517494\n",
      "2      \t [ 1.80704489  4.45026773 10.01104732  0.85803542  3.18517512  0.72712717]. \t  0.9916465861641749 \t 0.9957704817517494\n",
      "3      \t [2.2245225  3.6719132  9.70131485 0.98798209 2.88877069 0.94578229]. \t  0.9913585362739193 \t 0.9957704817517494\n",
      "4      \t [2.56009195 4.5792086  9.6388487  0.9626097  2.78655311 0.9693861 ]. \t  0.9913585363430631 \t 0.9957704817517494\n",
      "5      \t [2.09297827 4.20389009 9.53376559 0.5        2.63252156 0.44244508]. \t  0.8863263148310914 \t 0.9957704817517494\n",
      "6      \t [1.96004463 4.27749767 9.62526351 1.         2.83690839 1.        ]. \t  \u001b[92m0.9957752825429198\u001b[0m \t 0.9957752825429198\n",
      "7      \t [ 2.35496009  4.16617522 10.64962528  1.          2.44254545  0.59477689]. \t  0.8795955849362205 \t 0.9957752825429198\n",
      "8      \t [2.44550509 4.19259646 9.31835169 1.         3.68042668 0.37476813]. \t  0.8801236719649629 \t 0.9957752825429198\n",
      "9      \t [2.7376358  3.87818858 8.64446226 1.         2.62781073 0.80793166]. \t  0.9895006226234518 \t 0.9957752825429198\n",
      "10     \t [2.68198482 3.86598695 9.5431636  1.         1.83412751 0.92192415]. \t  0.9920882610261638 \t 0.9957752825429198\n",
      "11     \t [2.25208718 4.44614533 8.75257771 1.         1.91976558 0.93885968]. \t  0.9896254444384699 \t 0.9957752825429198\n",
      "12     \t [2.37208468 4.1458174  9.28244955 1.         2.44943653 0.65307563]. \t  0.8801476759208148 \t 0.9957752825429198\n",
      "13     \t [2.51160101 4.06766918 9.0726402  0.5        3.2990212  1.        ]. \t  0.995059963206513 \t 0.9957752825429198\n",
      "14     \t [ 2.31776618  4.10605586 10.14082515  0.5         2.33249845  1.        ]. \t  \u001b[92m0.9959097069774354\u001b[0m \t 0.9959097069774354\n",
      "15     \t [2.18205371 3.66572128 8.65405746 0.52720134 2.09321183 1.        ]. \t  0.9949783487194597 \t 0.9959097069774354\n",
      "16     \t [2.39697262 4.41382114 8.15526754 0.54845481 2.62194418 1.        ]. \t  0.9948727308988485 \t 0.9959097069774354\n",
      "17     \t [2.82368294 4.23105008 8.83660932 0.5        2.07506738 1.        ]. \t  0.9946951009341065 \t 0.9959097069774354\n",
      "18     \t [ 2.61145723  4.15402522 10.3257699   0.63839854  3.53026175  0.70223685]. \t  0.9907440306480516 \t 0.9959097069774354\n",
      "19     \t [2.07694622 4.17595684 9.50114203 0.60169527 1.22510026 1.        ]. \t  0.995367215016862 \t 0.9959097069774354\n",
      "20     \t [2.15455162 3.66267442 8.15831545 0.9277386  3.17757025 1.        ]. \t  0.9954296257860834 \t 0.9959097069774354\n",
      "21     \t [2.32690773 4.00755773 8.55848814 0.77327906 2.69313575 1.        ]. \t  0.9951271738680356 \t 0.9959097069774354\n",
      "22     \t [2.66039699 3.77610182 8.47077117 0.79921723 1.36412597 1.        ]. \t  0.9952759986708927 \t 0.9959097069774354\n",
      "23     \t [2.38839626 4.47394078 8.17674673 1.         3.61163642 0.90375417]. \t  0.9895246265793037 \t 0.9959097069774354\n",
      "24     \t [1.72469113 4.2364536  8.35255591 0.50120637 3.63455228 0.61203103]. \t  0.886283104184225 \t 0.9959097069774354\n",
      "25     \t [2.16797325 4.95337308 9.28009426 0.52300672 3.57584881 0.93352262]. \t  0.9908448477466362 \t 0.9959097069774354\n",
      "26     \t [2.2683828  3.91361577 9.05444585 0.67961784 1.57392212 1.        ]. \t  0.9953432106461474 \t 0.9959097069774354\n",
      "27     \t [2.75768943 3.66422849 7.90418589 0.69684062 2.10759591 0.79424459]. \t  0.9904271787073814 \t 0.9959097069774354\n",
      "28     \t [2.35274505 4.79593213 9.8361583  0.87987963 1.51507837 1.        ]. \t  0.9954824340049512 \t 0.9959097069774354\n",
      "29     \t [2.96493604 3.00966096 8.66485411 0.73628379 2.18700412 1.        ]. \t  0.9950839665400708 \t 0.9959097069774354\n",
      "30     \t [1.70186254 5.02217949 9.23552187 0.5        1.89779438 1.        ]. \t  0.9952423941007131 \t 0.9959097069774354\n"
     ]
    }
   ],
   "source": [
    "### 6(h). Bayesian optimization runs (x20): STP DF1 run number = 8\n",
    "\n",
    "np.random.seed(run_num_8)\n",
    "surrogate_stp_df1_8 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_8 = GPGO(surrogate_stp_df1_8, Acquisition_new(util_stp), f_syn_polarity8, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_8.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.7978351681019, -5.499138667834888)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(h). Training Regret Minimisation: run number = 8\n",
    "\n",
    "gp_output_8 = np.append(np.max(gpgo_gp_8.GP.y[0:n_init]),gpgo_gp_8.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_8 = np.append(np.max(gpgo_stp_df1_8.GP.y[0:n_init]),gpgo_stp_df1_8.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_8 = np.log(y_global_orig - gp_output_8)\n",
    "regret_stp_df1_8 = np.log(y_global_orig - stp_df1_output_8)\n",
    "\n",
    "train_regret_gp_8 = min_max_array(regret_gp_8)\n",
    "train_regret_stp_df1_8 = min_max_array(regret_stp_df1_8)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 8\n",
    "min_train_regret_gp_8 = min(train_regret_gp_8)\n",
    "min_train_regret_stp_df1_8 = min(train_regret_stp_df1_8)\n",
    "\n",
    "min_train_regret_gp_8, min_train_regret_stp_df1_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.0342804   5.27522296 13.          0.60741904 17.          0.15336222]. \t  0.8605604646092962 \t 0.8631049071619116\n",
      "init   \t [7.56063644 1.79876369 7.         0.84869647 5.         0.55956942]. \t  0.8608005020243582 \t 0.8631049071619116\n",
      "init   \t [2.43675539 0.20225128 6.         0.51669963 7.         0.4646446 ]. \t  0.8615638388834549 \t 0.8631049071619116\n",
      "init   \t [ 7.6043547   4.73758376  9.          0.87564624 15.          0.65751286]. \t  0.8622551547480161 \t 0.8631049071619116\n",
      "init   \t [ 4.70349314  6.71029586  6.          0.97680389 11.          0.41971652]. \t  0.8631049071619116 \t 0.8631049071619116\n",
      "1      \t [ 7.70430466  4.86261386  9.93007208  0.81326555 15.46507804  0.54026015]. \t  0.8614438159927249 \t 0.8631049071619116\n",
      "2      \t [ 7.46572015  5.51114961  9.62961201  0.81074992 14.73805026  0.57529175]. \t  0.8614294136192138 \t 0.8631049071619116\n",
      "3      \t [ 7.37582936  4.52172086  9.86019733  0.79186538 14.53870195  0.49196811]. \t  0.8618470861156577 \t 0.8631049071619116\n",
      "4      \t [ 8.29125419  4.90230227  9.68213689  0.89462811 14.66079608  0.69549118]. \t  \u001b[92m0.9872154332347486\u001b[0m \t 0.9872154332347486\n",
      "5      \t [ 7.98366965  4.92575708  9.51644564  0.5        14.82004999  0.1       ]. \t  0.8595426977109 \t 0.9872154332347486\n",
      "6      \t [ 7.80031153  4.88354075  9.6621977   0.5        14.83608758  1.        ]. \t  \u001b[92m0.993413283192098\u001b[0m \t 0.993413283192098\n",
      "7      \t [ 8.02780393  5.16707702 10.62715871  0.598097   14.62191595  0.63221788]. \t  0.8610645480279056 \t 0.993413283192098\n",
      "8      \t [ 8.01689233  5.21739318 12.14266807  0.62271117 16.24154232  0.31507608]. \t  0.8623367601772337 \t 0.993413283192098\n",
      "9      \t [ 8.18177751  5.49706984 13.12127369  0.5        15.99474243  0.23290926]. \t  0.8596723216999642 \t 0.993413283192098\n",
      "10     \t [ 7.26666638  5.49017015 12.79617471  0.5        16.35663835  0.1       ]. \t  0.8588081658754009 \t 0.993413283192098\n",
      "11     \t [ 7.77003817  4.595786   12.94603926  0.5        16.25544119  0.15191411]. \t  0.859067412332366 \t 0.993413283192098\n",
      "12     \t [ 7.73918895  5.20438002 12.89404192  0.5        16.44390346  0.97368272]. \t  0.9866345385402888 \t 0.993413283192098\n",
      "13     \t [ 7.75224863  5.20213722 12.9211683   1.         16.32975119  0.50637103]. \t  0.8461434988558012 \t 0.993413283192098\n",
      "14     \t [ 7.51207432  5.10642593 12.40252058  0.5        15.41687414  0.84924369]. \t  0.9867737614150861 \t 0.993413283192098\n",
      "15     \t [ 7.12751756  4.75701253 12.06781798  0.5        16.13506592  0.92593364]. \t  0.9871770284956926 \t 0.993413283192098\n",
      "16     \t [ 7.27412294  5.44479314 11.430229    0.5        15.61316345  1.        ]. \t  0.993264456453215 \t 0.993413283192098\n",
      "17     \t [ 7.62252664  4.63042666 11.38380207  0.5        15.42943023  1.        ]. \t  0.9932932620299627 \t 0.993413283192098\n",
      "18     \t [ 7.10847097  4.92162339 11.52480036  0.5        15.37603332  0.34803882]. \t  0.858952191062532 \t 0.993413283192098\n",
      "19     \t [ 7.25717657  5.00829972 10.98456095  1.         15.03160015  1.        ]. \t  \u001b[92m0.9950215527976667\u001b[0m \t 0.9950215527976667\n",
      "20     \t [ 7.49202858  5.0382262  11.70778901  1.         15.6564059   1.        ]. \t  \u001b[92m0.9952135884548214\u001b[0m \t 0.9952135884548214\n",
      "21     \t [ 7.54288451  5.15296948 12.20075483  0.5        15.97553647  0.84782466]. \t  0.9867353551548669 \t 0.9952135884548214\n",
      "22     \t [ 6.78635174  4.85199097 11.84823022  0.63109482 15.24105071  1.        ]. \t  0.9941718111010577 \t 0.9952135884548214\n",
      "23     \t [ 6.76020873  4.7540559  10.98969599  0.72640973 15.84241777  1.        ]. \t  0.9949207394328464 \t 0.9952135884548214\n",
      "24     \t [ 7.13268003  4.91273464 11.3830462   0.64771853 15.52031208  1.        ]. \t  0.994291833576925 \t 0.9952135884548214\n",
      "25     \t [ 6.08898217  5.29120657 11.57114167  0.88289023 16.11985958  0.86232577]. \t  0.9888669173592355 \t 0.9952135884548214\n",
      "26     \t [ 6.06059607  4.26425683 11.67732925  0.90486178 16.07410469  0.80909837]. \t  0.989759865830659 \t 0.9952135884548214\n",
      "27     \t [ 6.20511848  4.82046204 12.63002462  0.63378008 15.77141538  0.7896515 ]. \t  0.9878299358874335 \t 0.9952135884548214\n",
      "28     \t [ 8.33688741  4.38861263 10.93416706  1.         15.07716457  1.        ]. \t  0.9944502553986331 \t 0.9952135884548214\n",
      "29     \t [ 8.42491236  5.21779466 11.5403917   0.79053245 15.23992251  1.        ]. \t  0.9950551624844864 \t 0.9952135884548214\n",
      "30     \t [ 8.0158406   4.50884947 11.84778928  0.97420086 14.68734405  1.        ]. \t  0.9950263541419874 \t 0.9952135884548214\n"
     ]
    }
   ],
   "source": [
    "### 6(i). Bayesian optimization runs (x20): GP run number = 9\n",
    "\n",
    "np.random.seed(run_num_9)\n",
    "surrogate_gp_9 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "X_train9, X_test9, y_train9, y_test9 = train_test_split(X, y, test_size=0.15, random_state=run_num_9)\n",
    "\n",
    "def f_syn_polarity9(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    clf = XGBClassifier(reg_alpha=alpha, gamma=gamma,max_depth=int(max_depth),subsample=subsample,\n",
    "                              min_child_weight=min_child_weight,colsample_bytree=colsample,objective=obj_classifier, \n",
    "                             booster='gbtree', n_estimators = 5,\n",
    "                             silent=None, random_state=run_num_9)\n",
    "    score = np.array(cross_val_score(clf, X=X_train9, y=y_train9).mean())\n",
    "    return score\n",
    "\n",
    "gpgo_gp_9 = GPGO(surrogate_gp_9, Acquisition_new(util_gp), f_syn_polarity9, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_9.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.0342804   5.27522296 13.          0.60741904 17.          0.15336222]. \t  0.8605604646092962 \t 0.8631049071619116\n",
      "init   \t [7.56063644 1.79876369 7.         0.84869647 5.         0.55956942]. \t  0.8608005020243582 \t 0.8631049071619116\n",
      "init   \t [2.43675539 0.20225128 6.         0.51669963 7.         0.4646446 ]. \t  0.8615638388834549 \t 0.8631049071619116\n",
      "init   \t [ 7.6043547   4.73758376  9.          0.87564624 15.          0.65751286]. \t  0.8622551547480161 \t 0.8631049071619116\n",
      "init   \t [ 4.70349314  6.71029586  6.          0.97680389 11.          0.41971652]. \t  0.8631049071619116 \t 0.8631049071619116\n",
      "1      \t [ 7.72344127  4.88657408 10.10856216  0.80128598 15.55423503  0.51778829]. \t  0.8613814025960397 \t 0.8631049071619116\n",
      "2      \t [ 8.65074969  4.89216486  9.35697903  0.85305425 15.41198067  0.50098839]. \t  0.8610597536671071 \t 0.8631049071619116\n",
      "3      \t [ 7.75885712  5.42891486  9.13881871  0.77046553 15.83544718  0.30598403]. \t  0.8619478994113342 \t 0.8631049071619116\n",
      "4      \t [ 7.86434142  4.39680018  9.23300351  0.97374408 16.0175603   0.65663082]. \t  0.8616022376070016 \t 0.8631049071619116\n",
      "5      \t [ 7.89546679  4.57446228  9.35702468  0.5        15.46210903  0.1       ]. \t  0.8593266587893312 \t 0.8631049071619116\n",
      "6      \t [ 7.94780375  4.99062403  9.37353259  0.5        15.58512109  1.        ]. \t  \u001b[92m0.9932932620299627\u001b[0m \t 0.9932932620299627\n",
      "7      \t [ 8.04256673  5.17305396 11.86346878  0.63816595 16.60367249  0.30894225]. \t  0.8615110228513396 \t 0.9932932620299627\n",
      "8      \t [ 7.4701179   5.03591395 12.67187737  0.82571314 16.1480878   0.16085349]. \t  0.8616022420322039 \t 0.9932932620299627\n",
      "9      \t [ 8.54046907  5.13314995 12.67051696  0.89140343 16.15410529  0.1       ]. \t  0.8622119453457379 \t 0.9932932620299627\n",
      "10     \t [ 8.06323306  4.44981839 12.56265541  1.         16.73363044  0.34192326]. \t  0.8454233752710348 \t 0.9932932620299627\n",
      "11     \t [ 8.07049228  5.08441566 12.6938669   0.5        16.37694048  0.90553286]. \t  0.9866537417049703 \t 0.9932932620299627\n",
      "12     \t [ 7.98219747  5.36839364 12.5798429   1.         16.57780231  0.64174925]. \t  0.8457498336341107 \t 0.9932932620299627\n",
      "13     \t [ 8.06085783  4.90893436 12.60774453  0.5        16.4211176   0.31152052]. \t  0.8595234942696434 \t 0.9932932620299627\n",
      "14     \t [ 8.10339849  4.75628104 11.94175096  0.92703598 15.73767666  1.        ]. \t  \u001b[92m0.9951511781004627\u001b[0m \t 0.9951511781004627\n",
      "15     \t [ 8.05419756  4.68969476 12.99995951  1.         15.65445605  1.        ]. \t  0.9949639431653345 \t 0.9951511781004627\n",
      "16     \t [ 8.01819997  5.34202104 12.53209111  0.59085597 15.39250681  1.        ]. \t  0.9939365707434019 \t 0.9951511781004627\n",
      "17     \t [ 7.51338604  4.85567396 12.45220199  0.73923837 15.7407545   1.        ]. \t  0.9949015363373087 \t 0.9951511781004627\n",
      "18     \t [ 8.12679137  4.96991189 12.52706647  0.79840729 15.82030201  1.        ]. \t  \u001b[92m0.9952423938241379\u001b[0m \t 0.9952423938241379\n",
      "19     \t [ 7.77202379  5.29221031 13.57747192  0.5489904  15.91400042  0.85098883]. \t  0.9876283079823486 \t 0.9952423938241379\n",
      "20     \t [ 8.10122206  4.75071314 13.68508406  0.97705986 16.53931714  0.6472232 ]. \t  0.8616118392584863 \t 0.9952423938241379\n",
      "21     \t [ 7.85809187  4.94959696 11.98505254  1.         15.20484238  0.35564965]. \t  0.8457162280267743 \t 0.9952423938241379\n",
      "22     \t [ 7.98313159  5.35973254 11.25884481  0.5        15.78573011  1.        ]. \t  0.9933796776539054 \t 0.9952423938241379\n",
      "23     \t [ 8.4330193   4.81694265 10.81502342  0.75965687 16.20617604  0.81510929]. \t  0.9875754933331091 \t 0.9952423938241379\n",
      "24     \t [ 7.72229511  4.7179449  11.20751503  0.6864929  16.28140806  1.        ]. \t  0.9945174693790574 \t 0.9952423938241379\n",
      "25     \t [ 7.78456194  5.11499668 13.25663424  0.99826982 15.36371915  0.45813857]. \t  0.8617702666808408 \t 0.9952423938241379\n",
      "26     \t [ 7.99486648  4.97936087 11.27067723  1.         15.99598735  0.66426233]. \t  0.8456730209062409 \t 0.9952423938241379\n",
      "27     \t [ 8.15329336  4.62465982 11.16338972  0.5        15.75259597  1.        ]. \t  0.9932932621682503 \t 0.9952423938241379\n",
      "28     \t [ 8.26965596  4.9316096  11.35736891  0.5        16.48284548  1.        ]. \t  0.9933412700108103 \t 0.9952423938241379\n",
      "29     \t [ 8.17078939  4.99836034  9.98133287  0.5        16.22217373  0.77788509]. \t  0.9861880637859987 \t 0.9952423938241379\n",
      "30     \t [ 7.34753462  5.00660087 13.30341848  0.50693119 16.7096016   1.        ]. \t  0.9935189003904151 \t 0.9952423938241379\n"
     ]
    }
   ],
   "source": [
    "### 6(i). Bayesian optimization runs (x20): STP DF1 run number = 9\n",
    "\n",
    "np.random.seed(run_num_9)\n",
    "surrogate_stp_df1_9 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_9 = GPGO(surrogate_stp_df1_9, Acquisition_new(util_stp), f_syn_polarity9, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_9.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.341974303811602, -5.348010641468211)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(i). Training Regret Minimisation: run number = 9\n",
    "\n",
    "gp_output_9 = np.append(np.max(gpgo_gp_9.GP.y[0:n_init]),gpgo_gp_9.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_9 = np.append(np.max(gpgo_stp_df1_9.GP.y[0:n_init]),gpgo_stp_df1_9.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_9 = np.log(y_global_orig - gp_output_9)\n",
    "regret_stp_df1_9 = np.log(y_global_orig - stp_df1_output_9)\n",
    "\n",
    "train_regret_gp_9 = min_max_array(regret_gp_9)\n",
    "train_regret_stp_df1_9 = min_max_array(regret_stp_df1_9)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 9\n",
    "min_train_regret_gp_9 = min(train_regret_gp_9)\n",
    "min_train_regret_stp_df1_9 = min(train_regret_stp_df1_9)\n",
    "\n",
    "min_train_regret_gp_9, min_train_regret_stp_df1_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.53589585  1.15006943  6.          0.93623727 14.          0.13663866]. \t  0.8645259990834046 \t 0.9889005473743276\n",
      "init   \t [ 3.97194461  2.33132197 14.          0.9533253  19.          0.26403087]. \t  0.8644203854114204 \t 0.9889005473743276\n",
      "init   \t [ 7.43539415  0.69582081  9.          0.9763222  11.          0.12608349]. \t  0.8649820882116354 \t 0.9889005473743276\n",
      "init   \t [ 9.82027485  3.39637684  9.          0.78522537 19.          0.87618843]. \t  0.9889005473743276 \t 0.9889005473743276\n",
      "init   \t [ 0.57576207  5.82646405 12.          0.77704362 18.          0.11865044]. \t  0.8641707475203182 \t 0.9889005473743276\n",
      "1      \t [ 9.9991147   3.05308212  8.50496277  0.8800611  19.64020102  0.82679794]. \t  0.9873738932929689 \t 0.9889005473743276\n",
      "2      \t [ 9.24077056  3.01174943  8.47971536  0.86287492 19.17007918  0.78860799]. \t  0.987580328143021 \t 0.9889005473743276\n",
      "3      \t [ 9.80870086  3.59723578  8.15443941  0.94261941 19.06478688  0.8238466 ]. \t  0.9876907466856588 \t 0.9889005473743276\n",
      "4      \t [10.          2.8263382   8.3865824   0.83191558 18.80443411  0.96075242]. \t  0.9875899307625184 \t 0.9889005473743276\n",
      "5      \t [ 9.8660942   3.11358643  8.55270536  1.         19.04131058  0.2075511 ]. \t  0.8995957712538251 \t 0.9889005473743276\n",
      "6      \t [ 9.75271488  3.17077401  8.55999495  1.         19.14712459  1.        ]. \t  \u001b[92m0.9941142031281766\u001b[0m \t 0.9941142031281766\n",
      "7      \t [ 9.7846071   3.20791388  8.412229    0.5        19.15790165  0.70891875]. \t  0.9868025965853732 \t 0.9941142031281766\n",
      "8      \t [ 9.33851357  3.4170658   8.52305707  0.79003406 18.07974181  0.71704194]. \t  0.9876187356478283 \t 0.9941142031281766\n",
      "9      \t [ 9.31584258  2.64956278  9.14928577  0.5940887  18.28046116  0.69354291]. \t  0.9878491691296608 \t 0.9941142031281766\n",
      "10     \t [ 8.82968438  3.4458759   9.23477555  0.67752279 18.52547942  0.46474823]. \t  0.865750213139446 \t 0.9941142031281766\n",
      "11     \t [ 9.88302067  3.3502148   9.33654425  0.58638528 17.95117171  0.58164229]. \t  0.8643579601220045 \t 0.9941142031281766\n",
      "12     \t [ 9.80134361  2.22871558  9.21494134  0.61041929 19.4030943   0.67676447]. \t  0.9881516200105519 \t 0.9941142031281766\n",
      "13     \t [ 9.81834091  1.93109355  8.28662274  0.8179938  19.70582616  0.64594843]. \t  0.8654381587401884 \t 0.9941142031281766\n",
      "14     \t [ 9.57600565  4.43014528  8.65543683  0.8238478  18.43487634  0.48252852]. \t  0.8650444953853801 \t 0.9941142031281766\n",
      "15     \t [ 9.25873444  1.9371053   8.83005022  1.         18.93840744  0.52586498]. \t  0.899235709703445 \t 0.9941142031281766\n",
      "16     \t [10.          1.93991578  8.77950447  0.5        18.67409606  0.509006  ]. \t  0.8657694168572779 \t 0.9941142031281766\n",
      "17     \t [ 9.61864179  2.61524953  8.84541102  0.71016192 18.96118999  0.63814685]. \t  0.866163083185269 \t 0.9941142031281766\n",
      "18     \t [ 9.84254753  1.32395251  9.38449282  0.72440183 19.28235551  0.97792475]. \t  0.9886845105270722 \t 0.9941142031281766\n",
      "19     \t [10.          1.56936417  9.40258522  0.87807329 19.85137056  0.27983125]. \t  0.8653997583571907 \t 0.9941142031281766\n",
      "20     \t [ 9.21041187  3.99441752  8.64162666  0.83783697 19.53921606  0.3275422 ]. \t  0.8658174290558964 \t 0.9941142031281766\n",
      "21     \t [ 9.34790904  1.63474372  9.61838913  0.79933677 18.05864236  0.71679677]. \t  0.989294213564031 \t 0.9941142031281766\n",
      "22     \t [10.          4.13846423  9.10083901  0.5        19.02011874  0.1       ]. \t  0.8650828979118351 \t 0.9941142031281766\n",
      "23     \t [ 9.27558454  1.5939398   9.80552869  0.5        18.9988306   0.48787306]. \t  0.8662302973039808 \t 0.9941142031281766\n",
      "24     \t [ 9.4867847   2.14615935  8.81988036  0.90866031 17.60351971  1.        ]. \t  0.994099800201515 \t 0.9941142031281766\n",
      "25     \t [ 8.86432852  2.62388588  9.43691732  1.         17.70190605  1.        ]. \t  \u001b[92m0.9942774308576948\u001b[0m \t 0.9942774308576948\n",
      "26     \t [ 8.73617475  1.8593205   9.00533736  0.54870847 18.04813062  1.        ]. \t  0.9936293211456539 \t 0.9942774308576948\n",
      "27     \t [ 8.91135248  2.2306197   9.1515119   0.67290803 17.50839792  0.3249432 ]. \t  0.8658510310677561 \t 0.9942774308576948\n",
      "28     \t [ 9.29681519  2.29313255  9.44408489  0.5        17.62681386  1.        ]. \t  0.9932644598412604 \t 0.9942774308576948\n",
      "29     \t [ 9.29245776  1.84596766  9.23793302  0.68029678 20.          1.        ]. \t  0.9935861134028264 \t 0.9942774308576948\n",
      "30     \t [ 8.40130398  2.74692597  8.47788997  1.         18.25634575  1.        ]. \t  0.9941574102487097 \t 0.9942774308576948\n"
     ]
    }
   ],
   "source": [
    "### 6(j). Bayesian optimization runs (x20): GP run number = 10\n",
    "\n",
    "np.random.seed(run_num_10)\n",
    "surrogate_gp_10 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "X_train10, X_test10, y_train10, y_test10 = train_test_split(X, y, test_size=0.15, random_state=run_num_10)\n",
    "\n",
    "def f_syn_polarity10(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    clf = XGBClassifier(reg_alpha=alpha, gamma=gamma,max_depth=int(max_depth),subsample=subsample,\n",
    "                              min_child_weight=min_child_weight,colsample_bytree=colsample,objective=obj_classifier, \n",
    "                             booster='gbtree', n_estimators = 5,\n",
    "                             silent=None, random_state=run_num_10)\n",
    "    score = np.array(cross_val_score(clf, X=X_train10, y=y_train10).mean())\n",
    "    return score\n",
    "\n",
    "gpgo_gp_10 = GPGO(surrogate_gp_10, Acquisition_new(util_gp), f_syn_polarity10, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_10.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.53589585  1.15006943  6.          0.93623727 14.          0.13663866]. \t  0.8645259990834046 \t 0.9889005473743276\n",
      "init   \t [ 3.97194461  2.33132197 14.          0.9533253  19.          0.26403087]. \t  0.8644203854114204 \t 0.9889005473743276\n",
      "init   \t [ 7.43539415  0.69582081  9.          0.9763222  11.          0.12608349]. \t  0.8649820882116354 \t 0.9889005473743276\n",
      "init   \t [ 9.82027485  3.39637684  9.          0.78522537 19.          0.87618843]. \t  0.9889005473743276 \t 0.9889005473743276\n",
      "init   \t [ 0.57576207  5.82646405 12.          0.77704362 18.          0.11865044]. \t  0.8641707475203182 \t 0.9889005473743276\n",
      "1      \t [ 9.04745562  3.64684796  9.54823628  0.71322876 18.47622091  0.89962056]. \t  \u001b[92m0.9891165814558316\u001b[0m \t 0.9891165814558316\n",
      "2      \t [ 8.9123309   2.86940405  9.06377418  0.77671027 18.9721954   0.90632098]. \t  0.9889341531890955 \t 0.9891165814558316\n",
      "3      \t [ 9.27440234  3.28611076  8.67821898  0.80542315 18.22758932  0.85837846]. \t  0.987767559966679 \t 0.9891165814558316\n",
      "4      \t [ 9.07061619  3.5995623   8.9218286   0.90266396 18.89607696  0.32850251]. \t  0.8658318295625254 \t 0.9891165814558316\n",
      "5      \t [ 9.50269637  2.94942947  9.3786132   0.57579787 18.4799294   0.53484511]. \t  0.8643435515255528 \t 0.9891165814558316\n",
      "6      \t [ 9.19878837  3.44351012  8.96525093  0.5        18.78157552  1.        ]. \t  \u001b[92m0.9932404558854085\u001b[0m \t 0.9932404558854085\n",
      "7      \t [ 9.3075448   3.26923206  9.18366399  1.         18.65466314  1.        ]. \t  \u001b[92m0.9941670126607761\u001b[0m \t 0.9941670126607761\n",
      "8      \t [ 8.22310297  3.11724882  9.18814439  0.66366546 18.08827075  0.70083922]. \t  0.9885452870991246 \t 0.9941670126607761\n",
      "9      \t [ 8.1232996   3.15074876  9.91405427  0.52374521 18.84845796  0.6168584 ]. \t  0.8657454084762239 \t 0.9941670126607761\n",
      "10     \t [ 7.98770587  3.63609274  9.2547593   1.         18.6692587   1.        ]. \t  \u001b[92m0.9941718134519464\u001b[0m \t 0.9941718134519464\n",
      "11     \t [ 8.52305488  3.31516536  9.24978189  0.77759613 18.54744465  0.7984443 ]. \t  0.9895438535985904 \t 0.9941718134519464\n",
      "12     \t [ 7.73270912  3.77390707  9.82791455  0.67192854 17.99946498  1.        ]. \t  0.9937829467396814 \t 0.9941718134519464\n",
      "13     \t [ 7.26139669  3.08515178  9.48340833  0.84218014 18.34789593  1.        ]. \t  0.9940565942564259 \t 0.9941718134519464\n",
      "14     \t [ 7.35983248  3.69746712  9.29205836  0.60885241 18.319769    0.47830653]. \t  0.8654621681583992 \t 0.9941718134519464\n",
      "15     \t [ 7.59565463  3.54889993  9.03540771  1.         17.70239129  1.        ]. \t  \u001b[92m0.994291833231206\u001b[0m \t 0.994291833231206\n",
      "16     \t [ 7.73498004  3.37794297  9.59807604  1.         18.09865779  0.79105696]. \t  0.9882716075688084 \t 0.994291833231206\n",
      "17     \t [ 7.71410683  3.41043994  9.30606192  0.56384928 18.17076349  1.        ]. \t  0.9936053171206582 \t 0.994291833231206\n",
      "18     \t [ 8.52692888  4.19750907  9.15180195  0.84006518 17.58984321  0.87634911]. \t  0.9895534556649377 \t 0.994291833231206\n",
      "19     \t [ 8.14209093  4.61956885  9.57748022  1.         18.25999488  1.        ]. \t  0.9941526102872649 \t 0.994291833231206\n",
      "20     \t [ 7.88390004  4.10562153  9.29753893  1.         18.0368814   1.        ]. \t  0.9941622118696056 \t 0.994291833231206\n",
      "21     \t [ 8.50653702  4.43563468  9.93183559  0.5        18.02694565  0.59809504]. \t  0.8663455122125868 \t 0.994291833231206\n",
      "22     \t [ 7.7630952   4.16622922 10.12149983  0.82554566 18.97049945  1.        ]. \t  \u001b[92m0.9947479126101634\u001b[0m \t 0.9947479126101634\n",
      "23     \t [ 8.49928845  3.21502736  8.76850584  0.83616383 17.14397551  1.        ]. \t  0.9937445420006253 \t 0.9947479126101634\n",
      "24     \t [ 8.29293051  3.46208877  8.42346887  0.85663557 17.45934394  0.48981503]. \t  0.8656974101755059 \t 0.9947479126101634\n",
      "25     \t [ 8.58479235  4.31684885  9.70354579  0.77538334 19.1183702   1.        ]. \t  0.9940133861678794 \t 0.9947479126101634\n",
      "26     \t [ 8.20721575  4.1470585   9.78776835  0.78988052 18.58039941  1.        ]. \t  0.994008586344722 \t 0.9947479126101634\n",
      "27     \t [ 9.12232826  3.45777396  9.59661179  0.77142819 19.64596381  0.87625042]. \t  0.9889869615462507 \t 0.9947479126101634\n",
      "28     \t [ 7.8403848   3.51974576  9.65746256  0.90545574 19.65165322  1.        ]. \t  0.9941190046799283 \t 0.9947479126101634\n",
      "29     \t [ 8.17978019  3.53877135  9.30371626  0.5739623  17.20421801  0.72305731]. \t  0.9882620385531897 \t 0.9947479126101634\n",
      "30     \t [ 7.23421701  3.66071061  9.74607333  0.93695905 18.95228944  1.        ]. \t  0.9941814151725747 \t 0.9947479126101634\n"
     ]
    }
   ],
   "source": [
    "### 6(j). Bayesian optimization runs (x20): STP DF1 run number = 10\n",
    "\n",
    "np.random.seed(run_num_10)\n",
    "surrogate_stp_df1_10 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_10 = GPGO(surrogate_stp_df1_10, Acquisition_new(util_stp), f_syn_polarity10, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_10.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.163337423675783, -5.249129683335651)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(j). Training Regret Minimisation: run number = 10\n",
    "\n",
    "gp_output_10 = np.append(np.max(gpgo_gp_10.GP.y[0:n_init]),gpgo_gp_10.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_10 = np.append(np.max(gpgo_stp_df1_10.GP.y[0:n_init]),gpgo_stp_df1_10.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_10 = np.log(y_global_orig - gp_output_10)\n",
    "regret_stp_df1_10 = np.log(y_global_orig - stp_df1_output_10)\n",
    "\n",
    "train_regret_gp_10 = min_max_array(regret_gp_10)\n",
    "train_regret_stp_df1_10 = min_max_array(regret_stp_df1_10)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 10\n",
    "min_train_regret_gp_10 = min(train_regret_gp_10)\n",
    "min_train_regret_stp_df1_10 = min(train_regret_stp_df1_10)\n",
    "\n",
    "min_train_regret_gp_10, min_train_regret_stp_df1_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.95549199  9.25003702  7.          0.65523847 12.          0.6729611 ]. \t  0.992957199871945 \t 0.9942534235137975\n",
      "init   \t [ 4.16869519  5.05504008 11.          0.89221267 11.          0.51950888]. \t  0.880037272658953 \t 0.9942534235137975\n",
      "init   \t [ 2.37132124  4.35159182 13.          0.51218474 19.          0.43481903]. \t  0.8796339992862624 \t 0.9942534235137975\n",
      "init   \t [5.24304212 1.64265056 6.         0.81868728 1.         0.9370167 ]. \t  0.9912097053172655 \t 0.9942534235137975\n",
      "init   \t [ 8.16499915  7.69424925 10.          0.64918107 19.          0.99719505]. \t  0.9942534235137975 \t 0.9942534235137975\n",
      "1      \t [ 8.271983    7.77290851  9.38377945  0.64633445 18.38377945  0.83165214]. \t  0.9935813059047091 \t 0.9942534235137975\n",
      "2      \t [ 8.17970274  7.13182723  9.98609862  0.67636247 18.3315249   0.85327987]. \t  0.9938213451866532 \t 0.9942534235137975\n",
      "3      \t [ 7.72630198  7.23998797  9.42048224  0.62802282 18.81091664  0.93347503]. \t  0.9937733384503936 \t 0.9942534235137975\n",
      "4      \t [ 7.59173918  7.77049243  9.92070585  0.7746102  18.36851423  0.88080801]. \t  0.9939845726395965 \t 0.9942534235137975\n",
      "5      \t [ 8.0098178   7.50733833  9.74637691  0.97693959 18.7322773   0.31732632]. \t  0.8813094870208823 \t 0.9942534235137975\n",
      "6      \t [ 8.05539686  7.48678183  9.67715366  1.         18.56410288  1.        ]. \t  \u001b[92m0.9942918299814482\u001b[0m \t 0.9942918299814482\n",
      "7      \t [ 7.96233537  7.54387185  9.7739073   0.5        18.5641984   0.83628896]. \t  0.993797348352611 \t 0.9942918299814482\n",
      "8      \t [ 7.5964661   8.34805497  9.2152516   0.81106808 19.12212805  0.9526987 ]. \t  0.9941430020671209 \t 0.9942918299814482\n",
      "9      \t [ 7.10923163  7.82182752  9.90862747  0.93203892 19.44198512  1.        ]. \t  0.9941093999169736 \t 0.9942918299814482\n",
      "10     \t [ 7.54936149  8.62798016 10.18964041  1.         19.11915113  0.96114881]. \t  0.9900959172713902 \t 0.9942918299814482\n",
      "11     \t [ 7.4600883   6.86729703 10.55280322  0.91156929 18.94086411  0.97572869]. \t  \u001b[92m0.9947623072395707\u001b[0m \t 0.9947623072395707\n",
      "12     \t [ 7.48049021  7.79686817 10.88047889  1.         19.01000483  0.98328057]. \t  0.9900527089062687 \t 0.9947623072395707\n",
      "13     \t [ 0.98939661  8.82816712  7.00001287  0.65617869 11.20874054  0.71920536]. \t  0.9931492318644795 \t 0.9947623072395707\n",
      "14     \t [ 1.05776846  8.67107246  7.54841384  0.83514743 11.78808356  0.41977101]. \t  0.8784577988808587 \t 0.9947623072395707\n",
      "15     \t [ 1.57029844  9.31894011  7.34134887  0.59499349 11.48756625  0.81737496]. \t  0.9929379980209955 \t 0.9947623072395707\n",
      "16     \t [ 0.74614017  9.28748439  7.57423735  0.5        11.47958773  1.        ]. \t  0.9927747773441428 \t 0.9947623072395707\n",
      "17     \t [ 0.94152023  9.51748253  7.29214725  0.90971126 11.32839268  0.35964726]. \t  0.8797636294291232 \t 0.9947623072395707\n",
      "18     \t [ 1.08844037  9.10171724  7.23211523  1.         11.60222339  1.        ]. \t  0.9936341202773736 \t 0.9947623072395707\n",
      "19     \t [ 7.53335537  7.79940308 10.13289858  1.         19.04236914  0.98214226]. \t  0.9900527089062687 \t 0.9947623072395707\n",
      "20     \t [ 6.78215743  8.41526916 10.29431413  0.5        19.03677051  1.        ]. \t  0.9931876431721949 \t 0.9947623072395707\n",
      "21     \t [ 7.39253655  8.29166956 10.46350715  0.5        19.75373317  0.835091  ]. \t  0.9939893799994263 \t 0.9947623072395707\n",
      "22     \t [ 6.86760172  7.47478554 10.74186221  0.5        19.18836652  0.57029583]. \t  0.8778816963345965 \t 0.9947623072395707\n",
      "23     \t [ 7.03696187  8.50941116  9.78918748  0.63898373 19.33375933  0.34497583]. \t  0.8784242177504217 \t 0.9947623072395707\n",
      "24     \t [ 7.59539912  8.54556088 10.88698714  0.5        18.76746274  0.60157107]. \t  0.877267191261879 \t 0.9947623072395707\n",
      "25     \t [ 8.04778587  7.05399998 11.21113908  0.58390823 18.68729662  0.7354745 ]. \t  0.9941766107859277 \t 0.9947623072395707\n",
      "26     \t [ 7.39538312  7.13398848 11.05182458  0.71203377 18.10825015  0.97512662]. \t  0.9946902947497209 \t 0.9947623072395707\n",
      "27     \t [ 7.64658377  7.0098628  10.89398127  1.         18.33047998  0.33057091]. \t  0.8848668718261182 \t 0.9947623072395707\n",
      "28     \t [ 7.56258944  7.42936323 10.83841827  0.5        18.74661784  1.        ]. \t  0.9933700730292382 \t 0.9947623072395707\n",
      "29     \t [ 8.10260572  6.47872836 10.92887973  1.         18.24639283  1.        ]. \t  0.9947287030842538 \t 0.9947623072395707\n",
      "30     \t [ 8.32086223  7.47246779 11.01495189  1.         18.06960728  1.        ]. \t  \u001b[92m0.9949975509852721\u001b[0m \t 0.9949975509852721\n"
     ]
    }
   ],
   "source": [
    "### 6(k). Bayesian optimization runs (x20): GP run number = 11\n",
    "\n",
    "np.random.seed(run_num_11)\n",
    "surrogate_gp_11 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "X_train11, X_test11, y_train11, y_test11 = train_test_split(X, y, test_size=0.15, random_state=run_num_11)\n",
    "\n",
    "def f_syn_polarity11(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    clf = XGBClassifier(reg_alpha=alpha, gamma=gamma,max_depth=int(max_depth),subsample=subsample,\n",
    "                              min_child_weight=min_child_weight,colsample_bytree=colsample,objective=obj_classifier, \n",
    "                             booster='gbtree', n_estimators = 5,\n",
    "                             silent=None, random_state=run_num_11)\n",
    "    score = np.array(cross_val_score(clf, X=X_train11, y=y_train11).mean())\n",
    "    return score\n",
    "\n",
    "gpgo_gp_11 = GPGO(surrogate_gp_11, Acquisition_new(util_gp), f_syn_polarity11, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_11.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.95549199  9.25003702  7.          0.65523847 12.          0.6729611 ]. \t  0.992957199871945 \t 0.9942534235137975\n",
      "init   \t [ 4.16869519  5.05504008 11.          0.89221267 11.          0.51950888]. \t  0.880037272658953 \t 0.9942534235137975\n",
      "init   \t [ 2.37132124  4.35159182 13.          0.51218474 19.          0.43481903]. \t  0.8796339992862624 \t 0.9942534235137975\n",
      "init   \t [5.24304212 1.64265056 6.         0.81868728 1.         0.9370167 ]. \t  0.9912097053172655 \t 0.9942534235137975\n",
      "init   \t [ 8.16499915  7.69424925 10.          0.64918107 19.          0.99719505]. \t  0.9942534235137975 \t 0.9942534235137975\n",
      "1      \t [ 7.67502557  7.91440707 10.9300382   0.61722977 18.69001465  0.86427173]. \t  0.9940517892474846 \t 0.9942534235137975\n",
      "2      \t [ 7.66349604  6.97087878 10.44213859  0.65996873 18.61556039  0.86594976]. \t  0.9941814101942223 \t 0.9942534235137975\n",
      "3      \t [ 7.25016945  7.59926338 10.29436085  0.61576103 19.28458341  0.84676803]. \t  0.9941478055548988 \t 0.9942534235137975\n",
      "4      \t [ 8.00880274  7.32068674 10.81156027  0.70731586 19.36420567  1.        ]. \t  0.9938885620711164 \t 0.9942534235137975\n",
      "5      \t [ 7.84883185  7.54952843 10.45456739  0.98110095 18.97028214  0.32372262]. \t  0.8812950831953518 \t 0.9942534235137975\n",
      "6      \t [ 7.6577251   7.56269169 10.45509507  1.         18.8701086   1.        ]. \t  \u001b[92m0.9946182847490473\u001b[0m \t 0.9946182847490473\n",
      "7      \t [ 7.79128164  7.50573571 10.48355045  0.5        18.95442521  0.87240948]. \t  0.9941814117153855 \t 0.9946182847490473\n",
      "8      \t [ 7.88927871  6.78941047  9.72025301  0.89383225 19.50131525  0.96746109]. \t  0.9940949936022667 \t 0.9946182847490473\n",
      "9      \t [ 7.03054441  7.27832918 11.45472087  0.77979299 18.92938307  0.69761742]. \t  \u001b[92m0.9948775260202286\u001b[0m \t 0.9948775260202286\n",
      "10     \t [ 7.16590912  6.53258042 10.63907664  0.92257427 19.41061498  0.76722545]. \t  0.9948391201748721 \t 0.9948775260202286\n",
      "11     \t [ 7.26933145  8.20481608 11.43473715  0.85272289 19.47473952  0.76852161]. \t  0.9948199170793344 \t 0.9948775260202286\n",
      "12     \t [ 7.79393594  7.90892882 11.9430209   0.9795877  18.87102363  0.78732809]. \t  0.9946854931288253 \t 0.9948775260202286\n",
      "13     \t [ 7.09144675  6.63847412  9.65808571  0.80556956 18.93234596  0.6642795 ]. \t  0.8778001030746846 \t 0.9948775260202286\n",
      "14     \t [ 7.50926548  7.6533656  11.32405296  0.89591888 19.1453396   0.78186331]. \t  \u001b[92m0.9949399360980123\u001b[0m \t 0.9949399360980123\n",
      "15     \t [ 7.12759661  8.31704429 11.82813504  0.59858783 18.6665757   0.78968671]. \t  0.9944502562975023 \t 0.9949399360980123\n",
      "16     \t [ 7.96452187  8.73324501 11.57884307  0.67062145 19.04395204  1.        ]. \t  0.9937781424221782 \t 0.9949399360980123\n",
      "17     \t [ 7.64033364  6.96030957 10.19843231  0.84636456 19.23747411  0.87969761]. \t  0.9949015305292308 \t 0.9949399360980123\n",
      "18     \t [ 6.58570908  6.63276762 10.72348884  0.60671535 18.68496843  0.72137692]. \t  0.9941958131208838 \t 0.9949399360980123\n",
      "19     \t [ 8.09518173  8.28773036 10.46374988  0.84358945 19.74566343  1.        ]. \t  0.9943398399674656 \t 0.9949399360980123\n",
      "20     \t [ 8.57732872  7.41024926  9.85660712  0.84425398 19.97833704  1.        ]. \t  0.9939941806523093 \t 0.9949399360980123\n",
      "21     \t [ 7.23644992  6.20161966 11.14788653  0.81871267 18.63649826  0.5447802 ]. \t  0.8789090896379516 \t 0.9949399360980123\n",
      "22     \t [ 7.85864043  7.6494014   9.36130743  1.         19.79983154  0.96117347]. \t  0.9897454558513318 \t 0.9949399360980123\n",
      "23     \t [ 7.60400597  8.6569178  11.72385327  1.         18.90474683  0.3187668 ]. \t  0.8846652406021317 \t 0.9949399360980123\n",
      "24     \t [ 7.91451399  7.45228303  9.89396915  0.5        20.          1.        ]. \t  0.9930820241761394 \t 0.9949399360980123\n",
      "25     \t [ 7.67210897  8.27406758 11.96945874  0.5        19.20830679  0.74224156]. \t  0.9943542403358071 \t 0.9949399360980123\n",
      "26     \t [ 8.11589631  7.61383596 10.00268434  0.91460376 19.61126383  1.        ]. \t  0.9949015347470017 \t 0.9949399360980123\n",
      "27     \t [ 8.74334289  8.04452111 11.08468375  0.68103816 19.38178537  0.99724707]. \t  0.9942006138429104 \t 0.9949399360980123\n",
      "28     \t [ 6.88655141  7.02298953 10.65570694  1.         18.67848474  0.50756388]. \t  0.8843723826606086 \t 0.9949399360980123\n",
      "29     \t [ 8.23137358  7.15744324  9.12348495  0.56851628 19.76454426  0.62537857]. \t  0.8786546319411386 \t 0.9949399360980123\n",
      "30     \t [ 7.00965449  6.61982605 10.95350006  0.5        19.04996883  1.        ]. \t  0.9932692577975356 \t 0.9949399360980123\n"
     ]
    }
   ],
   "source": [
    "### 6(k). Bayesian optimization runs (x20): STP DF1 run number = 11\n",
    "\n",
    "np.random.seed(run_num_11)\n",
    "surrogate_stp_df1_11 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_11 = GPGO(surrogate_stp_df1_11, Acquisition_new(util_stp), f_syn_polarity11, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_11.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.297827683516764, -5.286376166911191)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(k). Training Regret Minimisation: run number = 11\n",
    "\n",
    "gp_output_11 = np.append(np.max(gpgo_gp_11.GP.y[0:n_init]),gpgo_gp_11.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_11 = np.append(np.max(gpgo_stp_df1_11.GP.y[0:n_init]),gpgo_stp_df1_11.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_11 = np.log(y_global_orig - gp_output_11)\n",
    "regret_stp_df1_11 = np.log(y_global_orig - stp_df1_output_11)\n",
    "\n",
    "train_regret_gp_11 = min_max_array(regret_gp_11)\n",
    "train_regret_stp_df1_11 = min_max_array(regret_stp_df1_11)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 11\n",
    "min_train_regret_gp_11 = min(train_regret_gp_11)\n",
    "min_train_regret_stp_df1_11 = min(train_regret_stp_df1_11)\n",
    "\n",
    "min_train_regret_gp_11, min_train_regret_stp_df1_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 9.6238432   6.94220364 14.          0.60068848 17.          0.64107078]. \t  0.8573627321615301 \t 0.9771961242579525\n",
      "init   \t [ 9.70248741  2.88838338 11.          0.56945199 12.          0.62104197]. \t  0.856993066332202 \t 0.9771961242579525\n",
      "init   \t [8.90655999 3.53839188 8.         0.98537995 1.         0.66949383]. \t  0.9752517909732029 \t 0.9771961242579525\n",
      "init   \t [ 7.3853305   5.34255009 10.          0.90708741 18.          0.78973723]. \t  0.9771961242579525 \t 0.9771961242579525\n",
      "init   \t [ 5.65590842  1.36833384 10.          0.86793408 17.          0.47248995]. \t  0.8570890771772571 \t 0.9771961242579525\n",
      "1      \t [ 7.02707205  4.51901425 10.00018655  0.89901162 17.79271223  0.72399773]. \t  \u001b[92m0.9773257463109908\u001b[0m \t 0.9773257463109908\n",
      "2      \t [ 6.97656902  4.85423633 10.28145084  0.87887616 18.58050551  0.8217898 ]. \t  0.9772489329608266 \t 0.9773257463109908\n",
      "3      \t [ 7.39940505  4.84102929 10.73915643  0.82971804 17.94009789  0.7105647 ]. \t  0.9765528078004097 \t 0.9773257463109908\n",
      "4      \t [ 7.64505704  4.63899522 10.07024253  0.53267138 18.23707713  1.        ]. \t  \u001b[92m0.9934468874857026\u001b[0m \t 0.9934468874857026\n",
      "5      \t [ 7.33087492  4.84689873 10.12741976  0.50594507 18.20529387  0.21123225]. \t  0.8565705934594502 \t 0.9934468874857026\n",
      "6      \t [ 6.9777001   4.99708299 10.29361495  0.5        17.94619786  1.        ]. \t  0.9933172652943769 \t 0.9934468874857026\n",
      "7      \t [ 7.33680921  4.79992368 10.22701822  1.         18.09893787  1.        ]. \t  \u001b[92m0.994983147920323\u001b[0m \t 0.994983147920323\n",
      "8      \t [8.94354702 3.81639148 8.82649882 0.89317435 1.27549395 0.67623572]. \t  0.9753958096608182 \t 0.994983147920323\n",
      "9      \t [8.60022609 3.05753093 8.68689255 0.99347543 1.00038569 0.84439084]. \t  0.9755446387505898 \t 0.994983147920323\n",
      "10     \t [8.37279156 3.4908557  8.32552032 0.87444298 1.64157969 0.69668171]. \t  0.9767400346457151 \t 0.994983147920323\n",
      "11     \t [9.18267291 3.12330147 8.42179861 0.80543633 1.59099754 0.72984269]. \t  0.9749589335848299 \t 0.994983147920323\n",
      "12     \t [8.76801401 3.28630829 8.51986507 1.         1.27134241 0.1       ]. \t  0.8870512391378843 \t 0.994983147920323\n",
      "13     \t [8.7404096  3.4163993  8.4341176  0.5        1.15113846 0.73553324]. \t  0.9734274827226405 \t 0.994983147920323\n",
      "14     \t [8.83835813 3.47057756 8.42831085 1.         1.36850536 1.        ]. \t  0.9945462748866613 \t 0.994983147920323\n",
      "15     \t [ 7.80179739  4.85948744 10.08995276  0.5        17.16816718  0.76444449]. \t  0.9774889745245156 \t 0.994983147920323\n",
      "16     \t [ 7.63687124  3.93573498 10.48824105  0.5        17.33321829  0.80216845]. \t  0.9775225794404143 \t 0.994983147920323\n",
      "17     \t [ 6.99907719  3.8566916  10.66716345  0.5        18.26873349  0.85574793]. \t  0.9777146129541118 \t 0.994983147920323\n",
      "18     \t [ 7.36210042  4.43711732 10.36491618  0.5        17.8000003   0.79194191]. \t  0.9775657869066666 \t 0.994983147920323\n",
      "19     \t [ 6.78750802  2.86810447 10.40274805  0.82107684 17.48934299  0.72861628]. \t  0.9768984674612847 \t 0.994983147920323\n",
      "20     \t [ 7.58178068  3.23382983 10.45061747  1.         17.98609496  0.71549171]. \t  \u001b[92m0.9952519956830539\u001b[0m \t 0.9952519956830539\n",
      "21     \t [ 7.13119446  3.42165572 11.11877125  1.         17.56416375  0.69022659]. \t  \u001b[92m0.9953912195258642\u001b[0m \t 0.9953912195258642\n",
      "22     \t [ 7.38801854  3.46312267 10.38382089  1.         17.15768995  0.18386404]. \t  0.8869120086572707 \t 0.9953912195258642\n",
      "23     \t [ 6.66043364  3.37341049 10.59084487  1.         18.1961546   0.28401786]. \t  0.8881218148082985 \t 0.9953912195258642\n",
      "24     \t [ 7.36100592  2.93235981 10.89190603  0.5        17.74726567  0.41550931]. \t  0.8573531270528565 \t 0.9953912195258642\n",
      "25     \t [ 8.45962188  4.32140885 10.15832496  1.         17.40347257  0.59714046]. \t  0.8879874012293568 \t 0.9953912195258642\n",
      "26     \t [ 7.86975193  3.90312247  9.58604967  0.9166776  17.03550061  1.        ]. \t  0.9938789612493574 \t 0.9953912195258642\n",
      "27     \t [ 7.82141567  3.17225118 10.44127387  1.         16.9728403   1.        ]. \t  0.9950695616773836 \t 0.9953912195258642\n",
      "28     \t [ 7.32615254  3.5663301  10.31977547  1.         17.39020499  1.        ]. \t  0.9949927490186573 \t 0.9953912195258642\n",
      "29     \t [ 8.08427955  4.13600631 10.23655188  0.99376041 16.39193641  0.85138586]. \t  0.9777050065317062 \t 0.9953912195258642\n",
      "30     \t [ 8.40368297  3.69154655  9.90153064  0.5        16.68316224  0.64762509]. \t  0.8573051146468066 \t 0.9953912195258642\n"
     ]
    }
   ],
   "source": [
    "### 6(l). Bayesian optimization runs (x20): GP run number = 12\n",
    "\n",
    "np.random.seed(run_num_12)\n",
    "surrogate_gp_12 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "X_train12, X_test12, y_train12, y_test12 = train_test_split(X, y, test_size=0.15, random_state=run_num_12)\n",
    "\n",
    "def f_syn_polarity12(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    clf = XGBClassifier(reg_alpha=alpha, gamma=gamma,max_depth=int(max_depth),subsample=subsample,\n",
    "                              min_child_weight=min_child_weight,colsample_bytree=colsample,objective=obj_classifier, \n",
    "                             booster='gbtree', n_estimators = 5,\n",
    "                             silent=None, random_state=run_num_12)\n",
    "    score = np.array(cross_val_score(clf, X=X_train12, y=y_train12).mean())\n",
    "    return score\n",
    "\n",
    "\n",
    "gpgo_gp_12 = GPGO(surrogate_gp_12, Acquisition_new(util_gp), f_syn_polarity12, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_12.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 9.6238432   6.94220364 14.          0.60068848 17.          0.64107078]. \t  0.8573627321615301 \t 0.9771961242579525\n",
      "init   \t [ 9.70248741  2.88838338 11.          0.56945199 12.          0.62104197]. \t  0.856993066332202 \t 0.9771961242579525\n",
      "init   \t [8.90655999 3.53839188 8.         0.98537995 1.         0.66949383]. \t  0.9752517909732029 \t 0.9771961242579525\n",
      "init   \t [ 7.3853305   5.34255009 10.          0.90708741 18.          0.78973723]. \t  0.9771961242579525 \t 0.9771961242579525\n",
      "init   \t [ 5.65590842  1.36833384 10.          0.86793408 17.          0.47248995]. \t  0.8570890771772571 \t 0.9771961242579525\n",
      "1      \t [ 6.94089072  4.32145209 10.00010209  0.89699567 17.74305067  0.70820354]. \t  \u001b[92m0.9773161425851926\u001b[0m \t 0.9773161425851926\n",
      "2      \t [ 7.45954075  4.91229848  9.72219475  0.9221859  17.06663999  0.68151234]. \t  0.9768552676008486 \t 0.9773161425851926\n",
      "3      \t [ 7.57226192  4.75939567 10.63291074  0.85026463 17.49440996  0.64583746]. \t  0.8570314651248925 \t 0.9773161425851926\n",
      "4      \t [ 6.67471611  5.18001945 10.14184408  0.72747042 17.36967616  0.67691135]. \t  \u001b[92m0.9776665968834415\u001b[0m \t 0.9776665968834415\n",
      "5      \t [ 7.23185976  4.90138588  9.83586201  0.50020333 17.69184669  0.14185266]. \t  0.8573435213910324 \t 0.9776665968834415\n",
      "6      \t [ 7.22198546  4.85739782  9.9228453   0.5        17.56965759  1.        ]. \t  \u001b[92m0.993398879159136\u001b[0m \t 0.993398879159136\n",
      "7      \t [ 7.11767136  4.92780178  9.96936566  1.         17.57577985  0.72428928]. \t  \u001b[92m0.9948247225031382\u001b[0m \t 0.9948247225031382\n",
      "8      \t [ 6.8084827   4.14753272 10.12100879  0.51845737 16.7494933   0.54318844]. \t  0.8565465855624027 \t 0.9948247225031382\n",
      "9      \t [ 7.59372335  5.93300876 10.15081416  0.51762796 17.22871789  0.69325754]. \t  0.9775369818139255 \t 0.9948247225031382\n",
      "10     \t [ 6.24977307  2.72365827 10.01026506  0.85751067 17.35673447  0.58266998]. \t  0.8573339171120842 \t 0.9948247225031382\n",
      "11     \t [ 6.18507767  3.81513297 10.62340916  0.5        17.64356113  0.622734  ]. \t  0.8576363762210854 \t 0.9948247225031382\n",
      "12     \t [ 6.05178294  3.8755996   9.56166008  0.52962035 17.45580784  0.67984066]. \t  0.9779498469505397 \t 0.9948247225031382\n",
      "13     \t [ 5.23392141  2.47369229  9.83549914  0.5        17.00579389  0.52956837]. \t  0.8559944946469522 \t 0.9948247225031382\n",
      "14     \t [ 5.99849552  3.75351013  9.98395779  1.         17.30912379  0.1       ]. \t  0.8878481732379195 \t 0.9948247225031382\n",
      "15     \t [ 6.03079753  3.75269724 10.05909782  1.         17.1628253   1.        ]. \t  \u001b[92m0.9950503587201333\u001b[0m \t 0.9950503587201333\n",
      "16     \t [ 7.30063611  5.84114742  9.35170509  0.59923787 17.45393618  0.77740237]. \t  0.977440962395041 \t 0.9950503587201333\n",
      "17     \t [ 7.42886356  5.45854286  9.90622589  0.62980574 17.41814016  0.70248018]. \t  0.9774841682709864 \t 0.9950503587201333\n",
      "18     \t [ 7.17958774  6.52775466 10.03099198  0.67691358 17.78365335  0.98161698]. \t  0.977076099292909 \t 0.9950503587201333\n",
      "19     \t [ 7.31766518  6.41624766  9.77668821  1.         16.88514695  1.        ]. \t  0.9941382050788586 \t 0.9950503587201333\n",
      "20     \t [ 7.93073318  6.49753999  9.71742667  0.97061887 17.52520321  0.8725468 ]. \t  0.9768744697975172 \t 0.9950503587201333\n",
      "21     \t [ 6.57616723  4.64173578  9.29791983  0.82284629 16.95642263  0.90892472]. \t  0.9766488278415881 \t 0.9950503587201333\n",
      "22     \t [ 7.33440152  6.53129109  9.85086522  0.90291538 17.33679615  0.32606196]. \t  0.8591102276150789 \t 0.9950503587201333\n",
      "23     \t [ 6.37388819  4.37145153  9.93013206  0.65907187 17.31297682  0.76559605]. \t  0.9767880498175163 \t 0.9950503587201333\n",
      "24     \t [ 7.3863197   6.12585081 10.39965513  1.         17.40882727  1.        ]. \t  0.9949591434804647 \t 0.9950503587201333\n",
      "25     \t [ 5.45730447  3.17822109  9.98078326  0.96538327 17.63191615  0.77595333]. \t  0.9776714063175849 \t 0.9950503587201333\n",
      "26     \t [ 5.71217986  3.17231224  9.46172553  1.         17.00292023  0.75442922]. \t  0.9949207383265458 \t 0.9950503587201333\n",
      "27     \t [ 5.70917139  3.29636654  9.99545126  0.70307907 17.12881531  0.69532682]. \t  0.9777050047339678 \t 0.9950503587201333\n",
      "28     \t [ 7.03449208  3.93874405  9.24793435  0.93080838 17.03669778  0.76298894]. \t  0.9769032750285461 \t 0.9950503587201333\n",
      "29     \t [ 7.39038416  6.23450391  9.84829243  0.72839336 17.3578255   1.        ]. \t  0.9936581207760363 \t 0.9950503587201333\n",
      "30     \t [ 7.01360328  5.70545857 10.78700955  0.5        18.11665499  0.72296893]. \t  0.9775897928676883 \t 0.9950503587201333\n"
     ]
    }
   ],
   "source": [
    "### 6(l). Bayesian optimization runs (x20): STP DF1 run number = 12\n",
    "\n",
    "np.random.seed(run_num_12)\n",
    "surrogate_stp_df1_12 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_12 = GPGO(surrogate_stp_df1_12, Acquisition_new(util_stp), f_syn_polarity12, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_12.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.379791996203266, -5.308440173741311)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(l). Training Regret Minimisation: run number = 12\n",
    "\n",
    "gp_output_12 = np.append(np.max(gpgo_gp_12.GP.y[0:n_init]),gpgo_gp_12.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_12 = np.append(np.max(gpgo_stp_df1_12.GP.y[0:n_init]),gpgo_stp_df1_12.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_12 = np.log(y_global_orig - gp_output_12)\n",
    "regret_stp_df1_12 = np.log(y_global_orig - stp_df1_output_12)\n",
    "\n",
    "train_regret_gp_12 = min_max_array(regret_gp_12)\n",
    "train_regret_stp_df1_12 = min_max_array(regret_stp_df1_12)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 12\n",
    "min_train_regret_gp_12 = min(train_regret_gp_12)\n",
    "min_train_regret_stp_df1_12 = min(train_regret_stp_df1_12)\n",
    "\n",
    "min_train_regret_gp_12, min_train_regret_stp_df1_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 2.99585426  0.12569841 12.          0.92487106 14.          0.12066157]. \t  0.8956879389264313 \t 0.9851511111471544\n",
      "init   \t [5.48040248 5.46008077 7.         0.70510802 9.         0.68228527]. \t  0.9851511111471544 \t 0.9851511111471544\n",
      "init   \t [ 8.36045309  6.9875368  14.          0.56453019 16.          0.1837942 ]. \t  0.8951454426097993 \t 0.9851511111471544\n",
      "init   \t [6.0919376  3.49625894 6.         0.83970528 4.         0.9658705 ]. \t  0.9822226212039785 \t 0.9851511111471544\n",
      "init   \t [ 4.66489813  6.85692341  9.          0.96730546 13.          0.54391367]. \t  0.896979361500538 \t 0.9851511111471544\n",
      "1      \t [5.32462041 5.72693227 7.38207785 0.75519582 9.76414294 0.65584904]. \t  0.8948285896319724 \t 0.9851511111471544\n",
      "2      \t [4.93566025 5.72647211 7.65581821 0.75839908 8.88083174 0.69646712]. \t  \u001b[92m0.9852903364419842\u001b[0m \t 0.9852903364419842\n",
      "3      \t [5.71763045 5.39736335 7.85597572 0.70334811 9.05584498 0.61388286]. \t  0.8967153126620957 \t 0.9852903364419842\n",
      "4      \t [4.97999306 4.96493569 7.45515277 0.66971657 9.26681139 0.83687557]. \t  0.9852183217395333 \t 0.9852903364419842\n",
      "5      \t [5.09841046 5.28428006 7.39390645 1.         9.16264173 0.1       ]. \t  0.9070898352420412 \t 0.9852903364419842\n",
      "6      \t [5.26232199 5.41807676 7.43580019 1.         9.1507476  1.        ]. \t  \u001b[92m0.9930052100653937\u001b[0m \t 0.9930052100653937\n",
      "7      \t [5.76609019 3.54845486 6.84055268 0.85609054 4.0019665  0.84285032]. \t  0.982294634108691 \t 0.9930052100653937\n",
      "8      \t [5.39916812 3.98386979 6.19842059 0.91856162 4.21777127 1.        ]. \t  0.9892365728169962 \t 0.9930052100653937\n",
      "9      \t [6.17028876 4.01353429 6.49161422 0.85299457 4.53446438 0.96972665]. \t  0.9823090367587773 \t 0.9930052100653937\n",
      "10     \t [5.6745234  3.34830943 6.30905946 0.78492721 4.67282192 0.74638876]. \t  0.9822610286396422 \t 0.9930052100653937\n",
      "11     \t [5.85404473 3.87608077 6.27780076 0.77630064 4.16805906 0.27210483]. \t  0.8943581013108445 \t 0.9930052100653937\n",
      "12     \t [5.77824426 3.71518018 6.37927152 0.5        4.24663839 1.        ]. \t  0.9888621100685495 \t 0.9930052100653937\n",
      "13     \t [5.83054003 3.64282531 6.37002675 1.         4.29730161 0.91538686]. \t  0.9924243128126139 \t 0.9930052100653937\n",
      "14     \t [5.25430651 4.11879545 7.0904397  0.74047076 4.82085962 0.62079371]. \t  0.8963840485986408 \t 0.9930052100653937\n",
      "15     \t [5.47997047 4.31776003 5.97203272 0.67560325 5.25541744 0.7277505 ]. \t  0.9740564505313918 \t 0.9930052100653937\n",
      "16     \t [6.01732552 3.9372715  5.28149656 0.68879643 4.70540761 0.89821211]. \t  0.9740180437180223 \t 0.9930052100653937\n",
      "17     \t [6.39080263 3.76275954 5.89991163 0.5        5.33697031 0.53873309]. \t  0.8950782204012647 \t 0.9930052100653937\n",
      "18     \t [5.37379576 3.52818131 5.42825507 0.5        5.08564581 0.3573062 ]. \t  0.8951454359028523 \t 0.9930052100653937\n",
      "19     \t [6.3867639  2.96866972 5.32896182 0.5        4.52100202 0.50622615]. \t  0.8948141768868937 \t 0.9930052100653937\n",
      "20     \t [5.55807904 4.54974076 7.27871064 0.89552907 3.76972683 0.93880249]. \t  0.9854055552917859 \t 0.9930052100653937\n",
      "21     \t [5.48783237 5.0422194  6.64696583 0.76400544 4.46756703 0.8982923 ]. \t  0.9822370248220779 \t 0.9930052100653937\n",
      "22     \t [5.93818438 3.16964089 5.43715221 0.79206109 5.28836341 1.        ]. \t  0.9840180868167253 \t 0.9930052100653937\n",
      "23     \t [4.8481526  4.56984747 6.83484706 0.60342298 3.93999695 0.60718645]. \t  0.8938780194971984 \t 0.9930052100653937\n",
      "24     \t [5.8558492  4.62389528 6.42399057 0.74680631 3.47207554 0.9729468 ]. \t  0.9822130210045135 \t 0.9930052100653937\n",
      "25     \t [5.62662336 4.43502888 6.73164098 0.71494432 4.09524536 0.77887537]. \t  0.9822802324957616 \t 0.9930052100653937\n",
      "26     \t [5.38465079 3.98689594 6.86404304 1.         2.92885161 1.        ]. \t  0.989370997251512 \t 0.9930052100653937\n",
      "27     \t [6.35221252 3.96231464 7.0181348  1.         3.06526786 1.        ]. \t  \u001b[92m0.993274055753811\u001b[0m \t 0.993274055753811\n",
      "28     \t [4.62184731 4.85048892 6.6734375  1.         4.86607211 1.        ]. \t  0.9893805985572776 \t 0.993274055753811\n",
      "29     \t [5.99311199 3.73651323 6.57003946 0.5        2.91570007 0.60631047]. \t  0.8944589191008667 \t 0.993274055753811\n",
      "30     \t [5.72457935 3.84140122 7.517125   0.5        3.11420758 1.        ]. \t  0.9924339128046477 \t 0.993274055753811\n"
     ]
    }
   ],
   "source": [
    "### 6(m). Bayesian optimization runs (x20): GP run number = 13\n",
    "\n",
    "np.random.seed(run_num_13)\n",
    "surrogate_gp_13 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "X_train13, X_test13, y_train13, y_test13 = train_test_split(X, y, test_size=0.15, random_state=run_num_13)\n",
    "\n",
    "def f_syn_polarity13(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    clf = XGBClassifier(reg_alpha=alpha, gamma=gamma,max_depth=int(max_depth),subsample=subsample,\n",
    "                              min_child_weight=min_child_weight,colsample_bytree=colsample,objective=obj_classifier, \n",
    "                             booster='gbtree', n_estimators = 5,\n",
    "                             silent=None, random_state=run_num_13)\n",
    "    score = np.array(cross_val_score(clf, X=X_train13, y=y_train13).mean())\n",
    "    return score\n",
    "\n",
    "\n",
    "gpgo_gp_13 = GPGO(surrogate_gp_13, Acquisition_new(util_gp), f_syn_polarity13, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_13.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 2.99585426  0.12569841 12.          0.92487106 14.          0.12066157]. \t  0.8956879389264313 \t 0.9851511111471544\n",
      "init   \t [5.48040248 5.46008077 7.         0.70510802 9.         0.68228527]. \t  0.9851511111471544 \t 0.9851511111471544\n",
      "init   \t [ 8.36045309  6.9875368  14.          0.56453019 16.          0.1837942 ]. \t  0.8951454426097993 \t 0.9851511111471544\n",
      "init   \t [6.0919376  3.49625894 6.         0.83970528 4.         0.9658705 ]. \t  0.9822226212039785 \t 0.9851511111471544\n",
      "init   \t [ 4.66489813  6.85692341  9.          0.96730546 13.          0.54391367]. \t  0.896979361500538 \t 0.9851511111471544\n",
      "1      \t [5.2858694  5.79307474 7.47696861 0.76763618 9.95391324 0.64930613]. \t  0.894660559382689 \t 0.9851511111471544\n",
      "2      \t [5.83488294 5.95850669 7.89055394 0.67018426 9.0281957  0.62837054]. \t  0.8960383959904314 \t 0.9851511111471544\n",
      "3      \t [5.88138594 6.30019362 6.914094   0.5959849  9.4189374  0.8430745 ]. \t  0.981785747478879 \t 0.9851511111471544\n",
      "4      \t [5.45396906 6.17209078 7.1332483  1.         9.21475392 0.1       ]. \t  0.9070130201632826 \t 0.9851511111471544\n",
      "5      \t [5.06115559 6.21797317 7.30043576 0.54422489 9.09378086 0.93509411]. \t  0.9849638779406208 \t 0.9851511111471544\n",
      "6      \t [5.55597005 5.96063029 7.27632008 1.         9.26305223 1.        ]. \t  \u001b[92m0.993024413990657\u001b[0m \t 0.993024413990657\n",
      "7      \t [5.52476659 5.93267738 7.24875164 0.5        9.31833154 0.5882264 ]. \t  0.8940124415808253 \t 0.993024413990657\n",
      "8      \t [5.71337683 6.28071713 6.81549025 0.80362265 8.30583826 0.79925698]. \t  0.9822850328720693 \t 0.993024413990657\n",
      "9      \t [5.1882704  6.17423993 6.20952608 0.91217113 9.05422847 0.86402661]. \t  0.9820641960633688 \t 0.993024413990657\n",
      "10     \t [4.78274941 5.72683268 6.91978022 1.         8.35285514 0.6483664 ]. \t  0.9078435581420597 \t 0.993024413990657\n",
      "11     \t [5.47604734 7.12239499 6.78238992 0.93573254 9.07457486 0.8730756 ]. \t  0.9823138387253921 \t 0.993024413990657\n",
      "12     \t [ 5.08764118  6.7123988   6.8063927   0.9963459  10.08053953  0.83663192]. \t  0.9822802318043237 \t 0.993024413990657\n",
      "13     \t [5.48970721 6.91335112 7.76531552 0.94207697 9.83763479 0.72806629]. \t  0.9852423277696988 \t 0.993024413990657\n",
      "14     \t [5.340421   6.54403517 7.00810984 0.95643575 9.31379244 0.77150439]. \t  0.9852039208180416 \t 0.993024413990657\n",
      "15     \t [ 5.7513644   7.33818413  6.89784029  0.54215128 10.07397291  1.        ]. \t  0.9886700782834467 \t 0.993024413990657\n",
      "16     \t [ 5.49250654  6.66751419  7.46571479  0.81435741 10.7955234   1.        ]. \t  0.9926979588773387 \t 0.993024413990657\n",
      "17     \t [ 5.87487651  6.82674982  7.09018829  1.         10.40906444  0.37059767]. \t  0.9068113863118322 \t 0.993024413990657\n",
      "18     \t [5.82352993 6.97423752 6.03782991 0.5        9.15280149 1.        ]. \t  0.9887468905964538 \t 0.993024413990657\n",
      "19     \t [ 4.98551038  6.97542133  7.46321197  0.52330714 10.51959905  0.51397022]. \t  0.8954526944892921 \t 0.993024413990657\n",
      "20     \t [5.30009561 6.56216963 6.46883796 0.5        8.68761656 1.        ]. \t  0.988674878728898 \t 0.993024413990657\n",
      "21     \t [4.99490387 5.51784657 7.91914599 1.         8.75678023 0.64782085]. \t  0.9070610300801561 \t 0.993024413990657\n",
      "22     \t [ 5.45896941  6.74815101  7.26320943  0.66592497 10.15913418  1.        ]. \t  0.991891424093832 \t 0.993024413990657\n",
      "23     \t [6.32942024 7.21949239 6.88143284 0.56496355 9.05072543 0.95949526]. \t  0.9818721613050831 \t 0.993024413990657\n",
      "24     \t [6.10019987 7.03116938 6.46597683 1.         9.57019476 1.        ]. \t  0.9889485245170477 \t 0.993024413990657\n",
      "25     \t [5.90697555 7.1970771  6.5209064  0.62383422 9.3542171  0.41166806]. \t  0.8938396200130697 \t 0.993024413990657\n",
      "26     \t [ 5.20145686  6.45782635  8.02513972  1.         10.6510304   0.4562132 ]. \t  0.9067009697743643 \t 0.993024413990657\n",
      "27     \t [5.90705771 6.57238317 6.53387678 0.72227479 8.9398312  1.        ]. \t  0.9887948984390138 \t 0.993024413990657\n",
      "28     \t [5.3370245  5.88947237 7.38200816 0.71130695 8.40558236 1.        ]. \t  0.991771400926527 \t 0.993024413990657\n",
      "29     \t [ 5.58415833  6.61360685  6.02593603  0.59646914 10.23474459  1.        ]. \t  0.9886796793817808 \t 0.993024413990657\n",
      "30     \t [5.49893029 7.03230772 6.27865587 0.60807968 9.6781567  1.        ]. \t  0.9886892808949778 \t 0.993024413990657\n"
     ]
    }
   ],
   "source": [
    "### 6(m). Bayesian optimization runs (x20): STP DF1 run number = 13\n",
    "\n",
    "np.random.seed(run_num_13)\n",
    "surrogate_stp_df1_13 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_13 = GPGO(surrogate_stp_df1_13, Acquisition_new(util_stp), f_syn_polarity13, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_13.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.001782954995323, -4.9653389391274105)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(m). Training Regret Minimisation: run number = 13\n",
    "\n",
    "gp_output_13 = np.append(np.max(gpgo_gp_13.GP.y[0:n_init]),gpgo_gp_13.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_13 = np.append(np.max(gpgo_stp_df1_13.GP.y[0:n_init]),gpgo_stp_df1_13.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_13 = np.log(y_global_orig - gp_output_13)\n",
    "regret_stp_df1_13 = np.log(y_global_orig - stp_df1_output_13)\n",
    "\n",
    "train_regret_gp_13 = min_max_array(regret_gp_13)\n",
    "train_regret_stp_df1_13 = min_max_array(regret_stp_df1_13)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 13\n",
    "min_train_regret_gp_13 = min(train_regret_gp_13)\n",
    "min_train_regret_stp_df1_13 = min(train_regret_stp_df1_13)\n",
    "\n",
    "min_train_regret_gp_13, min_train_regret_stp_df1_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 2.67559603  8.13471837  8.          0.78152314 13.          0.81243369]. \t  0.9931252362750252 \t 0.9938213539679138\n",
      "init   \t [6.74508797 3.16072807 7.         0.9769074  6.         0.6095684 ]. \t  0.8864703338644256 \t 0.9938213539679138\n",
      "init   \t [ 0.65196398  0.45835711  7.          0.85774934 12.          0.26042982]. \t  0.8885587002878413 \t 0.9938213539679138\n",
      "init   \t [ 2.6759138   7.69220817 11.          0.52843413 18.          0.91623157]. \t  0.9938213539679138 \t 0.9938213539679138\n",
      "init   \t [ 4.37302805  7.27284651  8.          0.94165902 10.          0.72862239]. \t  0.9930628255058037 \t 0.9938213539679138\n",
      "1      \t [ 3.15581067  7.89088647  7.99999858  0.82682706 12.15128086  0.78872245]. \t  0.9916993943138989 \t 0.9938213539679138\n",
      "2      \t [ 3.89619186  7.51496155  8.00000171  0.89668138 10.84274674  0.75216641]. \t  0.9931348392402416 \t 0.9938213539679138\n",
      "3      \t [ 2.40772411  8.34353008  8.31151903  0.87094247 12.24051086  0.99531326]. \t  0.9931444393014193 \t 0.9938213539679138\n",
      "4      \t [ 3.10233789  8.75300991  8.11223634  0.84087764 12.51739512  0.88471728]. \t  0.9931060335943501 \t 0.9938213539679138\n",
      "5      \t [ 3.03288762  8.14970709  8.72416465  0.9167316  12.65342565  0.95728594]. \t  0.9929188026695615 \t 0.9938213539679138\n",
      "6      \t [ 2.79021977  8.33475709  8.40847573  0.5841753  12.46351951  0.30219959]. \t  0.8864655379133201 \t 0.9938213539679138\n",
      "7      \t [ 2.90713962  8.24405894  8.24756153  0.5        12.5120012   1.        ]. \t  \u001b[92m0.994181416693738\u001b[0m \t 0.994181416693738\n",
      "8      \t [ 4.04887989  7.94909927  8.48931301  1.         10.21446475  0.846257  ]. \t  0.9818241210340887 \t 0.994181416693738\n",
      "9      \t [ 4.28901106  7.18824999  8.71755295  1.         10.54466388  0.75519883]. \t  0.9819969494470788 \t 0.994181416693738\n",
      "10     \t [ 3.57175067  7.20517034  8.38442111  1.         10.13230715  0.81988966]. \t  0.9818529269565553 \t 0.994181416693738\n",
      "11     \t [ 3.98509414  7.48723293  8.36198096  1.         10.30075064  0.10281408]. \t  0.8776992776097022 \t 0.994181416693738\n",
      "12     \t [ 3.30073519  8.0660103   8.74556231  1.         11.32909106  0.89204275]. \t  0.9814976684790907 \t 0.994181416693738\n",
      "13     \t [ 3.9812277   7.44983993  8.44219427  0.5        10.35413126  0.82194163]. \t  0.992962010412389 \t 0.994181416693738\n",
      "14     \t [ 2.90531279  8.26340531  8.2527813   1.         12.4032885   0.83939763]. \t  0.9819873489710386 \t 0.994181416693738\n",
      "15     \t [ 2.37217747  8.95976215  8.59755129  0.75996566 13.18274617  1.        ]. \t  \u001b[92m0.9945030694947224\u001b[0m \t 0.9945030694947224\n",
      "16     \t [ 3.11176532  7.15234895  8.63641212  0.75195565 11.57537851  0.82156691]. \t  0.9929524078620351 \t 0.9945030694947224\n",
      "17     \t [ 3.91370295  7.58739881  8.70956428  0.68488963 11.87428221  0.68358477]. \t  0.992928403975327 \t 0.9945030694947224\n",
      "18     \t [ 2.17534663  9.09472966  7.69031445  0.63548503 12.82943675  0.85159713]. \t  0.9913249329483277 \t 0.9945030694947224\n",
      "19     \t [ 2.9348892   9.13536568  7.87467695  0.64107533 13.48981288  0.68835734]. \t  0.9912481206353204 \t 0.9945030694947224\n",
      "20     \t [ 2.75104122  9.7226041   8.22873784  0.55846697 12.78009624  0.78278831]. \t  0.9929236044287447 \t 0.9945030694947224\n",
      "21     \t [ 3.60797048  7.4507251   8.51959039  1.         11.57984393  1.        ]. \t  0.9943110377787633 \t 0.9945030694947224\n",
      "22     \t [ 3.35329317  7.66083193  8.46822542  0.56703225 11.37922193  0.49005441]. \t  0.886734385952626 \t 0.9945030694947224\n",
      "23     \t [ 3.2925498   7.0470004   8.52913084  0.65692827 12.69695726  0.61645327]. \t  0.8867151725546645 \t 0.9945030694947224\n",
      "24     \t [ 3.16700453  7.50260992  9.5013133   0.73489084 12.06209358  0.89460416]. \t  0.9939077691078498 \t 0.9945030694947224\n",
      "25     \t [ 2.30348443  7.35513592  8.9034365   0.7632511  12.36176332  0.95229495]. \t  0.9929668098206835 \t 0.9945030694947224\n",
      "26     \t [ 2.3138857   7.8706545   9.33216838  0.94251332 11.6073259   1.        ]. \t  \u001b[92m0.9948871343095164\u001b[0m \t 0.9948871343095164\n",
      "27     \t [ 2.49346143  9.31976887  8.10560351  1.         13.05194477  0.38780464]. \t  0.8786738377332838 \t 0.9948871343095164\n",
      "28     \t [ 2.85918093  7.548255    9.00317659  1.         12.00266093  0.62695277]. \t  0.8781649524172037 \t 0.9948871343095164\n",
      "29     \t [ 2.82338597  7.69842274  9.03232329  0.5        11.86063726  1.        ]. \t  0.9942870343069178 \t 0.9948871343095164\n",
      "30     \t [ 3.28304479  7.543987    9.57321396  0.88803569 10.82920488  1.        ]. \t  0.9946903022172494 \t 0.9948871343095164\n"
     ]
    }
   ],
   "source": [
    "### 6(n). Bayesian optimization runs (x20): GP run number = 14\n",
    "\n",
    "np.random.seed(run_num_14)\n",
    "surrogate_gp_14 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "X_train14, X_test14, y_train14, y_test14 = train_test_split(X, y, test_size=0.15, random_state=run_num_14)\n",
    "\n",
    "def f_syn_polarity14(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    clf = XGBClassifier(reg_alpha=alpha, gamma=gamma,max_depth=int(max_depth),subsample=subsample,\n",
    "                              min_child_weight=min_child_weight,colsample_bytree=colsample,objective=obj_classifier, \n",
    "                             booster='gbtree', n_estimators = 5,\n",
    "                             silent=None, random_state=run_num_14)\n",
    "    score = np.array(cross_val_score(clf, X=X_train14, y=y_train14).mean())\n",
    "    return score\n",
    "\n",
    "\n",
    "gpgo_gp_14 = GPGO(surrogate_gp_14, Acquisition_new(util_gp), f_syn_polarity14, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_14.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 2.67559603  8.13471837  8.          0.78152314 13.          0.81243369]. \t  0.9931252362750252 \t 0.9938213539679138\n",
      "init   \t [6.74508797 3.16072807 7.         0.9769074  6.         0.6095684 ]. \t  0.8864703338644256 \t 0.9938213539679138\n",
      "init   \t [ 0.65196398  0.45835711  7.          0.85774934 12.          0.26042982]. \t  0.8885587002878413 \t 0.9938213539679138\n",
      "init   \t [ 2.6759138   7.69220817 11.          0.52843413 18.          0.91623157]. \t  0.9938213539679138 \t 0.9938213539679138\n",
      "init   \t [ 4.37302805  7.27284651  8.          0.94165902 10.          0.72862239]. \t  0.9930628255058037 \t 0.9938213539679138\n",
      "1      \t [ 3.33422733  7.8002983   7.9999997   0.84365887 11.83595014  0.77991396]. \t  0.9916417848198541 \t 0.9938213539679138\n",
      "2      \t [ 3.8726812   7.06170556  7.83941902  0.77402615 10.899581    0.75610712]. \t  0.9916033793893603 \t 0.9938213539679138\n",
      "3      \t [ 3.9728834   7.96595895  8.16913348  1.         10.69095516  0.74543554]. \t  0.9820977661999445 \t 0.9938213539679138\n",
      "4      \t [ 4.76873125  7.47094905  7.88234923  1.         10.97798986  0.8523808 ]. \t  0.9808927579360481 \t 0.9938213539679138\n",
      "5      \t [ 4.28237672  7.19451421  8.66535225  1.         10.78025571  0.92010821]. \t  0.9820017502382492 \t 0.9938213539679138\n",
      "6      \t [ 4.32037127  7.35147413  8.19646462  1.         10.77928655  0.10916452]. \t  0.8780017273151489 \t 0.9938213539679138\n",
      "7      \t [ 4.3478649   7.49578526  8.16972715  0.5        10.66773626  0.86026043]. \t  0.9929572095520748 \t 0.9938213539679138\n",
      "8      \t [ 3.55273508  7.81496108  8.2222734   1.         12.84936712  1.        ]. \t  \u001b[92m0.9943062365035864\u001b[0m \t 0.9943062365035864\n",
      "9      \t [ 2.82173811  7.73025594  8.62642584  1.         12.50609351  1.        ]. \t  \u001b[92m0.9945078689030171\u001b[0m \t 0.9945078689030171\n",
      "10     \t [ 3.15798491  8.49379045  8.36196672  1.         12.48605771  1.        ]. \t  0.9944790658154457 \t 0.9945078689030171\n",
      "11     \t [ 3.14484883  8.00550361  8.37354165  1.         12.64453811  0.33027488]. \t  0.8787506509451602 \t 0.9945078689030171\n",
      "12     \t [ 4.20984885  7.37801367  8.04820724  1.         10.68993516  0.89626683]. \t  0.9820113519588775 \t 0.9945078689030171\n",
      "13     \t [ 4.12320444  7.75069324  8.65995992  0.81151794 11.77496449  0.86327135]. \t  0.9933124676146767 \t 0.9945078689030171\n",
      "14     \t [ 3.17619282  7.97595595  8.38284387  0.5        12.59359958  1.        ]. \t  0.994181416693738 \t 0.9945078689030171\n",
      "15     \t [ 4.26281536  6.82531505  8.14657764  0.66250329 11.79976709  0.76810241]. \t  0.993091631220839 \t 0.9945078689030171\n",
      "16     \t [ 4.73972203  6.45566548  8.01506959  0.60955453 10.67124857  0.65017361]. \t  0.8864079163882571 \t 0.9945078689030171\n",
      "17     \t [ 3.53584104  7.00714786  8.65418342  0.59589928 11.41272407  0.67436574]. \t  0.9927843805859343 \t 0.9945078689030171\n",
      "18     \t [ 4.09824576  7.26990988  8.28916294  0.70615926 11.32894602  0.67012859]. \t  0.9930292209356241 \t 0.9945078689030171\n",
      "19     \t [ 4.91621521  8.00626541  8.48835335  1.         10.29545038  0.63244379]. \t  0.8778913084267922 \t 0.9945078689030171\n",
      "20     \t [ 3.5536884   6.98826114  8.42643295  1.         12.03489138  1.        ]. \t  0.9943398428715046 \t 0.9945078689030171\n",
      "21     \t [ 3.53767085  7.21903502  8.52688256  0.61828344 10.10663304  0.49564108]. \t  0.8864559390275869 \t 0.9945078689030171\n",
      "22     \t [ 3.32681079  7.95069154  8.98279347  0.98060371 11.26997092  0.9861415 ]. \t  0.9937397376139784 \t 0.9945078689030171\n",
      "23     \t [ 3.1141316   7.91092008  8.19588593  0.98608974 12.43880645  1.        ]. \t  0.9944310577654543 \t 0.9945078689030171\n",
      "24     \t [ 2.87503424  8.30836972  8.62744815  0.94155918 13.43624999  1.        ]. \t  0.9942774323788582 \t 0.9945078689030171\n",
      "25     \t [ 2.85194152  7.67469062  8.26441279  0.67811014 10.99323827  0.68672327]. \t  0.9931780464990627 \t 0.9945078689030171\n",
      "26     \t [ 3.32975454  8.42285002  8.52199991  0.57750346 11.28841914  0.58773362]. \t  0.8873344791791338 \t 0.9945078689030171\n",
      "27     \t [ 3.5751473   7.84202586  9.09180245  1.         12.5225638   1.        ]. \t  \u001b[92m0.9948151221653855\u001b[0m \t 0.9948151221653855\n",
      "28     \t [ 3.32281219  7.66092546  8.62884583  0.83624696 11.67396152  0.87109902]. \t  0.9931204356912863 \t 0.9948151221653855\n",
      "29     \t [ 3.15283574  6.71661693  7.79314917  0.5        11.77559257  0.59177726]. \t  0.8864127278275702 \t 0.9948151221653855\n",
      "30     \t [ 3.8380832   6.91086284  8.76227414  0.60092263 12.74162022  0.83926079]. \t  0.9927171697169803 \t 0.9948151221653855\n"
     ]
    }
   ],
   "source": [
    "### 6(n). Bayesian optimization runs (x20): STP DF1 run number = 14\n",
    "\n",
    "np.random.seed(run_num_14)\n",
    "surrogate_stp_df1_14 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_14 = GPGO(surrogate_stp_df1_14, Acquisition_new(util_stp), f_syn_polarity14, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_14.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.275995231470317, -5.262008998886893)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(n). Training Regret Minimisation: run number = 14\n",
    "\n",
    "gp_output_14 = np.append(np.max(gpgo_gp_14.GP.y[0:n_init]),gpgo_gp_14.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_14 = np.append(np.max(gpgo_stp_df1_14.GP.y[0:n_init]),gpgo_stp_df1_14.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_14 = np.log(y_global_orig - gp_output_14)\n",
    "regret_stp_df1_14 = np.log(y_global_orig - stp_df1_output_14)\n",
    "\n",
    "train_regret_gp_14 = min_max_array(regret_gp_14)\n",
    "train_regret_stp_df1_14 = min_max_array(regret_stp_df1_14)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 14\n",
    "min_train_regret_gp_14 = min(train_regret_gp_14)\n",
    "min_train_regret_stp_df1_14 = min(train_regret_stp_df1_14)\n",
    "\n",
    "min_train_regret_gp_14, min_train_regret_stp_df1_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.63169119  7.60718723 12.          0.76592956  8.          0.64018209]. \t  0.8828698120331415 \t 0.9863561448532284\n",
      "init   \t [ 9.12390807  2.52878942  6.          0.96871191 19.          0.99350327]. \t  0.9813824455471867 \t 0.9863561448532284\n",
      "init   \t [ 7.65391566  3.02373633 13.          0.86200388 16.          0.6857035 ]. \t  0.9862697427812742 \t 0.9863561448532284\n",
      "init   \t [ 2.62681906  0.8215349  11.          0.53794672 19.          0.85820702]. \t  0.9863561448532284 \t 0.9863561448532284\n",
      "init   \t [ 2.67644658  0.3407803  12.          0.89443357  1.          0.22476028]. \t  0.8838923528263877 \t 0.9863561448532284\n",
      "1      \t [ 2.51802678  0.62917855 11.85190531  0.5        19.21242069  0.83342494]. \t  \u001b[92m0.9864041591260788\u001b[0m \t 0.9864041591260788\n",
      "2      \t [ 3.01204654  0.7893069  11.50190392  0.89273435 19.09285564  0.34172719]. \t  0.8835515033600702 \t 0.9864041591260788\n",
      "3      \t [ 2.95082118  0.61007497 11.29138628  0.51237947 19.74638143  0.94898116]. \t  \u001b[92m0.9866874046297628\u001b[0m \t 0.9866874046297628\n",
      "4      \t [ 3.04615226  0.15628111 11.38918929  0.5        18.99576181  1.        ]. \t  \u001b[92m0.994099800062401\u001b[0m \t 0.994099800062401\n",
      "5      \t [ 2.43358332  0.15093204 11.21271717  0.75033447 19.38459805  0.61381014]. \t  0.883527505972778 \t 0.994099800062401\n",
      "6      \t [ 2.79623533  0.62515146 11.44197428  1.         19.21378877  1.        ]. \t  \u001b[92m0.995184782739129\u001b[0m \t 0.995184782739129\n",
      "7      \t [ 7.32802279  2.43870852 12.38944267  0.8539694  16.00112081  0.66027873]. \t  0.8833595080823211 \t 0.995184782739129\n",
      "8      \t [ 7.42182834  2.5741184  13.07232833  0.73082357 16.7106703   0.51216448]. \t  0.8832202661240846 \t 0.995184782739129\n",
      "9      \t [ 6.79748673  3.13259154 12.86315598  0.82634022 16.20122339  0.63471719]. \t  0.8827593961179118 \t 0.995184782739129\n",
      "10     \t [ 7.55745228  3.18602078 12.39612251  0.93511212 16.60655245  0.86284072]. \t  0.986413764442132 \t 0.995184782739129\n",
      "11     \t [ 7.5258005   3.14056111 12.49133664  0.50810156 16.30484226  0.14993579]. \t  0.8804934159090269 \t 0.995184782739129\n",
      "12     \t [ 7.41667227  2.93875689 12.69801999  0.5        16.30848544  1.        ]. \t  0.9935140938595022 \t 0.995184782739129\n",
      "13     \t [ 8.3763762   2.77431145  6.          0.81913086 18.59799306  0.8214184 ]. \t  0.9815648785156963 \t 0.995184782739129\n",
      "14     \t [ 9.04145172  3.25518436  5.99999992  0.88718069 18.79712989  0.51237151]. \t  0.8756014375699334 \t 0.995184782739129\n",
      "15     \t [ 8.79800197  2.69131004  6.76522707  0.88361577 18.78796132  0.78993838]. \t  0.9814544681318894 \t 0.995184782739129\n",
      "16     \t [ 3.30557629  1.05039055 11.60095742  0.5        19.04984858  1.        ]. \t  0.9940805948234317 \t 0.995184782739129\n",
      "17     \t [ 2.90702337  0.64452411 11.4247703   0.5        19.19755791  0.874654  ]. \t  0.9862601344229383 \t 0.995184782739129\n",
      "18     \t [ 2.66890827  1.77449106 11.56786801  0.79249489 19.50264445  0.84956304]. \t  0.9869034420993302 \t 0.995184782739129\n",
      "19     \t [ 2.65980714  1.57674236 11.78270471  0.83751175 18.45051271  0.84509787]. \t  0.9871050721479598 \t 0.995184782739129\n",
      "20     \t [ 3.21381829  1.7805113  10.96959485  0.95675929 18.79988853  0.91425014]. \t  0.9867642152142212 \t 0.995184782739129\n",
      "21     \t [ 8.49301446  2.86221459  6.25973483  0.90156732 19.43871739  0.82280607]. \t  0.9807295376713078 \t 0.995184782739129\n",
      "22     \t [ 8.71214103  2.38106792  6.17614598  0.80743319 19.01638747  0.25284989]. \t  0.8825769984122238 \t 0.995184782739129\n",
      "23     \t [ 3.47587093  1.84109317 11.98241111  1.         18.98496293  0.73410011]. \t  0.9727649711174701 \t 0.995184782739129\n",
      "24     \t [ 8.80301932  2.79369903  6.25139905  0.5        18.98712848  0.96018802]. \t  0.981195201623516 \t 0.995184782739129\n",
      "25     \t [ 2.37589306  0.05012055 11.62606225  0.6869744  18.16920505  1.        ]. \t  0.9949255389095898 \t 0.995184782739129\n",
      "26     \t [ 3.20431384  0.70177279 11.24357344  0.89282631 17.98217234  1.        ]. \t  \u001b[92m0.9952759961811011\u001b[0m \t 0.9952759961811011\n",
      "27     \t [ 3.13471112  0.42858868 12.25182903  0.83213652 18.24575654  1.        ]. \t  0.9950359573139599 \t 0.9952759961811011\n",
      "28     \t [ 3.54223094  1.58072677 11.14407147  0.92833921 19.98496167  1.        ]. \t  0.9951895822165874 \t 0.9952759961811011\n",
      "29     \t [ 2.75784262  1.3049838  10.5293578   0.82282741 20.          1.        ]. \t  0.9951511753340742 \t 0.9952759961811011\n",
      "30     \t [ 3.5126599   0.7713214  10.38740715  0.86286607 19.54698667  1.        ]. \t  0.9951607780227015 \t 0.9952759961811011\n"
     ]
    }
   ],
   "source": [
    "### 6(o). Bayesian optimization runs (x20): GP run number = 15\n",
    "\n",
    "np.random.seed(run_num_15)\n",
    "surrogate_gp_15 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "X_train15, X_test15, y_train15, y_test15 = train_test_split(X, y, test_size=0.15, random_state=run_num_15)\n",
    "\n",
    "def f_syn_polarity15(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    clf = XGBClassifier(reg_alpha=alpha, gamma=gamma,max_depth=int(max_depth),subsample=subsample,\n",
    "                              min_child_weight=min_child_weight,colsample_bytree=colsample,objective=obj_classifier, \n",
    "                             booster='gbtree', n_estimators = 5,\n",
    "                             silent=None, random_state=run_num_15)\n",
    "    score = np.array(cross_val_score(clf, X=X_train15, y=y_train15).mean())\n",
    "    return score\n",
    "\n",
    "gpgo_gp_15 = GPGO(surrogate_gp_15, Acquisition_new(util_gp), f_syn_polarity15, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_15.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.63169119  7.60718723 12.          0.76592956  8.          0.64018209]. \t  0.8828698120331415 \t 0.9863561448532284\n",
      "init   \t [ 9.12390807  2.52878942  6.          0.96871191 19.          0.99350327]. \t  0.9813824455471867 \t 0.9863561448532284\n",
      "init   \t [ 7.65391566  3.02373633 13.          0.86200388 16.          0.6857035 ]. \t  0.9862697427812742 \t 0.9863561448532284\n",
      "init   \t [ 2.62681906  0.8215349  11.          0.53794672 19.          0.85820702]. \t  0.9863561448532284 \t 0.9863561448532284\n",
      "init   \t [ 2.67644658  0.3407803  12.          0.89443357  1.          0.22476028]. \t  0.8838923528263877 \t 0.9863561448532284\n",
      "1      \t [ 3.39277204  0.89649855 10.43553043  0.61752598 18.4243293   0.6660522 ]. \t  0.8830666475825936 \t 0.9863561448532284\n",
      "2      \t [ 3.54938801  0.90323954 11.522016    0.5000118  18.78183602  0.90253835]. \t  0.9860392974759126 \t 0.9863561448532284\n",
      "3      \t [ 2.88231935  1.40758955 11.27137836  0.59237833 18.23365976  0.85683995]. \t  \u001b[92m0.9864569717701027\u001b[0m \t 0.9864569717701027\n",
      "4      \t [ 3.25537558  1.57796995 10.91455572  0.51637636 19.05608532  0.96337035]. \t  0.9863945569905935 \t 0.9864569717701027\n",
      "5      \t [ 3.12514971  1.1909686  11.19872719  0.81280349 18.83971044  0.2294887 ]. \t  0.8836955337329191 \t 0.9864569717701027\n",
      "6      \t [ 3.14902202  1.0408251  11.06320954  1.         18.70126834  1.        ]. \t  \u001b[92m0.9951991853200716\u001b[0m \t 0.9951991853200716\n",
      "7      \t [ 7.26648145  2.37672812 12.3251963   0.85037886 16.02290274  0.65920167]. \t  0.8830426427278804 \t 0.9951991853200716\n",
      "8      \t [ 3.12329773  1.08688232 11.06275475  0.5        18.67670016  0.8676932 ]. \t  0.9860969061402556 \t 0.9951991853200716\n",
      "9      \t [ 2.78276952  1.46640971 11.92389703  0.73294797 19.13172043  1.        ]. \t  0.995026352551049 \t 0.9951991853200716\n",
      "10     \t [ 3.67263231  1.86234514 11.85457432  0.83750994 18.57112416  0.96735226]. \t  0.986826628196018 \t 0.9951991853200716\n",
      "11     \t [ 2.65342687  2.23645974 11.32815123  1.         18.69462214  0.94219623]. \t  0.9730530163751837 \t 0.9951991853200716\n",
      "12     \t [ 2.24476392  1.51098656 10.44097827  0.92505813 18.8004977   0.74207669]. \t  0.9873931142251053 \t 0.9951991853200716\n",
      "13     \t [ 2.11558836  1.50186187 11.17497365  0.81887191 19.12295042  0.98692883]. \t  0.9871578829942717 \t 0.9951991853200716\n",
      "14     \t [ 2.95045591  1.61012977 11.38729345  0.84624389 18.81270098  0.90035563]. \t  0.9867594149070493 \t 0.9951991853200716\n",
      "15     \t [ 3.16588269  2.25273681 10.52482504  0.81390605 18.06314698  0.9124071 ]. \t  0.9863321474659363 \t 0.9951991853200716\n",
      "16     \t [ 2.197202    2.25138161 10.78113453  0.5        18.23215654  1.        ]. \t  0.9941910104620902 \t 0.9951991853200716\n",
      "17     \t [ 3.05286984  2.53852638 11.52832138  0.5        17.91330094  1.        ]. \t  0.9943110308636599 \t 0.9951991853200716\n",
      "18     \t [ 2.52288224  2.2293353  11.08161389  1.         17.71011392  0.56972939]. \t  0.7954325070207117 \t 0.9951991853200716\n",
      "19     \t [ 2.89231886  2.57041151 10.89073723  0.5        18.56626358  0.6068252 ]. \t  0.8827450022489605 \t 0.9951991853200716\n",
      "20     \t [ 4.02630824  1.74096316 11.14297624  0.61681148 18.00229204  1.        ]. \t  0.9948535231008803 \t 0.9951991853200716\n",
      "21     \t [ 3.01140405  0.74450766 11.69062502  0.75320831 19.85032035  0.92533347]. \t  0.9868026252773028 \t 0.9951991853200716\n",
      "22     \t [ 2.63195749  0.48136486 12.12174482  0.8356517  19.19267101  0.81356864]. \t  0.987455528866329 \t 0.9951991853200716\n",
      "23     \t [ 3.25223063  2.06349973 11.1191873   0.61181488 18.17681886  1.        ]. \t  0.9946470890113681 \t 0.9951991853200716\n",
      "24     \t [ 3.70441018  1.60665999 12.14687758  0.59868345 17.72126852  0.88938924]. \t  0.9871386806592994 \t 0.9951991853200716\n",
      "25     \t [ 2.67297978  2.0608832  12.37146696  0.55820613 18.103925    0.99560354]. \t  0.9870186561091625 \t 0.9951991853200716\n",
      "26     \t [ 2.86916305  1.94708727 10.09335994  0.77796645 18.84932409  1.        ]. \t  0.994963945515532 \t 0.9951991853200716\n",
      "27     \t [ 3.27613571  1.07987278 12.47178601  0.84315495 18.51562092  0.80103057]. \t  0.9874843291882187 \t 0.9951991853200716\n",
      "28     \t [ 4.07235193  1.19951015 11.59409483  1.         18.11427109  0.61896581]. \t  0.7954613137037381 \t 0.9951991853200716\n",
      "29     \t [ 3.95649673  1.70341395 10.29827522  0.96275344 18.69759488  0.86535371]. \t  0.9869130442348153 \t 0.9951991853200716\n",
      "30     \t [ 3.33671332  1.96876989 12.22457257  0.5        18.20615171  0.60124839]. \t  0.8830234383186246 \t 0.9951991853200716\n"
     ]
    }
   ],
   "source": [
    "### 6(o). Bayesian optimization runs (x20): STP DF1 run number = 15\n",
    "\n",
    "np.random.seed(run_num_15)\n",
    "surrogate_stp_df1_15 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_15 = GPGO(surrogate_stp_df1_15, Acquisition_new(util_stp), f_syn_polarity15, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_15.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.355098572243681, -5.338969650484858)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(o). Training Regret Minimisation: run number = 15\n",
    "\n",
    "gp_output_15 = np.append(np.max(gpgo_gp_15.GP.y[0:n_init]),gpgo_gp_15.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_15 = np.append(np.max(gpgo_stp_df1_15.GP.y[0:n_init]),gpgo_stp_df1_15.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_15 = np.log(y_global_orig - gp_output_15)\n",
    "regret_stp_df1_15 = np.log(y_global_orig - stp_df1_output_15)\n",
    "\n",
    "train_regret_gp_15 = min_max_array(regret_gp_15)\n",
    "train_regret_stp_df1_15 = min_max_array(regret_stp_df1_15)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 15\n",
    "min_train_regret_gp_15 = min(train_regret_gp_15)\n",
    "min_train_regret_stp_df1_15 = min(train_regret_stp_df1_15)\n",
    "\n",
    "min_train_regret_gp_15, min_train_regret_stp_df1_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [8.63948332 3.16366002 7.         0.75395583 1.         0.56999262]. \t  0.8839259643800098 \t 0.9899663962346213\n",
      "init   \t [ 3.3426621   9.77029134 14.          0.93751218 17.          0.80549151]. \t  0.9899663962346213 \t 0.9899663962346213\n",
      "init   \t [ 3.36871073  4.8308288  13.          0.91984982 14.          0.48829226]. \t  0.8833930391537385 \t 0.9899663962346213\n",
      "init   \t [ 8.84540736  1.59826081  8.          0.75772888 14.          0.29029227]. \t  0.8846748713467618 \t 0.9899663962346213\n",
      "init   \t [ 7.74981136  8.34377636 13.          0.83182409 15.          0.19378622]. \t  0.8834650783327104 \t 0.9899663962346213\n",
      "1      \t [ 8.48739889  2.49920084  8.31898691  0.67853978 14.00001186  0.36409417]. \t  0.8827545558462085 \t 0.9899663962346213\n",
      "2      \t [ 8.29914417  1.71301277  8.68229053  0.7442437  13.49726668  0.38294681]. \t  0.8826105496733353 \t 0.9899663962346213\n",
      "3      \t [ 8.84316762  2.16099488  8.03852729  0.70811266 13.15742317  0.29693992]. \t  0.8838155170739546 \t 0.9899663962346213\n",
      "4      \t [ 8.01570878  1.93413196  7.75652884  0.6649697  13.60991916  0.2249449 ]. \t  0.883431465534518 \t 0.9899663962346213\n",
      "5      \t [ 8.4610443   1.98112371  8.02459587  1.         13.63838448  1.        ]. \t  \u001b[92m0.9944070516653682\u001b[0m \t 0.9944070516653682\n",
      "6      \t [ 8.52698011  1.91527611  8.1121039   0.5        13.64994002  0.78807724]. \t  0.9860201533591676 \t 0.9944070516653682\n",
      "7      \t [ 7.93160176  1.7096127   8.26352781  1.         14.42685382  0.76074106]. \t  0.9684826576486064 \t 0.9944070516653682\n",
      "8      \t [ 4.11115449  9.5292804  13.82460544  0.91733254 16.6486388   0.69840687]. \t  0.9897311542175128 \t 0.9944070516653682\n",
      "9      \t [ 3.64276465  9.39274205 13.34180297  0.95969306 17.17860965  0.4951023 ]. \t  0.8837387207329104 \t 0.9944070516653682\n",
      "10     \t [ 3.73180991  9.03057192 14.10582682  0.73981879 17.10284582  0.98782503]. \t  0.9893038801386922 \t 0.9944070516653682\n",
      "11     \t [ 4.06739352  9.65889041 14.13809909  0.99994131 17.46118656  0.70441155]. \t  0.9894815180548817 \t 0.9944070516653682\n",
      "12     \t [ 3.74120895  9.45529423 14.18305811  0.50707733 17.02943296  0.2576769 ]. \t  0.8823081005901088 \t 0.9944070516653682\n",
      "13     \t [ 3.90075613  9.72403186 13.74407703  0.5        17.16907105  1.        ]. \t  0.99417181338197 \t 0.9944070516653682\n",
      "14     \t [ 8.32654308  1.92417032  8.16825501  1.         13.86867663  0.50279552]. \t  0.7926816518126553 \t 0.9944070516653682\n",
      "15     \t [ 7.97400934  1.86103512  7.55318724  0.57751587 14.67611392  1.        ]. \t  0.992817985016789 \t 0.9944070516653682\n",
      "16     \t [ 7.35517044  2.05817636  8.24595978  0.5        14.25460434  1.        ]. \t  0.9938021511480608 \t 0.9944070516653682\n",
      "17     \t [ 7.64765854  1.11010008  7.9580027   0.60852325 14.2070014   1.        ]. \t  0.9927219646299373 \t 0.9944070516653682\n",
      "18     \t [ 7.88857103  1.5043571   8.47878727  0.5        14.91494912  1.        ]. \t  0.9937589457560881 \t 0.9944070516653682\n",
      "19     \t [ 7.4582281   1.52072039  8.03085496  0.5        14.76337325  0.27388882]. \t  0.8824520937641381 \t 0.9944070516653682\n",
      "20     \t [ 8.54496966  2.68929507  7.25246808  0.7464811  13.60758609  1.        ]. \t  0.9932068526279529 \t 0.9944070516653682\n",
      "21     \t [ 8.00607319  2.77879103  7.98757328  0.68796474 13.11063194  1.        ]. \t  0.9927651812230417 \t 0.9944070516653682\n",
      "22     \t [ 8.09921231  2.03013832  7.26830991  0.82710276 12.96070723  1.        ]. \t  0.9935093007431802 \t 0.9944070516653682\n",
      "23     \t [ 7.63679938  2.33138986  7.36804937  0.70872429 13.69053789  1.        ]. \t  0.9930196196288427 \t 0.9944070516653682\n",
      "24     \t [ 7.53424309  1.69172554  8.29395072  0.68687564 12.92435278  1.        ]. \t  \u001b[92m0.9944214549377387\u001b[0m \t 0.9944214549377387\n",
      "25     \t [ 7.5108368   1.29280863  9.07605509  0.61153601 13.86999085  1.        ]. \t  0.9941958178218265 \t 0.9944214549377387\n",
      "26     \t [ 7.77630872  2.22416738  9.18072107  0.51575291 13.29574558  1.        ]. \t  0.9941430087732274 \t 0.9944214549377387\n",
      "27     \t [ 7.23133673  1.87368637  8.98518727  0.5        13.37854208  0.31103521]. \t  0.8810406740203108 \t 0.9944214549377387\n",
      "28     \t [ 8.77790185  1.58216864  6.98650356  0.83863036 13.87615136  1.        ]. \t  0.9890493500495547 \t 0.9944214549377387\n",
      "29     \t [ 8.67337796  0.9832235   7.64573505  0.62052864 14.87121474  0.63543699]. \t  0.882255276191811 \t 0.9944214549377387\n",
      "30     \t [ 9.17772408  2.06889489  7.19865305  0.5        14.39164009  0.28476025]. \t  0.8818327879001767 \t 0.9944214549377387\n"
     ]
    }
   ],
   "source": [
    "### 6(p). Bayesian optimization runs (x20): GP run number = 16\n",
    "\n",
    "np.random.seed(run_num_16)\n",
    "surrogate_gp_16 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "X_train16, X_test16, y_train16, y_test16 = train_test_split(X, y, test_size=0.15, random_state=run_num_16)\n",
    "\n",
    "def f_syn_polarity16(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    clf = XGBClassifier(reg_alpha=alpha, gamma=gamma,max_depth=int(max_depth),subsample=subsample,\n",
    "                              min_child_weight=min_child_weight,colsample_bytree=colsample,objective=obj_classifier, \n",
    "                             booster='gbtree', n_estimators = 5,\n",
    "                             silent=None, random_state=run_num_16)\n",
    "    score = np.array(cross_val_score(clf, X=X_train16, y=y_train16).mean())\n",
    "    return score\n",
    "\n",
    "\n",
    "gpgo_gp_16 = GPGO(surrogate_gp_16, Acquisition_new(util_gp), f_syn_polarity16, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_16.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [8.63948332 3.16366002 7.         0.75395583 1.         0.56999262]. \t  0.8839259643800098 \t 0.9899663962346213\n",
      "init   \t [ 3.3426621   9.77029134 14.          0.93751218 17.          0.80549151]. \t  0.9899663962346213 \t 0.9899663962346213\n",
      "init   \t [ 3.36871073  4.8308288  13.          0.91984982 14.          0.48829226]. \t  0.8833930391537385 \t 0.9899663962346213\n",
      "init   \t [ 8.84540736  1.59826081  8.          0.75772888 14.          0.29029227]. \t  0.8846748713467618 \t 0.9899663962346213\n",
      "init   \t [ 7.74981136  8.34377636 13.          0.83182409 15.          0.19378622]. \t  0.8834650783327104 \t 0.9899663962346213\n",
      "1      \t [ 8.25371895  1.54615323  8.76202186  0.79958981 14.76200776  0.37976884]. \t  0.8835131000729817 \t 0.9899663962346213\n",
      "2      \t [ 8.49783913  1.54259882  9.0621643   0.79829316 13.64197847  0.44462546]. \t  0.8835851149136923 \t 0.9899663962346213\n",
      "3      \t [ 8.90917591  2.29411224  8.83927073  0.60214325 14.28123169  0.14828897]. \t  0.8827161456448415 \t 0.9899663962346213\n",
      "4      \t [ 7.95310204  2.09625589  8.40348262  0.61437703 13.95792442  0.16980858]. \t  0.8813047404209994 \t 0.9899663962346213\n",
      "5      \t [ 8.51432587  2.07606941  8.5195565   0.99046562 14.15874222  0.99459151]. \t  0.9861401774944477 \t 0.9899663962346213\n",
      "6      \t [ 8.54005477  1.75030386  8.58170809  0.5        14.16794612  0.70446575]. \t  0.9859865491346809 \t 0.9899663962346213\n",
      "7      \t [ 8.51747265  1.8806647   8.59347643  1.         14.14410753  0.33497936]. \t  0.7926912532567124 \t 0.9899663962346213\n",
      "8      \t [ 7.97953691  2.3672147   9.40009364  0.57517502 14.34920696  0.90702485]. \t  0.9870139180304996 \t 0.9899663962346213\n",
      "9      \t [ 8.49403928  2.63865508  8.89061029  0.5        13.43512686  0.95036712]. \t  0.9860441569693105 \t 0.9899663962346213\n",
      "10     \t [ 8.30150398  2.90672504  8.49587935  0.5        14.46402375  0.97817719]. \t  0.9860057495336548 \t 0.9899663962346213\n",
      "11     \t [ 8.90207867  2.54568608  7.77599485  0.5        13.6946129   0.73966505]. \t  0.9836965731979307 \t 0.9899663962346213\n",
      "12     \t [ 8.898656    1.86027476  8.19055335  0.5        12.96169607  0.44639718]. \t  0.8809254387144998 \t 0.9899663962346213\n",
      "13     \t [ 8.06362027  2.35433628  8.10682988  0.56594441 13.60162808  1.        ]. \t  \u001b[92m0.9940325902305718\u001b[0m \t 0.9940325902305718\n",
      "14     \t [ 8.39726422  2.44241313  8.60313386  0.5        13.97167702  0.80588511]. \t  0.9859049347859111 \t 0.9940325902305718\n",
      "15     \t [ 8.49898426  2.43573836  9.04096126  0.69759168 15.27901716  1.        ]. \t  \u001b[92m0.9941238053319706\u001b[0m \t 0.9941238053319706\n",
      "16     \t [ 7.66794326  2.29439626  8.50925122  0.73014232 15.04067402  1.        ]. \t  \u001b[92m0.9943158409891076\u001b[0m \t 0.9943158409891076\n",
      "17     \t [ 7.96016567  1.96871599  9.05728643  0.73703878 12.86144128  1.        ]. \t  \u001b[92m0.9945222746637631\u001b[0m \t 0.9945222746637631\n",
      "18     \t [ 8.58410126  2.28568141  7.84421998  0.62552494 14.9743849   0.83475583]. \t  0.9835573464511075 \t 0.9945222746637631\n",
      "19     \t [ 8.12787417  2.5865918   8.73458795  0.5        15.18113178  0.28333228]. \t  0.8808582254255312 \t 0.9945222746637631\n",
      "20     \t [ 8.6566765   2.09364557  9.73599302  0.64711302 13.16826386  1.        ]. \t  0.9943398415569679 \t 0.9945222746637631\n",
      "21     \t [ 8.7525259   2.17692831  8.9161329   0.96764127 12.89485349  1.        ]. \t  \u001b[92m0.9945750816380786\u001b[0m \t 0.9945750816380786\n",
      "22     \t [ 8.34065346  1.87092791  7.3331459   0.6106449  13.98197135  0.85181318]. \t  0.9835573517059594 \t 0.9945750816380786\n",
      "23     \t [ 8.4243315   2.28709457  9.34886379  0.51361898 12.83579748  0.36852384]. \t  0.8812135109379007 \t 0.9945750816380786\n",
      "24     \t [ 8.26096614  2.284519    8.60644427  0.66981329 14.84915447  1.        ]. \t  0.9940661943167729 \t 0.9945750816380786\n",
      "25     \t [ 8.04432588  2.76278908  7.56935015  0.93181278 14.46526367  0.8594178 ]. \t  0.9842006539892137 \t 0.9945750816380786\n",
      "26     \t [ 8.94407295  2.70800595  9.6781049   0.73963824 14.50406232  0.89460428]. \t  0.9868410920374702 \t 0.9945750816380786\n",
      "27     \t [ 8.14341286  3.07567524  9.23246099  1.         14.92919635  1.        ]. \t  0.9944646619891383 \t 0.9945750816380786\n",
      "28     \t [ 8.54000154  1.88174217  8.94665793  0.52946479 13.19602496  1.        ]. \t  0.9938837667414008 \t 0.9945750816380786\n",
      "29     \t [ 8.26556373  2.40016775  9.43541651  1.         13.44238124  1.        ]. \t  0.9945654779814522 \t 0.9945750816380786\n",
      "30     \t [ 7.62067389  1.81633402  7.79440931  0.66434918 14.54966814  0.69675375]. \t  0.9836677701103295 \t 0.9945750816380786\n"
     ]
    }
   ],
   "source": [
    "### 6(p). Bayesian optimization runs (x20): STP DF1 run number = 16\n",
    "\n",
    "np.random.seed(run_num_16)\n",
    "surrogate_stp_df1_16 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_16 = GPGO(surrogate_stp_df1_16, Acquisition_new(util_stp), f_syn_polarity16, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_16.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.188827278118134, -5.216752428162598)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(p). Training Regret Minimisation: run number = 16\n",
    "\n",
    "gp_output_16 = np.append(np.max(gpgo_gp_16.GP.y[0:n_init]),gpgo_gp_16.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_16 = np.append(np.max(gpgo_stp_df1_16.GP.y[0:n_init]),gpgo_stp_df1_16.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_16 = np.log(y_global_orig - gp_output_16)\n",
    "regret_stp_df1_16 = np.log(y_global_orig - stp_df1_output_16)\n",
    "\n",
    "train_regret_gp_16 = min_max_array(regret_gp_16)\n",
    "train_regret_stp_df1_16 = min_max_array(regret_stp_df1_16)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 16\n",
    "min_train_regret_gp_16 = min(train_regret_gp_16)\n",
    "min_train_regret_stp_df1_16 = min(train_regret_stp_df1_16)\n",
    "\n",
    "min_train_regret_gp_16, min_train_regret_stp_df1_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.30620418  0.79287811  5.          0.60928035 13.          0.288655  ]. \t  0.895068527203411 \t 0.9928227623692543\n",
      "init   \t [ 6.82247115  3.27872116 13.          0.57928621 11.          0.78490425]. \t  0.9928227623692543 \t 0.9928227623692543\n",
      "init   \t [ 6.41853716  0.86073186 14.          0.50788149  3.          0.58773362]. \t  0.8947708639072278 \t 0.9928227623692543\n",
      "init   \t [ 9.74735437  6.00734076 14.          0.55734082 15.          0.94580565]. \t  0.991958607512704 \t 0.9928227623692543\n",
      "init   \t [2.95635136 5.2249123  9.         0.7208594  2.         0.51090679]. \t  0.8946556519718044 \t 0.9928227623692543\n",
      "1      \t [ 7.21340739  2.69328408 13.24757793  0.53668841 11.49520421  0.84571409]. \t  \u001b[92m0.9929331791141537\u001b[0m \t 0.9929331791141537\n",
      "2      \t [ 7.20959858  3.48191732 12.95683417  0.55942626 11.77115796  0.77663984]. \t  0.9928275631604248 \t 0.9929331791141537\n",
      "3      \t [ 7.18439066  3.42547142 13.68919338  0.52293503 11.32991263  0.98113992]. \t  0.992769947650871 \t 0.9929331791141537\n",
      "4      \t [ 7.69237681  3.26895618 13.05970664  0.51011808 11.07872106  0.80806988]. \t  \u001b[92m0.9932116273529248\u001b[0m \t 0.9932116273529248\n",
      "5      \t [ 7.23959703  3.23395466 13.36946133  0.7561689  11.33266449  0.22515037]. \t  0.8952797446598183 \t 0.9932116273529248\n",
      "6      \t [ 7.26060268  3.21653217 13.14739323  1.         11.33276235  1.        ]. \t  \u001b[92m0.9956744660666293\u001b[0m \t 0.9956744660666293\n",
      "7      \t [ 9.30303659  5.5422796  13.84680858  0.54612682 14.38921661  0.93072806]. \t  0.9923426756463997 \t 0.9956744660666293\n",
      "8      \t [ 9.95512341  5.56514122 14.44547294  0.5571845  14.40840167  0.99987265]. \t  0.9921074336984369 \t 0.9956744660666293\n",
      "9      \t [ 7.20926785  3.22549673 13.17526384  0.5        11.33763712  0.86831759]. \t  0.9927123379493951 \t 0.9956744660666293\n",
      "10     \t [ 8.05170444  3.11455416 13.61483197  0.74619947 12.0303677   0.84340443]. \t  0.9933460484685387 \t 0.9956744660666293\n",
      "11     \t [ 7.95002997  4.15689047 13.45089665  0.81996877 11.65208427  0.79801647]. \t  0.9933796549056005 \t 0.9956744660666293\n",
      "12     \t [ 7.0897153   4.29191681 13.03955421  0.84314376 10.8614924   0.70011336]. \t  0.9938501381792323 \t 0.9956744660666293\n",
      "13     \t [ 9.78093795  6.08537323 13.97621347  1.         14.25368773  0.80314899]. \t  0.9804270768364624 \t 0.9956744660666293\n",
      "14     \t [10.          5.66194203 13.59601718  0.5        14.42967795  1.        ]. \t  0.9936245193864707 \t 0.9956744660666293\n",
      "15     \t [ 7.09819506  3.61031519 13.47918751  0.87114591 10.14680998  0.7967695 ]. \t  0.9938885445085953 \t 0.9956744660666293\n",
      "16     \t [ 9.82690648  5.38555378 13.96181913  1.         14.76241957  0.58861955]. \t  0.8404497332159302 \t 0.9956744660666293\n",
      "17     \t [ 7.92072604  3.83401348 13.83967607  0.91741911 10.75127072  0.86296406]. \t  0.9939029471586817 \t 0.9956744660666293\n",
      "18     \t [ 9.75072539  5.93032389 14.07176828  0.5        14.39431627  0.38858229]. \t  0.889158718285913 \t 0.9956744660666293\n",
      "19     \t [ 7.73941361  2.49857183 12.75370452  0.79341923 12.24190421  0.58988349]. \t  0.8950589058459482 \t 0.9956744660666293\n",
      "20     \t [ 7.11096489  2.71359839 13.53622043  0.82184437 12.53178144  0.81168837]. \t  0.9934276627481607 \t 0.9956744660666293\n",
      "21     \t [ 7.80942812  3.88872393 12.87217891  0.9805661  10.4258402   0.59314856]. \t  0.8948908865905262 \t 0.9956744660666293\n",
      "22     \t [ 7.5924745   1.99877973 13.72471885  0.86346193 12.03644393  0.83085491]. \t  0.993744522018072 \t 0.9956744660666293\n",
      "23     \t [ 7.53748192  4.09164104 13.39873083  0.5        10.62190442  1.        ]. \t  0.9942726310345374 \t 0.9956744660666293\n",
      "24     \t [ 7.39774763  3.81146559 13.40454994  0.86422073 10.93753979  0.74287831]. \t  0.9938213343310792 \t 0.9956744660666293\n",
      "25     \t [ 6.67853673  3.90521895 12.53327185  0.63775323 10.13799015  0.82835568]. \t  0.9936725084910654 \t 0.9956744660666293\n",
      "26     \t [ 7.368868    2.70423653 14.25272452  1.         11.7132338   0.85745032]. \t  0.9810175767778836 \t 0.9956744660666293\n",
      "27     \t [ 7.90967426  2.69370289 13.99018511  0.76294944 10.67714225  1.        ]. \t  0.9951415770021285 \t 0.9956744660666293\n",
      "28     \t [ 8.18057925  3.40805018 14.34346836  0.5        11.24497365  1.        ]. \t  0.9940037852078326 \t 0.9956744660666293\n",
      "29     \t [ 7.21009328  4.15903648 12.1150895   0.64681358 11.02426272  0.91677583]. \t  0.9935668900481601 \t 0.9956744660666293\n",
      "30     \t [ 6.93829042  3.96195063 12.34440321  0.5        10.74471972  0.1625846 ]. \t  0.8929705565701908 \t 0.9956744660666293\n"
     ]
    }
   ],
   "source": [
    "### 6(q). Bayesian optimization runs (x20): GP run number = 17\n",
    "\n",
    "np.random.seed(run_num_17)\n",
    "surrogate_gp_17 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "X_train17, X_test17, y_train17, y_test17 = train_test_split(X, y, test_size=0.15, random_state=run_num_17)\n",
    "\n",
    "def f_syn_polarity17(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    clf = XGBClassifier(reg_alpha=alpha, gamma=gamma,max_depth=int(max_depth),subsample=subsample,\n",
    "                              min_child_weight=min_child_weight,colsample_bytree=colsample,objective=obj_classifier, \n",
    "                             booster='gbtree', n_estimators = 5,\n",
    "                             silent=None, random_state=run_num_17)\n",
    "    score = np.array(cross_val_score(clf, X=X_train17, y=y_train17).mean())\n",
    "    return score\n",
    "\n",
    "gpgo_gp_17 = GPGO(surrogate_gp_17, Acquisition_new(util_gp), f_syn_polarity17, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_17.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.30620418  0.79287811  5.          0.60928035 13.          0.288655  ]. \t  0.895068527203411 \t 0.9928227623692543\n",
      "init   \t [ 6.82247115  3.27872116 13.          0.57928621 11.          0.78490425]. \t  0.9928227623692543 \t 0.9928227623692543\n",
      "init   \t [ 6.41853716  0.86073186 14.          0.50788149  3.          0.58773362]. \t  0.8947708639072278 \t 0.9928227623692543\n",
      "init   \t [ 9.74735437  6.00734076 14.          0.55734082 15.          0.94580565]. \t  0.991958607512704 \t 0.9928227623692543\n",
      "init   \t [2.95635136 5.2249123  9.         0.7208594  2.         0.51090679]. \t  0.8946556519718044 \t 0.9928227623692543\n",
      "1      \t [ 7.28994256  3.70637847 13.92275839  0.5        11.00002402  0.83592635]. \t  0.9927363420435346 \t 0.9928227623692543\n",
      "2      \t [ 6.87928201  3.50638932 13.54321296  0.54157303 11.88024217  0.83667482]. \t  \u001b[92m0.9930724039941209\u001b[0m \t 0.9930724039941209\n",
      "3      \t [ 7.65617022  3.64239152 13.08872205  0.51724579 11.43779434  0.76109656]. \t  0.9926787323420584 \t 0.9930724039941209\n",
      "4      \t [ 6.88222829  4.17936285 13.24228311  0.58612972 11.26173737  0.68070997]. \t  0.9929571834848683 \t 0.9930724039941209\n",
      "5      \t [ 7.14140249  3.50631627 13.47825929  0.97680875 11.31336191  0.2867403 ]. \t  0.8952845532642363 \t 0.9930724039941209\n",
      "6      \t [ 7.11983432  3.65396436 13.36017545  0.86031622 11.29034018  1.        ]. \t  \u001b[92m0.995468032253734\u001b[0m \t 0.995468032253734\n",
      "7      \t [ 7.07753926  3.59472578 13.38413834  0.5        11.30083128  0.66158878]. \t  0.8931913929640286 \t 0.995468032253734\n",
      "8      \t [ 7.62453233  4.32240488 14.08195991  0.74346397 11.8171683   0.81170853]. \t  0.9933076424157509 \t 0.995468032253734\n",
      "9      \t [ 6.70635503  3.4414847  12.2341456   0.9148282  11.58210428  0.727251  ]. \t  0.9941621938230781 \t 0.995468032253734\n",
      "10     \t [ 7.10349113  3.60539975 12.27452019  0.90533527 10.52999056  0.67631343]. \t  0.9938501396312519 \t 0.995468032253734\n",
      "11     \t [ 7.90868967  4.37988673 13.75471284  0.79381605 10.68903051  0.7138961 ]. \t  0.9934468649448291 \t 0.995468032253734\n",
      "12     \t [ 7.10201553  4.01210665 13.12018869  0.88521783 12.52100601  0.74350167]. \t  0.9938693427959334 \t 0.995468032253734\n",
      "13     \t [ 6.12564915  3.32572869 12.51004269  0.98546404 10.67846573  0.74794626]. \t  0.9942342052066274 \t 0.995468032253734\n",
      "14     \t [ 6.81522412  2.6591119  12.32976273  0.92847536 10.81284637  0.82767927]. \t  0.9943974341115903 \t 0.995468032253734\n",
      "15     \t [ 7.14882284  4.39559798 14.47329345  0.83026983 10.84470478  0.78469598]. \t  0.9938693419662079 \t 0.995468032253734\n",
      "16     \t [ 6.48317184  4.14917041 13.94628941  0.95511917 12.17056728  0.77671012]. \t  0.9940853801963391 \t 0.995468032253734\n",
      "17     \t [ 6.13887687  3.48156566 13.1257101   1.         12.18186543  0.79456027]. \t  0.9811327997763123 \t 0.995468032253734\n",
      "18     \t [ 7.93618562  3.80993513 14.50443977  0.78130645 10.95278542  0.84769567]. \t  0.9933316455418774 \t 0.995468032253734\n",
      "19     \t [ 6.62136043  3.26494361 12.23678568  0.5        10.81405473  1.        ]. \t  0.9942054199581521 \t 0.995468032253734\n",
      "20     \t [ 6.89685359  3.38708754 13.8145549   1.         12.63498993  0.85188393]. \t  0.9808159452081782 \t 0.995468032253734\n",
      "21     \t [ 6.74419421  3.28702781 12.54980182  0.9616256  10.96070922  0.70084303]. \t  0.9942486073035633 \t 0.995468032253734\n",
      "22     \t [ 6.69636879  2.98138845 12.85503596  0.64300298  9.9899786   0.89001508]. \t  0.9935188802004303 \t 0.995468032253734\n",
      "23     \t [ 7.86640079  3.20553903 12.83291909  0.61985437 10.37561178  0.92442195]. \t  0.993446869784894 \t 0.995468032253734\n",
      "24     \t [ 7.01010762  2.96560553 12.65990422  0.72531738 12.28837489  1.        ]. \t  0.995203986803337 \t 0.995468032253734\n",
      "25     \t [ 7.45381629  4.07375624 14.05758018  1.         11.20181565  0.6662477 ]. \t  0.8402577028137029 \t 0.995468032253734\n",
      "26     \t [ 7.62942437  4.23225008 14.32167722  0.5        10.4929407   1.        ]. \t  0.9942630294521967 \t 0.995468032253734\n",
      "27     \t [ 7.65828425  4.31513608 12.78102097  0.55742498 10.56154524  1.        ]. \t  0.9945558801336226 \t 0.995468032253734\n",
      "28     \t [ 6.31188999  4.16136813 13.48680314  0.73193762 10.35056958  0.95306536]. \t  0.9938405372883294 \t 0.995468032253734\n",
      "29     \t [ 6.82970295  4.86375911 14.06488266  0.5        11.2756305   1.        ]. \t  0.9940709980819564 \t 0.995468032253734\n",
      "30     \t [ 7.32982201  3.88776156 13.19007853  0.74873681 10.08566663  0.49574246]. \t  0.8931097631270553 \t 0.995468032253734\n"
     ]
    }
   ],
   "source": [
    "### 6(q). Bayesian optimization runs (x20): STP DF1 run number = 17\n",
    "\n",
    "np.random.seed(run_num_17)\n",
    "surrogate_stp_df1_17 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_17 = GPGO(surrogate_stp_df1_17, Acquisition_new(util_stp), f_syn_polarity17, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_17.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.443219693415502, -5.396599052729178)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(q). Training Regret Minimisation: run number = 17\n",
    "\n",
    "gp_output_17 = np.append(np.max(gpgo_gp_17.GP.y[0:n_init]),gpgo_gp_17.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_17 = np.append(np.max(gpgo_stp_df1_17.GP.y[0:n_init]),gpgo_stp_df1_17.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_17 = np.log(y_global_orig - gp_output_17)\n",
    "regret_stp_df1_17 = np.log(y_global_orig - stp_df1_output_17)\n",
    "\n",
    "train_regret_gp_17 = min_max_array(regret_gp_17)\n",
    "train_regret_stp_df1_17 = min_max_array(regret_stp_df1_17)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 17\n",
    "min_train_regret_gp_17 = min(train_regret_gp_17)\n",
    "min_train_regret_stp_df1_17 = min(train_regret_stp_df1_17)\n",
    "\n",
    "min_train_regret_gp_17, min_train_regret_stp_df1_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.69581135  9.97907709 13.          0.58943329  7.          0.63606469]. \t  0.8779252814417621 \t 0.9929332456288602\n",
      "init   \t [ 7.64685675  0.54457673  9.          0.73765712 14.          0.31028049]. \t  0.8786694026213274 \t 0.9929332456288602\n",
      "init   \t [ 5.64226636  3.5897164  11.          0.71441011 19.          0.77998073]. \t  0.9929332456288602 \t 0.9929332456288602\n",
      "init   \t [ 8.71058044  6.55813874 11.          0.85444395  7.          0.24686855]. \t  0.8797975984340205 \t 0.9929332456288602\n",
      "init   \t [ 7.14894493  1.08735777 13.          0.77064415  7.          0.49799441]. \t  0.878280530585025 \t 0.9929332456288602\n",
      "1      \t [ 5.9777548   3.30751949 11.37040071  0.7620342  18.31070826  0.7553482 ]. \t  \u001b[92m0.9931492821304285\u001b[0m \t 0.9931492821304285\n",
      "2      \t [ 5.39889713  3.26732489 10.70127591  0.72696012 18.2689946   0.74946055]. \t  0.9928516363966567 \t 0.9931492821304285\n",
      "3      \t [ 6.20912032  3.38500553 10.56645786  0.68720277 18.53532303  0.87958004]. \t  0.9924963810305428 \t 0.9931492821304285\n",
      "4      \t [ 5.80315365  2.75976271 10.93324171  0.79688309 18.79930202  0.85390448]. \t  \u001b[92m0.9932020903493143\u001b[0m \t 0.9932020903493143\n",
      "5      \t [ 5.88995611  3.2193278  10.85040916  0.73869281 18.63037495  0.14010848]. \t  0.8789526499227239 \t 0.9932020903493143\n",
      "6      \t [ 5.79704432  3.33086556 10.89720243  1.         18.56825474  0.93121291]. \t  0.9904655833759692 \t 0.9932020903493143\n",
      "7      \t [ 5.78337458  3.22121201 10.96366237  0.5        18.58008632  0.87560962]. \t  0.9916898510177479 \t 0.9932020903493143\n",
      "8      \t [ 5.46866917  3.15875811  9.95289023  0.72339161 19.16003946  0.73783285]. \t  0.9928996444466561 \t 0.9932020903493143\n",
      "9      \t [ 6.22815534  3.15555634 10.38276176  0.77257007 19.70305751  0.78854185]. \t  0.9928852402062868 \t 0.9932020903493143\n",
      "10     \t [ 6.07061254  2.70746568 11.01025246  0.85556943 17.47016836  0.65647549]. \t  0.8783669500809618 \t 0.9932020903493143\n",
      "11     \t [ 5.31489545  3.04122848 10.62390828  0.85836738 19.86862721  0.65225046]. \t  0.8783909514785355 \t 0.9932020903493143\n",
      "12     \t [ 5.95894934  3.79289602 11.10227461  0.69869433 17.33783275  0.51882311]. \t  0.8778196652805619 \t 0.9932020903493143\n",
      "13     \t [ 5.74892635  3.9539821  10.1571132   0.65380361 19.65455714  0.66930624]. \t  0.992529984425316 \t 0.9932020903493143\n",
      "14     \t [ 5.32065901  3.1278221  11.6685667   0.89461857 17.49098189  0.42635569]. \t  0.8785109786560983 \t 0.9932020903493143\n",
      "15     \t [ 5.79326983  3.38768841 10.41208428  0.72319061 19.2966859   0.73677705]. \t  0.9928948426183429 \t 0.9932020903493143\n",
      "16     \t [ 5.78484914  3.33063276  9.41222271  0.78005803 20.          0.99160939]. \t  0.9927796230770874 \t 0.9932020903493143\n",
      "17     \t [ 5.9053006   2.25814559  9.82700386  0.90586414 19.58406639  0.99384225]. \t  0.9929668524807728 \t 0.9932020903493143\n",
      "18     \t [ 6.25936865  2.45678236  9.80442786  0.83923027 18.56266903  0.84382798]. \t  0.9929044412966886 \t 0.9932020903493143\n",
      "19     \t [ 6.15896977  2.86596049  9.44864464  0.85719117 19.59513439  0.35495686]. \t  0.8783621493589332 \t 0.9932020903493143\n",
      "20     \t [ 6.31292913  2.88038996  9.59706542  0.5        19.32593688  1.        ]. \t  \u001b[92m0.9933652783908946\u001b[0m \t 0.9933652783908946\n",
      "21     \t [ 6.83464052  2.33087032 10.37713323  0.9329432  19.08413845  1.        ]. \t  \u001b[92m0.9948631244066518\u001b[0m \t 0.9948631244066518\n",
      "22     \t [ 5.24083579  4.20069732 11.38801131  0.69507624 18.24553815  0.39956977]. \t  0.8780549088188118 \t 0.9948631244066518\n",
      "23     \t [ 6.29131116  2.15826228 10.28308445  0.5        19.13367524  0.57618814]. \t  0.8794375285173205 \t 0.9948631244066518\n",
      "24     \t [ 4.82281504  4.05821137 10.12903308  0.77312081 19.84857562  1.        ]. \t  0.9945318718209674 \t 0.9948631244066518\n",
      "25     \t [ 4.85143583  4.30072215 10.81459821  0.55688456 19.53276163  0.37386754]. \t  0.8782613216123454 \t 0.9948631244066518\n",
      "26     \t [ 4.99828026  3.74349569  9.65941985  0.5        20.          0.38358258]. \t  0.879672770949286 \t 0.9948631244066518\n",
      "27     \t [ 6.28498711  2.71761372 10.01336362  1.         19.23518544  1.        ]. \t  0.9946854925750276 \t 0.9948631244066518\n",
      "28     \t [ 5.49301846  3.09122137  9.92125496  0.5        20.          1.        ]. \t  0.9933028678982382 \t 0.9948631244066518\n",
      "29     \t [ 6.9129699   2.58937232 10.47275423  0.5        18.2020671   1.        ]. \t  0.9934948985770943 \t 0.9948631244066518\n",
      "30     \t [ 4.96179138  4.41444935 10.06093799  0.70876904 18.90926009  0.74596155]. \t  0.9928132293758578 \t 0.9948631244066518\n"
     ]
    }
   ],
   "source": [
    "### 6(r). Bayesian optimization runs (x20): GP run number = 18\n",
    "\n",
    "np.random.seed(run_num_18)\n",
    "surrogate_gp_18 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "X_train18, X_test18, y_train18, y_test18 = train_test_split(X, y, test_size=0.15, random_state=run_num_18)\n",
    "\n",
    "def f_syn_polarity18(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    clf = XGBClassifier(reg_alpha=alpha, gamma=gamma,max_depth=int(max_depth),subsample=subsample,\n",
    "                              min_child_weight=min_child_weight,colsample_bytree=colsample,objective=obj_classifier, \n",
    "                             booster='gbtree', n_estimators = 5,\n",
    "                             silent=None, random_state=run_num_18)\n",
    "    score = np.array(cross_val_score(clf, X=X_train18, y=y_train18).mean())\n",
    "    return score\n",
    "\n",
    "\n",
    "gpgo_gp_18 = GPGO(surrogate_gp_18, Acquisition_new(util_gp), f_syn_polarity18, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_18.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.69581135  9.97907709 13.          0.58943329  7.          0.63606469]. \t  0.8779252814417621 \t 0.9929332456288602\n",
      "init   \t [ 7.64685675  0.54457673  9.          0.73765712 14.          0.31028049]. \t  0.8786694026213274 \t 0.9929332456288602\n",
      "init   \t [ 5.64226636  3.5897164  11.          0.71441011 19.          0.77998073]. \t  0.9929332456288602 \t 0.9929332456288602\n",
      "init   \t [ 8.71058044  6.55813874 11.          0.85444395  7.          0.24686855]. \t  0.8797975984340205 \t 0.9929332456288602\n",
      "init   \t [ 7.14894493  1.08735777 13.          0.77064415  7.          0.49799441]. \t  0.878280530585025 \t 0.9929332456288602\n",
      "1      \t [ 6.05605272  3.24881325 11.48050831  0.77458452 18.15389726  0.75113695]. \t  \u001b[92m0.9932549005041981\u001b[0m \t 0.9932549005041981\n",
      "2      \t [ 5.37234205  3.20445926 10.68552074  0.73232451 18.11473861  0.7445465 ]. \t  0.9930100603618858 \t 0.9932549005041981\n",
      "3      \t [ 6.28416578  3.20461196 10.57225346  0.81741791 18.49912157  0.83799313]. \t  0.9931396806863715 \t 0.9932549005041981\n",
      "4      \t [ 5.70552054  2.68108985 11.09100111  0.76228932 18.7163888   0.8784268 ]. \t  0.993226095618884 \t 0.9932549005041981\n",
      "5      \t [ 5.86292669  3.15855918 10.95755524  0.50628019 18.53094946  0.17586953]. \t  0.8802008778912604 \t 0.9932549005041981\n",
      "6      \t [ 5.81725261  3.28909333 10.99198605  0.5        18.43961468  1.        ]. \t  \u001b[92m0.9935765154150045\u001b[0m \t 0.9935765154150045\n",
      "7      \t [ 5.76354358  3.25089192 10.99456923  1.         18.49735981  0.72196562]. \t  0.9904655833759692 \t 0.9935765154150045\n",
      "8      \t [ 6.12019743  2.47367117 10.88990481  0.72422767 17.61814646  0.74563772]. \t  0.9928660380096002 \t 0.9935765154150045\n",
      "9      \t [ 5.38955298  3.13442986 10.07500224  0.59057165 19.20207441  0.80582055]. \t  0.992467574485802 \t 0.9935765154150045\n",
      "10     \t [ 6.08471758  3.17203282 10.579558    0.58892528 19.80007276  0.84843474]. \t  0.9920547101787739 \t 0.9935765154150045\n",
      "11     \t [ 5.14841547  3.13575007 10.83899557  0.5        19.84081836  0.75720636]. \t  0.9917954670406625 \t 0.9935765154150045\n",
      "12     \t [ 6.18800345  3.35054646 10.95356971  0.75044829 17.24492359  0.60538834]. \t  0.8787174108096124 \t 0.9935765154150045\n",
      "13     \t [ 5.62426015  3.12293615 10.65946335  0.570432   19.31615124  0.77789723]. \t  0.9924867780653442 \t 0.9935765154150045\n",
      "14     \t [ 5.431114    3.80261474 10.35338501  0.76037082 19.99114817  0.94620138]. \t  0.9930388670449123 \t 0.9935765154150045\n",
      "15     \t [ 5.7426903   3.56074618 11.35884241  0.90373448 20.          1.        ]. \t  \u001b[92m0.9947623077229101\u001b[0m \t 0.9947623077229101\n",
      "16     \t [ 5.72717866  3.83150726 10.98417597  0.5        20.          0.3903632 ]. \t  0.8790342651012071 \t 0.9947623077229101\n",
      "17     \t [ 5.80582812  2.67479476  9.91536319  0.59354406 18.01084276  0.70928304]. \t  0.9918050704207179 \t 0.9947623077229101\n",
      "18     \t [ 5.70464152  3.70499241  9.59591587  0.60123206 18.37734907  0.66849324]. \t  0.9922707410106337 \t 0.9947623077229101\n",
      "19     \t [ 5.96699931  2.98413955 10.6428809   0.62580976 17.94508738  0.66260721]. \t  0.8786645970593038 \t 0.9947623077229101\n",
      "20     \t [ 5.46337816  2.98430798  9.5363202   0.9983575  18.35997702  1.        ]. \t  0.9940181805970649 \t 0.9947623077229101\n",
      "21     \t [ 5.92308835  2.65844643 11.53849468  1.         17.07573422  1.        ]. \t  \u001b[92m0.9948967291842808\u001b[0m \t 0.9948967291842808\n",
      "22     \t [ 6.26868853  3.06644764  9.463067    0.66160026 18.8116394   0.93045458]. \t  0.9923475547756603 \t 0.9948967291842808\n",
      "23     \t [ 5.41326879  1.99431099 10.6797323   1.         17.73831071  1.        ]. \t  \u001b[92m0.9949015301828806\u001b[0m \t 0.9949015301828806\n",
      "24     \t [ 6.12622608  1.98531995 10.09503931  1.         18.32914852  1.        ]. \t  0.9947046925591451 \t 0.9949015301828806\n",
      "25     \t [ 6.12501901  2.00159227 11.35739975  1.         17.87982037  1.        ]. \t  0.9948631231620816 \t 0.9949015301828806\n",
      "26     \t [ 6.15656338  3.75189557 10.01496503  0.91338632 19.09243568  1.        ]. \t  0.9948295200438796 \t 0.9949015301828806\n",
      "27     \t [ 5.78098121  2.93762677  9.56658171  0.91493465 18.67409719  0.27446071]. \t  0.8779588830388226 \t 0.9949015301828806\n",
      "28     \t [ 5.43051505  2.23088268  9.78451715  0.5        18.37974398  1.        ]. \t  0.9934564941837215 \t 0.9949015301828806\n",
      "29     \t [ 5.75472749  3.1242047   9.85627807  0.6662788  18.59262075  1.        ]. \t  0.9937445377820082 \t 0.9949015301828806\n",
      "30     \t [ 5.76640842  2.08082686  9.4418378   0.88481437 17.7252214   0.94730812]. \t  0.9929236458442299 \t 0.9949015301828806\n"
     ]
    }
   ],
   "source": [
    "### 6(r). Bayesian optimization runs (x20): STP DF1 run number = 18\n",
    "\n",
    "np.random.seed(run_num_18)\n",
    "surrogate_stp_df1_18 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_18 = GPGO(surrogate_stp_df1_18, Acquisition_new(util_stp), f_syn_polarity18, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_18.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.2713102455539, -5.2788148201305685)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(r). Training Regret Minimisation: run number = 18\n",
    "\n",
    "gp_output_18 = np.append(np.max(gpgo_gp_18.GP.y[0:n_init]),gpgo_gp_18.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_18 = np.append(np.max(gpgo_stp_df1_18.GP.y[0:n_init]),gpgo_stp_df1_18.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_18 = np.log(y_global_orig - gp_output_18)\n",
    "regret_stp_df1_18 = np.log(y_global_orig - stp_df1_output_18)\n",
    "\n",
    "train_regret_gp_18 = min_max_array(regret_gp_18)\n",
    "train_regret_stp_df1_18 = min_max_array(regret_stp_df1_18)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 18\n",
    "min_train_regret_gp_18 = min(train_regret_gp_18)\n",
    "min_train_regret_stp_df1_18 = min(train_regret_stp_df1_18)\n",
    "\n",
    "min_train_regret_gp_18, min_train_regret_stp_df1_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.24520174  9.84626759 12.          0.81846905 12.          0.27594309]. \t  0.8764750462677148 \t 0.9816081067968505\n",
      "init   \t [ 8.6021676   8.64300514 14.          0.95067208 18.          0.13363564]. \t  0.8766094565968986 \t 0.9816081067968505\n",
      "init   \t [ 3.02486657  6.06229509 10.          0.50494482  9.          0.10218675]. \t  0.8772431925609546 \t 0.9816081067968505\n",
      "init   \t [ 6.57586195  8.01231848  7.          0.63945407 18.          0.49206406]. \t  0.8780737118709103 \t 0.9816081067968505\n",
      "init   \t [ 6.05008856  8.15558548 14.          0.64718943  8.          0.90611106]. \t  0.9816081067968505 \t 0.9816081067968505\n",
      "1      \t [ 5.79646882  8.64092303 13.36249417  0.61086825  7.68150068  0.75412879]. \t  0.9807199573880028 \t 0.9816081067968505\n",
      "2      \t [ 6.12524953  8.05021069 13.15749517  0.74860964  8.22661141  0.74722432]. \t  \u001b[92m0.9819105535291145\u001b[0m \t 0.9819105535291145\n",
      "3      \t [ 6.24674196  8.79318416 13.5874405   0.61505194  8.40866619  0.76392737]. \t  0.9805615303805109 \t 0.9819105535291145\n",
      "4      \t [ 5.46585087  8.36755873 13.56929341  0.52734846  8.4335391   0.75297142]. \t  0.9804655099244698 \t 0.9819105535291145\n",
      "5      \t [ 5.96601615  8.35174412 13.66573231  0.67728298  8.10503784  0.11594932]. \t  0.8785538034338297 \t 0.9819105535291145\n",
      "6      \t [ 5.82603939  8.48982551 13.57197561  1.          8.17771916  0.90553839]. \t  \u001b[92m0.9870618046675385\u001b[0m \t 0.9870618046675385\n",
      "7      \t [ 5.9839473   8.3716478  13.49292378  0.5         8.13295538  0.88536352]. \t  0.9804366994385134 \t 0.9870618046675385\n",
      "8      \t [ 6.20347241  7.92447874 13.92464117  0.86852956  9.071514    0.66975499]. \t  0.9823570281451169 \t 0.9870618046675385\n",
      "9      \t [ 7.01807322  8.03764959 13.8578293   1.          8.4723085   0.67644009]. \t  0.9870185967864235 \t 0.9870618046675385\n",
      "10     \t [ 6.76188419  8.29292876 13.18457627  1.          9.10692207  0.42995326]. \t  0.8959231440899013 \t 0.9870618046675385\n",
      "11     \t [ 6.73595584  8.67738404 14.31256384  1.          8.97644606  0.54842032]. \t  0.8959231440899013 \t 0.9870618046675385\n",
      "12     \t [ 5.02547932  7.90300235 13.40964431  0.74905584  7.43593083  0.66167502]. \t  0.877944090440166 \t 0.9870618046675385\n",
      "13     \t [ 5.02491294  8.57465107 12.63278522  0.74087074  7.86217126  0.41586904]. \t  0.8779152888046139 \t 0.9870618046675385\n",
      "14     \t [ 4.7209595   8.93586653 13.57360788  0.53435132  7.54810527  0.53967009]. \t  0.8767726922779522 \t 0.9870618046675385\n",
      "15     \t [ 5.15634061  8.77140319 13.00717793  1.          6.81965788  0.44136862]. \t  0.8976706387137247 \t 0.9870618046675385\n",
      "16     \t [ 6.43042376  8.26471608 13.76582237  0.88305207  8.63253649  0.66905857]. \t  0.9821986022439256 \t 0.9870618046675385\n",
      "17     \t [ 7.25236713  7.8187315  14.076279    0.56496181  9.36935984  0.65462428]. \t  0.8792451240693122 \t 0.9870618046675385\n",
      "18     \t [ 6.79699201  7.06591622 13.44274996  0.92635482  8.87280066  0.55769048]. \t  0.8768302938204617 \t 0.9870618046675385\n",
      "19     \t [ 6.75257977  7.36565731 14.70190328  0.90776405  8.66790465  0.68692529]. \t  0.9822466111236424 \t 0.9870618046675385\n",
      "20     \t [ 5.88605253  7.04493413 14.06945347  0.7640232   8.40258869  0.89171746]. \t  0.9819297585606782 \t 0.9870618046675385\n",
      "21     \t [ 5.40869128  9.44181112 13.22123433  1.          7.98851387  0.27970764]. \t  0.8978338650603673 \t 0.9870618046675385\n",
      "22     \t [ 5.12732359  8.62140459 13.39125579  1.          7.55739264  0.10550381]. \t  0.8977426505812804 \t 0.9870618046675385\n",
      "23     \t [ 6.75621062  7.20385051 14.05251101  0.5         8.0090702   0.6128737 ]. \t  0.8743146850552578 \t 0.9870618046675385\n",
      "24     \t [ 6.25696735  7.28283855 14.29055462  0.5         8.92004664  0.16918462]. \t  0.8744827171022796 \t 0.9870618046675385\n",
      "25     \t [ 6.49616716  7.32845489 14.18351097  0.5         8.85728094  1.        ]. \t  \u001b[92m0.9940085739679847\u001b[0m \t 0.9940085739679847\n",
      "26     \t [ 5.91744459  9.15837    12.61088494  0.6924029   8.77806036  0.63282313]. \t  0.8775744406521956 \t 0.9940085739679847\n",
      "27     \t [ 7.28409305  8.273157   14.77194794  0.5         8.3673392   0.57446805]. \t  0.8753900620008489 \t 0.9940085739679847\n",
      "28     \t [ 5.00332298  8.94304435 13.09735211  1.          7.61617147  1.        ]. \t  \u001b[92m0.9959481104027595\u001b[0m \t 0.9959481104027595\n",
      "29     \t [ 7.44668391  8.97092981 13.70056004  0.5         8.75204152  0.14075302]. \t  0.8753276461149874 \t 0.9959481104027595\n",
      "30     \t [ 6.36046593  9.43491891 13.46189558  0.5         9.2187595   0.1       ]. \t  0.8742426689007875 \t 0.9959481104027595\n"
     ]
    }
   ],
   "source": [
    "### 6(s). Bayesian optimization runs (x20): GP run number = 19\n",
    "\n",
    "np.random.seed(run_num_19)\n",
    "surrogate_gp_19 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "X_train19, X_test19, y_train19, y_test19 = train_test_split(X, y, test_size=0.15, random_state=run_num_19)\n",
    "\n",
    "def f_syn_polarity19(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    clf = XGBClassifier(reg_alpha=alpha, gamma=gamma,max_depth=int(max_depth),subsample=subsample,\n",
    "                              min_child_weight=min_child_weight,colsample_bytree=colsample,objective=obj_classifier, \n",
    "                             booster='gbtree', n_estimators = 5,\n",
    "                             silent=None, random_state=run_num_19)\n",
    "    score = np.array(cross_val_score(clf, X=X_train19, y=y_train19).mean())\n",
    "    return score\n",
    "\n",
    "\n",
    "gpgo_gp_19 = GPGO(surrogate_gp_19, Acquisition_new(util_gp), f_syn_polarity19, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_19.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.24520174  9.84626759 12.          0.81846905 12.          0.27594309]. \t  0.8764750462677148 \t 0.9816081067968505\n",
      "init   \t [ 8.6021676   8.64300514 14.          0.95067208 18.          0.13363564]. \t  0.8766094565968986 \t 0.9816081067968505\n",
      "init   \t [ 3.02486657  6.06229509 10.          0.50494482  9.          0.10218675]. \t  0.8772431925609546 \t 0.9816081067968505\n",
      "init   \t [ 6.57586195  8.01231848  7.          0.63945407 18.          0.49206406]. \t  0.8780737118709103 \t 0.9816081067968505\n",
      "init   \t [ 6.05008856  8.15558548 14.          0.64718943  8.          0.90611106]. \t  0.9816081067968505 \t 0.9816081067968505\n",
      "1      \t [ 5.23246631  8.21441795 14.00043322  0.85355424  8.24637445  0.20621436]. \t  0.8782177528919677 \t 0.9816081067968505\n",
      "2      \t [ 5.93292611  7.865432   13.21193117  0.86168511  8.48885098  0.50863243]. \t  0.8785489995311891 \t 0.9816081067968505\n",
      "3      \t [ 6.03851104  8.69451658 13.90986739  0.70474603  8.79085513  0.51556316]. \t  0.877814483391329 \t 0.9816081067968505\n",
      "4      \t [ 5.39903788  8.55505185 13.42142966  0.5         8.25084242  1.        ]. \t  \u001b[92m0.9947286996270646\u001b[0m \t 0.9947286996270646\n",
      "5      \t [ 5.50107273  7.93158612 13.93719084  0.50195378  8.73584223  1.        ]. \t  \u001b[92m0.994839119898297\u001b[0m \t 0.994839119898297\n",
      "6      \t [ 5.60242759  8.29934757 13.77845569  1.          8.42438803  1.        ]. \t  \u001b[92m0.9957032677021814\u001b[0m \t 0.9957032677021814\n",
      "7      \t [ 5.68051729  8.21442629 13.72164682  0.5         8.35493974  0.64902186]. \t  0.8742618796021414 \t 0.9957032677021814\n",
      "8      \t [ 6.3578781   7.57253817 14.46905569  1.          8.7379328   0.69183056]. \t  0.9870762069719059 \t 0.9957032677021814\n",
      "9      \t [ 6.88058028  7.96127711 13.72692576  0.88052581  8.68929159  1.        ]. \t  0.9955784476849017 \t 0.9957032677021814\n",
      "10     \t [ 6.38521188  7.14274087 13.78302071  1.          8.19991266  1.        ]. \t  \u001b[92m0.9957656786788341\u001b[0m \t 0.9957656786788341\n",
      "11     \t [ 4.48841547  8.83589464 13.75111336  0.52705035  8.66972984  1.        ]. \t  0.9953528098084558 \t 0.9957656786788341\n",
      "12     \t [ 4.89253444  8.5542977  12.98739803  0.64247175  9.03830935  1.        ]. \t  0.9953288061291788 \t 0.9957656786788341\n",
      "13     \t [ 4.48501723  8.12154279 13.25326482  0.69332111  8.30347942  1.        ]. \t  0.9954872329292394 \t 0.9957656786788341\n",
      "14     \t [ 6.83429273  7.79976031 14.15520525  1.          7.97298007  0.67375573]. \t  0.9871050115114968 \t 0.9957656786788341\n",
      "15     \t [ 4.68137025  8.95414629 13.09988086  0.90060093  8.29832044  0.61709054]. \t  0.8763646343628803 \t 0.9957656786788341\n",
      "16     \t [ 6.40084966  7.76961815 14.04849936  0.91948025  8.38536929  1.        ]. \t  0.9957080692539334 \t 0.9957656786788341\n",
      "17     \t [ 6.80441363  7.32185212 13.68150908  1.          8.7072499   0.30706512]. \t  0.8959375464634124 \t 0.9957656786788341\n",
      "18     \t [ 4.73321446  8.10034326 13.77663184  0.94410138  9.03809098  0.76632281]. \t  0.9823522309494234 \t 0.9957656786788341\n",
      "19     \t [ 5.45902894  7.1007125  14.29416218  1.          8.18716388  0.57313272]. \t  0.8963168104870359 \t 0.9957656786788341\n",
      "20     \t [ 6.87956788  7.89288825 13.11716432  0.92597983  7.78026715  0.91902267]. \t  0.9821890001775785 \t 0.9957656786788341\n",
      "21     \t [ 4.86563877  8.44285039 13.53859081  0.7211773   8.60769578  1.        ]. \t  0.9954920335129783 \t 0.9957656786788341\n",
      "22     \t [ 5.42633468  7.71873312 14.49085169  1.          9.21557476  0.4809398 ]. \t  0.8962784042268167 \t 0.9957656786788341\n",
      "23     \t [ 6.42896864  8.85884731 13.35924681  1.          7.94106115  1.        ]. \t  0.9957512763053229 \t 0.9957656786788341\n",
      "24     \t [ 5.75869647  8.11964979 13.29705234  1.          7.34746253  1.        ]. \t  \u001b[92m0.9959241065851953\u001b[0m \t 0.9959241065851953\n",
      "25     \t [ 3.98372594  8.46095198 13.21516337  0.5         8.96569865  0.56832754]. \t  0.8756060924868763 \t 0.9959241065851953\n",
      "26     \t [ 5.47594721  7.1453596  13.64096764  0.91862182  9.14770177  1.        ]. \t  0.9956408589381297 \t 0.9959241065851953\n",
      "27     \t [ 5.88407026  8.01340948 13.58843175  0.79744574  9.65702967  1.        ]. \t  0.9955304401189166 \t 0.9959241065851953\n",
      "28     \t [ 4.92375741  7.77080892 13.51815313  0.5         9.58354586  1.        ]. \t  0.9951271733148855 \t 0.9959241065851953\n",
      "29     \t [ 5.46746968  8.99566536 13.86296097  0.81790397  7.44958374  0.94139037]. \t  0.9822466103630609 \t 0.9959241065851953\n",
      "30     \t [ 6.18700853  8.41115872 13.71146687  1.          7.58594941  0.62050986]. \t  0.8959135425075605 \t 0.9959241065851953\n"
     ]
    }
   ],
   "source": [
    "### 6(s). Bayesian optimization runs (x20): STP DF1 run number = 19\n",
    "\n",
    "np.random.seed(run_num_19)\n",
    "surrogate_stp_df1_19 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_19 = GPGO(surrogate_stp_df1_19, Acquisition_new(util_stp), f_syn_polarity19, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_19.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.508571939452466, -5.5026653134226535)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(s). Training Regret Minimisation: run number = 19\n",
    "\n",
    "gp_output_19 = np.append(np.max(gpgo_gp_19.GP.y[0:n_init]),gpgo_gp_19.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_19 = np.append(np.max(gpgo_stp_df1_19.GP.y[0:n_init]),gpgo_stp_df1_19.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_19 = np.log(y_global_orig - gp_output_19)\n",
    "regret_stp_df1_19 = np.log(y_global_orig - stp_df1_output_19)\n",
    "\n",
    "train_regret_gp_19 = min_max_array(regret_gp_19)\n",
    "train_regret_stp_df1_19 = min_max_array(regret_stp_df1_19)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 19\n",
    "min_train_regret_gp_19 = min(train_regret_gp_19)\n",
    "min_train_regret_stp_df1_19 = min(train_regret_stp_df1_19)\n",
    "\n",
    "min_train_regret_gp_19, min_train_regret_stp_df1_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.70517285  5.64528755 11.          0.51213999 19.          0.57883228]. \t  0.8542085347679844 \t 0.9836195641736719\n",
      "init   \t [0.68106288 5.8452906  6.         0.58037829 8.         0.87974899]. \t  0.9824338042245909 \t 0.9836195641736719\n",
      "init   \t [ 4.04121426  7.17971417 11.          0.57603584 10.          0.19954225]. \t  0.8541221293771765 \t 0.9836195641736719\n",
      "init   \t [ 5.27556761  8.67655329 13.          0.54252273  4.          0.48570796]. \t  0.8537524600907028 \t 0.9836195641736719\n",
      "init   \t [ 5.30003889  5.73946822 12.          0.52666698 16.          0.77467019]. \t  0.9836195641736719 \t 0.9836195641736719\n",
      "1      \t [ 5.44372839  5.70606598 11.6453299   0.52150811 17.06401712  0.70521233]. \t  0.9826881971344378 \t 0.9836195641736719\n",
      "2      \t [ 4.91423572  6.027353   11.91026383  1.         16.59949681  0.68273873]. \t  \u001b[92m0.9894526106999565\u001b[0m \t 0.9894526106999565\n",
      "3      \t [ 4.97856465  5.31817839 11.46129805  0.79514961 16.42045684  0.72186073]. \t  0.9836435706878119 \t 0.9894526106999565\n",
      "4      \t [ 5.42998782  6.03170592 11.31044822  0.78949974 16.31778507  0.51047422]. \t  0.8544053953471256 \t 0.9894526106999565\n",
      "5      \t [ 5.44218714  5.4231314  12.07450296  0.99999368 16.54286406  0.35925721]. \t  0.8543909830861929 \t 0.9894526106999565\n",
      "6      \t [ 4.83031562  5.72889744 11.8174268   0.5        16.53469628  0.26924759]. \t  0.8532915741816888 \t 0.9894526106999565\n",
      "7      \t [ 5.13324053  5.62095869 11.13747156  1.         18.0237252   0.68581926]. \t  0.9893757976263577 \t 0.9894526106999565\n",
      "8      \t [ 5.6221328   6.15022768 11.61752422  0.89049129 18.32186601  0.76624717]. \t  0.9838404070669774 \t 0.9894526106999565\n",
      "9      \t [ 5.96090146  5.47375827 11.23497614  0.95167357 18.14291903  1.        ]. \t  \u001b[92m0.9952855927851632\u001b[0m \t 0.9952855927851632\n",
      "10     \t [ 5.36565003  5.38440689 11.71648458  0.60993803 18.38401167  1.        ]. \t  0.9947335014547405 \t 0.9952855927851632\n",
      "11     \t [ 5.28583051  5.60073483 11.76622947  1.         17.5497361   1.        ]. \t  0.9952375870860185 \t 0.9952855927851632\n",
      "12     \t [ 5.70332384  5.42297729 11.60216905  0.90437166 18.27754046  0.27973176]. \t  0.8551014837235739 \t 0.9952855927851632\n",
      "13     \t [ 5.52744678  5.81271675 11.18754968  0.5        18.12528225  0.9854471 ]. \t  0.9822897287706444 \t 0.9952855927851632\n",
      "14     \t [ 5.45618101  5.66352579 11.31297476  1.         18.71225292  1.        ]. \t  0.9951799777302476 \t 0.9952855927851632\n",
      "15     \t [1.23144693 5.85197325 6.48847773 0.74955703 8.48849574 0.73305095]. \t  0.98294749461878 \t 0.9952855927851632\n",
      "16     \t [1.36246342 5.26761315 6.02045907 0.67723944 8.02045963 0.82422932]. \t  0.9826882567355205 \t 0.9952855927851632\n",
      "17     \t [0.8716044  5.4377858  6.70288751 0.82952515 7.82121259 0.77483168]. \t  0.9843589085546133 \t 0.9952855927851632\n",
      "18     \t [1.3925918  5.98678594 6.37216489 0.54306783 7.65648227 0.77433504]. \t  0.9804750563969784 \t 0.9952855927851632\n",
      "19     \t [1.03027388 5.62393477 6.2622863  0.51503887 7.99869208 0.13981842]. \t  0.8526770724968954 \t 0.9952855927851632\n",
      "20     \t [1.1421043  5.80777639 6.15335746 1.         7.91994751 0.76766661]. \t  0.9848054333683572 \t 0.9952855927851632\n",
      "21     \t [1.1060852  5.62064555 6.41770283 0.5        8.05879462 1.        ]. \t  0.989025334685138 \t 0.9952855927851632\n",
      "22     \t [1.96514915 5.34324043 6.92581975 0.93926432 7.97665128 0.50697567]. \t  0.8533972027885909 \t 0.9952855927851632\n",
      "23     \t [0.75204563 5.33493028 5.8971384  0.5        6.98208838 0.84873805]. \t  0.980820764803552 \t 0.9952855927851632\n",
      "24     \t [1.53190737 5.04001089 6.44717664 0.71055155 6.95917079 0.63550503]. \t  0.8539445075021136 \t 0.9952855927851632\n",
      "25     \t [0.38092742 4.72691689 5.83966195 0.87386778 7.53496813 0.76499962]. \t  0.9818913419953404 \t 0.9952855927851632\n",
      "26     \t [0.54305182 5.12824791 5.05386159 0.5        7.58541829 0.79214693]. \t  0.980839968106523 \t 0.9952855927851632\n",
      "27     \t [1.06117456 4.81774332 5.38752597 0.89105395 7.19632873 0.48429556]. \t  0.8482026787046539 \t 0.9952855927851632\n",
      "28     \t [0.72283048 5.14409805 5.79973689 0.5        7.58443103 0.7043625 ]. \t  0.980739150178211 \t 0.9952855927851632\n",
      "29     \t [0.14032403 5.38337767 5.33672421 1.         7.26861171 1.        ]. \t  0.9848006206154837 \t 0.9952855927851632\n",
      "30     \t [0.16080518 4.57914608 5.13035198 0.62639294 6.83103029 1.        ]. \t  0.9844357586887463 \t 0.9952855927851632\n"
     ]
    }
   ],
   "source": [
    "### 6(t). Bayesian optimization runs (x20): GP run number = 20\n",
    "\n",
    "np.random.seed(run_num_20)\n",
    "surrogate_gp_20 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "X_train20, X_test20, y_train20, y_test20 = train_test_split(X, y, test_size=0.15, random_state=run_num_20)\n",
    "\n",
    "def f_syn_polarity20(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    clf = XGBClassifier(reg_alpha=alpha, gamma=gamma,max_depth=int(max_depth),subsample=subsample,\n",
    "                              min_child_weight=min_child_weight,colsample_bytree=colsample,objective=obj_classifier, \n",
    "                             booster='gbtree', n_estimators = 5,\n",
    "                             silent=None, random_state=run_num_20)\n",
    "    score = np.array(cross_val_score(clf, X=X_train20, y=y_train20).mean())\n",
    "    return score\n",
    "\n",
    "\n",
    "gpgo_gp_20 = GPGO(surrogate_gp_20, Acquisition_new(util_gp), f_syn_polarity20, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_20.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.70517285  5.64528755 11.          0.51213999 19.          0.57883228]. \t  0.8542085347679844 \t 0.9836195641736719\n",
      "init   \t [0.68106288 5.8452906  6.         0.58037829 8.         0.87974899]. \t  0.9824338042245909 \t 0.9836195641736719\n",
      "init   \t [ 4.04121426  7.17971417 11.          0.57603584 10.          0.19954225]. \t  0.8541221293771765 \t 0.9836195641736719\n",
      "init   \t [ 5.27556761  8.67655329 13.          0.54252273  4.          0.48570796]. \t  0.8537524600907028 \t 0.9836195641736719\n",
      "init   \t [ 5.30003889  5.73946822 12.          0.52666698 16.          0.77467019]. \t  0.9836195641736719 \t 0.9836195641736719\n",
      "1      \t [ 5.47765929  5.69818856 11.56158201  0.52029865 17.31523192  0.6888129 ]. \t  0.9826593925256949 \t 0.9836195641736719\n",
      "2      \t [ 5.48116166  5.01327975 12.26843182  0.5        16.72951388  0.73129729]. \t  0.9824865647349531 \t 0.9836195641736719\n",
      "3      \t [ 5.23991261  5.10153159 11.32490354  0.50626954 16.45165728  0.75082507]. \t  0.9831250756995983 \t 0.9836195641736719\n",
      "4      \t [ 4.69979552  5.4991846  11.95967826  0.50060834 16.73940791  0.60942564]. \t  0.8539732944103027 \t 0.9836195641736719\n",
      "5      \t [ 5.5783426   5.45761951 11.78572725  1.         16.54464222  0.31457343]. \t  0.8491392198369697 \t 0.9836195641736719\n",
      "6      \t [ 5.51217471  5.51085934 11.80326558  0.5        16.61640823  1.        ]. \t  \u001b[92m0.9938213435264714\u001b[0m \t 0.9938213435264714\n",
      "7      \t [ 5.48585609  4.86247564 11.35233629  0.5        18.00456935  0.59079781]. \t  0.8534163944755736 \t 0.9938213435264714\n",
      "8      \t [ 5.35249292  5.6460667  10.51488109  0.5        17.99599939  0.45446277]. \t  0.8538484739781323 \t 0.9938213435264714\n",
      "9      \t [ 5.15713794  5.66332685 11.21492435  1.         18.28738659  1.        ]. \t  \u001b[92m0.995218383921333\u001b[0m \t 0.995218383921333\n",
      "10     \t [ 5.14730126  5.72999269 11.40443599  0.76245577 18.34425211  0.18487615]. \t  0.854054930055051 \t 0.995218383921333\n",
      "11     \t [ 5.90660466  5.62454461 11.11066333  1.         18.15168898  0.67440397]. \t  0.9893037879022134 \t 0.995218383921333\n",
      "12     \t [ 5.09605624  4.97070137 12.07418845  0.8629601  15.97170928  0.98101922]. \t  0.9840228408652004 \t 0.995218383921333\n",
      "13     \t [ 5.55299885  5.70608143 11.22437535  0.5        18.22381159  0.90421132]. \t  0.9822945296309585 \t 0.995218383921333\n",
      "14     \t [ 5.21994934  5.19307973 11.45922354  1.         17.28960944  0.90001343]. \t  0.989390199654158 \t 0.995218383921333\n",
      "15     \t [ 5.26557673  5.16138418 11.96597058  0.5        16.2152043   0.67483302]. \t  0.9826305910975205 \t 0.995218383921333\n",
      "16     \t [ 4.90826807  5.67538968 11.43007785  1.         15.99543336  1.        ]. \t  \u001b[92m0.9952471889449325\u001b[0m \t 0.9952471889449325\n",
      "17     \t [ 5.14540822  5.41823426 11.91572322  0.88888394 16.36946938  0.95809153]. \t  0.983667576233953 \t 0.9952471889449325\n",
      "18     \t [ 5.42309328  5.42098922 11.45908321  0.86155091 15.32226498  1.        ]. \t  \u001b[92m0.9952615897281625\u001b[0m \t 0.9952615897281625\n",
      "19     \t [ 4.71933547  5.48447376 11.59408484  0.52536506 15.38041916  1.        ]. \t  0.994757501607744 \t 0.9952615897281625\n",
      "20     \t [ 5.1418124   5.47576188 11.52533091  0.64301492 15.7750636   1.        ]. \t  0.9949927436248527 \t 0.9952615897281625\n",
      "21     \t [ 4.89935859  5.61676292 11.90841089  1.         15.19935481  0.82168196]. \t  0.9896254400118397 \t 0.9952615897281625\n",
      "22     \t [ 4.69837892  4.96071104 11.24927126  1.         15.33464783  0.82318359]. \t  0.9896254393204118 \t 0.9952615897281625\n",
      "23     \t [ 5.10321462  4.73845797 11.88044804  0.8325843  14.76280606  1.        ]. \t  \u001b[92m0.9953432053215024\u001b[0m \t 0.9953432053215024\n",
      "24     \t [ 4.97745834  5.11156601 11.67857882  1.         15.21391552  1.        ]. \t  0.9952567896975618 \t 0.9953432053215024\n",
      "25     \t [ 4.85364263  5.84239864 11.11749848  0.80960822 15.38259978  0.41070865]. \t  0.854400580243397 \t 0.9953432053215024\n",
      "26     \t [ 5.6397902   5.27634717 12.38514542  0.51227884 14.92645244  0.92152858]. \t  0.9834515335095338 \t 0.9953432053215024\n",
      "27     \t [ 5.24344986  5.18564678 11.75571891  0.5        14.69851094  0.57284691]. \t  0.8531955525502671 \t 0.9953432053215024\n",
      "28     \t [ 5.96294426  4.5337319  12.15957452  0.78926185 15.41455184  1.        ]. \t  \u001b[92m0.9954200183951012\u001b[0m \t 0.9954200183951012\n",
      "29     \t [ 5.88921478  5.20939077 12.56730721  0.794703   15.82612533  1.        ]. \t  0.9953288026022743 \t 0.9954200183951012\n",
      "30     \t [ 4.1678748   5.24923821 10.88657784  0.85279893 16.13717789  0.6904382 ]. \t  0.9837635937168075 \t 0.9954200183951012\n"
     ]
    }
   ],
   "source": [
    "### 6(t). Bayesian optimization runs (x20): STP DF1 run number = 20\n",
    "\n",
    "np.random.seed(run_num_20)\n",
    "surrogate_stp_df1_20 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_20 = GPGO(surrogate_stp_df1_20, Acquisition_new(util_stp), f_syn_polarity20, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_20.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.357132094058905, -5.386060297261749)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(t). Training Regret Minimisation: run number = 20\n",
    "\n",
    "gp_output_20 = np.append(np.max(gpgo_gp_20.GP.y[0:n_init]),gpgo_gp_20.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_20 = np.append(np.max(gpgo_stp_df1_20.GP.y[0:n_init]),gpgo_stp_df1_20.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_20 = np.log(y_global_orig - gp_output_20)\n",
    "regret_stp_df1_20 = np.log(y_global_orig - stp_df1_output_20)\n",
    "\n",
    "train_regret_gp_20 = min_max_array(regret_gp_20)\n",
    "train_regret_stp_df1_20 = min_max_array(regret_stp_df1_20)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 20\n",
    "min_train_regret_gp_20 = min(train_regret_gp_20)\n",
    "min_train_regret_stp_df1_20 = min(train_regret_stp_df1_20)\n",
    "\n",
    "min_train_regret_gp_20, min_train_regret_stp_df1_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7(a). Sort GP results:\n",
    "\n",
    "gp_regret = [min_train_regret_gp_1,\n",
    "                 min_train_regret_gp_2,\n",
    "                 min_train_regret_gp_3,\n",
    "                 min_train_regret_gp_4,\n",
    "                 min_train_regret_gp_5,\n",
    "                 min_train_regret_gp_6,\n",
    "                 min_train_regret_gp_7,\n",
    "                 min_train_regret_gp_8,\n",
    "                 min_train_regret_gp_9,\n",
    "                 min_train_regret_gp_10,\n",
    "                 min_train_regret_gp_11,\n",
    "                 min_train_regret_gp_12,\n",
    "                 min_train_regret_gp_13,\n",
    "                 min_train_regret_gp_14,\n",
    "                 min_train_regret_gp_15,\n",
    "                 min_train_regret_gp_16,\n",
    "                 min_train_regret_gp_17,\n",
    "                 min_train_regret_gp_18,\n",
    "                 min_train_regret_gp_19,\n",
    "                 min_train_regret_gp_20]\n",
    "\n",
    "fields = [\"Run 1\",\"Run 2\",\"Run 3\",\"Run 4\",\"Run 5\",\"Run 6\",\"Run 7\",\"Run 8\",\"Run 9\",\"Run 10\",\n",
    "          \"Run 11\",\"Run 12\",\"Run 13\",\"Run 14\",\"Run 15\",\"Run 16\",\"Run 17\",\"Run 18\",\"Run 19\",\"Run 20\"]\n",
    "\n",
    "IndexTitle = [\"GP\"]\n",
    "\n",
    "gp_results = pd.DataFrame(gp_regret, fields, IndexTitle).sort_values(by=[\"GP\"], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp = gp_results[4:5]\n",
    "median_gp = gp_results[9:10]\n",
    "upper_gp = gp_results[14:15]\n",
    "best_gp = gp_results[19:20]\n",
    "\n",
    "#gp_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(              GP\n",
       " Run 10 -5.163337,              GP\n",
       " Run 9 -5.341974,               GP\n",
       " Run 12 -5.379792)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 7(b). Training regret minimization - GP:\n",
    "lower_gp, median_gp, upper_gp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7(c). Sort STP DF1 results:\n",
    "\n",
    "stp_df1_regret = [min_train_regret_stp_df1_1,\n",
    "                  min_train_regret_stp_df1_2,\n",
    "                  min_train_regret_stp_df1_3,\n",
    "                  min_train_regret_stp_df1_4,\n",
    "                  min_train_regret_stp_df1_5,\n",
    "                  min_train_regret_stp_df1_6,\n",
    "                  min_train_regret_stp_df1_7,\n",
    "                  min_train_regret_stp_df1_8,\n",
    "                  min_train_regret_stp_df1_9,\n",
    "                  min_train_regret_stp_df1_10,\n",
    "                  min_train_regret_stp_df1_11,\n",
    "                  min_train_regret_stp_df1_12,\n",
    "                  min_train_regret_stp_df1_13,\n",
    "                  min_train_regret_stp_df1_14,\n",
    "                  min_train_regret_stp_df1_15,\n",
    "                  min_train_regret_stp_df1_16,\n",
    "                  min_train_regret_stp_df1_17,\n",
    "                  min_train_regret_stp_df1_18,\n",
    "                  min_train_regret_stp_df1_19,\n",
    "                  min_train_regret_stp_df1_20]\n",
    "\n",
    "fields = [\"Run 1\",\"Run 2\",\"Run 3\",\"Run 4\",\"Run 5\",\"Run 6\",\"Run 7\",\"Run 8\",\"Run 9\",\"Run 10\",\n",
    "          \"Run 11\",\"Run 12\",\"Run 13\",\"Run 14\",\"Run 15\",\"Run 16\",\"Run 17\",\"Run 18\",\"Run 19\",\"Run 20\"]\n",
    "\n",
    "IndexTitle = [\"STP DF 1\"]\n",
    "\n",
    "stp_df1_results = pd.DataFrame(stp_df1_regret, fields, IndexTitle).sort_values(by=[\"STP DF 1\"], ascending=False)\n",
    "\n",
    "### training regret minimization IQR - STP DF1:\n",
    "lower_stp_df1 = stp_df1_results[4:5]\n",
    "median_stp_df1 = stp_df1_results[9:10]\n",
    "upper_stp_df1 = stp_df1_results[14:15]\n",
    "best_stp_df1 = stp_df1_results[19:20]\n",
    "\n",
    "#stp_df1_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(        STP DF 1\n",
       " Run 16 -5.216752,         STP DF 1\n",
       " Run 12  -5.30844,         STP DF 1\n",
       " Run 17 -5.396599)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 7(d). Sort STP DF 1 results:\n",
    "\n",
    "### Training regret minimization - STP DF1:\n",
    "lower_stp_df1, median_stp_df1, upper_stp_df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(              GP\n",
       " Run 10 -5.163337,               GP\n",
       " Run 12 -5.379792,         STP DF 1\n",
       " Run 16 -5.216752,         STP DF 1\n",
       " Run 17 -5.396599)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 8(a). IQR inputs:\n",
    "\n",
    "lower_gp, upper_gp, lower_stp_df1, upper_stp_df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 8(b). Regret minimisation plot: GP v STP DF 1\n",
    "\n",
    "title = obj_func\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(train_regret_gp_9, label = 'GP ERM Regret: Median', marker = 'D', color = 'Red')\n",
    "plt.plot(train_regret_stp_df1_12, label = 'STP ERM Regret: Median ' r'($\\nu$' ' = {})'.format(df1), marker = '*', color = 'Blue')\n",
    "\n",
    "plt.title(title, weight = 'bold')\n",
    "plt.xlabel('(post-initialization) iteration $\\it{k}$', weight = 'bold') # x-axis label\n",
    "plt.ylabel('ln(Regret)', weight = 'bold') # y-axis label\n",
    "plt.legend(loc=0) # add plot legend\n",
    "\n",
    "plt.show() #visualise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 8(c). Regret minimisation plot: IQR GP v STP DF 1\n",
    "\n",
    "title = obj_func\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(train_regret_gp_9, marker = 'D', color = 'Red')\n",
    "plt.plot(train_regret_stp_df1_12, marker = '*', color = 'Blue')\n",
    "\n",
    "xstar = np.arange(0, 31, step=1)\n",
    "plt.fill_between(xstar, train_regret_gp_10, train_regret_gp_12, facecolor = 'Red', alpha=0.4, label='GP ERM Regret: IQR')\n",
    "plt.fill_between(xstar, train_regret_stp_df1_16, train_regret_stp_df1_8, facecolor = 'Blue', alpha=0.4, label='STP ERM Regret: IQR ' r'($\\nu$' ' = {})'.format(df1))\n",
    "\n",
    "plt.title(title, weight = 'bold')\n",
    "plt.xlabel('(post-initialization) iteration $\\it{k}$', weight = 'bold') # x-axis label\n",
    "plt.ylabel('ln(Regret)', weight = 'bold') # y-axis label\n",
    "plt.legend(loc=0) # add plot legend\n",
    "\n",
    "plt.show() #visualise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.209830241257138,\n",
       " -4.537426461675017,\n",
       " -4.913043412021056,\n",
       " -4.209830241257138,\n",
       " -4.537426461675017,\n",
       " -4.913043412021056)"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iteration1 :\n",
    "\n",
    "slice1 = 0\n",
    "\n",
    "gp1 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp1 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp1_results = pd.DataFrame(gp1).sort_values(by=[0], ascending=False)\n",
    "stp1_results = pd.DataFrame(stp1).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp1 = np.asarray(gp1_results[4:5][0])[0]\n",
    "median_gp1 = np.asarray(gp1_results[9:10][0])[0]\n",
    "upper_gp1 = np.asarray(gp1_results[14:15][0])[0]\n",
    "\n",
    "lower_stp1 = np.asarray(stp1_results[4:5][0])[0]\n",
    "median_stp1 = np.asarray(stp1_results[9:10][0])[0]\n",
    "upper_stp1 = np.asarray(stp1_results[14:15][0])[0]\n",
    "\n",
    "lower_gp1, median_gp1, upper_gp1, lower_stp1, median_stp1, upper_stp1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.9625897021081435,\n",
       " -5.165856793799957,\n",
       " -5.344989113198684,\n",
       " -4.628479230876084,\n",
       " -5.204438920825461,\n",
       " -5.3704622566126305)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iteration11 :\n",
    "\n",
    "slice11 = 10\n",
    "\n",
    "gp11 = [train_regret_gp_1[slice11],\n",
    "       train_regret_gp_2[slice11],\n",
    "       train_regret_gp_3[slice11],\n",
    "       train_regret_gp_4[slice11],\n",
    "       train_regret_gp_5[slice11],\n",
    "       train_regret_gp_6[slice11],\n",
    "       train_regret_gp_7[slice11],\n",
    "       train_regret_gp_8[slice11],\n",
    "       train_regret_gp_9[slice11],\n",
    "       train_regret_gp_10[slice11],\n",
    "       train_regret_gp_11[slice11],\n",
    "       train_regret_gp_12[slice11],\n",
    "       train_regret_gp_13[slice11],\n",
    "       train_regret_gp_14[slice11],\n",
    "       train_regret_gp_15[slice11],\n",
    "       train_regret_gp_16[slice11],\n",
    "       train_regret_gp_17[slice11],\n",
    "       train_regret_gp_18[slice11],\n",
    "       train_regret_gp_19[slice11],\n",
    "       train_regret_gp_20[slice11]]\n",
    "\n",
    "stp11 = [train_regret_stp_df1_1[slice11],\n",
    "       train_regret_stp_df1_2[slice11],\n",
    "       train_regret_stp_df1_3[slice11],\n",
    "       train_regret_stp_df1_4[slice11],\n",
    "       train_regret_stp_df1_5[slice11],\n",
    "       train_regret_stp_df1_6[slice11],\n",
    "       train_regret_stp_df1_7[slice11],\n",
    "       train_regret_stp_df1_8[slice11],\n",
    "       train_regret_stp_df1_9[slice11],\n",
    "       train_regret_stp_df1_10[slice11],\n",
    "       train_regret_stp_df1_11[slice11],\n",
    "       train_regret_stp_df1_12[slice11],\n",
    "       train_regret_stp_df1_13[slice11],\n",
    "       train_regret_stp_df1_14[slice11],\n",
    "       train_regret_stp_df1_15[slice11],\n",
    "       train_regret_stp_df1_16[slice11],\n",
    "       train_regret_stp_df1_17[slice11],\n",
    "       train_regret_stp_df1_18[slice11],\n",
    "       train_regret_stp_df1_19[slice11],\n",
    "       train_regret_stp_df1_20[slice11]]\n",
    "\n",
    "gp11_results = pd.DataFrame(gp11).sort_values(by=[0], ascending=False)\n",
    "stp11_results = pd.DataFrame(stp11).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp11 = np.asarray(gp11_results[4:5][0])[0]\n",
    "median_gp11 = np.asarray(gp11_results[9:10][0])[0]\n",
    "upper_gp11 = np.asarray(gp11_results[14:15][0])[0]\n",
    "\n",
    "lower_stp11 = np.asarray(stp11_results[4:5][0])[0]\n",
    "median_stp11 = np.asarray(stp11_results[9:10][0])[0]\n",
    "upper_stp11 = np.asarray(stp11_results[14:15][0])[0]\n",
    "\n",
    "lower_gp11, median_gp11, upper_gp11, lower_stp11, median_stp11, upper_stp11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.015438570012605,\n",
       " -5.251874190510547,\n",
       " -5.350030893021542,\n",
       " -5.165857363117043,\n",
       " -5.308440173741311,\n",
       " -5.3704622566126305)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iteration21 :\n",
    "\n",
    "slice21 = 20\n",
    "\n",
    "gp21 = [train_regret_gp_1[slice21],\n",
    "       train_regret_gp_2[slice21],\n",
    "       train_regret_gp_3[slice21],\n",
    "       train_regret_gp_4[slice21],\n",
    "       train_regret_gp_5[slice21],\n",
    "       train_regret_gp_6[slice21],\n",
    "       train_regret_gp_7[slice21],\n",
    "       train_regret_gp_8[slice21],\n",
    "       train_regret_gp_9[slice21],\n",
    "       train_regret_gp_10[slice21],\n",
    "       train_regret_gp_11[slice21],\n",
    "       train_regret_gp_12[slice21],\n",
    "       train_regret_gp_13[slice21],\n",
    "       train_regret_gp_14[slice21],\n",
    "       train_regret_gp_15[slice21],\n",
    "       train_regret_gp_16[slice21],\n",
    "       train_regret_gp_17[slice21],\n",
    "       train_regret_gp_18[slice21],\n",
    "       train_regret_gp_19[slice21],\n",
    "       train_regret_gp_20[slice21]]\n",
    "\n",
    "stp21 = [train_regret_stp_df1_1[slice21],\n",
    "       train_regret_stp_df1_2[slice21],\n",
    "       train_regret_stp_df1_3[slice21],\n",
    "       train_regret_stp_df1_4[slice21],\n",
    "       train_regret_stp_df1_5[slice21],\n",
    "       train_regret_stp_df1_6[slice21],\n",
    "       train_regret_stp_df1_7[slice21],\n",
    "       train_regret_stp_df1_8[slice21],\n",
    "       train_regret_stp_df1_9[slice21],\n",
    "       train_regret_stp_df1_10[slice21],\n",
    "       train_regret_stp_df1_11[slice21],\n",
    "       train_regret_stp_df1_12[slice21],\n",
    "       train_regret_stp_df1_13[slice21],\n",
    "       train_regret_stp_df1_14[slice21],\n",
    "       train_regret_stp_df1_15[slice21],\n",
    "       train_regret_stp_df1_16[slice21],\n",
    "       train_regret_stp_df1_17[slice21],\n",
    "       train_regret_stp_df1_18[slice21],\n",
    "       train_regret_stp_df1_19[slice21],\n",
    "       train_regret_stp_df1_20[slice21]]\n",
    "\n",
    "gp21_results = pd.DataFrame(gp21).sort_values(by=[0], ascending=False)\n",
    "stp21_results = pd.DataFrame(stp21).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp21 = np.asarray(gp21_results[4:5][0])[0]\n",
    "median_gp21 = np.asarray(gp21_results[9:10][0])[0]\n",
    "upper_gp21 = np.asarray(gp21_results[14:15][0])[0]\n",
    "\n",
    "lower_stp21 = np.asarray(stp21_results[4:5][0])[0]\n",
    "median_stp21 = np.asarray(stp21_results[9:10][0])[0]\n",
    "upper_stp21 = np.asarray(stp21_results[14:15][0])[0]\n",
    "\n",
    "lower_gp21, median_gp21, upper_gp21, lower_stp21, median_stp21, upper_stp21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.163337423675783,\n",
       " -5.341974303811602,\n",
       " -5.379791996203266,\n",
       " -5.216752428162598,\n",
       " -5.308440173741311,\n",
       " -5.396599052729178)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iteration31 :\n",
    "\n",
    "slice31 = 30\n",
    "\n",
    "gp31 = [train_regret_gp_1[slice31],\n",
    "       train_regret_gp_2[slice31],\n",
    "       train_regret_gp_3[slice31],\n",
    "       train_regret_gp_4[slice31],\n",
    "       train_regret_gp_5[slice31],\n",
    "       train_regret_gp_6[slice31],\n",
    "       train_regret_gp_7[slice31],\n",
    "       train_regret_gp_8[slice31],\n",
    "       train_regret_gp_9[slice31],\n",
    "       train_regret_gp_10[slice31],\n",
    "       train_regret_gp_11[slice31],\n",
    "       train_regret_gp_12[slice31],\n",
    "       train_regret_gp_13[slice31],\n",
    "       train_regret_gp_14[slice31],\n",
    "       train_regret_gp_15[slice31],\n",
    "       train_regret_gp_16[slice31],\n",
    "       train_regret_gp_17[slice31],\n",
    "       train_regret_gp_18[slice31],\n",
    "       train_regret_gp_19[slice31],\n",
    "       train_regret_gp_20[slice31]]\n",
    "\n",
    "stp31 = [train_regret_stp_df1_1[slice31],\n",
    "       train_regret_stp_df1_2[slice31],\n",
    "       train_regret_stp_df1_3[slice31],\n",
    "       train_regret_stp_df1_4[slice31],\n",
    "       train_regret_stp_df1_5[slice31],\n",
    "       train_regret_stp_df1_6[slice31],\n",
    "       train_regret_stp_df1_7[slice31],\n",
    "       train_regret_stp_df1_8[slice31],\n",
    "       train_regret_stp_df1_9[slice31],\n",
    "       train_regret_stp_df1_10[slice31],\n",
    "       train_regret_stp_df1_11[slice31],\n",
    "       train_regret_stp_df1_12[slice31],\n",
    "       train_regret_stp_df1_13[slice31],\n",
    "       train_regret_stp_df1_14[slice31],\n",
    "       train_regret_stp_df1_15[slice31],\n",
    "       train_regret_stp_df1_16[slice31],\n",
    "       train_regret_stp_df1_17[slice31],\n",
    "       train_regret_stp_df1_18[slice31],\n",
    "       train_regret_stp_df1_19[slice31],\n",
    "       train_regret_stp_df1_20[slice31]]\n",
    "\n",
    "gp31_results = pd.DataFrame(gp31).sort_values(by=[0], ascending=False)\n",
    "stp31_results = pd.DataFrame(stp31).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp31 = np.asarray(gp31_results[4:5][0])[0]\n",
    "median_gp31 = np.asarray(gp31_results[9:10][0])[0]\n",
    "upper_gp31 = np.asarray(gp31_results[14:15][0])[0]\n",
    "\n",
    "lower_stp31 = np.asarray(stp31_results[4:5][0])[0]\n",
    "median_stp31 = np.asarray(stp31_results[9:10][0])[0]\n",
    "upper_stp31 = np.asarray(stp31_results[14:15][0])[0]\n",
    "\n",
    "lower_gp31, median_gp31, upper_gp31, lower_stp31, median_stp31, upper_stp31\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration2 :\n",
    "\n",
    "slice1 = 1\n",
    "\n",
    "gp2 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp2 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp2_results = pd.DataFrame(gp2).sort_values(by=[0], ascending=False)\n",
    "stp2_results = pd.DataFrame(stp2).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp2 = np.asarray(gp2_results[4:5][0])[0]\n",
    "median_gp2 = np.asarray(gp2_results[9:10][0])[0]\n",
    "upper_gp2 = np.asarray(gp2_results[14:15][0])[0]\n",
    "\n",
    "lower_stp2 = np.asarray(stp2_results[4:5][0])[0]\n",
    "median_stp2 = np.asarray(stp2_results[9:10][0])[0]\n",
    "upper_stp2 = np.asarray(stp2_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration12 :\n",
    "\n",
    "slice11 = 11\n",
    "\n",
    "gp12 = [train_regret_gp_1[slice11],\n",
    "       train_regret_gp_2[slice11],\n",
    "       train_regret_gp_3[slice11],\n",
    "       train_regret_gp_4[slice11],\n",
    "       train_regret_gp_5[slice11],\n",
    "       train_regret_gp_6[slice11],\n",
    "       train_regret_gp_7[slice11],\n",
    "       train_regret_gp_8[slice11],\n",
    "       train_regret_gp_9[slice11],\n",
    "       train_regret_gp_10[slice11],\n",
    "       train_regret_gp_11[slice11],\n",
    "       train_regret_gp_12[slice11],\n",
    "       train_regret_gp_13[slice11],\n",
    "       train_regret_gp_14[slice11],\n",
    "       train_regret_gp_15[slice11],\n",
    "       train_regret_gp_16[slice11],\n",
    "       train_regret_gp_17[slice11],\n",
    "       train_regret_gp_18[slice11],\n",
    "       train_regret_gp_19[slice11],\n",
    "       train_regret_gp_20[slice11]]\n",
    "\n",
    "stp12 = [train_regret_stp_df1_1[slice11],\n",
    "       train_regret_stp_df1_2[slice11],\n",
    "       train_regret_stp_df1_3[slice11],\n",
    "       train_regret_stp_df1_4[slice11],\n",
    "       train_regret_stp_df1_5[slice11],\n",
    "       train_regret_stp_df1_6[slice11],\n",
    "       train_regret_stp_df1_7[slice11],\n",
    "       train_regret_stp_df1_8[slice11],\n",
    "       train_regret_stp_df1_9[slice11],\n",
    "       train_regret_stp_df1_10[slice11],\n",
    "       train_regret_stp_df1_11[slice11],\n",
    "       train_regret_stp_df1_12[slice11],\n",
    "       train_regret_stp_df1_13[slice11],\n",
    "       train_regret_stp_df1_14[slice11],\n",
    "       train_regret_stp_df1_15[slice11],\n",
    "       train_regret_stp_df1_16[slice11],\n",
    "       train_regret_stp_df1_17[slice11],\n",
    "       train_regret_stp_df1_18[slice11],\n",
    "       train_regret_stp_df1_19[slice11],\n",
    "       train_regret_stp_df1_20[slice11]]\n",
    "\n",
    "gp12_results = pd.DataFrame(gp12).sort_values(by=[0], ascending=False)\n",
    "stp12_results = pd.DataFrame(stp12).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp12 = np.asarray(gp12_results[4:5][0])[0]\n",
    "median_gp12 = np.asarray(gp12_results[9:10][0])[0]\n",
    "upper_gp12 = np.asarray(gp12_results[14:15][0])[0]\n",
    "\n",
    "lower_stp12 = np.asarray(stp12_results[4:5][0])[0]\n",
    "median_stp12 = np.asarray(stp12_results[9:10][0])[0]\n",
    "upper_stp12 = np.asarray(stp12_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration22 :\n",
    "\n",
    "slice21 = 21\n",
    "\n",
    "gp22 = [train_regret_gp_1[slice21],\n",
    "       train_regret_gp_2[slice21],\n",
    "       train_regret_gp_3[slice21],\n",
    "       train_regret_gp_4[slice21],\n",
    "       train_regret_gp_5[slice21],\n",
    "       train_regret_gp_6[slice21],\n",
    "       train_regret_gp_7[slice21],\n",
    "       train_regret_gp_8[slice21],\n",
    "       train_regret_gp_9[slice21],\n",
    "       train_regret_gp_10[slice21],\n",
    "       train_regret_gp_11[slice21],\n",
    "       train_regret_gp_12[slice21],\n",
    "       train_regret_gp_13[slice21],\n",
    "       train_regret_gp_14[slice21],\n",
    "       train_regret_gp_15[slice21],\n",
    "       train_regret_gp_16[slice21],\n",
    "       train_regret_gp_17[slice21],\n",
    "       train_regret_gp_18[slice21],\n",
    "       train_regret_gp_19[slice21],\n",
    "       train_regret_gp_20[slice21]]\n",
    "\n",
    "stp22 = [train_regret_stp_df1_1[slice21],\n",
    "       train_regret_stp_df1_2[slice21],\n",
    "       train_regret_stp_df1_3[slice21],\n",
    "       train_regret_stp_df1_4[slice21],\n",
    "       train_regret_stp_df1_5[slice21],\n",
    "       train_regret_stp_df1_6[slice21],\n",
    "       train_regret_stp_df1_7[slice21],\n",
    "       train_regret_stp_df1_8[slice21],\n",
    "       train_regret_stp_df1_9[slice21],\n",
    "       train_regret_stp_df1_10[slice21],\n",
    "       train_regret_stp_df1_11[slice21],\n",
    "       train_regret_stp_df1_12[slice21],\n",
    "       train_regret_stp_df1_13[slice21],\n",
    "       train_regret_stp_df1_14[slice21],\n",
    "       train_regret_stp_df1_15[slice21],\n",
    "       train_regret_stp_df1_16[slice21],\n",
    "       train_regret_stp_df1_17[slice21],\n",
    "       train_regret_stp_df1_18[slice21],\n",
    "       train_regret_stp_df1_19[slice21],\n",
    "       train_regret_stp_df1_20[slice21]]\n",
    "\n",
    "gp22_results = pd.DataFrame(gp22).sort_values(by=[0], ascending=False)\n",
    "stp22_results = pd.DataFrame(stp22).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp22 = np.asarray(gp22_results[4:5][0])[0]\n",
    "median_gp22 = np.asarray(gp22_results[9:10][0])[0]\n",
    "upper_gp22 = np.asarray(gp22_results[14:15][0])[0]\n",
    "\n",
    "lower_stp22 = np.asarray(stp22_results[4:5][0])[0]\n",
    "median_stp22 = np.asarray(stp22_results[9:10][0])[0]\n",
    "upper_stp22 = np.asarray(stp22_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration3 :\n",
    "\n",
    "slice1 = 2\n",
    "\n",
    "gp3 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp3 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp3_results = pd.DataFrame(gp3).sort_values(by=[0], ascending=False)\n",
    "stp3_results = pd.DataFrame(stp3).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp3 = np.asarray(gp3_results[4:5][0])[0]\n",
    "median_gp3 = np.asarray(gp3_results[9:10][0])[0]\n",
    "upper_gp3 = np.asarray(gp3_results[14:15][0])[0]\n",
    "\n",
    "lower_stp3 = np.asarray(stp3_results[4:5][0])[0]\n",
    "median_stp3 = np.asarray(stp3_results[9:10][0])[0]\n",
    "upper_stp3 = np.asarray(stp3_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration13 :\n",
    "\n",
    "slice11 = 12\n",
    "\n",
    "gp13 = [train_regret_gp_1[slice11],\n",
    "       train_regret_gp_2[slice11],\n",
    "       train_regret_gp_3[slice11],\n",
    "       train_regret_gp_4[slice11],\n",
    "       train_regret_gp_5[slice11],\n",
    "       train_regret_gp_6[slice11],\n",
    "       train_regret_gp_7[slice11],\n",
    "       train_regret_gp_8[slice11],\n",
    "       train_regret_gp_9[slice11],\n",
    "       train_regret_gp_10[slice11],\n",
    "       train_regret_gp_11[slice11],\n",
    "       train_regret_gp_12[slice11],\n",
    "       train_regret_gp_13[slice11],\n",
    "       train_regret_gp_14[slice11],\n",
    "       train_regret_gp_15[slice11],\n",
    "       train_regret_gp_16[slice11],\n",
    "       train_regret_gp_17[slice11],\n",
    "       train_regret_gp_18[slice11],\n",
    "       train_regret_gp_19[slice11],\n",
    "       train_regret_gp_20[slice11]]\n",
    "\n",
    "stp13 = [train_regret_stp_df1_1[slice11],\n",
    "       train_regret_stp_df1_2[slice11],\n",
    "       train_regret_stp_df1_3[slice11],\n",
    "       train_regret_stp_df1_4[slice11],\n",
    "       train_regret_stp_df1_5[slice11],\n",
    "       train_regret_stp_df1_6[slice11],\n",
    "       train_regret_stp_df1_7[slice11],\n",
    "       train_regret_stp_df1_8[slice11],\n",
    "       train_regret_stp_df1_9[slice11],\n",
    "       train_regret_stp_df1_10[slice11],\n",
    "       train_regret_stp_df1_11[slice11],\n",
    "       train_regret_stp_df1_12[slice11],\n",
    "       train_regret_stp_df1_13[slice11],\n",
    "       train_regret_stp_df1_14[slice11],\n",
    "       train_regret_stp_df1_15[slice11],\n",
    "       train_regret_stp_df1_16[slice11],\n",
    "       train_regret_stp_df1_17[slice11],\n",
    "       train_regret_stp_df1_18[slice11],\n",
    "       train_regret_stp_df1_19[slice11],\n",
    "       train_regret_stp_df1_20[slice11]]\n",
    "\n",
    "gp13_results = pd.DataFrame(gp12).sort_values(by=[0], ascending=False)\n",
    "stp13_results = pd.DataFrame(stp12).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp13 = np.asarray(gp13_results[4:5][0])[0]\n",
    "median_gp13 = np.asarray(gp13_results[9:10][0])[0]\n",
    "upper_gp13 = np.asarray(gp13_results[14:15][0])[0]\n",
    "\n",
    "lower_stp13 = np.asarray(stp13_results[4:5][0])[0]\n",
    "median_stp13 = np.asarray(stp13_results[9:10][0])[0]\n",
    "upper_stp13 = np.asarray(stp13_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration23 :\n",
    "\n",
    "slice21 = 22\n",
    "\n",
    "gp23 = [train_regret_gp_1[slice21],\n",
    "       train_regret_gp_2[slice21],\n",
    "       train_regret_gp_3[slice21],\n",
    "       train_regret_gp_4[slice21],\n",
    "       train_regret_gp_5[slice21],\n",
    "       train_regret_gp_6[slice21],\n",
    "       train_regret_gp_7[slice21],\n",
    "       train_regret_gp_8[slice21],\n",
    "       train_regret_gp_9[slice21],\n",
    "       train_regret_gp_10[slice21],\n",
    "       train_regret_gp_11[slice21],\n",
    "       train_regret_gp_12[slice21],\n",
    "       train_regret_gp_13[slice21],\n",
    "       train_regret_gp_14[slice21],\n",
    "       train_regret_gp_15[slice21],\n",
    "       train_regret_gp_16[slice21],\n",
    "       train_regret_gp_17[slice21],\n",
    "       train_regret_gp_18[slice21],\n",
    "       train_regret_gp_19[slice21],\n",
    "       train_regret_gp_20[slice21]]\n",
    "\n",
    "stp23 = [train_regret_stp_df1_1[slice21],\n",
    "       train_regret_stp_df1_2[slice21],\n",
    "       train_regret_stp_df1_3[slice21],\n",
    "       train_regret_stp_df1_4[slice21],\n",
    "       train_regret_stp_df1_5[slice21],\n",
    "       train_regret_stp_df1_6[slice21],\n",
    "       train_regret_stp_df1_7[slice21],\n",
    "       train_regret_stp_df1_8[slice21],\n",
    "       train_regret_stp_df1_9[slice21],\n",
    "       train_regret_stp_df1_10[slice21],\n",
    "       train_regret_stp_df1_11[slice21],\n",
    "       train_regret_stp_df1_12[slice21],\n",
    "       train_regret_stp_df1_13[slice21],\n",
    "       train_regret_stp_df1_14[slice21],\n",
    "       train_regret_stp_df1_15[slice21],\n",
    "       train_regret_stp_df1_16[slice21],\n",
    "       train_regret_stp_df1_17[slice21],\n",
    "       train_regret_stp_df1_18[slice21],\n",
    "       train_regret_stp_df1_19[slice21],\n",
    "       train_regret_stp_df1_20[slice21]]\n",
    "\n",
    "gp23_results = pd.DataFrame(gp23).sort_values(by=[0], ascending=False)\n",
    "stp23_results = pd.DataFrame(stp23).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp23 = np.asarray(gp23_results[4:5][0])[0]\n",
    "median_gp23 = np.asarray(gp23_results[9:10][0])[0]\n",
    "upper_gp23 = np.asarray(gp23_results[14:15][0])[0]\n",
    "\n",
    "lower_stp23 = np.asarray(stp23_results[4:5][0])[0]\n",
    "median_stp23 = np.asarray(stp23_results[9:10][0])[0]\n",
    "upper_stp23 = np.asarray(stp23_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration4 :\n",
    "\n",
    "slice1 = 3\n",
    "\n",
    "gp4 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp4 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp4_results = pd.DataFrame(gp4).sort_values(by=[0], ascending=False)\n",
    "stp4_results = pd.DataFrame(stp4).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp4 = np.asarray(gp4_results[4:5][0])[0]\n",
    "median_gp4 = np.asarray(gp4_results[9:10][0])[0]\n",
    "upper_gp4 = np.asarray(gp4_results[14:15][0])[0]\n",
    "\n",
    "lower_stp4 = np.asarray(stp4_results[4:5][0])[0]\n",
    "median_stp4 = np.asarray(stp4_results[9:10][0])[0]\n",
    "upper_stp4 = np.asarray(stp4_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration14 :\n",
    "\n",
    "slice11 = 13\n",
    "\n",
    "gp14 = [train_regret_gp_1[slice11],\n",
    "       train_regret_gp_2[slice11],\n",
    "       train_regret_gp_3[slice11],\n",
    "       train_regret_gp_4[slice11],\n",
    "       train_regret_gp_5[slice11],\n",
    "       train_regret_gp_6[slice11],\n",
    "       train_regret_gp_7[slice11],\n",
    "       train_regret_gp_8[slice11],\n",
    "       train_regret_gp_9[slice11],\n",
    "       train_regret_gp_10[slice11],\n",
    "       train_regret_gp_11[slice11],\n",
    "       train_regret_gp_12[slice11],\n",
    "       train_regret_gp_13[slice11],\n",
    "       train_regret_gp_14[slice11],\n",
    "       train_regret_gp_15[slice11],\n",
    "       train_regret_gp_16[slice11],\n",
    "       train_regret_gp_17[slice11],\n",
    "       train_regret_gp_18[slice11],\n",
    "       train_regret_gp_19[slice11],\n",
    "       train_regret_gp_20[slice11]]\n",
    "\n",
    "stp14 = [train_regret_stp_df1_1[slice11],\n",
    "       train_regret_stp_df1_2[slice11],\n",
    "       train_regret_stp_df1_3[slice11],\n",
    "       train_regret_stp_df1_4[slice11],\n",
    "       train_regret_stp_df1_5[slice11],\n",
    "       train_regret_stp_df1_6[slice11],\n",
    "       train_regret_stp_df1_7[slice11],\n",
    "       train_regret_stp_df1_8[slice11],\n",
    "       train_regret_stp_df1_9[slice11],\n",
    "       train_regret_stp_df1_10[slice11],\n",
    "       train_regret_stp_df1_11[slice11],\n",
    "       train_regret_stp_df1_12[slice11],\n",
    "       train_regret_stp_df1_13[slice11],\n",
    "       train_regret_stp_df1_14[slice11],\n",
    "       train_regret_stp_df1_15[slice11],\n",
    "       train_regret_stp_df1_16[slice11],\n",
    "       train_regret_stp_df1_17[slice11],\n",
    "       train_regret_stp_df1_18[slice11],\n",
    "       train_regret_stp_df1_19[slice11],\n",
    "       train_regret_stp_df1_20[slice11]]\n",
    "\n",
    "gp14_results = pd.DataFrame(gp14).sort_values(by=[0], ascending=False)\n",
    "stp14_results = pd.DataFrame(stp14).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp14 = np.asarray(gp14_results[4:5][0])[0]\n",
    "median_gp14 = np.asarray(gp14_results[9:10][0])[0]\n",
    "upper_gp14 = np.asarray(gp14_results[14:15][0])[0]\n",
    "\n",
    "lower_stp14 = np.asarray(stp14_results[4:5][0])[0]\n",
    "median_stp14 = np.asarray(stp14_results[9:10][0])[0]\n",
    "upper_stp14 = np.asarray(stp14_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration24 :\n",
    "\n",
    "slice21 = 23\n",
    "\n",
    "gp24 = [train_regret_gp_1[slice21],\n",
    "       train_regret_gp_2[slice21],\n",
    "       train_regret_gp_3[slice21],\n",
    "       train_regret_gp_4[slice21],\n",
    "       train_regret_gp_5[slice21],\n",
    "       train_regret_gp_6[slice21],\n",
    "       train_regret_gp_7[slice21],\n",
    "       train_regret_gp_8[slice21],\n",
    "       train_regret_gp_9[slice21],\n",
    "       train_regret_gp_10[slice21],\n",
    "       train_regret_gp_11[slice21],\n",
    "       train_regret_gp_12[slice21],\n",
    "       train_regret_gp_13[slice21],\n",
    "       train_regret_gp_14[slice21],\n",
    "       train_regret_gp_15[slice21],\n",
    "       train_regret_gp_16[slice21],\n",
    "       train_regret_gp_17[slice21],\n",
    "       train_regret_gp_18[slice21],\n",
    "       train_regret_gp_19[slice21],\n",
    "       train_regret_gp_20[slice21]]\n",
    "\n",
    "stp24 = [train_regret_stp_df1_1[slice21],\n",
    "       train_regret_stp_df1_2[slice21],\n",
    "       train_regret_stp_df1_3[slice21],\n",
    "       train_regret_stp_df1_4[slice21],\n",
    "       train_regret_stp_df1_5[slice21],\n",
    "       train_regret_stp_df1_6[slice21],\n",
    "       train_regret_stp_df1_7[slice21],\n",
    "       train_regret_stp_df1_8[slice21],\n",
    "       train_regret_stp_df1_9[slice21],\n",
    "       train_regret_stp_df1_10[slice21],\n",
    "       train_regret_stp_df1_11[slice21],\n",
    "       train_regret_stp_df1_12[slice21],\n",
    "       train_regret_stp_df1_13[slice21],\n",
    "       train_regret_stp_df1_14[slice21],\n",
    "       train_regret_stp_df1_15[slice21],\n",
    "       train_regret_stp_df1_16[slice21],\n",
    "       train_regret_stp_df1_17[slice21],\n",
    "       train_regret_stp_df1_18[slice21],\n",
    "       train_regret_stp_df1_19[slice21],\n",
    "       train_regret_stp_df1_20[slice21]]\n",
    "\n",
    "gp24_results = pd.DataFrame(gp24).sort_values(by=[0], ascending=False)\n",
    "stp24_results = pd.DataFrame(stp24).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp24 = np.asarray(gp24_results[4:5][0])[0]\n",
    "median_gp24 = np.asarray(gp24_results[9:10][0])[0]\n",
    "upper_gp24 = np.asarray(gp24_results[14:15][0])[0]\n",
    "\n",
    "lower_stp24 = np.asarray(stp24_results[4:5][0])[0]\n",
    "median_stp24 = np.asarray(stp24_results[9:10][0])[0]\n",
    "upper_stp24 = np.asarray(stp24_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration5 :\n",
    "\n",
    "slice1 = 4\n",
    "\n",
    "gp5 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp5 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp5_results = pd.DataFrame(gp5).sort_values(by=[0], ascending=False)\n",
    "stp5_results = pd.DataFrame(stp5).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp5 = np.asarray(gp5_results[4:5][0])[0]\n",
    "median_gp5 = np.asarray(gp5_results[9:10][0])[0]\n",
    "upper_gp5 = np.asarray(gp5_results[14:15][0])[0]\n",
    "\n",
    "lower_stp5 = np.asarray(stp5_results[4:5][0])[0]\n",
    "median_stp5 = np.asarray(stp5_results[9:10][0])[0]\n",
    "upper_stp5 = np.asarray(stp5_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration15 :\n",
    "\n",
    "slice11 = 14\n",
    "\n",
    "gp15 = [train_regret_gp_1[slice11],\n",
    "       train_regret_gp_2[slice11],\n",
    "       train_regret_gp_3[slice11],\n",
    "       train_regret_gp_4[slice11],\n",
    "       train_regret_gp_5[slice11],\n",
    "       train_regret_gp_6[slice11],\n",
    "       train_regret_gp_7[slice11],\n",
    "       train_regret_gp_8[slice11],\n",
    "       train_regret_gp_9[slice11],\n",
    "       train_regret_gp_10[slice11],\n",
    "       train_regret_gp_11[slice11],\n",
    "       train_regret_gp_12[slice11],\n",
    "       train_regret_gp_13[slice11],\n",
    "       train_regret_gp_14[slice11],\n",
    "       train_regret_gp_15[slice11],\n",
    "       train_regret_gp_16[slice11],\n",
    "       train_regret_gp_17[slice11],\n",
    "       train_regret_gp_18[slice11],\n",
    "       train_regret_gp_19[slice11],\n",
    "       train_regret_gp_20[slice11]]\n",
    "\n",
    "stp15 = [train_regret_stp_df1_1[slice11],\n",
    "       train_regret_stp_df1_2[slice11],\n",
    "       train_regret_stp_df1_3[slice11],\n",
    "       train_regret_stp_df1_4[slice11],\n",
    "       train_regret_stp_df1_5[slice11],\n",
    "       train_regret_stp_df1_6[slice11],\n",
    "       train_regret_stp_df1_7[slice11],\n",
    "       train_regret_stp_df1_8[slice11],\n",
    "       train_regret_stp_df1_9[slice11],\n",
    "       train_regret_stp_df1_10[slice11],\n",
    "       train_regret_stp_df1_11[slice11],\n",
    "       train_regret_stp_df1_12[slice11],\n",
    "       train_regret_stp_df1_13[slice11],\n",
    "       train_regret_stp_df1_14[slice11],\n",
    "       train_regret_stp_df1_15[slice11],\n",
    "       train_regret_stp_df1_16[slice11],\n",
    "       train_regret_stp_df1_17[slice11],\n",
    "       train_regret_stp_df1_18[slice11],\n",
    "       train_regret_stp_df1_19[slice11],\n",
    "       train_regret_stp_df1_20[slice11]]\n",
    "\n",
    "gp15_results = pd.DataFrame(gp15).sort_values(by=[0], ascending=False)\n",
    "stp15_results = pd.DataFrame(stp15).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp15 = np.asarray(gp15_results[4:5][0])[0]\n",
    "median_gp15 = np.asarray(gp15_results[9:10][0])[0]\n",
    "upper_gp15 = np.asarray(gp15_results[14:15][0])[0]\n",
    "\n",
    "lower_stp15 = np.asarray(stp15_results[4:5][0])[0]\n",
    "median_stp15 = np.asarray(stp15_results[9:10][0])[0]\n",
    "upper_stp15 = np.asarray(stp15_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration25 :\n",
    "\n",
    "slice21 = 24\n",
    "\n",
    "gp25 = [train_regret_gp_1[slice21],\n",
    "       train_regret_gp_2[slice21],\n",
    "       train_regret_gp_3[slice21],\n",
    "       train_regret_gp_4[slice21],\n",
    "       train_regret_gp_5[slice21],\n",
    "       train_regret_gp_6[slice21],\n",
    "       train_regret_gp_7[slice21],\n",
    "       train_regret_gp_8[slice21],\n",
    "       train_regret_gp_9[slice21],\n",
    "       train_regret_gp_10[slice21],\n",
    "       train_regret_gp_11[slice21],\n",
    "       train_regret_gp_12[slice21],\n",
    "       train_regret_gp_13[slice21],\n",
    "       train_regret_gp_14[slice21],\n",
    "       train_regret_gp_15[slice21],\n",
    "       train_regret_gp_16[slice21],\n",
    "       train_regret_gp_17[slice21],\n",
    "       train_regret_gp_18[slice21],\n",
    "       train_regret_gp_19[slice21],\n",
    "       train_regret_gp_20[slice21]]\n",
    "\n",
    "stp25 = [train_regret_stp_df1_1[slice21],\n",
    "       train_regret_stp_df1_2[slice21],\n",
    "       train_regret_stp_df1_3[slice21],\n",
    "       train_regret_stp_df1_4[slice21],\n",
    "       train_regret_stp_df1_5[slice21],\n",
    "       train_regret_stp_df1_6[slice21],\n",
    "       train_regret_stp_df1_7[slice21],\n",
    "       train_regret_stp_df1_8[slice21],\n",
    "       train_regret_stp_df1_9[slice21],\n",
    "       train_regret_stp_df1_10[slice21],\n",
    "       train_regret_stp_df1_11[slice21],\n",
    "       train_regret_stp_df1_12[slice21],\n",
    "       train_regret_stp_df1_13[slice21],\n",
    "       train_regret_stp_df1_14[slice21],\n",
    "       train_regret_stp_df1_15[slice21],\n",
    "       train_regret_stp_df1_16[slice21],\n",
    "       train_regret_stp_df1_17[slice21],\n",
    "       train_regret_stp_df1_18[slice21],\n",
    "       train_regret_stp_df1_19[slice21],\n",
    "       train_regret_stp_df1_20[slice21]]\n",
    "\n",
    "gp25_results = pd.DataFrame(gp25).sort_values(by=[0], ascending=False)\n",
    "stp25_results = pd.DataFrame(stp25).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp25 = np.asarray(gp25_results[4:5][0])[0]\n",
    "median_gp25 = np.asarray(gp25_results[9:10][0])[0]\n",
    "upper_gp25 = np.asarray(gp25_results[14:15][0])[0]\n",
    "\n",
    "lower_stp25 = np.asarray(stp25_results[4:5][0])[0]\n",
    "median_stp25 = np.asarray(stp25_results[9:10][0])[0]\n",
    "upper_stp25= np.asarray(stp25_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration6 :\n",
    "\n",
    "slice1 = 5\n",
    "\n",
    "gp6 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp6 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp6_results = pd.DataFrame(gp6).sort_values(by=[0], ascending=False)\n",
    "stp6_results = pd.DataFrame(stp6).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp6 = np.asarray(gp6_results[4:5][0])[0]\n",
    "median_gp6 = np.asarray(gp6_results[9:10][0])[0]\n",
    "upper_gp6 = np.asarray(gp6_results[14:15][0])[0]\n",
    "\n",
    "lower_stp6 = np.asarray(stp6_results[4:5][0])[0]\n",
    "median_stp6 = np.asarray(stp6_results[9:10][0])[0]\n",
    "upper_stp6 = np.asarray(stp6_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration16 :\n",
    "\n",
    "slice11 = 15\n",
    "\n",
    "gp16 = [train_regret_gp_1[slice11],\n",
    "       train_regret_gp_2[slice11],\n",
    "       train_regret_gp_3[slice11],\n",
    "       train_regret_gp_4[slice11],\n",
    "       train_regret_gp_5[slice11],\n",
    "       train_regret_gp_6[slice11],\n",
    "       train_regret_gp_7[slice11],\n",
    "       train_regret_gp_8[slice11],\n",
    "       train_regret_gp_9[slice11],\n",
    "       train_regret_gp_10[slice11],\n",
    "       train_regret_gp_11[slice11],\n",
    "       train_regret_gp_12[slice11],\n",
    "       train_regret_gp_13[slice11],\n",
    "       train_regret_gp_14[slice11],\n",
    "       train_regret_gp_15[slice11],\n",
    "       train_regret_gp_16[slice11],\n",
    "       train_regret_gp_17[slice11],\n",
    "       train_regret_gp_18[slice11],\n",
    "       train_regret_gp_19[slice11],\n",
    "       train_regret_gp_20[slice11]]\n",
    "\n",
    "stp16 = [train_regret_stp_df1_1[slice11],\n",
    "       train_regret_stp_df1_2[slice11],\n",
    "       train_regret_stp_df1_3[slice11],\n",
    "       train_regret_stp_df1_4[slice11],\n",
    "       train_regret_stp_df1_5[slice11],\n",
    "       train_regret_stp_df1_6[slice11],\n",
    "       train_regret_stp_df1_7[slice11],\n",
    "       train_regret_stp_df1_8[slice11],\n",
    "       train_regret_stp_df1_9[slice11],\n",
    "       train_regret_stp_df1_10[slice11],\n",
    "       train_regret_stp_df1_11[slice11],\n",
    "       train_regret_stp_df1_12[slice11],\n",
    "       train_regret_stp_df1_13[slice11],\n",
    "       train_regret_stp_df1_14[slice11],\n",
    "       train_regret_stp_df1_15[slice11],\n",
    "       train_regret_stp_df1_16[slice11],\n",
    "       train_regret_stp_df1_17[slice11],\n",
    "       train_regret_stp_df1_18[slice11],\n",
    "       train_regret_stp_df1_19[slice11],\n",
    "       train_regret_stp_df1_20[slice11]]\n",
    "\n",
    "gp16_results = pd.DataFrame(gp16).sort_values(by=[0], ascending=False)\n",
    "stp16_results = pd.DataFrame(stp16).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp16 = np.asarray(gp16_results[4:5][0])[0]\n",
    "median_gp16 = np.asarray(gp16_results[9:10][0])[0]\n",
    "upper_gp16 = np.asarray(gp16_results[14:15][0])[0]\n",
    "\n",
    "lower_stp16 = np.asarray(stp16_results[4:5][0])[0]\n",
    "median_stp16 = np.asarray(stp16_results[9:10][0])[0]\n",
    "upper_stp16 = np.asarray(stp16_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration26 :\n",
    "\n",
    "slice21 = 25\n",
    "\n",
    "gp26 = [train_regret_gp_1[slice21],\n",
    "       train_regret_gp_2[slice21],\n",
    "       train_regret_gp_3[slice21],\n",
    "       train_regret_gp_4[slice21],\n",
    "       train_regret_gp_5[slice21],\n",
    "       train_regret_gp_6[slice21],\n",
    "       train_regret_gp_7[slice21],\n",
    "       train_regret_gp_8[slice21],\n",
    "       train_regret_gp_9[slice21],\n",
    "       train_regret_gp_10[slice21],\n",
    "       train_regret_gp_11[slice21],\n",
    "       train_regret_gp_12[slice21],\n",
    "       train_regret_gp_13[slice21],\n",
    "       train_regret_gp_14[slice21],\n",
    "       train_regret_gp_15[slice21],\n",
    "       train_regret_gp_16[slice21],\n",
    "       train_regret_gp_17[slice21],\n",
    "       train_regret_gp_18[slice21],\n",
    "       train_regret_gp_19[slice21],\n",
    "       train_regret_gp_20[slice21]]\n",
    "\n",
    "stp26 = [train_regret_stp_df1_1[slice21],\n",
    "       train_regret_stp_df1_2[slice21],\n",
    "       train_regret_stp_df1_3[slice21],\n",
    "       train_regret_stp_df1_4[slice21],\n",
    "       train_regret_stp_df1_5[slice21],\n",
    "       train_regret_stp_df1_6[slice21],\n",
    "       train_regret_stp_df1_7[slice21],\n",
    "       train_regret_stp_df1_8[slice21],\n",
    "       train_regret_stp_df1_9[slice21],\n",
    "       train_regret_stp_df1_10[slice21],\n",
    "       train_regret_stp_df1_11[slice21],\n",
    "       train_regret_stp_df1_12[slice21],\n",
    "       train_regret_stp_df1_13[slice21],\n",
    "       train_regret_stp_df1_14[slice21],\n",
    "       train_regret_stp_df1_15[slice21],\n",
    "       train_regret_stp_df1_16[slice21],\n",
    "       train_regret_stp_df1_17[slice21],\n",
    "       train_regret_stp_df1_18[slice21],\n",
    "       train_regret_stp_df1_19[slice21],\n",
    "       train_regret_stp_df1_20[slice21]]\n",
    "\n",
    "gp26_results = pd.DataFrame(gp26).sort_values(by=[0], ascending=False)\n",
    "stp26_results = pd.DataFrame(stp26).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp26 = np.asarray(gp26_results[4:5][0])[0]\n",
    "median_gp26 = np.asarray(gp26_results[9:10][0])[0]\n",
    "upper_gp26 = np.asarray(gp26_results[14:15][0])[0]\n",
    "\n",
    "lower_stp26 = np.asarray(stp26_results[4:5][0])[0]\n",
    "median_stp26 = np.asarray(stp26_results[9:10][0])[0]\n",
    "upper_stp26 = np.asarray(stp26_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration7 :\n",
    "\n",
    "slice1 = 6\n",
    "\n",
    "gp7 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp7 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp7_results = pd.DataFrame(gp7).sort_values(by=[0], ascending=False)\n",
    "stp7_results = pd.DataFrame(stp7).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp7 = np.asarray(gp7_results[4:5][0])[0]\n",
    "median_gp7 = np.asarray(gp7_results[9:10][0])[0]\n",
    "upper_gp7 = np.asarray(gp7_results[14:15][0])[0]\n",
    "\n",
    "lower_stp7 = np.asarray(stp7_results[4:5][0])[0]\n",
    "median_stp7 = np.asarray(stp7_results[9:10][0])[0]\n",
    "upper_stp7 = np.asarray(stp7_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration17 :\n",
    "\n",
    "slice11 = 16\n",
    "\n",
    "gp17 = [train_regret_gp_1[slice11],\n",
    "       train_regret_gp_2[slice11],\n",
    "       train_regret_gp_3[slice11],\n",
    "       train_regret_gp_4[slice11],\n",
    "       train_regret_gp_5[slice11],\n",
    "       train_regret_gp_6[slice11],\n",
    "       train_regret_gp_7[slice11],\n",
    "       train_regret_gp_8[slice11],\n",
    "       train_regret_gp_9[slice11],\n",
    "       train_regret_gp_10[slice11],\n",
    "       train_regret_gp_11[slice11],\n",
    "       train_regret_gp_12[slice11],\n",
    "       train_regret_gp_13[slice11],\n",
    "       train_regret_gp_14[slice11],\n",
    "       train_regret_gp_15[slice11],\n",
    "       train_regret_gp_16[slice11],\n",
    "       train_regret_gp_17[slice11],\n",
    "       train_regret_gp_18[slice11],\n",
    "       train_regret_gp_19[slice11],\n",
    "       train_regret_gp_20[slice11]]\n",
    "\n",
    "stp17 = [train_regret_stp_df1_1[slice11],\n",
    "       train_regret_stp_df1_2[slice11],\n",
    "       train_regret_stp_df1_3[slice11],\n",
    "       train_regret_stp_df1_4[slice11],\n",
    "       train_regret_stp_df1_5[slice11],\n",
    "       train_regret_stp_df1_6[slice11],\n",
    "       train_regret_stp_df1_7[slice11],\n",
    "       train_regret_stp_df1_8[slice11],\n",
    "       train_regret_stp_df1_9[slice11],\n",
    "       train_regret_stp_df1_10[slice11],\n",
    "       train_regret_stp_df1_11[slice11],\n",
    "       train_regret_stp_df1_12[slice11],\n",
    "       train_regret_stp_df1_13[slice11],\n",
    "       train_regret_stp_df1_14[slice11],\n",
    "       train_regret_stp_df1_15[slice11],\n",
    "       train_regret_stp_df1_16[slice11],\n",
    "       train_regret_stp_df1_17[slice11],\n",
    "       train_regret_stp_df1_18[slice11],\n",
    "       train_regret_stp_df1_19[slice11],\n",
    "       train_regret_stp_df1_20[slice11]]\n",
    "\n",
    "gp17_results = pd.DataFrame(gp17).sort_values(by=[0], ascending=False)\n",
    "stp17_results = pd.DataFrame(stp17).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp17 = np.asarray(gp17_results[4:5][0])[0]\n",
    "median_gp17 = np.asarray(gp17_results[9:10][0])[0]\n",
    "upper_gp17 = np.asarray(gp17_results[14:15][0])[0]\n",
    "\n",
    "lower_stp17 = np.asarray(stp17_results[4:5][0])[0]\n",
    "median_stp17 = np.asarray(stp17_results[9:10][0])[0]\n",
    "upper_stp17 = np.asarray(stp17_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration27 :\n",
    "\n",
    "slice21 = 26\n",
    "\n",
    "gp27 = [train_regret_gp_1[slice21],\n",
    "       train_regret_gp_2[slice21],\n",
    "       train_regret_gp_3[slice21],\n",
    "       train_regret_gp_4[slice21],\n",
    "       train_regret_gp_5[slice21],\n",
    "       train_regret_gp_6[slice21],\n",
    "       train_regret_gp_7[slice21],\n",
    "       train_regret_gp_8[slice21],\n",
    "       train_regret_gp_9[slice21],\n",
    "       train_regret_gp_10[slice21],\n",
    "       train_regret_gp_11[slice21],\n",
    "       train_regret_gp_12[slice21],\n",
    "       train_regret_gp_13[slice21],\n",
    "       train_regret_gp_14[slice21],\n",
    "       train_regret_gp_15[slice21],\n",
    "       train_regret_gp_16[slice21],\n",
    "       train_regret_gp_17[slice21],\n",
    "       train_regret_gp_18[slice21],\n",
    "       train_regret_gp_19[slice21],\n",
    "       train_regret_gp_20[slice21]]\n",
    "\n",
    "stp27 = [train_regret_stp_df1_1[slice21],\n",
    "       train_regret_stp_df1_2[slice21],\n",
    "       train_regret_stp_df1_3[slice21],\n",
    "       train_regret_stp_df1_4[slice21],\n",
    "       train_regret_stp_df1_5[slice21],\n",
    "       train_regret_stp_df1_6[slice21],\n",
    "       train_regret_stp_df1_7[slice21],\n",
    "       train_regret_stp_df1_8[slice21],\n",
    "       train_regret_stp_df1_9[slice21],\n",
    "       train_regret_stp_df1_10[slice21],\n",
    "       train_regret_stp_df1_11[slice21],\n",
    "       train_regret_stp_df1_12[slice21],\n",
    "       train_regret_stp_df1_13[slice21],\n",
    "       train_regret_stp_df1_14[slice21],\n",
    "       train_regret_stp_df1_15[slice21],\n",
    "       train_regret_stp_df1_16[slice21],\n",
    "       train_regret_stp_df1_17[slice21],\n",
    "       train_regret_stp_df1_18[slice21],\n",
    "       train_regret_stp_df1_19[slice21],\n",
    "       train_regret_stp_df1_20[slice21]]\n",
    "\n",
    "gp27_results = pd.DataFrame(gp27).sort_values(by=[0], ascending=False)\n",
    "stp27_results = pd.DataFrame(stp27).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp27 = np.asarray(gp27_results[4:5][0])[0]\n",
    "median_gp27 = np.asarray(gp27_results[9:10][0])[0]\n",
    "upper_gp27 = np.asarray(gp27_results[14:15][0])[0]\n",
    "\n",
    "lower_stp27 = np.asarray(stp27_results[4:5][0])[0]\n",
    "median_stp27 = np.asarray(stp27_results[9:10][0])[0]\n",
    "upper_stp27 = np.asarray(stp27_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration8 :\n",
    "\n",
    "slice1 = 7\n",
    "\n",
    "gp8 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp8 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp8_results = pd.DataFrame(gp8).sort_values(by=[0], ascending=False)\n",
    "stp8_results = pd.DataFrame(stp8).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp8 = np.asarray(gp8_results[4:5][0])[0]\n",
    "median_gp8 = np.asarray(gp8_results[9:10][0])[0]\n",
    "upper_gp8 = np.asarray(gp8_results[14:15][0])[0]\n",
    "\n",
    "lower_stp8 = np.asarray(stp8_results[4:5][0])[0]\n",
    "median_stp8 = np.asarray(stp8_results[9:10][0])[0]\n",
    "upper_stp8 = np.asarray(stp8_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration18 :\n",
    "\n",
    "slice11 = 17\n",
    "\n",
    "gp18 = [train_regret_gp_1[slice11],\n",
    "       train_regret_gp_2[slice11],\n",
    "       train_regret_gp_3[slice11],\n",
    "       train_regret_gp_4[slice11],\n",
    "       train_regret_gp_5[slice11],\n",
    "       train_regret_gp_6[slice11],\n",
    "       train_regret_gp_7[slice11],\n",
    "       train_regret_gp_8[slice11],\n",
    "       train_regret_gp_9[slice11],\n",
    "       train_regret_gp_10[slice11],\n",
    "       train_regret_gp_11[slice11],\n",
    "       train_regret_gp_12[slice11],\n",
    "       train_regret_gp_13[slice11],\n",
    "       train_regret_gp_14[slice11],\n",
    "       train_regret_gp_15[slice11],\n",
    "       train_regret_gp_16[slice11],\n",
    "       train_regret_gp_17[slice11],\n",
    "       train_regret_gp_18[slice11],\n",
    "       train_regret_gp_19[slice11],\n",
    "       train_regret_gp_20[slice11]]\n",
    "\n",
    "stp18 = [train_regret_stp_df1_1[slice11],\n",
    "       train_regret_stp_df1_2[slice11],\n",
    "       train_regret_stp_df1_3[slice11],\n",
    "       train_regret_stp_df1_4[slice11],\n",
    "       train_regret_stp_df1_5[slice11],\n",
    "       train_regret_stp_df1_6[slice11],\n",
    "       train_regret_stp_df1_7[slice11],\n",
    "       train_regret_stp_df1_8[slice11],\n",
    "       train_regret_stp_df1_9[slice11],\n",
    "       train_regret_stp_df1_10[slice11],\n",
    "       train_regret_stp_df1_11[slice11],\n",
    "       train_regret_stp_df1_12[slice11],\n",
    "       train_regret_stp_df1_13[slice11],\n",
    "       train_regret_stp_df1_14[slice11],\n",
    "       train_regret_stp_df1_15[slice11],\n",
    "       train_regret_stp_df1_16[slice11],\n",
    "       train_regret_stp_df1_17[slice11],\n",
    "       train_regret_stp_df1_18[slice11],\n",
    "       train_regret_stp_df1_19[slice11],\n",
    "       train_regret_stp_df1_20[slice11]]\n",
    "\n",
    "gp18_results = pd.DataFrame(gp18).sort_values(by=[0], ascending=False)\n",
    "stp18_results = pd.DataFrame(stp18).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp18 = np.asarray(gp18_results[4:5][0])[0]\n",
    "median_gp18 = np.asarray(gp18_results[9:10][0])[0]\n",
    "upper_gp18 = np.asarray(gp18_results[14:15][0])[0]\n",
    "\n",
    "lower_stp18 = np.asarray(stp18_results[4:5][0])[0]\n",
    "median_stp18 = np.asarray(stp18_results[9:10][0])[0]\n",
    "upper_stp18 = np.asarray(stp18_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration28 :\n",
    "\n",
    "slice21 = 27\n",
    "\n",
    "gp28 = [train_regret_gp_1[slice21],\n",
    "       train_regret_gp_2[slice21],\n",
    "       train_regret_gp_3[slice21],\n",
    "       train_regret_gp_4[slice21],\n",
    "       train_regret_gp_5[slice21],\n",
    "       train_regret_gp_6[slice21],\n",
    "       train_regret_gp_7[slice21],\n",
    "       train_regret_gp_8[slice21],\n",
    "       train_regret_gp_9[slice21],\n",
    "       train_regret_gp_10[slice21],\n",
    "       train_regret_gp_11[slice21],\n",
    "       train_regret_gp_12[slice21],\n",
    "       train_regret_gp_13[slice21],\n",
    "       train_regret_gp_14[slice21],\n",
    "       train_regret_gp_15[slice21],\n",
    "       train_regret_gp_16[slice21],\n",
    "       train_regret_gp_17[slice21],\n",
    "       train_regret_gp_18[slice21],\n",
    "       train_regret_gp_19[slice21],\n",
    "       train_regret_gp_20[slice21]]\n",
    "\n",
    "stp28 = [train_regret_stp_df1_1[slice21],\n",
    "       train_regret_stp_df1_2[slice21],\n",
    "       train_regret_stp_df1_3[slice21],\n",
    "       train_regret_stp_df1_4[slice21],\n",
    "       train_regret_stp_df1_5[slice21],\n",
    "       train_regret_stp_df1_6[slice21],\n",
    "       train_regret_stp_df1_7[slice21],\n",
    "       train_regret_stp_df1_8[slice21],\n",
    "       train_regret_stp_df1_9[slice21],\n",
    "       train_regret_stp_df1_10[slice21],\n",
    "       train_regret_stp_df1_11[slice21],\n",
    "       train_regret_stp_df1_12[slice21],\n",
    "       train_regret_stp_df1_13[slice21],\n",
    "       train_regret_stp_df1_14[slice21],\n",
    "       train_regret_stp_df1_15[slice21],\n",
    "       train_regret_stp_df1_16[slice21],\n",
    "       train_regret_stp_df1_17[slice21],\n",
    "       train_regret_stp_df1_18[slice21],\n",
    "       train_regret_stp_df1_19[slice21],\n",
    "       train_regret_stp_df1_20[slice21]]\n",
    "\n",
    "gp28_results = pd.DataFrame(gp28).sort_values(by=[0], ascending=False)\n",
    "stp28_results = pd.DataFrame(stp28).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp28 = np.asarray(gp28_results[4:5][0])[0]\n",
    "median_gp28 = np.asarray(gp28_results[9:10][0])[0]\n",
    "upper_gp28 = np.asarray(gp28_results[14:15][0])[0]\n",
    "\n",
    "lower_stp28 = np.asarray(stp28_results[4:5][0])[0]\n",
    "median_stp28 = np.asarray(stp28_results[9:10][0])[0]\n",
    "upper_stp28 = np.asarray(stp28_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration9 :\n",
    "\n",
    "slice1 = 8\n",
    "\n",
    "gp9 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp9 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp9_results = pd.DataFrame(gp9).sort_values(by=[0], ascending=False)\n",
    "stp9_results = pd.DataFrame(stp9).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp9 = np.asarray(gp9_results[4:5][0])[0]\n",
    "median_gp9 = np.asarray(gp9_results[9:10][0])[0]\n",
    "upper_gp9 = np.asarray(gp9_results[14:15][0])[0]\n",
    "\n",
    "lower_stp9 = np.asarray(stp9_results[4:5][0])[0]\n",
    "median_stp9 = np.asarray(stp9_results[9:10][0])[0]\n",
    "upper_stp9 = np.asarray(stp9_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration19 :\n",
    "\n",
    "slice11 = 18\n",
    "\n",
    "gp19 = [train_regret_gp_1[slice11],\n",
    "       train_regret_gp_2[slice11],\n",
    "       train_regret_gp_3[slice11],\n",
    "       train_regret_gp_4[slice11],\n",
    "       train_regret_gp_5[slice11],\n",
    "       train_regret_gp_6[slice11],\n",
    "       train_regret_gp_7[slice11],\n",
    "       train_regret_gp_8[slice11],\n",
    "       train_regret_gp_9[slice11],\n",
    "       train_regret_gp_10[slice11],\n",
    "       train_regret_gp_11[slice11],\n",
    "       train_regret_gp_12[slice11],\n",
    "       train_regret_gp_13[slice11],\n",
    "       train_regret_gp_14[slice11],\n",
    "       train_regret_gp_15[slice11],\n",
    "       train_regret_gp_16[slice11],\n",
    "       train_regret_gp_17[slice11],\n",
    "       train_regret_gp_18[slice11],\n",
    "       train_regret_gp_19[slice11],\n",
    "       train_regret_gp_20[slice11]]\n",
    "\n",
    "stp19 = [train_regret_stp_df1_1[slice11],\n",
    "       train_regret_stp_df1_2[slice11],\n",
    "       train_regret_stp_df1_3[slice11],\n",
    "       train_regret_stp_df1_4[slice11],\n",
    "       train_regret_stp_df1_5[slice11],\n",
    "       train_regret_stp_df1_6[slice11],\n",
    "       train_regret_stp_df1_7[slice11],\n",
    "       train_regret_stp_df1_8[slice11],\n",
    "       train_regret_stp_df1_9[slice11],\n",
    "       train_regret_stp_df1_10[slice11],\n",
    "       train_regret_stp_df1_11[slice11],\n",
    "       train_regret_stp_df1_12[slice11],\n",
    "       train_regret_stp_df1_13[slice11],\n",
    "       train_regret_stp_df1_14[slice11],\n",
    "       train_regret_stp_df1_15[slice11],\n",
    "       train_regret_stp_df1_16[slice11],\n",
    "       train_regret_stp_df1_17[slice11],\n",
    "       train_regret_stp_df1_18[slice11],\n",
    "       train_regret_stp_df1_19[slice11],\n",
    "       train_regret_stp_df1_20[slice11]]\n",
    "\n",
    "gp19_results = pd.DataFrame(gp19).sort_values(by=[0], ascending=False)\n",
    "stp19_results = pd.DataFrame(stp19).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp19 = np.asarray(gp19_results[4:5][0])[0]\n",
    "median_gp19 = np.asarray(gp19_results[9:10][0])[0]\n",
    "upper_gp19 = np.asarray(gp19_results[14:15][0])[0]\n",
    "\n",
    "lower_stp19 = np.asarray(stp19_results[4:5][0])[0]\n",
    "median_stp19 = np.asarray(stp19_results[9:10][0])[0]\n",
    "upper_stp19 = np.asarray(stp19_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration29 :\n",
    "\n",
    "slice21 = 28\n",
    "\n",
    "gp29 = [train_regret_gp_1[slice21],\n",
    "       train_regret_gp_2[slice21],\n",
    "       train_regret_gp_3[slice21],\n",
    "       train_regret_gp_4[slice21],\n",
    "       train_regret_gp_5[slice21],\n",
    "       train_regret_gp_6[slice21],\n",
    "       train_regret_gp_7[slice21],\n",
    "       train_regret_gp_8[slice21],\n",
    "       train_regret_gp_9[slice21],\n",
    "       train_regret_gp_10[slice21],\n",
    "       train_regret_gp_11[slice21],\n",
    "       train_regret_gp_12[slice21],\n",
    "       train_regret_gp_13[slice21],\n",
    "       train_regret_gp_14[slice21],\n",
    "       train_regret_gp_15[slice21],\n",
    "       train_regret_gp_16[slice21],\n",
    "       train_regret_gp_17[slice21],\n",
    "       train_regret_gp_18[slice21],\n",
    "       train_regret_gp_19[slice21],\n",
    "       train_regret_gp_20[slice21]]\n",
    "\n",
    "stp29 = [train_regret_stp_df1_1[slice21],\n",
    "       train_regret_stp_df1_2[slice21],\n",
    "       train_regret_stp_df1_3[slice21],\n",
    "       train_regret_stp_df1_4[slice21],\n",
    "       train_regret_stp_df1_5[slice21],\n",
    "       train_regret_stp_df1_6[slice21],\n",
    "       train_regret_stp_df1_7[slice21],\n",
    "       train_regret_stp_df1_8[slice21],\n",
    "       train_regret_stp_df1_9[slice21],\n",
    "       train_regret_stp_df1_10[slice21],\n",
    "       train_regret_stp_df1_11[slice21],\n",
    "       train_regret_stp_df1_12[slice21],\n",
    "       train_regret_stp_df1_13[slice21],\n",
    "       train_regret_stp_df1_14[slice21],\n",
    "       train_regret_stp_df1_15[slice21],\n",
    "       train_regret_stp_df1_16[slice21],\n",
    "       train_regret_stp_df1_17[slice21],\n",
    "       train_regret_stp_df1_18[slice21],\n",
    "       train_regret_stp_df1_19[slice21],\n",
    "       train_regret_stp_df1_20[slice21]]\n",
    "\n",
    "gp29_results = pd.DataFrame(gp29).sort_values(by=[0], ascending=False)\n",
    "stp29_results = pd.DataFrame(stp29).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp29 = np.asarray(gp29_results[4:5][0])[0]\n",
    "median_gp29 = np.asarray(gp29_results[9:10][0])[0]\n",
    "upper_gp29 = np.asarray(gp29_results[14:15][0])[0]\n",
    "\n",
    "lower_stp29 = np.asarray(stp29_results[4:5][0])[0]\n",
    "median_stp29 = np.asarray(stp29_results[9:10][0])[0]\n",
    "upper_stp29 = np.asarray(stp29_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration10 :\n",
    "\n",
    "slice1 = 9\n",
    "\n",
    "gp10 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp10 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp10_results = pd.DataFrame(gp10).sort_values(by=[0], ascending=False)\n",
    "stp10_results = pd.DataFrame(stp10).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp10 = np.asarray(gp10_results[4:5][0])[0]\n",
    "median_gp10 = np.asarray(gp10_results[9:10][0])[0]\n",
    "upper_gp10 = np.asarray(gp10_results[14:15][0])[0]\n",
    "\n",
    "lower_stp10 = np.asarray(stp10_results[4:5][0])[0]\n",
    "median_stp10 = np.asarray(stp10_results[9:10][0])[0]\n",
    "upper_stp10 = np.asarray(stp10_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration20 :\n",
    "\n",
    "slice1 = 19\n",
    "\n",
    "gp20 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp20 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp20_results = pd.DataFrame(gp20).sort_values(by=[0], ascending=False)\n",
    "stp20_results = pd.DataFrame(stp20).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp20 = np.asarray(gp20_results[4:5][0])[0]\n",
    "median_gp20 = np.asarray(gp20_results[9:10][0])[0]\n",
    "upper_gp20 = np.asarray(gp20_results[14:15][0])[0]\n",
    "\n",
    "lower_stp20 = np.asarray(stp20_results[4:5][0])[0]\n",
    "median_stp20 = np.asarray(stp20_results[9:10][0])[0]\n",
    "upper_stp20 = np.asarray(stp20_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration30 :\n",
    "\n",
    "slice1 = 29\n",
    "\n",
    "gp30 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp30 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp30_results = pd.DataFrame(gp30).sort_values(by=[0], ascending=False)\n",
    "stp30_results = pd.DataFrame(stp30).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp30 = np.asarray(gp30_results[4:5][0])[0]\n",
    "median_gp30 = np.asarray(gp30_results[9:10][0])[0]\n",
    "upper_gp30 = np.asarray(gp30_results[14:15][0])[0]\n",
    "\n",
    "lower_stp30 = np.asarray(stp30_results[4:5][0])[0]\n",
    "median_stp30 = np.asarray(stp30_results[9:10][0])[0]\n",
    "upper_stp30 = np.asarray(stp30_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9(a). Summarize Arrays: GPs\n",
    "\n",
    "lower_gp = [lower_gp1,\n",
    "            lower_gp2,\n",
    "            lower_gp3,\n",
    "            lower_gp4,\n",
    "            lower_gp5,\n",
    "            lower_gp6,\n",
    "            lower_gp7,\n",
    "            lower_gp8,\n",
    "            lower_gp9,\n",
    "            lower_gp10,\n",
    "            lower_gp11,\n",
    "            lower_gp12,\n",
    "            lower_gp13,\n",
    "            lower_gp14,\n",
    "            lower_gp15,\n",
    "            lower_gp16,\n",
    "            lower_gp17,\n",
    "            lower_gp18,\n",
    "            lower_gp19,\n",
    "            lower_gp20,\n",
    "            lower_gp21,\n",
    "            lower_gp22,\n",
    "            lower_gp23,\n",
    "            lower_gp24,\n",
    "            lower_gp25,\n",
    "            lower_gp26,\n",
    "            lower_gp27,\n",
    "            lower_gp28,\n",
    "            lower_gp29,\n",
    "            lower_gp30,\n",
    "            lower_gp31]\n",
    "\n",
    "median_gp = [median_gp1,\n",
    "            median_gp2,\n",
    "            median_gp3,\n",
    "            median_gp4,\n",
    "            median_gp5,\n",
    "            median_gp6,\n",
    "            median_gp7,\n",
    "            median_gp8,\n",
    "            median_gp9,\n",
    "            median_gp10,\n",
    "            median_gp11,\n",
    "            median_gp12,\n",
    "            median_gp13,\n",
    "            median_gp14,\n",
    "            median_gp15,\n",
    "            median_gp16,\n",
    "            median_gp17,\n",
    "            median_gp18,\n",
    "            median_gp19,\n",
    "            median_gp20,\n",
    "            median_gp21,\n",
    "            median_gp22,\n",
    "            median_gp23,\n",
    "            median_gp24,\n",
    "            median_gp25,\n",
    "            median_gp26,\n",
    "            median_gp27,\n",
    "            median_gp28,\n",
    "            median_gp29,\n",
    "            median_gp30,\n",
    "            median_gp31]\n",
    "\n",
    "upper_gp = [upper_gp1,\n",
    "            upper_gp2,\n",
    "            upper_gp3,\n",
    "            upper_gp4,\n",
    "            upper_gp5,\n",
    "            upper_gp6,\n",
    "            upper_gp7,\n",
    "            upper_gp8,\n",
    "            upper_gp9,\n",
    "            upper_gp10,\n",
    "            upper_gp11,\n",
    "            upper_gp12,\n",
    "            upper_gp13,\n",
    "            upper_gp14,\n",
    "            upper_gp15,\n",
    "            upper_gp16,\n",
    "            upper_gp17,\n",
    "            upper_gp18,\n",
    "            upper_gp19,\n",
    "            upper_gp20,\n",
    "            upper_gp21,\n",
    "            upper_gp22,\n",
    "            upper_gp23,\n",
    "            upper_gp24,\n",
    "            upper_gp25,\n",
    "            upper_gp26,\n",
    "            upper_gp27,\n",
    "            upper_gp28,\n",
    "            upper_gp29,\n",
    "            upper_gp30,\n",
    "            upper_gp31]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9(a). Summarize Arrays: STPs\n",
    "\n",
    "lower_stp = [lower_stp1,\n",
    "            lower_stp2,\n",
    "            lower_stp3,\n",
    "            lower_stp4,\n",
    "            lower_stp5,\n",
    "            lower_stp6,\n",
    "            lower_stp7,\n",
    "            lower_stp8,\n",
    "            lower_stp9,\n",
    "            lower_stp10,\n",
    "            lower_stp11,\n",
    "            lower_stp12,\n",
    "            lower_stp13,\n",
    "            lower_stp14,\n",
    "            lower_stp15,\n",
    "            lower_stp16,\n",
    "            lower_stp17,\n",
    "            lower_stp18,\n",
    "            lower_stp19,\n",
    "            lower_stp20,\n",
    "            lower_stp21,\n",
    "            lower_stp22,\n",
    "            lower_stp23,\n",
    "            lower_stp24,\n",
    "            lower_stp25,\n",
    "            lower_stp26,\n",
    "            lower_stp27,\n",
    "            lower_stp28,\n",
    "            lower_stp29,\n",
    "            lower_stp30,\n",
    "            lower_stp31]\n",
    "\n",
    "median_stp = [median_stp1,\n",
    "            median_stp2,\n",
    "            median_stp3,\n",
    "            median_stp4,\n",
    "            median_stp5,\n",
    "            median_stp6,\n",
    "            median_stp7,\n",
    "            median_stp8,\n",
    "            median_stp9,\n",
    "            median_stp10,\n",
    "            median_stp11,\n",
    "            median_stp12,\n",
    "            median_stp13,\n",
    "            median_stp14,\n",
    "            median_stp15,\n",
    "            median_stp16,\n",
    "            median_stp17,\n",
    "            median_stp18,\n",
    "            median_stp19,\n",
    "            median_stp20,\n",
    "            median_stp21,\n",
    "            median_stp22,\n",
    "            median_stp23,\n",
    "            median_stp24,\n",
    "            median_stp25,\n",
    "            median_stp26,\n",
    "            median_stp27,\n",
    "            median_stp28,\n",
    "            median_stp29,\n",
    "            median_stp30,\n",
    "            median_stp31]\n",
    "\n",
    "upper_stp = [upper_stp1,\n",
    "            upper_stp2,\n",
    "            upper_stp3,\n",
    "            upper_stp4,\n",
    "            upper_stp5,\n",
    "            upper_stp6,\n",
    "            upper_stp7,\n",
    "            upper_stp8,\n",
    "            upper_stp9,\n",
    "            upper_stp10,\n",
    "            upper_stp11,\n",
    "            upper_stp12,\n",
    "            upper_stp13,\n",
    "            upper_stp14,\n",
    "            upper_stp15,\n",
    "            upper_stp16,\n",
    "            upper_stp17,\n",
    "            upper_stp18,\n",
    "            upper_stp19,\n",
    "            upper_stp20,\n",
    "            upper_stp21,\n",
    "            upper_stp22,\n",
    "            upper_stp23,\n",
    "            upper_stp24,\n",
    "            upper_stp25,\n",
    "            upper_stp26,\n",
    "            upper_stp27,\n",
    "            upper_stp28,\n",
    "            upper_stp29,\n",
    "            upper_stp30,\n",
    "            upper_stp31]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEaCAYAAAAPGBBTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdeXxU5b348c83y5CQHbJDNnZkhygqYhUQKSqItVZvf663V71a61a9tr2tttbWulRri7ZYW2tdqtXiVdQqICq4AcGwL2EnGwlJyD5Zn98fZxKTkGWSzJKZfN+v13ll5pwzz3nOJJnvPLsYY1BKKaW6EuDtDCillBrYNFAopZTqlgYKpZRS3dJAoZRSqlsaKJRSSnVLA4VSSqluaaBQSinVLQ0UatASkW+JiBGRUhFJcuwLFJEvHPufcOwbISLPiMghEakTkRIR2SQiP2qT1keO1xgRaRaRYhH5QERmefB+Wq6f7qlrqsFBdMCdGsxE5DXg28BbxpilIvJD4FHgADAVGAl8CsQCxcAaoKHlmDEmzpHOR8A3gE+AbGAOMAs4ZIwZ5aF7aflnzjDGHPbENdXgoCUKNdjdihUAlojIz4BfAAb4T2NMDfA7rCCxF5hgjPkPY8y1xpgZwLxO0ltpjLkduN7xPENEbAAiEiYij4rIARGpEpFsEbm65YViuVFEtotItYjsF5FfikiI43iMiPxTRE6IiN1RwvmT41jbb3yHHCWL81z4PqlBTAOFGtSMMcXA9x1Pfw6EAk8bYz4WkVBggePYk8aY0g6v3d5JkstE5HfAXxzPVxlj6h2P/wr8EGgCXgPGAi+IyFWO4/8N/AlIAV4FgoCfYAUrgLuBy4EcR1q7gbMdx1rOabnO74BcZ94DpXpkjNFNt0G9AYFYH6rGsY137B/RZt8ix75FbfYZ4DzH/o867DdAM/Ajx/H4NvvTHPtudzz/zPF8l+P5tY7n0xzPm4AQ4DeO508CZwDhQGCb+2hJP93b76lu/rVpiUIpuIuvgwLA446fpUCj43GK4+dhrG/rLaWEju40xggw3vH6X4nIuUC643itMeaI4/Eex880x8+Wc3Z3OB7guP6TwPvALcCXwEmsEon+Hyu30j8wNaiJyHi+bpf4NlZ7xUUicrUxphZY6zj1NhGJMMbsMcbcAdR2l64xZh9Q4Hg6DivAAISKSKrj8XjHz5bA0XLOhA7Hm4FjQKkxZhEQgVXa2An8B1bDect5oP/XysWCvJ0BpbzF8U38L1jVOsuNMW849r0GPCkiq4E7sHo9TQF2i8harKAS1kWyyxzdU8cBk7E+vDcaY4pE5HWsNobVIvIpcIXjNX9w/FzuePw7EfkGXzeWP2eMsYvI/SKyBNiOVaJJdxwvd/w8hlU6+YOI7AN+Yoyp7tu7o1Qb3q770k03b21YQcAAh4DwNvtfd+xf6XieBvwZqx2jHigCvgB+BEQ7zvmI9u0TFcBm4Ntt0o0Afuu4XjWwDbiuzXHBatDeAdRgddH9FRDqOH4JX1c52YF9wG1tXv8drGDR7MhDrLffY938Y9NxFEoppbqldZlKKaW6pYFCKaVUtzRQKKWU6pYGCqWUUt3yy+6xsbGxJj093dvZUEopn5GVlXXCOCa57MgvA0V6ejqbN2/2djaUUspniMiRro5p1ZNSSqluaaBQSinVLQ0USimluuXVNgoRuRt4DIgzxpzocGw68AwQiTXN8kPGmFc9n0ulLA0NDeTm5mK3272dFaX6LCQkhJEjRxIcHOz0a7wWKEQkBVgIHO3ilBrgGmNMjogkA1ki8r4x5qTHMqlUG7m5uURERJCeno6IeDs7SvWaMYaSkhJyc3PJyMhw+nXerHp6AriXr9cAaMcYs88Yk+N4nI81EVunXbeU8gS73c7w4cM1SCifJSIMHz6816VirwQKEVkK5Bljtjp5/hmADWs2TaW8RoOE8nV9+Rt2W9WTiKwBEjs59BPgx1jVTs6kkwT8HWt5yOZuzrsRuBEgNTW1q9OUUkr1ktsChTFmQWf7RWQKkAFsdUS2kcAWETnDGFPY4dxI4B2sBVi+6OF6K4AVAJmZmX2aO/3kkXI+WbGn5xMBhgyBsLCvt6FDoReNQx0FB0NUFERHf70NHdrn5JQnrFjh2vRuvLHHU44fP86dd97JF198QUxMDDabjXvvvZdly5bx0UcfsXTpUjIyMqirq+PKK6/k/vvvb/f6w4cPM3HiRMaPH9+676677uKaa64hPT2diIgIRISYmBheeOEF0tKsVVpFhO9+97u8+OKLADQ2NpKUlMTs2bNZtWpVu2u0zYfdbufiiy/mscce6++706Pnn3+ehQsXkpyc3ON5mzdv5g9/sNaLWrFiBb/97W8BCA8P57HHHuO8884D4LzzzqOgoICQkBBsNhvPPvss06dPd+t9DEQeb8w2xmzHWmgeABE5DGR20uvJBqwEXjDGvO6JvDXUNFB4pM7Js+uw1qZpw2azPt2HhkLoUAgNBVsw1no0PQgN4Vhg+19HcHD7wBEdDRER4EzJMTRUA42/McZw6aWXcu211/Lyyy8DcOTIEd56663Wc+bOncuqVauorq5m+vTpXHLJJcycObNdOqNHjyY7O7vTa6xbt47Y2Fjuv/9+fvnLX/Lss88CEBYWxo4dO6itrSU0NJTVq1czYsSILvPako/a2lpmzJjBsmXLmDNnTpfnO6upqYnAwMBOjz3//PNMnjy5x0DR1qpVq/jTn/7Ehg0biI2NZcuWLSxZsoQvv/yy9f5eeuklMjMz+etf/8o999zD6tWr+30fvmZATeEhIpnAzcaY72EtE3kuMFxErnOccp0xpvO/8IGgvt7aTvahY1ZKCnTohdDQAMXF1tZb48aB40uR8hMffvghNpuNm2++uXVfWloat9122ynnhoWFMWvWLPbv339KoHDGWWedxVNPPdVu3+LFi3nnnXe4/PLLeeWVV7jqqqtYv359t+mEhoYyffp08vLyAKiurua2225jx44dNDQ08MADD7B06VJqamq47rrr2LFjB+PHjyc/P5/ly5eTmZlJeHg4N910E2vWrGH58uWEhoZy1113UVVVRWxsLM8//zyffvopmzdv5rvf/S6hoaF8/vnnhIaG9nifv/nNb3j00UeJjY0FYObMmVx//fUsX76cX/3qV6e8J48++mhv3ka/4fUBd8aY9JbShDFmsyNIYIx50RgTbIyZ3mYbuEGivwoLoKnJZcmVl/d8jvItO3fudPpDv6SkhC+++IJJkyadcuzAgQNMnz69devsw/7f//43l156abt9V155Jf/4xz+w2+1s27aN2bNn95iPsrIycnJyOPfccwF46KGHmDdvHhs3bmTdunXcc889VFdX8/TTTxMTE8OuXbt48MEHycrKak2jurqa2bNns3XrVmbPns1tt93G66+/TlZWFjfccAM/+clPuPzyy8nMzOSll14iOzub0NBQfvazn7UrbXVm586dzJo1q92+zMxMdu3a5dR7MlgMqBLFoNbQCEVFkJTkkuQ0UPi/W2+9lQ0bNmCz2di0aRMA69evZ8aMGQQEBHDfffd1Gii6q3o6//zzKS0tJTw8nAcffLDdsalTp3L48GFeeeUVFi9e3G3e1q9fz7Rp08jJyeGOO+4gMdHq1/LBBx/w1ltvtbZZ2O12jh49yoYNG7j99tsBmDx5MlOnTm1NKzAwkG9961sA7N27lx07dnDBBRcAVlVUUhf/M7/4xS+6zaOzvvvd71JfX09VVVWX75u/83qJQrWRn+eypOx2qHO2uUX5hEmTJrFly5bW58uXL2ft2rUUt6mbnDt3Ll999RVZWVntqqictW7dOo4cOcL06dNPaQgHWLJkCT/84Q+56qqruk1n7ty5bN26lZ07d/Lcc8+1fsAaY3jjjTfIzs4mOzubo0ePMnHixG7TCgkJaW2XMMYwadKk1tdv376dDz74oNf32eK0005rV3oByMrKIjMzs/X5Sy+9xMGDB7n22ms7reYbDDRQDCTVNVBW5rLktFThX+bNm4fdbueZZ55p3VdTU+Py6wQFBfHkk0/ywgsvUFpa2u7YDTfcwP3338+UKVOcSisjI4P77ruP3/zmNwBceOGF/P73v8cYq2PiV199BcCcOXN47bXXANi1axfbt2/vNL3x48dTXFzM559/DljTquzcuROAiIgIKisre3Wv9957L//zP/9DSUkJANnZ2axcuZKbbrqp3XkiwoMPPsgXX3zBnj1O9oz0I1r1NNDk5UFMjEuSKi+H+Piez1N95ER3VlcSEd58803uvPNOHnnkEeLi4ggLC2v9EHZWSxtFixtuuIEf/OAH7c5JSkriqquuYvny5fz0pz9t3T9y5MhTzu3JzTffzGOPPcbhw4f56U9/yh133MHUqVNpbm4mIyODVatWccstt3Dttddy2mmnMWHCBCZNmkRUVNQpadlsNl5//XV+8IMfUF5eTmNjI3fccQeTJk3iuuuu4+abb25tzP71r39NZmYmS5Ys6TJvS5YsIT8/nzlz5tDY2EhhYSFbt24lLu7USSBCQ0O5++67efTRR3nuued69R74OmmJ7P4kMzPT9GXhouLdJ1j50KmNWB53+ulW/9Z+mjkT2pSgVT/t3r27x2oS1TdNTU00NDQQEhLCgQMHWLBgAXv37sVms3ksD42NjVx//fU0Nzfz4osv+vUo/M7+lkUkyxjT6SeGligGovx8GD2638n0pZeuUt5QU1PD+eefT0NDA8YYnn76aY8GCbCq3P7+97979Jq+QgPFQFRYCGlpENS/X4+2UShfERERocsXD2DamD0QNTXB8cKez+tBeTn4Yc2iUsrDNFAMVPkF/f6Ub2wEN3SKUUoNMhooBqraWujQNbEvtPpJKdVfGigGsrz+D8DTBm2lVH9poBjITp6E6up+JaElCqVUf2mgGOj6WarQQKGU6i/tHjvQFRVZ04/3cVEkDRTu44V1iwBrBtaXX36ZwMBAAgIC+NOf/tQ65URhYSGBgYGtI4s3btxIaGgoU6ZMobGxkYkTJ/K3v/2NoR0WKwkMDGw3LceVV17Jfffd17q/sbGRjIwM/v73vxMdHQ30bjGjttfoLC13OXnyJC+//DK33HJLj+eGh4dTVVUFQG5uLrfeeiu7du2iqamJxYsX8/jjjzNkyJBe3UttbS2LFi3iww8/7HIdjf5qWXAqMDCQoKAgNm/eTH19PQsWLODDDz8kqJ/d7EFLFANfczMUFPT55RUVVhLKP3z++eesWrWKLVu2sG3bNtasWUNKSkrrJHk333wzd955Z+tzm81GaGgo2dnZ7NixA5vNxh//+MdT0m05p2W777772u3fsWMHw4YNY/ny5a2vabuYEdDjYkbdpdUfxhiau/gjP3nyJE8//XSv07vsssu49NJLycnJIScnh9raWu69997Wc5y9l7/85S9cdtllbgsSLdatW0d2dnbrWBSbzcb8+fN59dVXXZK+BgpfUFDQ5097Y6CX86SpAaygoIDY2NjWb7axsbG9WtFt7ty57N+/v0/XPuuss1oXIGrRspgR0LqYUV/SevHFFznjjDOYPn06N910E02OtVkefPBBxo8fzznnnMNVV13VOj354cOHGT9+PNdccw2TJ0/m2LFjnaZx3333tc5tdc899ziVtw8//JCQkBCuv/56wCo9PPHEE7zwwgutJY6e3pcWL730EkuXLgWgvLychISE1mOzZs2i3I1F/ksvvZSXXnrJJWlpoPAFdXVQcqLn87qgPZ/8x8KFCzl27Bjjxo3jlltu4eOPP3b6tY2Njbz33nudzvxaW1vbbjGjjt9Em5qaWLt27SkT7PVlMaOOae3evZtXX32VTz/9lOzsbAIDA3nppZfYtGkTb7zxBlu3buW99947ZeR2Tk4Ot9xyCzt37qSmpqbTNB5++OHW9TdaVqdbvHgx+fn5Xeavs8WMIiMjSU9PPyXIdvW+ANTX13Pw4EHS09MBiIqKoqamhsbGRgCmTZvGtm3bTnnd3Llz2/0uWrY1a9Z0ml8RYeHChcyaNYsVbepDJ0+e3LpOSX9pG4WvyMuDuL5NBavtFP4jPDycrKws1q9fz7p16/jOd77Dww8/zHXXXdfla1qCAFgfQv/5n/95yjktVSldvTYvL4+JEye2LhjUojeLGXWV1tq1a8nKyuL0009vPS8+Pp7S0lKWLl1KSEgIISEhXHLJJe3SS0tL48wzz+w2jZaV9dp69913u82nM3p6XwBOnDhxSrtFYmIiBQUFpKSksGfPntYFndrqaXnZjjZs2MCIESMoKiriggsuYMKECZx77rkEBgZis9morKwkIiKidzfYgZYofEVFpdXg0AcaKPxLYGAg5513Hj//+c/5wx/+wBtvvNHt+W3bH37/+9/3arK9ltceOXIEY0yndfHOLmbUVVrGGK699trWPO7du5cHHnigx7yFhYW1Pu5rGp3pbDGjiooKCgsLGT9+fLf30vF+7XZ7u33Jycnk5+fz+uuvExsby9ixY095XW9LFC3tQvHx8SxbtoyNGze2HqurqyMkJKR3b0AnNFD4km6Ky93RQOE/9u7dS05OTuvz7Oxs0tLS3H7doUOH8tRTT/H444+3Vp206O1iRh3Tmj9/Pq+//jpFRUUAlJaWcuTIEebMmcPbb7+N3W6nqqqq055ULbpKoy+LGc2fP5+amhpeeOEFwKpeuvvuu/n+979PaIfp/7t7X2JiYmhqamoXLJKTk3n33Xd55JFH+Mtf/tLp9devX9+uY0HLtmDBglPOra6ubr2/6upqPvjgAyZPngxY66bHxsYS3Mcek21p1ZMvOVEMdRngaMh0lrZRuIeH1y0CoKqqittuu42TJ08SFBTEmDFj2tVL91Xb6imARYsW8fDDD7c7Z8aMGUydOpVXXnmFq6++unV/XxYz6pjWL3/5SxYuXEhzczPBwcEsX76cM888kyVLljB16lQSEhKYMmVKp4sZgVUK6CqNOXPmMHnyZL75zW/y6KOPsnjxYv785z932QlARFi5ciW33norDz74IMXFxXznO9/hJz/5iVP30tbChQvZsGFD64d8cnIyL7/8Mh9++CGxsbG9es86c/z4cZYtWwZYbVD/8R//waJFiwCrJ9RFF13U72uALlzUzoBZuKg7Q4Y4N/14XBykprY+vf76Pg/FUA66cJHnVVVVER4eTk1NDeeeey4rVqxg5syZHs3DZ599xlVXXcXKlSt7fe0tW7bwxBNPeGWdi8suu4yHH36YcePGnXJMFy7yd3V11taTDqWO8nJwwRcYpTzqxhtvZNeuXdjtdq699lqPBwmAs88+myNHjvTptTNnzuT888+nqanJ7WMp2qqvr+fSSy/tNEj0hQYKf+UYBNVCA4XyRS+//LK3s9BvN9xwg8evabPZuOaaa1yWnjZm+6s6e7tBetqgrZTqK68HChG5W0SMiHT5fVdEIkUkV0T+4Mm8+bRm066KSgOFUqqvvBooRCQFWAgc7eHUB4FP3J8jP9OmW572fHINf+z8oQaXvvwNe7tE8QRwL9BlzkVkFpAAfOCpTPmNNu0UWqLov5CQEEpKSjRYKJ9ljKGkpKTXg/C81pgtIkuBPGPMVhHp6pwA4HHg/wGnjjZpf+6NwI0AqW26hQ5qbQJFfb31tMN4IdULI0eOJDc3l+LiYm9nRak+CwkJYeTIkb16jVsDhYisAU6dzAR+AvwYq9qpO7cA7xpjcrsKJi2MMSuAFWCNo+h9bv1QJz2fNFD0XXBwMBkZGd7OhlIe59ZAYYzptBQgIlOADKClNDES2CIiZxhjCtucehYwV0RuAcIBm4hUGWPuc2e+/UYngaKTOciUUqpbXql6MsZsB1qnQhWRw0CmMeZEh/O+2+ac6xznaJBwVksX2QCrKUrbKZRSfeHtxuxTiEimiPzZ2/nwCx26yGrPJ6VUXwyIkdnGmPQ2jzcD3+vknOeB5z2WKX9ht7c2TGiJQinVFwOuRKFcrE07RUWFtTSqUkr1hgYKf9cmUDQ1QSdL/iqlVLc0UPi7Tno+KaVUb2ig8HcdAoU2aCuleksDhb/TWWSVUv2kgcLf6SyySql+0kAxGLSZRVYDhVKqtzRQDAZt2ikqK63eT0op5SwNFINBhwbtigov5UMp5ZM0UAwG2kVWKdUPGigGA+0iq5TqBw0Ug4F2kVVK9YMGisFAu8gqpfpBA8Vg0aaLrFY9KaV6QwPFYNGmncJub1fAUEqpbmmgaKOqqNrbWXAf7fmklOojDRQODbWNzJsvvP4GVO3L97+FGzRQKKX6SAOFQ0N1PVfN2Mtm+xTu3nQlL70WRGX2fmhq9HbWXEMDhVKqjwbEUqgDwdDYodz5wgxG/O9X7NhUw79yz+CTnXM4Y/cmvp2+keEz02kYEuHtbPZdSxfZAOu7gQYKpZSzNFB0EBwaxIxzI5nUsIfdX2XzfwemcM/B05lycBtXJ60l+fQRVEckejubvdfSRVbXz1ZK9ZIGii7YgmHaGSFMztzPrh37eGf3KO4tuJOxb+3jhmFvMj61FnFhxV1j5HBKR0x1XYKdsdtbA4V2kVVKOUsDRQ8CAwxTpgqTJh9i14E8Vm9L4Eel90Kpa68TRhXfHfERU+dGERzopob02lqIiQGgsRFqamDoUPdcSinlPzRQOCkgACaPrWfSmGPsyy+lpMx1xQnBkLe3ihV5F5O0spSlswuYPrIEEZddwtLJnE8aKJRSPdFA0UsiMH5ENYxwbbqBE+u4/v1beKjsFv74yWQmJJZxxawDjIiucd1FOun5lJzsuuSVUv7Jq91jReRuETEiEtvF8VQR+UBEdovILhFJ92wOPacpcAi1C5fy79ir+T23kXsihAffncUrm0ZTXeeieK5dZJVSfeC1QCEiKcBC4Gg3p70APGqMmQicARR5Im/e0hgUyurzH+KKYavZ3zSKxSO28nFOMj9963TW7U2iqbnnNLrVYRZZbdBWSjnDm1VPTwD3Av/X2UEROQ0IMsasBjDGVHkwb17TYAvnvXmPcsnq21lZcBZPz/4LzxxaxD82j+WTnGSWTjtMVGh9j+nYAptIjq5p386hXWSVUn3glUAhIkuBPGPMVum6xXYccFJE/gVkAGuA+4wxna74LCI3AjcCpKamuj7THlQ3JIp35j/OJatv57+z/ouR855gTe3ZvL5lFM98MsnpdO65IJsx8R3WPW3TRbayst0YPKWU6pTbAoWIrAE6G5n2E+DHWNVO3QkC5gIzsKqnXgWuA57r7GRjzApgBUBmZqbPT9RUGzqcd+b/liWrb+OidT+k+YLfMfniUnKKomhu7r47VE1DEM99OpG8k2GnBoo2XWSbm61gERXlrrtQSvkDtwUKY8yCzvaLyBSsEkJLaWIksEVEzjDGFLY5NRfINsYcdLzuTeBMuggU/qg6LJ5V83/LktU/4KK1d/PWBb8nOKnnGNhs4G+fj6e4KuTUg500aGugUEp1x+OVDsaY7caYeGNMujEmHSsgzOwQJAA2AdEiEud4Pg/Y5cGsDgiVESN4Z/7jiGni4rV3El7V8W06VYBAXHgtxVWhpx7Unk9KqV4aULXTIpIpIn8GcLRF/BBYKyLbAQGe9Wb+vOVkVDrvznuc4MYaLlp7F0GNtT2+JjbCzonKTkoUbVa6Aygrs6qgBvLmbzO+K+VrvD7gzlGqaHm8Gfhem+erATdPgOQbSoaNZc05D3DRhz9k1JF17Bu9uNvz48Jr2Xc8GmNo3/PJXtuuBXvPHmsbyObPh9GjvZ0LpQavAVWiUN3LS8zkZGQqEw682+O5cRF26hoDqbQHtz/QbKC+5+61A0mpi+fVUkr1jgYKXyLCntGLSSzeTlT5kW5PjQu3qqecaacY6EpKvJ0DpQY3DRQ+JifjQpolsMdSRVyE1RZR3Fk7hY8FCi1RKOVdPQYKEckQkf8RkVUissOxvSMi94pIhicyqb5WGzqMIyPOYuyh95HmrpdpHR5mRzB+UaKoqrIGlCulvKPbQCEiK4Ec4NfAFKASqHI8fhjIEZE33J1J1d7e0YsZai8jNe/zLs8JDjTEDK3zixIFaKlCKW/qqUSRDNwEJBpj0owxZxljzjTGpGKNur4Za8Cc8qBjybOpDh3uVPVTp4PuOnSR9QXaTqGU93QbKIwxs40xzxljTpm11RhTZIz5szFmtvuypzpjAoLYl7GIlPwvCa3t+hO0y0F3LV1kfYiWKJTyHqcas0WkSUSuaPN8sYjsc1+2VE/2jl5MgGli3MF/d3lOXISdSrsNe0Ng+wPaRVYp1Qs9tVGkisi5WKOiTxORcx3PvwmM8kQGVecqIkeSHz+N8Qfe7XLo8tddZH2/naK0VEdoK+UtPZUorgfWAQb4qePxOuBWYK97s6Z6snf0YqIrc0ks2tbpcX/qItvYCBUVPZ+nlHK9ngLFRuAZrBLFauBpYDnwILDMvVlTPTmU+g3qg4Yy4cA7nR73p0F3oA3aSnlLt3M9GWPeA94TkU3AR8aY7ocDK49qDArlQPp8xh76gE9Pv52G4LB2x0NtTYQNaeh8ckAfDBSlpTBKKzyV8jhnR2a/AzwmImUiskBE/iki33dnxpRz9oy+iKCmOkYfXtvp8a57PmkXWaWUc5wNFMuBRUAk0AwcxhpfobysePgESqMyuhxTERfe1ViKWp9rHdaeT0p5h7OBYiHwWJvnu7BWqVPeJsKeMRcRX7KbmJMHTzkcF1FLaXUITR2XT202PjcvRmWlz/XqVcovOBsoqoEEx+NAYAGgFQEDRE76BTQFBHVaqogLt9NshJLqIae+0EfbKZRSnuVsoPgH1nQdAKuAK4FX3JIj1Wt1IdEcGTmHsYc+IKCp/VfuuAhHz6dK/+j5pIFCKc9zNlD8CHgAyAK2Aj/HGlehBog9oy8ipK6ctNxP2+2PC3eMpfCDQXegDdpKeYMz04wHYpUeso0xZzi2XxhjGtyfPeWsvMRMqobGWyO124gKrSc4sMlvShQaKJTyvB4DhTGmCZgApLg/O6qvTEAge0ctIqVgE2HVX8/hKNJdzyff6yKrU3ko5XnOVj3tAB4UkUdF5K6WzZ0ZU723b/RiBMO4g++12x8bbu+8ROGDXWQbG63eT0opz3E2UFwBRAN3Y3WTfTSq+hAAACAASURBVAx41F2ZUn1TGZ5EbuIsx0SBX08jHhdRy4mqkFNjgg92kQWtflLK07qdwqONG7AmBlQD3N7Ri5n/6YMkH/+K/MRZgFX1VN8USIXdRlRoh4EItbUQ0km11ABWWgoZOopHKY9xKlAYY553cz6UixweOZc6WzgT9r/TJlC0dJEN6TxQxMR4Opv9oiUKpTzL2YWLDnaybRGR34hIv76OisjdImJEJLaL44+IyE4R2S0iT4mIdHaesjQFDWF/+gWkH1uPrc6qzG+dbtxPZpHVsRRKeZazVU/xwFCseZ7ACjANwDTABtzZl4uLSArW9CBHuzh+NjAHmOrYtQH4BvBRX643WOwbdSGT9q0kNf8L9mdcwPAwOyKm83UpioqgqqrnRAMC4LSJEOjsn4z7VFRAQwMEB3s7J0oNDs7+1y8HhmMtWCTAH4CTjseX08dAATwB3Av8XxfHDRCCFYwECAaO9/Fag8aJmHHU2cJJOv4V+zMuICjQMGxoXeclioYGKC93LuFjuZCe7tK89lVpKSQk9HyeUqr/nO31dAtQaIypM8bYgULgOqzpx/v07yoiS4E8Y8zWrs4xxnyOtaJegWN73xizuy/XG0xMQCAF8dNJPp7dui82vLbzEkVv5OVB/cDoJaXVT0p5jrOBYhvwIxE5KiJHsKb02AuMAPK7epGIrBGRHZ1sS4EfAz/r7qIiMgaYCIx0XGueiMzt4twbRWSziGwuLi528rb8V37CdKKq8loH38VF2DnR2aC73mhqgsMDY+0qbdBWynOcDRTfwaoeCgcigDexJgbcBvy/rl5kjFlgjJnccQMOYk1TvlVEDmMFgi0iktghiWXAF8aYKmNMFfAecFYX11phjMk0xmTGxcU5eVv+Kz9hBgDJx78CrJ5PlXU27A2B/Uu4sBCqq/ubvX7TEoVSnuNUoDDG5BpjLjPGDHNs3zLGHDPGbDXGfNbbixpjthtj4o0x6caYdCAXmGmMKexw6lHgGyISJCLBWA3ZWvXkhNLoUdhtkSQVWdVPrT2f+lv9BHDw1HUvPE1LFEp5jrPdY4c7lj91+1KoIpIpIn92PH0dOABsx5q1dqsx5m13XNfvSAAFCdPalSigiy6yvVVWZm1e1NCgU3ko5SnOVj09gxuXQnWULE44Hm82xnzP8bjJGHOTMWaiMeY0Y4zOL9UL+fHTiawqILyq0LUlCrBKFV6eJ0pLFUp5hrOB4gJ0KVSf09JOkVSUTWhwE+FD6l1TogCrneK4d3sqazuFUp6hS6H6sbLoDOxDotpUP3Ux3XhfHT5s9YTyEg0USnmGLoXqzySA/DbjKeIiuphuvK/q6yEv13Xp9ZJWPSnlGb1ZCvXntF8K9X/dlSnlOgUJ04moLiSiqoC48FpKa4bQ2OTC6bKO5XptEF55ubU+hVLKvZztHttgjPl526VQgQvdnDflAq3tFMe/Ii7CjjFCSfUQ112gqQmOdDpVl0do9ZNS7tdtoBCREMfsrstF5BrHvkUikgW85ZEcqn4pi0qndkg0yce/cm0X2ba8OAhPA4VS7tfTpIDPYbVHCHCziCzBGi0NsNKdGVMuIkJ+gtVOETf963UpXMoYOHQIJk92bbpO0HYKpdyvp6qnhViN1+cAvwAuA74CZhhjLndz3pSL5CfMILymiBGNh7EFNrm+RAHWV/uTJ12frhOXVUq5V0+BYjjwimOajqcd+35pjNnm3mwpVypImA7AiKJsYsNdMDlgV7wwCE9LFEq5nzON2b8RkW1YiwUZ4HER2SYiXU4PrgaWk5Fp1IQMs9opImpd20W2raoqayEkD6qvd27dJaVU3zmzcFGKY2uhI7J9jQgFCdNJOp5NbKqdXQUxGANuWVT28GEYNsyjy8+VlkJ4uMcup9Sg022JwhgT0N3mqUyq/stPmEF4bTGpwQU0NAVSXmtzz4Xq6mDnTo+O2NbqJ6Xcq6fusRN6SsCZc5T35TvaKSbWW6O0XTqVR0cVFbB7NzQ393yuC2igUMq9eioV7BKRT0TkhyLyDREZKyLjROQ8x75PgJ2eyKgnhMcPZcxoQ0CAd2dFdYfyiBSqQ4czo2oDgPvaKVqUlsK+fR5p3NaeT0q5V09tFJcCPwQewWrIbkuA9Y5z/ELo8KHMu38uZ2/cxv4PDrInL4JSV45i9iYRCuKnk1n4PiLGvSWKFkVFVlvF6NFuvUzLVB5BzrS4KaV6rdt/LWPMW8BbIpKCNZaipVH7KPCpMeaYm/PneQEBhJw5nckT0pn8yScU7StgT2E0B4ojaWjy7WaZ/MQZjDmyltjQaveXKFrk5VnBIjXVbZcwxlpHSVfAVco9nPoO5ggIg2u22OhouOQS4nfvJv7LLznLfpyDxZHsKYzmeIWHPmRdLD/emvcpNTif4qqOy5O70eHDVrBISnLbJUpLNVAo5S5OBQoRmQM8AKRjrUcBYIwx7q1T8DYROO00SE0leMMGxgceZXxiOWXVNtbuGeFz1VIVESOoCo1jbPM+9lR5uJdzTo5VN+SmT3Nt0FbKfZytS3kFmA+MBOIcW7y7MjXghIfDokUwbx6EhBATVs9FU44SPbTe2znrHcd4ikn2zVTXBVNbH9jza1xp7x63rbWtDdpKuU9vKt3/1xgTaoyJaNnclquBaswYuOIKGDuWUFsTF089QlSobwWL/IQZTGzcDrhhFtmeNBvYtQsqK12etJYolHIfZwPFm8BiEZkvIjNbNndmbMAKCYHzz4dlyxg6OpmLpx4lMrTB27lyWn7CDEZzAHDDLLLOaGqCHTugpsalydbVeW2mc6X8nrMdCr/v+PlBh/0errsYQOLi4JvfJOz4cS5O+oq3/w2Vds9NW9FXleFJJIRWQK0XShQtGhpgx3ZITXPpPCIlm6oJS3ZiybugIEhLg8DB++erVG84Gyj+1sk+/xuV1hcJCYRfvoiLpxXy9h/zqDo+wL/WilCZOI64Q8XeKVG0sNdZA/JcaO3+ZgKdGCw5JKiJsyZlk3r+aJg4EWxums5EKT/RbaAQEV3FzkkRYxO5+P5E3v5bKdW7j0CF6+vhXSU/YTqjD+2n7GS6t7PiUg1NATQ4McWUvSGQf2+OZfSRY5w9cRuhMyZYiy6F+ma3Z6XcracSxcXdHNMSRQeRkXDxNcN4++1h1OSWwpEjbmm47a+Wdoo1leO9nRWvOlAcSd7JMM46foCx27bB+PEwbRpEDL5+Gkp1p6dA4ZbO9iLyAPBfQLFj14+NMe92ct4i4HdYbSF/NsY87I78uFJUFFx8Mbz99jBqhw2D+jrnXpibB7m57s2cQ1V4EiODN1FUH01DkxAcOHhjvr0hkHV7k9lfXM3c6n2E795tTTkyfbo1XbpSqscpPI648dpPGGMe6+qgiAQCy4ELgFxgk4i8ZYzZ5cY8uUR0dEuwADtODspLS7XmRqr3THfbmGiDKQ6gpMpGYpSTwcyPHSsN47Ws0ZyRXsSk5v3I/v0wYoTrF7oQsRrTAwPb/+y4LyDATQuG9CAwEOLjresr5TCQp1E7A9hvjDkIICL/AJYCAz5QAMTEWMFi82bnzi8qCqJm1CjYs8e9GXMYGh8OxWAvrIAo3xph7i6NTcJnBxI4UBzJuWMLiMnL83aWvCM01CpVjRsHsbHezo0aALwZKL4vItcAm4G7jTEdh+yOANpOOpgLzO4qMRG5EbgRINWNE9D1xrBhsHChc+cWFcH//V88pqDAmg7VzQJSkmEn1B4vh/GDZ5C9M45XhPLGVxnMTD3B9JElg+/LdW2tNdZlxw7rG8/YsdZgU11GcNBy27+AiKwRkR2dbEuBZ4DRwHSgAHi8v9czxqwwxmQaYzLjfHB2uPh4mDED65ucB6ocgoZFE0Y1J8s8s7iQr2luFjYfjuOtbWlU1A788TFuU1YGGzfCyy/DqlVWl+YG3xlgqlzDbSUKY8wCZ84TkWeBVZ0cyqP9Wt0jHfv81syZcPRoOCcKEyG/wK3XEoERtiKKqiPANIMMtq/NzimqCOWNLRnMGXOccQnuL+kNaPn51rZhg9VDbM4cb+dIeYhXPh1EpO1808uAHZ2ctgkYKyIZImIDrgT8elxHQIA172DgqHQIdn+tYFx4LYdMGsPLDrj9Wr6soSmAj/YmsWb3COoaNKDS2Giti56f7+2cKA/x1l/9IyKyXUS2AecDdwKISLKIvAtgjGnEmjrkfWA38Joxxm+WXe1KdDTMPicY0tLdfq3IYUEcZBSjDq11+7X8wcHiCF7fMor8k0O9nZWBISvL2zlQHuKVxmxjzNVd7M8HFrd5/i5wyvgKfzdpEhw9kkRuYSFUVbntOtExQh0hDNv7ORHjllAZkey2a/mL6rogVm1LZerIUk5PL3ZqyhC/VVBgrWA4YoS3c6LcTMvRA5AIfOM8wTbRvetCxYXbAdgvY5id/Ue3XsvfbMsdxpvZ6ZRVD/J5opzt/618mgaKASosDOZeHAUJ7uu6Gh9RC8ATUfeTeHQjiUVb3XYtf1RSNYR/fZXBrvxob2fFe44fh2PHej5P+TQNFAPY6NEwZn6626bDjg23c/74PN4pO4dJspu6L76yekAppzU1Cxv2J3KkZBCPMdBShd/TQDHAzZkfQtj4kW5JWwSuzDzA3Qu20jAkjCsq/8IbqyOpqhvIA/YHpk9ykrA3DNL1LYqL4ehRb+dCuZEGigFuyBA47+oUGOq+njbjEsr50dJd3BH6Rz4snsL9b2ey8VAcZhC30/ZWbX0gG/Ynejsb3qOlCr+mgcIHjEgJYPJi905LEhwE553TxBZmkhZwjOc+m8jv102mpErngXLWweII9hdFejsb3nHiBBw+7O1cKDfRQOEjzrg4nui0KLde43j8FIamxZNVN4Wrp2azvziKn7+TyZo9I2jWpgunfHoggZr6QVoFtXkzWgz1TxoofERQEMz73igCAt07D9SX028ikCZ+WnEv91+0mbHxJ/ln1mh+88F0Cst1Bbie1DUE8vG+QToepbQUDh3ydi6UG2irpQ+JzYhgyX+PwP7JRmjufs3PY6Xh7MyP6fU1qsIT2T7x28zY+RITxn+L759Xx6Yjcby6eTSPfDCdH8zbTvpw9w0C9AfHSsPYXRDNxKST3s6K52VlQUaGd9bSUG6jJQofE3/mKFL/8wJSM4JIHVbd5TY+se8fUtmT/h81IcM4K+sPCIYz0ou578JsQoKbeGLNVPYdd28VmD/4/GDC4Jx1tqwMDujcYf5GA4UvGj4cli2DUaO6PiWsDltQ3xoWGoKHsmna90g8sYNRR9YBEBdh556F2UQPreepdZPZ0YfSymDS2CR8tC95cFbZZ2WhjVr+RQOFr7LZYMECOOecTpetFIHEyJo+J79v1CJOxIxhdvafCGyylkqNGVrP3RdsJTGylqc/nsSWo7r6WXcKy0PZnjcI190uL9dShZ/RQOHrTjsNli6FiIhTDiVG1fY5WRMQyOczbyWiupApu//Zuj8ypIG7FmwlfVglKzZM5PODCX2+xmCw8XDc4JwPSksVfkUDhT+Ii4PLLoP09Ha7k6L6XqIAKEicyeGR5zB954uE1pa07h9qa+L2+duZkHCS5z8fz7q9g7SXjxOam4V1e5MH32dmRQXk5Hg7F8pFNFD4iyFDrAW6zzyztSoqLry239NgfzHjvwlsbuD0rc+1v1xQM7eet4NpI0/wj81jeG9HShcpqBNVIYOzmm7LFi1V+AkNFP5m6lS45BIICyMgABIi+179BFAROZKd4y5j/IF3yTj6MUPqKlqPBQcabpq7mzPSj/Pm1gz+9VX64Gy8dcJXx2IprgzxdjY8q7IS9u71di6UC+g4Cn+UkABnnw2rV5MUVdPvFdm2TLmG0Uc+5IL1PwOgcmgCJcPGUBIzhhMxY7ltaj7PBs3l/V2p2BuCuPL0/QRoN/p2jIF1e5NZPPko4SGN3s6O52zebE1F7g1Dh8KYMTBsEHYocDENFP5qxAgICOhXz6cW9bYIXrv4b8SX7Ca2NIfhZfsZXraf1LzPCXBMS355UDh3Df09z+Rch628mCvOLaB5iC4Z2tbJGhv/2DyaMXEVTE8pIXpovbez5H61tbBvn/eun51tdScfO9YKGm6cXNOfaaDwVzYbJCeT0JBHQIChubl/X/EbbOHkJZ1OXtLprfsCG+0MO3mQ2LL9DC/L4WeljxJXe5RfFP2MuDd28GDSQxxLPZsjI+ZgDxnEi/u00dws7Dsexb7jUWTEVjIj9QSx4XXezpZ/Kymxti+/hJEjraCRnm7Ni6OcIsYPK5UzMzPNZp32GHbsgM8+483sNIoqPDNPkzQ38snGEF48cDaXB/6LV5quIEAMhXFTOJwyl8Mjz6EqPMkjefEVI2OqmZF6gqR+dGdWvRQcbA1YHTsWkpJ0yhFARLKMMZmdHtNA4ccqK+GVV/jyUBxbjw336KX/vXMkK7NHcXbSAX477JeMyfuE4ScPAnAiZgyHR87leNwkoOd/0DpbBCeGjQXx774XiVG1TE85Qeqwam9nZXCx2Vy/iuSQIVY1V9stNLT98yEDawr/7gKFlr38WUQExMSQVFrj8UCxaFIuAQJvfDWaW4If4Xvf3ENM9THScj8l49h6Zm1/HsH5LylVQ+M4mHoeB9POp2j4aX75DbCwPJR/l6cwPLyO1GH+MfGiYBiXUE5kaIO3s9K1eje0FdXWwske5lsLCHB99deyZRDl+rnYNFD4u7Q0Eou3eeXSC0/LJUAM/9wymmcNfG8OVEz8DtsnfofQ2lIiK3OdSieyqoBRRz9i0r43mbrnn1QNjXcEjfP8MmiUVA3xqwWjtuYOZ1baCaaOKOlstpnBq7nZ9UHKTTVEGij8XVoatuxshofXeeXDZ8HEPEQMr2WNYcUG4cZzdhMUaKgNHUZtqHPdFo/HTyVn1IUE11eRnvspo46uY9K+lUzd8xqVQxM4mHYeB1PPo3j4RL8LGv6gqVnYeCiO/UWRnDu2gPhIu7ezpHrJK4FCRB4A/gsoduz6sTHm3Q7npAAvAAmAAVYYY37nyXz6hfh4CAkhKarGa99S50/IJ0DgH5vH8Kf1p3Hj3F0EB/b+m0+DLZycUReSM+pCbPWVpOV+yqijHzF57xtM2/0qtUOiaQjyzqC2k5GpHEuezbHkM6mIHOmVPAx0pdVDeDM7nckjyjg9vahPfwPKO7zSmO0IFFXGmMe6OScJSDLGbBGRCCALuNQYs6un9LUxu4OPPuLgpwWs2T3Cq9n4eF8SL28ay5TkEm46t2/BojMtQSP5eDZiPD9lhJhm4kr2EF15DIDy8BEcSz6DY8lnkp8wnSYvBa+BLGxII+eMKSRNF8FyrSuugOi+dUX3ycZsY0wBUOB4XCkiu4ERQI+BQnWQmkrSDu9P+/yNcQWIGF7aOI5nPp7Ef39jp0uCRb0tgpxRi8gZtcgFuey7iMp8UvK/JCX/SyYceJfJ+1bSGGCjIGGao7Qxm/KIFK0eA6rrgnh/50gyYiuZM6aQobbuV2xU3uXNEsV1QAWwGbjbGFPWzfnpwCfAZGNMRRfn3AjcCJCamjrryJEjLs2zT6uvhxde4NUv0ymv9f6U1xv2J/Lil2OJCq0nJLjnD4jYcDvXnrWXyJAB3HOmg8CmOhKLtrUGjpiKowA0SwDOdAl2tXpbOBtOv4ODafM8fu2e2IKamZ1RRPrwSm9npVsiOPX36lVuKlG4LVCIyBogsZNDPwG+AE5gtT08iFXFdEMX6YQDHwMPGWP+5cy1teqpE++8wyfrmthTODBGSGcdjSXriDMzqgrb8oYRG2bnzgXbiQr1zWkvwqsKSMnfSFhNcc8nu8GIwiwSSnaxdeKVbJz+X5iAAVuZMKCF2pqIC68lLsJOXEQtseH2gVUa8rVA4SxHaWGVMWZyJ8eCgVXA+8aY3zqbpgaKTuzYQc7KHT65dsS+41H84aPJRIfWceeCbcQMhjmSXCygqYGzs57itJy3yE2cxdo5P6NOp1VxiaG2xnaBIzbcji3QO9OrB175bSTGTwKFiCQ52iAQkTuB2caYKzucI8DfgFJjzB29SV8DRScqKqh6/nVe3jjG2znpk/1Fkfx+3WQiQhq4a8E2hoXp/Eh9Mf7AO8zZ+CS1ocP44NwHKRk2zttZUi50xUPTiE7r24C77gKFt4a/PCIi20VkG3A+cCeAiCSLSEs32TnA1cA8Ecl2bIu9lF/fFxlJeFIE4T5Uz9/WmPgK7pi/naq6YB5bPY0TVdqTqC/2jr6Itxc+hZgmln5wK2MP/tvbWVI+wCuBwhhztTFmijFmqjFmSUvpwhiTb4xZ7Hi8wRgjjnOmO7Z3u09ZdSstjcR+LmTkTRmxldw5fxv2hkAeWz2N4xUaLPqiePhE/vXNZymKPY3zP/81Z296EmkeRGtkqF7TAfWDSWpqv9fR9ra04VXcuWAbDU3C42umUVjumVlx/Y09JIZ35j3OtgnfZvK+lVy85o5266Ir1ZYGisEkIYGk+AHUQ6OPUmKquXvBNowRHlszjbx+ruA3WJmAIL6Y9X3Wnv2/xJXu47L3biS+eIe3s6UGIO0jN5iIED0hkZCNTdgbXDytsoclR9dw9wVbeWLNVH67Zip3zN9OSoxOz90XBzIuoCw6g4Wf/C9LVt9GY1DPpTQjQtHwiRxOOZfDI+dQG+rZ2YmVZ3m9e6w7aK+nbhw8yOqndnPoRIS3c+ISRZUh/HbNVOoaA7lj3nadEqIfhtRVMHnv69gaeq6eDGiqZ0RhFtGVuRiE43GTOZQyl8Mj51IZ4XtdsP2Fu3o9aaAYbOrr2fHrt/ksJ87bOXGZE1VWsCirGYItyHVVawFiGBtfzpkZRUwZUaKT2HVkDDHlh0k/9gkZx9YTW5YDOBamSpnLoZRzKYvK0ClLPMhdgUKrngYbm43EsRGQ4+2MuE5suJ17Fmbz0d5kGppd1+xW3xjItrxhbM2NJczWQGZaMWeNOk768Er97AMQoSw6g7LoDL6aci0RVQWtQWPWtufJ3PZXTkaMtOa3GshEsA+JxD4kijpbJPaQKOxDorDbolof19kiBvVo9sF754PY8CnJ2D44QX2j//RliBlaz7IZh12eblMz7CmM4fODCXx2MIGPc5JJiKzhzIzjnJlRpAP/2qgMT2J768JUJaTnfkpa7gaG2ku9nbVuiWliWNkBQurKCW7qeq2MuuBwTEDPbXvNEsiWydewa/wyV2bTqzRQDEKSlkpi5FGOloZ7OysDXmAATEouY1JyGbX1gWQdjePzgwn839YM3tqazvjEk5yZcZwZKScICfbOtA0DUW3ocHaPXcLusUu8nZVeCWysI6S+nBB7OSF11jakrsL6WV9BgBPT2MecPMg5m5+kITjU6zMau4oGisEoMpLElCCODuwvegNOqK2Jc8YUcs6YQoorQ/jiUDxfHErg+c8n8PLGJmaknGB2RhETEssI9J/C2qDSFDSE6qB4qofG9zmNgKZ6Fn10H9/44hHqbREcGTnHhTn0Dg0Ug1TSpOGwVbuT9lVchJ1Lph7l4ilHOVAcyReHEsg6GsuXhxOIDKnjjPRiZmccJyWmWtszBpnmQBurz/0lF629k/nrH+C9eY9SkDDd29nqF+31NEg15xfy1/v20tSsn2Ku0tAkbM8bzpeH4tmeP4ym5gCSo6qZnXGcM9KLtT1jkBliP8mS1bcRVlvC2wue9MgEjNo9thc0UDjBGFbd/SH5RcHezolfqqoLIutIHF8ciufgiSgEw7iEcjJiK7ywbJHyluCGasYdfB8xTeRkXEjdkEi3Xi9zcTy/eKRvc6BpoFCdyvrTJrLW++4kgb6ipT1j4+F4nfV2EBKsnlUARgJx5yduYgLkFfStgUzHUahOJU2Nh/W6ZKy7tbRnXDL1qLezorwktmQvF6+5naqwBN6+4CnqhvSteqgnVzw0DXB92to3YxCLn5ZEQKBWhCjlbieGj+f9b/yKyMp8Fq27jyAnpkkZSDRQDGJBQ23Epug03Up5QkHiTNae8zPiSvew8JOfEtDkO0v6aqAY5JKmxkFYmHObzebt7Crl046kzOWT2fcwsnAz53/2ENLsG9P+axvFIDfhwjSKQtMoKHDyBU2NYK8Du/3rrbYW6uzW/ibf+MNXylv2jV7MkPpKztryNCn5GzEBrvu+HvR5Ihxw/URuGigGuagouOQSyMuDTZugqKiHFwQGQViQVcLoTH0dNDaBMWCaodnRx8M0g+HU/WpgMQYaG6Ghoc1WDw2NUF8PjQ36u3OB7RO/Q50tkuFlrv1QH3tBBu5YaUYDhQJgxAhrO3IENm+Gkr6uimkbAlpD5d8aG63gUVcPdXVflybr2mxasuzRvtHfBL7p0jRTfjgNd3TA1kCh2klLg9RUOHTIChgnT3o7R2rACQqyttBulqBtaPg6aPhIPbzLGKxg2tgA9Q0dSmeOzcfGr2mgUKcQgVGjICMDcnIgKwsqK72dK+VTgoOtLVxnKD5F2+o9Z4NorR1KS6GszKoC9DANFKpLIjBuHIwZA/v2WX+nSvmK5maoqbG26mrr54D4Ii/ydSB1VngExMVZN1BdZf0zlpZBRYX78tmGBgrVo4AAmDDB27lQqn+MsTrodQwetbWuDSDGQFUVlJe7oSQuYgWN8AhITbNKJWVlUOYIHG7ilUAhIg8A/wUUO3b92BjzbhfnBgKbgTxjzMWeyaFSyt+IwNCh1uYpjY3Wl/6TJ9tv5eXWZ3y/BQdDfLy1GQMR7ikyebNE8YQx5jEnzrsd2A24d9pFpZRysaAgGDbM2jqqqXG+uaG29usmitJSazvltSIQ4J4peQZ01ZOIjAQuAh4C7vJydpRSymV6U7qJjoakpPb7qqu/DhotAcRdi2R5M1B8X0SuwapWutsY01kF25PAvUBET4mJyI3AjQCpqamuzKdSSg04LTPrpKS4/1pum+tJRNaIyI5OtqXAM8BoYDpQADzeyesvBoqMMVnOXM8Ys8IYk2mMyYyLi3PlrSilK5+GZwAACWpJREFU1KDmthKFMWaBM+eJyLPAqk4OzQGWiMhiIASIFJEXjTH/z4XZVEop1QOvzB4rIm1r25YBOzqeY4z5kTFmpDEmHbgS+FCDhFJKeZ63phl/RES2i8g24HzgTgARSRaRTrvJKqWU8g6vNGYbY67uYn8+sLiT/R8BH7k3V0oppTqjCxcppZTqlgYKpZRS3dJAoZRSqlsaKJRSSnVLzICYd9e1RKQYONLHl8cCJ1yYHW/yl3vxl/sAvZeByF/uA/p3L2nGmE5HK/tloOgPEdlsjMn0dj5cwV/uxV/uA/ReBiJ/uQ9w371o1ZNSSqluaaBQSinVLQ0Up1rh7Qy4kL/ci7/cB+i9DET+ch/gpnvRNgqllFLd0hKFUkqpbmmgUEop1S0NFA4iskhE9orIfhG5z9v56Q8ROeyYnTdbRDZ7Oz+9ISJ/EZEiEdnRZt8wEVktIjmOnzHezKOzuriXB0Qkz/G7yXastzKgiUiKiKwTkV0islNEbnfs97nfSzf34ou/lxAR2SgiWx338nPH/gwR+dLxWfaqiNj6fS1towARCQT2ARcAucAm4CpjzC6vZqyPROQwkGmM8blBRCJyLlAFvGCMmezY9whQaox52BHEY4wx/+PNfDqji3t5AKgyxjzmzbz1hmP9mCRjzBYRiQCygEuB6/Cx30s393IFvvd7ESDMGFMlIsHABuB24C7gX8aYf4jIH4Gtxphn+nMtLVFYzgD2G2MOGmPqgX8AS72cp0HJGPMJUNph91Lgb47Hf8P6xx7wurgXn2OMKTDGbHE8rgR2AyPwwd9LN/fic4ylyvE02LEZYB7wumO/S34vGigsI4BjbZ7n4qN/PA4G+EBEskTkRm9nxgUSjDEFjseFQII3M+MC3xeRbY6qqQFfXdOWiKQDM4Av8fHfS4d7AR/8vYhIoIhkA0XAauAAcNIY0+g4xSWfZRoo/NM5xpiZwDeBWx1VIH7BWHWlvlxf+gwwGpgOFACPezc7zhORcOAN4A5jTEXbY772e+nkXnzy92KMaTLGTAdGYtWMTHDHdTRQWPKAlDbPRzr2+SRjTJ7jZxGwEusPyJcdb1ln3fGzyMv56TNjzHHHP3cz8Cw+8rtx1IG/AbxkjPmXY7dP/l46uxdf/b20MMacBNYBZwHR8v/bO7dQrYoojv8WVlr6YF568KG08iECBQvUsDJNe1CDSrygpSnmQxChRGGEdrHsJlIUFWWWtxKRsmMXUPFSpyioKFOyQCOSIsUKS7ro6mHm82y/s/fsfW4cD/x/MLD3ntlr1pr5zl4zs/dZY1bbvbRdnmVyFIHPgMHxa4FzgGnA5k7WqVWYWc/4kg4z6wmMB/ak7zrj2QzMisezgLc7UZc2UXuwRm6iC/RNfGn6CrDP3ZdnsrpcvxTZ0kX7pb+Z9Y7H5xI+xtlHcBiTY7F26Rd99RSJn8OtALoBK919aSer1CrM7GLCLALCnujrupItZrYeGE0Il/wLsBh4C9gAXEgIHz/F3c/4l8QFtowmLG84cBCYn1nnPyMxs1HAbuBr4GS8vIiwtt+l+iVhy3S6Xr8MIbys7kYY9G9w94fiM+ANoA/wBTDT3f9uU11yFEIIIVJo6UkIIUQSOQohhBBJ5CiEEEIkkaMQQgiRRI5CCCFEEjkKIYQQSeQohBBCJJGjEElizPtDZvZ4B8g+L+4DMDtRZqCZuZk1VJB3qmye7Kqy6uRUrr9AVqv1qCC7r5kdN7O7C/KT7dFedKSNOXWNNbPV7SlTlKN/uBNJzGwu8DIw2N2/b2fZ/YBfgZ3uPrqgTE9gEvCTu+8ukXeqLCGUwWmyq8qKUUUPAFuAqVXrL5DVzMaW2FRB/hpgFDDI6/6Yy9qjhfWclYlIWp/XoTbW1bUAoC6UiOho3F1JqTAB24C98XggIcTBh4SH6G/AaqB7zJ8HfAf8CXxKiGILcEGUcwz4gxD6oT8hVIJn0pKc+mt1NmSOG4H3oqx1NA14smWbya7L708Ib3Aspt3A5Yk6G2Le7Dq5Hq/lyivTI2Nns7YrszfeNzWWGVnSdrltDcwBvo31NgLD6u5tBLYSQpC02sY8+3LqybWxzqbXgOuA7sAq4NGiskrtl7T0JAqJO/+NIARNzDIC2AFsB2YC881sDPASYWS5gBD/Z7OZ9QVmEDZTeRpYCHxJiE+zKMrbR4i1szEuY/SLqVeBasOBXYQH3HTCQ7WeZrLr8k8Cmwg7gi0DhhJifZWxM8q7DTgM/ENT3KA8eWV6UNR2QN8K9tb65uoSvfPaejQhQN5B4JFY3ztm1iNz30jCLnAPtNbGkt9GjSp9CjCEEKX2A2Cruy/y6EFEB9LZnkrpzE2EjWgceCyeD4znu+P5JfF8E/BUPB4X85bG8wnARJpmIsuAMbFMv3h9R6bOJTSNTFdRMKOIZe+L57fW6ddQIDubPwD4iPDwq9X3c065U8d1bbMyXp8Rz3PllekRz4va7s6UvfFaj3jt+Zz+K2uPJzO6ZtOwzL2fZ8q3ysaEfRPK+rTOnrOB34GvyJlBKXVc0oxCVMEKzuuvQ9PmNadGee7eQJiFvE8YKW4zs+uzZTK8TgiXPA54okCfWoTS2pp5t4QeRdwFXEUYEY8n7ATWI3lHxMzuB24HFrv72hJ5LRntNmu7SMrevD5Iyc5jIU1tfgPh/UyNQ5njttpYZB9U69PLCDOo/4ATFesU7YAchUhxGDhOGElmGWFm99D0IN8BvBuPHzSz+cBc4CjwiZlNJswqfgS+ieUGENajTwKXmtkMM7vIw77lW2Pa2wbdm8kuKHc+cA1hg5dSzGwS8DBhvX2/mU0zs0EJeVX0KGy7CirV+uaHknJ5emyJedMJy0HDgWfc/WiJrJba2Bb7sgwlvMuYBrxqZl1q69WujByFKMTdTwAfA1fWZTUS9lUYC6wFXnT37cAdhBfXywmjzRvd/QjwF3AL8AIwBXgT2Oju/xKWP3oDayhfZ2+J7mWynyWMTqcS9hSuulHNFYRR/GBgfUzXFsmrYmNR2wFHKuhT65tdqUJ5erj7DsLMqBfwXNShMSGmVTaW/DZawlBgj7vvB+4FNsTd6kQHo89jRRIzm0N44TmYMOU/AGxx94mdqpgA0p/HCtFeaEYhylhL2Gx+XmcrIk7HzPoANwMr5CRER6IZhRBCiCSaUQghhEgiRyGEECKJHIUQQogkchRCCCGSyFEIIYRIIkchhBAiiRyFEEKIJP8Dxo3+bIYbjikAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 10. Visualise!\n",
    "\n",
    "title = obj_func\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(median_gp, color = 'Red')\n",
    "plt.plot(median_stp, color = 'Blue')\n",
    "\n",
    "xstar = np.arange(0, 31, step=1)\n",
    "plt.fill_between(xstar, lower_gp, upper_gp, facecolor = 'Red', alpha=0.4, label='GP ERM Regret: IQR')\n",
    "plt.fill_between(xstar, lower_stp, upper_stp, facecolor = 'Blue', alpha=0.4, label='STP ERM Regret: IQR ' r'($\\nu$' ' = {})'.format(df1))\n",
    "\n",
    "plt.title(title, weight = 'bold')\n",
    "plt.xlabel('(post-initialization) iteration $\\it{k}$', weight = 'bold') # x-axis label\n",
    "plt.ylabel('ln(Regret)', weight = 'bold') # y-axis label\n",
    "plt.legend(loc=0) # add plot legend\n",
    "\n",
    "plt.show() #visualise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
