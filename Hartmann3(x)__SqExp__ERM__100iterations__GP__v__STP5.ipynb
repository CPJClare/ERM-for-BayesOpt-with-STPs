{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Import modules:\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['text.latex.preamble']=[r'\\usepackage{amsmath}']\n",
    "plt.rcParams['text.latex.preamble'] = [r'\\boldmath']\n",
    "from matplotlib.pyplot import rc\n",
    "rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
    "rc('text', usetex=False)\n",
    "\n",
    "from collections import OrderedDict\n",
    "from numpy.linalg import slogdet\n",
    "from scipy.linalg import inv\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import gamma\n",
    "from scipy.stats import norm, t\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from pyGPGO.logger import EventLogger\n",
    "from pyGPGO.GPGO import GPGO\n",
    "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\n",
    "from pyGPGO.surrogates.tStudentProcess import tStudentProcess\n",
    "from pyGPGO.surrogates.tStudentProcess import logpdf\n",
    "from pyGPGO.acquisition import Acquisition\n",
    "from pyGPGO.covfunc import squaredExponential, matern32, matern52\n",
    "\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. User-defined - inputs:\n",
    "\n",
    "### Objective Function:\n",
    "obj_func = 'Hartmann3' # 6-D;\n",
    "\n",
    "### Data inputs:\n",
    "n_test = 50\n",
    "\n",
    "### Student-t parameter input:\n",
    "df1 = 5 # Degree(s)-of-freedom (DF)\n",
    "\n",
    "### Acquisition / Utility function - MLE/Type II:\n",
    "util_gp = 'RegretMinimized' # Gaussian MLE\n",
    "util_stp = 'tRegretMinimized' # Student-t MLE\n",
    "\n",
    "#util_gp = 'ExpectedImprovement' # Gaussian MLE\n",
    "#util_stp = 'tExpectedImprovement' # Student-t MLE\n",
    "\n",
    "### Probabilistic / Surrogate / Stochastic model - MLE/Type II: \n",
    "#surrogate_model_gp = 'Gaussian Process'\n",
    "surrogate_model_stp = 'Student-t Process'\n",
    "\n",
    "### Covariance Function:\n",
    "cov_func = squaredExponential()\n",
    "#cov_func = matern32()\n",
    "#cov_func = matern52()\n",
    "\n",
    "n_init = 5  # Number of iterations used to initialise Bayesian optimisation; minimum 2\n",
    "\n",
    "### MLE / Type II Empirical Bayes:\n",
    "optimize = False # MLE Boolean\n",
    "usegrads = False # MLE Boolean (pyGPGO not programmed for Student-t MLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Objective Function - Hartmann3(x) 3-D:\n",
    "\n",
    "if obj_func == 'Hartmann3':\n",
    "            \n",
    "    # True y bounds:\n",
    "    y_lb = -3.86278\n",
    "    operator = -1 # targets global minimum \n",
    "    y_global_orig = y_lb * operator # targets global minimum\n",
    "            \n",
    "# Constraints:\n",
    "    lb = 0\n",
    "    ub = 1\n",
    "    \n",
    "# Input array dimension(s):\n",
    "    dim = 3\n",
    "\n",
    "# 3-D inputs' parameter bounds:\n",
    "    param = {'x1_training': ('cont', [lb, ub]),\n",
    "             'x2_training': ('cont', [lb, ub]),\n",
    "             'x3_training': ('cont', [lb, ub])}\n",
    "    \n",
    "    max_iter = (10 * dim)*0 + 100  # iterations of Bayesian optimisation\n",
    "    \n",
    "# Test data:\n",
    "    x1_test = np.linspace(lb, ub, n_test) \n",
    "    x2_test = np.linspace(lb, ub, n_test)\n",
    "    x3_test = np.linspace(lb, ub, n_test)\n",
    "    Xstar_d = np.column_stack((x1_test, x2_test, x3_test))\n",
    "    \n",
    "    def f_syn_polarity(x1_training, x2_training, x3_training):\n",
    "       \n",
    "        value = np.array([x1_training, x2_training, x3_training])\n",
    "      \n",
    "        a = np.array([[3.0, 10, 30],\n",
    "                      [0.1, 10, 35],\n",
    "                      [3.0, 10, 30],\n",
    "                      [0.1, 10, 35]])\n",
    "        \n",
    "        alpha = np.array([1.0, 1.2, 3.0, 3.2])\n",
    "      \n",
    "        p = np.array([[.3689, .1170, .2673],\n",
    "                      [.4699, .4387, .7470],\n",
    "                      [.1091, .8732, .5547],\n",
    "                      [.3810, .5743, .8828]])\n",
    "  \n",
    "        s = 0\n",
    "        for i in [0,1,2,3]:\n",
    "            sm = a[i,0]*(value[0]-p[i,0])**2\n",
    "            sm += a[i,1]*(value[1]-p[i,1])**2\n",
    "            sm += a[i,2]*(value[2]-p[i,2])**2\n",
    "            s += alpha[i]*np.exp(-sm)\n",
    "        result = -s\n",
    "        \n",
    "        return operator * result\n",
    "      \n",
    "    def f_syn_transform_polarity(x1_training, x2_training, x3_training):\n",
    "            return operator * (np.sqrt(2 * (y_global_orig - f_syn_polarity(x1_training, x2_training, x3_training))))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4a. Add new acquisition functions: add CBM & ERM (Nyugen and Osborne, 2019) method .\n",
    "\n",
    "### Inherits from class Acquisition()\n",
    "\n",
    "class Acquisition_new(Acquisition):    \n",
    "    def __init__(self, mode, eps=1e-06, **params):\n",
    "        \"\"\"\n",
    "        Acquisition function class.\n",
    "        Parameters\n",
    "        ----------\n",
    "        mode: str\n",
    "            Defines the behaviour of the acquisition strategy.\n",
    "        eps: float\n",
    "            Small floating value to avoid `np.sqrt` or zero-division warnings.\n",
    "        params: float\n",
    "            Extra parameters needed for certain acquisition functions, e.g. UCB needs\n",
    "            to be supplied with `beta`.\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "        self.eps = eps\n",
    "\n",
    "        mode_dict = {\n",
    "            'ExpectedImprovement': self.ExpectedImprovement,\n",
    "            'tExpectedImprovement': self.tExpectedImprovement,\n",
    "            'RegretMinimized': self.RegretMinimized,\n",
    "            'tRegretMinimized': self.tRegretMinimized\n",
    "        }\n",
    "\n",
    "        self.f = mode_dict[mode]\n",
    "   \n",
    "    def ExpectedImprovement(self, tau, mean, std):\n",
    "        \"\"\"\n",
    "        Expected Improvement acquisition function.\n",
    "        Parameters\n",
    "        ----------\n",
    "        tau: float\n",
    "            Best observed function evaluation.\n",
    "        mean: float\n",
    "            Point mean of the posterior process.\n",
    "        std: float\n",
    "            Point std of the posterior process.\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Expected improvement.\n",
    "        \"\"\"\n",
    "        z = (mean - tau - self.eps) / (std + self.eps)\n",
    "        return (mean - tau) * norm.cdf(z) + std * norm.pdf(z)[0]\n",
    "\n",
    "\n",
    "    def RegretMinimized(self, tau, mean, std):\n",
    "        \"\"\"\n",
    "        Regret Minimized acquisition function.\n",
    "        Parameters\n",
    "        ----------\n",
    "        tau: float\n",
    "            Best observed function evaluation.\n",
    "        mean: float\n",
    "            Point mean of the posterior process.\n",
    "        std: float\n",
    "            Point std of the posterior process.\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Expected improvement.\n",
    "        \"\"\"\n",
    "        \n",
    "        z = (mean - y_global_orig - self.eps) / (std + self.eps)\n",
    "        return z * (std + self.eps) * norm.cdf(z) + std * norm.pdf(z)[0]\n",
    "    \n",
    "    \n",
    "    def tExpectedImprovement(self, tau, mean, std, nu=3.0):\n",
    "        \"\"\"\n",
    "        Expected Improvement acquisition function. Only to be used with `tStudentProcess` surrogate.\n",
    "        Parameters\n",
    "        ----------\n",
    "        tau: float\n",
    "            Best observed function evaluation.\n",
    "        mean: float\n",
    "            Point mean of the posterior process.\n",
    "        std: float\n",
    "            Point std of the posterior process.\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Expected improvement.\n",
    "        \"\"\"\n",
    "        gamma = (mean - tau - self.eps) / (std + self.eps)\n",
    "        return gamma * std * t.cdf(gamma, df=nu) + std * (1 + (gamma ** 2 - 1)/(nu - 1)) * t.pdf(gamma, df=nu)\n",
    "    \n",
    "    \n",
    "    def tRegretMinimized(self, tau, mean, std, nu=3.0):\n",
    "        \"\"\"\n",
    "        Regret Minimized acquisition function. Only to be used with `tStudentProcess` surrogate.\n",
    "        Parameters\n",
    "        ----------\n",
    "        tau: float\n",
    "            Best observed function evaluation.\n",
    "        mean: float\n",
    "            Point mean of the posterior process.\n",
    "        std: float\n",
    "            Point std of the posterior process.\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Expected improvement.\n",
    "        \"\"\"\n",
    "        \n",
    "        gamma = (mean - y_global_orig - self.eps) / (std + self.eps)\n",
    "        return gamma * (std + self.eps) * t.cdf(gamma, df=nu) + std * (nu + gamma ** 2)/(nu - 1) * t.pdf(gamma, df=nu)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4b. Re-define tStudentProcess class with non-zero prior mean function:\n",
    "\n",
    "### [Nyugen and Osborne, 2019] \"Knowing The What But Not The Where in Bayesian Optimization\"\n",
    "\n",
    "### Inherits from class tStudentProcess()\n",
    "\n",
    "class tStudentProcess_prior(tStudentProcess):\n",
    "    def __init__(self, covfunc, nu, optimize=False, mprior=0):\n",
    "        \"\"\"\n",
    "        t-Student Process regressor class.\n",
    "        This class DOES NOT support gradients in ML estimation yet.\n",
    "        Parameters\n",
    "        ----------\n",
    "        covfunc: instance from a class of covfunc module\n",
    "            An instance from a class from the `covfunc` module.\n",
    "        nu: float\n",
    "            (>2.0) Degrees of freedom\n",
    "        Attributes\n",
    "        ----------\n",
    "        covfunc: object\n",
    "            Internal covariance function.\n",
    "        nu: float\n",
    "            Degrees of freedom.\n",
    "        optimize: bool\n",
    "            Whether to optimize covariance function hyperparameters.\n",
    "        \"\"\"\n",
    "        self.covfunc = covfunc\n",
    "        self.nu = nu\n",
    "        self.optimize = optimize\n",
    "        self.mprior = mprior\n",
    "        \n",
    "    def logpdf(x, nu, Sigma):\n",
    "        \"\"\"\n",
    "        Marginal log-likelihood of a Student-t Process\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: array-like\n",
    "            Point to be evaluated\n",
    "        df: float\n",
    "            Degrees of freedom (>2.0)\n",
    "        mu: array-like\n",
    "            Mean of the process.\n",
    "        Sigma: array-like\n",
    "            Covariance matrix of the process.\n",
    "        Returns\n",
    "        -------\n",
    "        logp: float\n",
    "            log-likelihood \n",
    "        \"\"\"\n",
    "        d = len(x)\n",
    "        x = np.atleast_2d(x)\n",
    "        xm = x - self.mprior\n",
    "        V = nu * Sigma\n",
    "        V_inv = np.linalg.inv(V)\n",
    "        _, logdet = slogdet(np.pi * V)\n",
    "\n",
    "        logz = -gamma(nu / 2.0 + d / 2.0) + gamma(nu / 2.0) + 0.5 * logdet\n",
    "        logp = -0.5 * (nu + d) * np.log(1 + np.sum(np.dot(xm, V_inv) * xm, axis=1))\n",
    "\n",
    "        logp = logp - logz\n",
    "\n",
    "        return logp[0]\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits a t-Student Process regressor\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: np.ndarray, shape=(nsamples, nfeatures)\n",
    "            Training instances to fit the GP.\n",
    "        y: np.ndarray, shape=(nsamples,)\n",
    "            Corresponding continuous target values to `X`.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n1 = X.shape[0]\n",
    "\n",
    "        if self.optimize:\n",
    "            self.optHyp(param_key=self.covfunc.parameters, param_bounds=self.covfunc.bounds)\n",
    "\n",
    "        self.K11 = self.covfunc.K(self.X, self.X)\n",
    "        self.beta1 = np.dot(np.dot(self.y.T, inv(self.K11)), self.y)\n",
    "        self.logp = logpdf(self.y, self.nu, mu=self.mprior, Sigma=self.K11)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5a. Cumulative Regret Calculator:\n",
    "\n",
    "def min_max_array(x):\n",
    "    new_list = []\n",
    "    for i, num in enumerate(x):\n",
    "            new_list.append(np.min(x[0:i+1]))\n",
    "    return new_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5b. Set-seeds:\n",
    "\n",
    "run_num_1 = 111\n",
    "run_num_2 = 222\n",
    "run_num_3 = 333\n",
    "run_num_4 = 444\n",
    "run_num_5 = 555\n",
    "run_num_6 = 666\n",
    "run_num_7 = 777\n",
    "run_num_8 = 888\n",
    "run_num_9 = 999\n",
    "run_num_10 = 1000\n",
    "run_num_11 = 1111\n",
    "run_num_12 = 1222\n",
    "run_num_13 = 1333\n",
    "run_num_14 = 1444\n",
    "run_num_15 = 1555\n",
    "run_num_16 = 1666\n",
    "run_num_17 = 1777\n",
    "run_num_18 = 1888\n",
    "run_num_19 = 1999\n",
    "run_num_20 = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.61217018 0.16906975 0.43605902]. \t  0.37345684462559386 \t 2.3951473341797507\n",
      "init   \t [0.76926247 0.2953253  0.14916296]. \t  0.296162062221701 \t 2.3951473341797507\n",
      "init   \t [0.02247832 0.42022449 0.23868214]. \t  0.2904113287153621 \t 2.3951473341797507\n",
      "init   \t [0.33765619 0.99071246 0.23772645]. \t  0.11013785080555143 \t 2.3951473341797507\n",
      "init   \t [0.08119266 0.66960024 0.62124292]. \t  2.3951473341797507 \t 2.3951473341797507\n",
      "1      \t [0.         0.86482727 1.        ]. \t  0.8663385652929555 \t 2.3951473341797507\n",
      "2      \t [0.9694624  0.81729872 0.76150236]. \t  1.3879340688336017 \t 2.3951473341797507\n",
      "3      \t [0.00746842 0.39300358 0.94016277]. \t  2.339565033428072 \t 2.3951473341797507\n",
      "4      \t [0.99271916 0.97953791 0.19194934]. \t  0.0051249034030868 \t 2.3951473341797507\n",
      "5      \t [0.89539121 0.31310218 0.96935805]. \t  1.3903958518762796 \t 2.3951473341797507\n",
      "6      \t [0.43154031 0.53910525 0.95905966]. \t  \u001b[92m2.8079980209670032\u001b[0m \t 2.8079980209670032\n",
      "7      \t [0.48961894 0.53008348 0.60431267]. \t  1.3107116768279226 \t 2.8079980209670032\n",
      "8      \t [0.08393622 0.02257993 0.82276374]. \t  0.30471528635060674 \t 2.8079980209670032\n",
      "9      \t [0.85262173 0.90853957 0.974701  ]. \t  0.7859874906235119 \t 2.8079980209670032\n",
      "10     \t [0.97309633 0.19016164 0.29778079]. \t  0.3092980434831704 \t 2.8079980209670032\n",
      "11     \t [0.41453023 0.189136   0.98671538]. \t  0.5835678811098508 \t 2.8079980209670032\n",
      "12     \t [0.38250177 0.04518189 0.08113646]. \t  0.3356017180695572 \t 2.8079980209670032\n",
      "13     \t [0.13178792 0.01843852 0.37992013]. \t  0.5266166458834225 \t 2.8079980209670032\n",
      "14     \t [0.29631203 0.21583422 0.16238708]. \t  0.6420229022556283 \t 2.8079980209670032\n",
      "15     \t [0.27903694 0.29800729 0.85858851]. \t  2.1003956761271243 \t 2.8079980209670032\n",
      "16     \t [0.61325549 0.30243981 0.90825932]. \t  1.8874160789018661 \t 2.8079980209670032\n",
      "17     \t [0.2483362  0.96867155 0.62386134]. \t  2.345384616481004 \t 2.8079980209670032\n",
      "18     \t [0.75511073 0.31098202 0.11627379]. \t  0.22145948662279805 \t 2.8079980209670032\n",
      "19     \t [0.6988929  0.82224792 0.81285555]. \t  1.8184288957347945 \t 2.8079980209670032\n",
      "20     \t [0.74629129 0.39196569 0.04424157]. \t  0.06887424994128499 \t 2.8079980209670032\n",
      "21     \t [0.65530889 0.99070857 0.91077714]. \t  0.5916856864635031 \t 2.8079980209670032\n",
      "22     \t [0.085087   0.97222274 0.28108833]. \t  0.28790122550111297 \t 2.8079980209670032\n",
      "23     \t [0.83604037 0.55025856 0.68514336]. \t  1.8388292114924272 \t 2.8079980209670032\n",
      "24     \t [0.57095976 0.81673629 0.74317659]. \t  1.7102203992439355 \t 2.8079980209670032\n",
      "25     \t [0.63342171 0.0610053  0.61277941]. \t  0.1942697040063976 \t 2.8079980209670032\n",
      "26     \t [0.34353819 0.78447438 0.42476058]. \t  1.4333216348280542 \t 2.8079980209670032\n",
      "27     \t [0.51315413 0.21560717 0.66828498]. \t  0.7868197231909237 \t 2.8079980209670032\n",
      "28     \t [0.52769422 0.79558163 0.20534277]. \t  0.05118149121556392 \t 2.8079980209670032\n",
      "29     \t [0.65439246 0.40648609 0.80296072]. \t  \u001b[92m3.0000629220142074\u001b[0m \t 3.0000629220142074\n",
      "30     \t [0.31703275 0.38679538 0.87263802]. \t  2.925033424807816 \t 3.0000629220142074\n",
      "31     \t [0.20163824 0.26175231 0.28072878]. \t  0.7494070337353412 \t 3.0000629220142074\n",
      "32     \t [0.80015172 0.95292565 0.34818642]. \t  0.1877165275170739 \t 3.0000629220142074\n",
      "33     \t [0.19810728 0.12835955 0.30677975]. \t  0.8755510102119044 \t 3.0000629220142074\n",
      "34     \t [0.92713944 0.37449199 0.1992468 ]. \t  0.17686171776661236 \t 3.0000629220142074\n",
      "35     \t [0.97646817 0.72199139 0.23044572]. \t  0.01886879752069358 \t 3.0000629220142074\n",
      "36     \t [0.7132736  0.45084692 0.77419227]. \t  2.9988847266872862 \t 3.0000629220142074\n",
      "37     \t [0.62059475 0.19878146 0.39672362]. \t  0.48416330717961403 \t 3.0000629220142074\n",
      "38     \t [0.93771887 0.12502626 0.69952327]. \t  0.5349600897796973 \t 3.0000629220142074\n",
      "39     \t [0.91155269 0.4824978  0.5395311 ]. \t  0.4077509563328468 \t 3.0000629220142074\n",
      "40     \t [0.10508875 0.32971212 0.91216338]. \t  2.1021775737118262 \t 3.0000629220142074\n",
      "41     \t [0.44206035 0.23963528 0.53817297]. \t  0.32404118535996623 \t 3.0000629220142074\n",
      "42     \t [0.33387899 0.53342683 0.11634561]. \t  0.09134636202859761 \t 3.0000629220142074\n",
      "43     \t [0.10933948 0.39434738 0.48843668]. \t  0.47463495201539446 \t 3.0000629220142074\n",
      "44     \t [0.18147797 0.00427161 0.93758596]. \t  0.16191639832850643 \t 3.0000629220142074\n",
      "45     \t [0.64603072 0.98356685 0.85169221]. \t  0.6965671981222739 \t 3.0000629220142074\n",
      "46     \t [0.60435642 0.84482559 0.20250671]. \t  0.03825490005082811 \t 3.0000629220142074\n",
      "47     \t [0.31632777 0.08667041 0.17033392]. \t  0.7412096226284234 \t 3.0000629220142074\n",
      "48     \t [0.11224833 0.81636102 0.05510213]. \t  0.0032231561413997357 \t 3.0000629220142074\n",
      "49     \t [0.18856892 0.58701826 0.71088235]. \t  2.6686946187175598 \t 3.0000629220142074\n",
      "50     \t [0.27851208 0.8787004  0.92779283]. \t  1.2761988007569391 \t 3.0000629220142074\n",
      "51     \t [0.86535767 0.22722627 0.11850734]. \t  0.2176305506090892 \t 3.0000629220142074\n",
      "52     \t [0.63427989 0.87306528 0.75324288]. \t  1.306823750539479 \t 3.0000629220142074\n",
      "53     \t [0.75308494 0.52011    0.17899753]. \t  0.10370341809742749 \t 3.0000629220142074\n",
      "54     \t [0.56886202 0.04790407 0.91984523]. \t  0.2817905816351073 \t 3.0000629220142074\n",
      "55     \t [0.72579525 0.35976761 0.33217317]. \t  0.3519326230068731 \t 3.0000629220142074\n",
      "56     \t [0.96154876 0.6577064  0.49507711]. \t  0.28915685733921626 \t 3.0000629220142074\n",
      "57     \t [0.55035505 0.35769172 0.58668171]. \t  0.6870939250800232 \t 3.0000629220142074\n",
      "58     \t [0.02752459 0.61563494 0.42928929]. \t  0.9991490514822108 \t 3.0000629220142074\n",
      "59     \t [0.85815424 0.3766133  0.43959801]. \t  0.17764358099336147 \t 3.0000629220142074\n",
      "60     \t [0.66539115 0.70870415 0.14928367]. \t  0.02179028217114431 \t 3.0000629220142074\n",
      "61     \t [0.68751657 0.93769228 0.33449312]. \t  0.24729420645382472 \t 3.0000629220142074\n",
      "62     \t [0.61867025 0.21017493 0.90488381]. \t  1.1280061567678494 \t 3.0000629220142074\n",
      "63     \t [0.45139881 0.75764658 0.1536751 ]. \t  0.025809063494270642 \t 3.0000629220142074\n",
      "64     \t [0.90472964 0.9160513  0.18428633]. \t  0.007771975311035936 \t 3.0000629220142074\n",
      "65     \t [0.58464209 0.25549428 0.4077631 ]. \t  0.4304226878949818 \t 3.0000629220142074\n",
      "66     \t [0.73167071 0.04646788 0.63602023]. \t  0.20109147747427045 \t 3.0000629220142074\n",
      "67     \t [0.50159174 0.39568288 0.3605373 ]. \t  0.4049837552433268 \t 3.0000629220142074\n",
      "68     \t [0.284838   0.7672172  0.22291601]. \t  0.1034113167291186 \t 3.0000629220142074\n",
      "69     \t [0.1213989  0.52283173 0.43351905]. \t  0.6735992321504352 \t 3.0000629220142074\n",
      "70     \t [0.98892067 0.72767522 0.86826642]. \t  2.734855490734007 \t 3.0000629220142074\n",
      "71     \t [0.33554799 0.1960815  0.29285402]. \t  0.9219313064285296 \t 3.0000629220142074\n",
      "72     \t [0.98804586 0.0449322  0.30230819]. \t  0.29004380755883535 \t 3.0000629220142074\n",
      "73     \t [0.95901186 0.25076518 0.23941641]. \t  0.28784139865955954 \t 3.0000629220142074\n",
      "74     \t [0.19916068 0.6271735  0.40368678]. \t  0.8597929973599497 \t 3.0000629220142074\n",
      "75     \t [0.11165879 0.28524441 0.48207772]. \t  0.3208186689144355 \t 3.0000629220142074\n",
      "76     \t [0.34104117 0.1003848  0.35929984]. \t  0.7758886749514444 \t 3.0000629220142074\n",
      "77     \t [0.4699236  0.89020772 0.37975838]. \t  0.8113985516385133 \t 3.0000629220142074\n",
      "78     \t [0.4392073  0.78519199 0.10636872]. \t  0.010029344977685532 \t 3.0000629220142074\n",
      "79     \t [0.21961907 0.6015288  0.09956847]. \t  0.04121021759401271 \t 3.0000629220142074\n",
      "80     \t [0.95655451 0.88808101 0.58809126]. \t  0.4553290610022084 \t 3.0000629220142074\n",
      "81     \t [0.78657803 0.00727757 0.24876982]. \t  0.5199892114983524 \t 3.0000629220142074\n",
      "82     \t [0.78933727 0.90472402 0.82215516]. \t  1.126411336305204 \t 3.0000629220142074\n",
      "83     \t [0.8777519  0.14790529 0.76351869]. \t  0.8108427181301892 \t 3.0000629220142074\n",
      "84     \t [0.33935565 0.09674464 0.31095635]. \t  0.9396204683691252 \t 3.0000629220142074\n",
      "85     \t [0.43361283 0.20523394 0.62461828]. \t  0.5329662580521853 \t 3.0000629220142074\n",
      "86     \t [0.35345103 0.44407979 0.5372887 ]. \t  0.7310458138475027 \t 3.0000629220142074\n",
      "87     \t [0.39871357 0.2735528  0.87510818]. \t  1.8095336899165377 \t 3.0000629220142074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.52645571 0.75007053 0.81331628]. \t  2.5756383452991445 \t 3.0000629220142074\n",
      "89     \t [0.67162765 0.16588182 0.85316793]. \t  0.9636196694046057 \t 3.0000629220142074\n",
      "90     \t [0.1353118  0.41816142 0.57977477]. \t  0.9330594111440559 \t 3.0000629220142074\n",
      "91     \t [0.84515891 0.7984235  0.27681832]. \t  0.06007456863337144 \t 3.0000629220142074\n",
      "92     \t [0.65259893 0.40543277 0.61169084]. \t  0.9410805313922265 \t 3.0000629220142074\n",
      "93     \t [0.89712384 0.86740939 0.57199769]. \t  0.5705142395729424 \t 3.0000629220142074\n",
      "94     \t [0.10050002 0.70316347 0.1260103 ]. \t  0.02331172194873839 \t 3.0000629220142074\n",
      "95     \t [0.49266886 0.21165411 0.91177336]. \t  1.110705187350084 \t 3.0000629220142074\n",
      "96     \t [0.02291614 0.84016591 0.0677154 ]. \t  0.0034917025704461944 \t 3.0000629220142074\n",
      "97     \t [0.09047156 0.52056546 0.37167556]. \t  0.43681458744751706 \t 3.0000629220142074\n",
      "98     \t [0.30024739 0.21118253 0.95814266]. \t  0.8514100172171297 \t 3.0000629220142074\n",
      "99     \t [0.05766926 0.62078636 0.81547445]. \t  \u001b[92m3.5677144595940287\u001b[0m \t 3.5677144595940287\n",
      "100    \t [0.31882033 0.71137107 0.41807763]. \t  1.1847704621319368 \t 3.5677144595940287\n"
     ]
    }
   ],
   "source": [
    "### 6(a). Bayesian optimization runs (x20): GP run number = 1\n",
    "\n",
    "np.random.seed(run_num_1)\n",
    "surrogate_gp_1 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "gpgo_gp_1 = GPGO(surrogate_gp_1, Acquisition_new(util_gp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_1.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.61217018 0.16906975 0.43605902]. \t  0.37345684462559386 \t 2.3951473341797507\n",
      "init   \t [0.76926247 0.2953253  0.14916296]. \t  0.296162062221701 \t 2.3951473341797507\n",
      "init   \t [0.02247832 0.42022449 0.23868214]. \t  0.2904113287153621 \t 2.3951473341797507\n",
      "init   \t [0.33765619 0.99071246 0.23772645]. \t  0.11013785080555143 \t 2.3951473341797507\n",
      "init   \t [0.08119266 0.66960024 0.62124292]. \t  2.3951473341797507 \t 2.3951473341797507\n",
      "1      \t [0. 1. 1.]. \t  0.330219860606422 \t 2.3951473341797507\n",
      "2      \t [1.         0.77045317 1.        ]. \t  1.3380072484105159 \t 2.3951473341797507\n",
      "3      \t [0.         0.12567546 1.        ]. \t  0.30752227328286746 \t 2.3951473341797507\n",
      "4      \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.3951473341797507\n",
      "5      \t [1.         1.         0.29332195]. \t  0.03057630306425369 \t 2.3951473341797507\n",
      "6      \t [0.46078378 0.53914416 1.        ]. \t  2.0702885314883517 \t 2.3951473341797507\n",
      "7      \t [1. 0. 0.]. \t  0.03095471703300515 \t 2.3951473341797507\n",
      "8      \t [1.         0.43584923 0.58475565]. \t  0.6230165261191098 \t 2.3951473341797507\n",
      "9      \t [0.58576215 1.         0.82300217]. \t  0.6501349103425291 \t 2.3951473341797507\n",
      "10     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 2.3951473341797507\n",
      "11     \t [1.        0.7739125 0.       ]. \t  0.0004989192341518355 \t 2.3951473341797507\n",
      "12     \t [ 0.00000000e+00  0.00000000e+00 -5.55111512e-17]. \t  0.06797411659013224 \t 2.3951473341797507\n",
      "13     \t [0.         1.         0.50880002]. \t  2.3246596005241105 \t 2.3951473341797507\n",
      "14     \t [0.44611937 0.         1.        ]. \t  0.09171708488740994 \t 2.3951473341797507\n",
      "15     \t [0.         0.         0.52433744]. \t  0.11276500879856079 \t 2.3951473341797507\n",
      "16     \t [0.45691683 0.         0.        ]. \t  0.09989770336711608 \t 2.3951473341797507\n",
      "17     \t [0.83901734 0.36443125 1.        ]. \t  1.3666252945263095 \t 2.3951473341797507\n",
      "18     \t [0.         0.58081052 1.        ]. \t  2.0545621869001685 \t 2.3951473341797507\n",
      "19     \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.3951473341797507\n",
      "20     \t [ 7.44849005e-01  1.00000000e+00 -6.93889390e-18]. \t  0.00010598015604950489 \t 2.3951473341797507\n",
      "21     \t [1.         0.         0.41726418]. \t  0.138391195217384 \t 2.3951473341797507\n",
      "22     \t [0.732377   0.72951967 0.58063763]. \t  1.042405318728132 \t 2.3951473341797507\n",
      "23     \t [0.3212586  0.50056564 0.        ]. \t  0.0268062223708316 \t 2.3951473341797507\n",
      "24     \t [0.         0.36996584 0.75804829]. \t  2.386554616911762 \t 2.3951473341797507\n",
      "25     \t [0.79796709 0.         0.76214158]. \t  0.24209588687683847 \t 2.3951473341797507\n",
      "26     \t [1.         1.         0.71822728]. \t  0.3493058340950802 \t 2.3951473341797507\n",
      "27     \t [0.         0.64031736 0.        ]. \t  0.005204889026176055 \t 2.3951473341797507\n",
      "28     \t [0.23561532 0.32165396 0.84863214]. \t  2.3542830195465796 \t 2.3951473341797507\n",
      "29     \t [ 1.0000000e+00  3.1729948e-01 -6.9388939e-18]. \t  0.02376629226568175 \t 2.3951473341797507\n",
      "30     \t [1.00000000e+00 1.00000000e+00 3.46944695e-18]. \t  3.7727185179443916e-05 \t 2.3951473341797507\n",
      "31     \t [0.1370181  1.         0.80991375]. \t  0.8366811497279676 \t 2.3951473341797507\n",
      "32     \t [0.         0.         0.84160792]. \t  0.23519315793480194 \t 2.3951473341797507\n",
      "33     \t [2.22210610e-09 7.63344712e-01 7.87400952e-01]. \t  \u001b[92m2.496246205335159\u001b[0m \t 2.496246205335159\n",
      "34     \t [0.25850133 0.         0.6174145 ]. \t  0.12937761167921152 \t 2.496246205335159\n",
      "35     \t [0.30648526 0.94333095 1.        ]. \t  0.5232277861683667 \t 2.496246205335159\n",
      "36     \t [0.78750705 0.894904   0.99834827]. \t  0.7240048718247031 \t 2.496246205335159\n",
      "37     \t [0.99999998 0.12375193 0.68560986]. \t  0.48514569407651575 \t 2.496246205335159\n",
      "38     \t [0.68620004 0.10779815 0.        ]. \t  0.0866082849089967 \t 2.496246205335159\n",
      "39     \t [1.         0.42077792 1.        ]. \t  1.628260249609915 \t 2.496246205335159\n",
      "40     \t [0.00464866 0.87432325 0.17790738]. \t  0.042744687621556976 \t 2.496246205335159\n",
      "41     \t [0.55533528 0.51608405 0.79341357]. \t  \u001b[92m3.4625294520897447\u001b[0m \t 3.4625294520897447\n",
      "42     \t [0.34070094 0.59294943 0.75616448]. \t  3.1051474108135375 \t 3.4625294520897447\n",
      "43     \t [0.30664353 1.         0.        ]. \t  0.0002702617752443132 \t 3.4625294520897447\n",
      "44     \t [0.         0.25654108 0.        ]. \t  0.06416106085019953 \t 3.4625294520897447\n",
      "45     \t [0.56033954 0.45376926 0.77551395]. \t  3.071547811732317 \t 3.4625294520897447\n",
      "46     \t [0.7185328  0.00648171 0.17707771]. \t  0.4804394508147164 \t 3.4625294520897447\n",
      "47     \t [0.80979463 0.55297167 0.82070552]. \t  \u001b[92m3.622916096227853\u001b[0m \t 3.622916096227853\n",
      "48     \t [1.79231087e-01 4.06042952e-07 1.69835779e-01]. \t  0.5887469541610201 \t 3.622916096227853\n",
      "49     \t [0.79898275 0.46805372 0.78515241]. \t  3.158697503111249 \t 3.622916096227853\n",
      "50     \t [0.66678267 0.57174151 0.84742409]. \t  \u001b[92m3.7777423644594146\u001b[0m \t 3.7777423644594146\n",
      "51     \t [0.80577988 0.50399102 0.87560035]. \t  3.631284867854779 \t 3.7777423644594146\n",
      "52     \t [0.72461188 0.60903319 0.84996932]. \t  3.659336639079948 \t 3.7777423644594146\n",
      "53     \t [0.26212259 0.54450501 0.85321913]. \t  \u001b[92m3.856996655758139\u001b[0m \t 3.856996655758139\n",
      "54     \t [0.58134017 0.51852564 0.88473367]. \t  3.6847066403012727 \t 3.856996655758139\n",
      "55     \t [0.75238725 0.60579578 0.85803426]. \t  3.670341550308704 \t 3.856996655758139\n",
      "56     \t [0.07699087 0.50775658 0.87446323]. \t  3.7004565592772956 \t 3.856996655758139\n",
      "57     \t [0.89892828 0.58736284 0.84946496]. \t  3.6604637682021046 \t 3.856996655758139\n",
      "58     \t [0.67003893 0.51370853 0.84154449]. \t  3.7355449719342917 \t 3.856996655758139\n",
      "59     \t [0.69347949 0.54470352 0.84827183]. \t  3.7857533348717944 \t 3.856996655758139\n",
      "60     \t [0.07414375 0.54304308 0.82810303]. \t  3.7754459468420682 \t 3.856996655758139\n",
      "61     \t [0.20097007 0.56458805 0.93558455]. \t  3.1979291279668947 \t 3.856996655758139\n",
      "62     \t [0.18486576 0.5247528  0.80260636]. \t  3.6140431162896736 \t 3.856996655758139\n",
      "63     \t [0.05791946 0.5468211  0.79469008]. \t  3.5467569855900027 \t 3.856996655758139\n",
      "64     \t [0.15112422 0.57169865 0.8695877 ]. \t  3.8131284252695528 \t 3.856996655758139\n",
      "65     \t [0.77587353 0.51426965 0.80072071]. \t  3.4510627829451606 \t 3.856996655758139\n",
      "66     \t [0.15043162 0.60970521 0.85780357]. \t  3.7470332461824105 \t 3.856996655758139\n",
      "67     \t [0.48825941 0.54786943 0.84111149]. \t  3.825718612689603 \t 3.856996655758139\n",
      "68     \t [0.50699119 0.56642632 0.82044433]. \t  3.718089006813016 \t 3.856996655758139\n",
      "69     \t [0.33613559 0.54255644 0.83945141]. \t  3.8386553458036796 \t 3.856996655758139\n",
      "70     \t [0.38721004 0.57612644 0.92320677]. \t  3.373790110281817 \t 3.856996655758139\n",
      "71     \t [0.87121634 0.51465973 0.8645512 ]. \t  3.675271968418247 \t 3.856996655758139\n",
      "72     \t [0.57211507 0.59941331 0.88418629]. \t  3.6759366690353437 \t 3.856996655758139\n",
      "73     \t [0.23277252 0.57523029 0.81683594]. \t  3.7269259582957996 \t 3.856996655758139\n",
      "74     \t [0.08670431 0.56884321 0.87902806]. \t  3.762783044167821 \t 3.856996655758139\n",
      "75     \t [0.61272452 0.59809047 0.85115516]. \t  3.7384110191989857 \t 3.856996655758139\n",
      "76     \t [0.72699877 0.57082533 0.88398842]. \t  3.6952669608346023 \t 3.856996655758139\n",
      "77     \t [0.16337384 0.55157081 0.84533827]. \t  3.8465186063083476 \t 3.856996655758139\n",
      "78     \t [0.67565162 0.62323581 0.80546915]. \t  3.3595103007960563 \t 3.856996655758139\n",
      "79     \t [0.18477608 0.54516106 0.86883026]. \t  3.8234603586733824 \t 3.856996655758139\n",
      "80     \t [0.41368788 0.56729512 0.83319442]. \t  3.8051167767258494 \t 3.856996655758139\n",
      "81     \t [0.43129676 0.5334239  0.82135133]. \t  3.742661626962778 \t 3.856996655758139\n",
      "82     \t [0.44806228 0.58681829 0.86771351]. \t  3.7963371090209606 \t 3.856996655758139\n",
      "83     \t [0.49917339 0.45547393 0.90041582]. \t  3.2789746267198554 \t 3.856996655758139\n",
      "84     \t [0.0864966  0.57036396 0.81728976]. \t  3.717182532635504 \t 3.856996655758139\n",
      "85     \t [0.53409613 0.56246123 0.84181688]. \t  3.8133836982771534 \t 3.856996655758139\n",
      "86     \t [0.23332233 0.62877131 0.83387494]. \t  3.641124651149619 \t 3.856996655758139\n",
      "87     \t [0.83826163 0.55417167 0.85918682]. \t  3.7413436192615395 \t 3.856996655758139\n",
      "88     \t [0.47366281 0.57681019 0.76499252]. \t  3.1694053282071843 \t 3.856996655758139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.13983935 0.60982176 0.8363651 ]. \t  3.7212576581323322 \t 3.856996655758139\n",
      "90     \t [0.18475198 0.62537676 0.85740846]. \t  3.686725681417821 \t 3.856996655758139\n",
      "91     \t [0.70822321 0.5796581  0.85518904]. \t  3.759830622179515 \t 3.856996655758139\n",
      "92     \t [0.13551095 0.6147752  0.86099234]. \t  3.7217456557246953 \t 3.856996655758139\n",
      "93     \t [0.39429329 0.5761656  0.87345389]. \t  3.8035064243724515 \t 3.856996655758139\n",
      "94     \t [0.03374698 0.59886308 0.8778744 ]. \t  3.700730318087712 \t 3.856996655758139\n",
      "95     \t [0.78403425 0.56597061 0.87503343]. \t  3.7226708827540422 \t 3.856996655758139\n",
      "96     \t [0.24940303 0.48230424 0.79459449]. \t  3.4271358926072937 \t 3.856996655758139\n",
      "97     \t [0.17092155 0.54018489 0.81752972]. \t  3.7374931594371636 \t 3.856996655758139\n",
      "98     \t [0.35321781 0.54229618 0.81064778]. \t  3.691339919948006 \t 3.856996655758139\n",
      "99     \t [0.05766922 0.62078601 0.81547454]. \t  3.567716407624374 \t 3.856996655758139\n",
      "100    \t [0.33421112 0.49737904 0.8374782 ]. \t  3.732271801975862 \t 3.856996655758139\n"
     ]
    }
   ],
   "source": [
    "### 6(a). Bayesian optimization runs (x20): STP DF1 run number = 1\n",
    "\n",
    "np.random.seed(run_num_1)\n",
    "surrogate_stp_df1_1 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_1 = GPGO(surrogate_stp_df1_1, Acquisition_new(util_stp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_1.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.2205577764509106, -5.152773175058427)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(a). Training Regret Minimisation: run number = 1\n",
    "\n",
    "gp_output_1 = np.append(np.max(gpgo_gp_1.GP.y[0:n_init]),gpgo_gp_1.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_1 = np.append(np.max(gpgo_stp_df1_1.GP.y[0:n_init]),gpgo_stp_df1_1.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_1 = np.log(y_global_orig - gp_output_1)\n",
    "regret_stp_df1_1 = np.log(y_global_orig - stp_df1_output_1)\n",
    "\n",
    "train_regret_gp_1 = min_max_array(regret_gp_1)\n",
    "train_regret_stp_df1_1 = min_max_array(regret_stp_df1_1)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 1\n",
    "min_train_regret_gp_1 = min(train_regret_gp_1)\n",
    "min_train_regret_stp_df1_1 = min(train_regret_stp_df1_1)\n",
    "\n",
    "min_train_regret_gp_1, min_train_regret_stp_df1_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.52602843 0.68531719 0.83567419]. \t  3.224923929499112 \t 3.224923929499112\n",
      "init   \t [0.65376526 0.03702195 0.18015566]. \t  0.5855524570206746 \t 3.224923929499112\n",
      "init   \t [0.76056942 0.22430462 0.73513507]. \t  1.1851880051371382 \t 3.224923929499112\n",
      "init   \t [0.28749046 0.73278001 0.10764748]. \t  0.015865788013002657 \t 3.224923929499112\n",
      "init   \t [0.09385216 0.78318907 0.51959739]. \t  2.745038971968515 \t 3.224923929499112\n",
      "1      \t [0. 1. 1.]. \t  0.330219860606422 \t 3.224923929499112\n",
      "2      \t [0.         0.16632652 0.58367067]. \t  0.2970939654230494 \t 3.224923929499112\n",
      "3      \t [0.7136153  1.         0.74262439]. \t  0.6066905443016801 \t 3.224923929499112\n",
      "4      \t [0.35130055 0.67793646 0.64016205]. \t  2.2003797354774415 \t 3.224923929499112\n",
      "5      \t [0.84206795 0.66050508 1.        ]. \t  1.8762732128158859 \t 3.224923929499112\n",
      "6      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.224923929499112\n",
      "7      \t [0.67734943 0.         1.        ]. \t  0.0910285601967416 \t 3.224923929499112\n",
      "8      \t [0.         0.36855587 0.        ]. \t  0.04141934682525585 \t 3.224923929499112\n",
      "9      \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.224923929499112\n",
      "10     \t [0.40826445 0.95926939 0.94169291]. \t  0.6888324436517632 \t 3.224923929499112\n",
      "11     \t [0.48081132 0.52752716 1.        ]. \t  2.053454746268059 \t 3.224923929499112\n",
      "12     \t [0.91464797 0.70538308 0.04787513]. \t  0.0031733385212958575 \t 3.224923929499112\n",
      "13     \t [0.00949011 0.94075138 0.45072523]. \t  2.0173741236496294 \t 3.224923929499112\n",
      "14     \t [0.48502313 0.07041087 0.86123701]. \t  0.44424601739724556 \t 3.224923929499112\n",
      "15     \t [0.00341482 0.69181859 0.94610726]. \t  2.56369413855202 \t 3.224923929499112\n",
      "16     \t [0.12463787 0.60335743 0.76404596]. \t  3.208022582275852 \t 3.224923929499112\n",
      "17     \t [0.63562884 0.37474211 0.21901071]. \t  0.39140319981627697 \t 3.224923929499112\n",
      "18     \t [0.06814596 0.59943673 0.6280838 ]. \t  2.0827655361095423 \t 3.224923929499112\n",
      "19     \t [1.         0.67110258 0.65983872]. \t  1.145811683804232 \t 3.224923929499112\n",
      "20     \t [0.77850122 0.28389911 0.        ]. \t  0.053648223033446456 \t 3.224923929499112\n",
      "21     \t [0.01350013 0.85800373 0.76663837]. \t  1.8369484395364406 \t 3.224923929499112\n",
      "22     \t [0.19527371 0.67171596 0.94475929]. \t  2.7320358320827944 \t 3.224923929499112\n",
      "23     \t [0. 0. 1.]. \t  0.0902894676548261 \t 3.224923929499112\n",
      "24     \t [0.33596922 0.695014   0.42434653]. \t  1.1588977333402906 \t 3.224923929499112\n",
      "25     \t [0.7398862  0.77055275 0.67075155]. \t  1.3150259844591248 \t 3.224923929499112\n",
      "26     \t [0.40499818 0.85418882 0.78408864]. \t  1.716986932133803 \t 3.224923929499112\n",
      "27     \t [1.         1.         0.94732162]. \t  0.4492927702486861 \t 3.224923929499112\n",
      "28     \t [1.         0.42585969 0.05761295]. \t  0.031206247829179398 \t 3.224923929499112\n",
      "29     \t [0.17736474 0.11437627 0.10306241]. \t  0.39880065346637084 \t 3.224923929499112\n",
      "30     \t [0.00155165 0.77690659 0.19492725]. \t  0.061703901485432144 \t 3.224923929499112\n",
      "31     \t [0.65557083 0.476947   0.9003737 ]. \t  \u001b[92m3.3821255816151727\u001b[0m \t 3.3821255816151727\n",
      "32     \t [0.08987027 0.65155095 0.91660688]. \t  3.1827265079974145 \t 3.3821255816151727\n",
      "33     \t [0.94311059 0.94349388 0.23208651]. \t  0.016004822715217765 \t 3.3821255816151727\n",
      "34     \t [0.68664934 0.3856685  0.64945096]. \t  1.245291137859965 \t 3.3821255816151727\n",
      "35     \t [0.08778952 0.44119934 0.38184904]. \t  0.38670197953672336 \t 3.3821255816151727\n",
      "36     \t [0.76858181 0.57403703 0.67369193]. \t  1.7207559454923649 \t 3.3821255816151727\n",
      "37     \t [0.33945169 0.87527288 0.70994165]. \t  1.8657918821944248 \t 3.3821255816151727\n",
      "38     \t [0.25751417 0.17247047 0.07851639]. \t  0.3207492356109673 \t 3.3821255816151727\n",
      "39     \t [0.60320244 0.06023291 0.75186474]. \t  0.4114388625605052 \t 3.3821255816151727\n",
      "40     \t [0.5738723  0.55353908 0.22707465]. \t  0.14754580473919207 \t 3.3821255816151727\n",
      "41     \t [0.63649055 0.14967659 0.39513935]. \t  0.49898356601725347 \t 3.3821255816151727\n",
      "42     \t [0.35347595 0.381024   0.9447087 ]. \t  2.2232957810815464 \t 3.3821255816151727\n",
      "43     \t [0.85941265 0.33385241 0.49947355]. \t  0.2221112138355711 \t 3.3821255816151727\n",
      "44     \t [0.35652651 0.2551975  0.33299507]. \t  0.7401731216461875 \t 3.3821255816151727\n",
      "45     \t [0.82177956 0.69354565 0.52182825]. \t  0.5944912820124095 \t 3.3821255816151727\n",
      "46     \t [0.20553955 0.4154865  0.58829551]. \t  0.9742592957601619 \t 3.3821255816151727\n",
      "47     \t [0.63622771 0.63621474 0.28898836]. \t  0.1436340371084973 \t 3.3821255816151727\n",
      "48     \t [0.07739229 0.27530566 0.04361215]. \t  0.1344748294211738 \t 3.3821255816151727\n",
      "49     \t [0.76024826 0.34812979 0.09803338]. \t  0.15683672350742572 \t 3.3821255816151727\n",
      "50     \t [0.79007309 0.44329455 0.74481246]. \t  2.5884915470406176 \t 3.3821255816151727\n",
      "51     \t [0.25419586 0.0276046  0.95612062]. \t  0.18098402865528798 \t 3.3821255816151727\n",
      "52     \t [0.24469018 0.61771928 0.87825347]. \t  \u001b[92m3.6702491470308027\u001b[0m \t 3.6702491470308027\n",
      "53     \t [0.62615361 0.41101627 0.76022685]. \t  2.6657140921132676 \t 3.6702491470308027\n",
      "54     \t [0.58515042 0.35566734 0.24937076]. \t  0.4935300394247305 \t 3.6702491470308027\n",
      "55     \t [0.8840954  0.20702956 0.94825935]. \t  0.8641184094305842 \t 3.6702491470308027\n",
      "56     \t [0.35237796 0.85700745 0.39376341]. \t  1.1574727210294813 \t 3.6702491470308027\n",
      "57     \t [0.86775468 0.54575366 0.1672563 ]. \t  0.0578779683882728 \t 3.6702491470308027\n",
      "58     \t [0.15690466 0.42721064 0.33416099]. \t  0.38980058768968673 \t 3.6702491470308027\n",
      "59     \t [0.18803751 0.75803518 0.22973877]. \t  0.1228383928996438 \t 3.6702491470308027\n",
      "60     \t [0.12398893 0.57893632 0.11000933]. \t  0.050420411522196784 \t 3.6702491470308027\n",
      "61     \t [0.36393743 0.15560536 0.6630096 ]. \t  0.5414257910872503 \t 3.6702491470308027\n",
      "62     \t [0.59115004 0.62308882 0.18061097]. \t  0.06515392602919333 \t 3.6702491470308027\n",
      "63     \t [0.13687477 0.05862078 0.74768394]. \t  0.39951107033195704 \t 3.6702491470308027\n",
      "64     \t [0.79096641 0.76810502 0.36144264]. \t  0.22600404898738813 \t 3.6702491470308027\n",
      "65     \t [0.67309763 0.80569127 0.53626753]. \t  1.1868127046780126 \t 3.6702491470308027\n",
      "66     \t [0.92791101 0.61945101 0.64115533]. \t  1.1358303125333447 \t 3.6702491470308027\n",
      "67     \t [0.2719489  0.36250173 0.4213103 ]. \t  0.40960326941803965 \t 3.6702491470308027\n",
      "68     \t [0.04170105 0.7687367  0.17815753]. \t  0.04589183601968321 \t 3.6702491470308027\n",
      "69     \t [0.7569151  0.49992428 0.94109872]. \t  2.9595576020731387 \t 3.6702491470308027\n",
      "70     \t [0.47947045 0.79739664 0.98454753]. \t  1.4060614237817395 \t 3.6702491470308027\n",
      "71     \t [0.51323393 0.11344288 0.5118943 ]. \t  0.22476762381393295 \t 3.6702491470308027\n",
      "72     \t [0.15283198 0.89615871 0.67492179]. \t  2.294386249749974 \t 3.6702491470308027\n",
      "73     \t [0.21403219 0.02216772 0.92030081]. \t  0.2176308585226132 \t 3.6702491470308027\n",
      "74     \t [0.53984055 0.82356104 0.32670266]. \t  0.358897480494095 \t 3.6702491470308027\n",
      "75     \t [0.45844122 0.59148814 0.87909955]. \t  \u001b[92m3.743022291423356\u001b[0m \t 3.743022291423356\n",
      "76     \t [0.0923824  0.96993046 0.30700232]. \t  0.43387124984388 \t 3.743022291423356\n",
      "77     \t [0.01222418 0.34742323 0.97437503]. \t  1.5847562559887505 \t 3.743022291423356\n",
      "78     \t [0.0733937  0.20847175 0.45811363]. \t  0.30361041792910143 \t 3.743022291423356\n",
      "79     \t [0.34492577 0.13395999 0.13721664]. \t  0.5992039913249965 \t 3.743022291423356\n",
      "80     \t [0.10184398 0.66174526 0.16409439]. \t  0.049898178094504794 \t 3.743022291423356\n",
      "81     \t [0.81748137 0.61121763 0.08526748]. \t  0.01804401539233174 \t 3.743022291423356\n",
      "82     \t [0.47577702 0.80651082 0.2205715 ]. \t  0.07512450231849598 \t 3.743022291423356\n",
      "83     \t [0.34526946 0.25685233 0.33120074]. \t  0.7410780404716316 \t 3.743022291423356\n",
      "84     \t [0.62126842 0.20900034 0.9356641 ]. \t  0.9631203330957123 \t 3.743022291423356\n",
      "85     \t [0.53701169 0.65866666 0.29393705]. \t  0.1905560574984148 \t 3.743022291423356\n",
      "86     \t [0.81420474 0.32509377 0.88655369]. \t  2.2152389266721944 \t 3.743022291423356\n",
      "87     \t [0.18060749 0.79281205 0.87800911]. \t  2.2821759115591447 \t 3.743022291423356\n",
      "88     \t [0.62599414 0.09174589 0.50492766]. \t  0.20084017874219262 \t 3.743022291423356\n",
      "89     \t [0.14881789 0.56011499 0.02973387]. \t  0.022614841809741933 \t 3.743022291423356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.76821153 0.51587517 0.81728031]. \t  3.5930061371538042 \t 3.743022291423356\n",
      "91     \t [0.96645967 0.1587612  0.88166805]. \t  0.8334989128506818 \t 3.743022291423356\n",
      "92     \t [0.15561261 0.07593028 0.96635313]. \t  0.2672322275701534 \t 3.743022291423356\n",
      "93     \t [0.27296138 0.89042181 0.48567342]. \t  2.411592501709204 \t 3.743022291423356\n",
      "94     \t [0.87345409 0.93913188 0.31161207]. \t  0.08520323240628584 \t 3.743022291423356\n",
      "95     \t [0.60913689 0.99516506 0.13754537]. \t  0.006826552412049399 \t 3.743022291423356\n",
      "96     \t [0.44252475 0.49994431 0.22823292]. \t  0.23875974433597896 \t 3.743022291423356\n",
      "97     \t [0.3073207  0.50090373 0.66132716]. \t  1.911208801117378 \t 3.743022291423356\n",
      "98     \t [0.99095956 0.1429846  0.56498283]. \t  0.18994732210356183 \t 3.743022291423356\n",
      "99     \t [0.55308075 0.3500606  0.01959509]. \t  0.08328961131340018 \t 3.743022291423356\n",
      "100    \t [0.82171241 0.23616843 0.51541852]. \t  0.21400579157084154 \t 3.743022291423356\n"
     ]
    }
   ],
   "source": [
    "### 6(b). Bayesian optimization runs (x20): GP run number = 2\n",
    "\n",
    "np.random.seed(run_num_2)\n",
    "surrogate_gp_2 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "gpgo_gp_2 = GPGO(surrogate_gp_2, Acquisition_new(util_gp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_2.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.52602843 0.68531719 0.83567419]. \t  3.224923929499112 \t 3.224923929499112\n",
      "init   \t [0.65376526 0.03702195 0.18015566]. \t  0.5855524570206746 \t 3.224923929499112\n",
      "init   \t [0.76056942 0.22430462 0.73513507]. \t  1.1851880051371382 \t 3.224923929499112\n",
      "init   \t [0.28749046 0.73278001 0.10764748]. \t  0.015865788013002657 \t 3.224923929499112\n",
      "init   \t [0.09385216 0.78318907 0.51959739]. \t  2.745038971968515 \t 3.224923929499112\n",
      "1      \t [0. 1. 1.]. \t  0.330219860606422 \t 3.224923929499112\n",
      "2      \t [0.         0.         0.61961303]. \t  0.1226327058266145 \t 3.224923929499112\n",
      "3      \t [1.         1.         0.78596933]. \t  0.4570515388446086 \t 3.224923929499112\n",
      "4      \t [ 0.00000000e+00  0.00000000e+00 -1.38777878e-17]. \t  0.06797411659013229 \t 3.224923929499112\n",
      "5      \t [0.40977364 1.         0.65608584]. \t  1.5557436000353566 \t 3.224923929499112\n",
      "6      \t [1.         0.49458523 0.        ]. \t  0.008537503547999802 \t 3.224923929499112\n",
      "7      \t [0.28705421 0.53048164 0.62236768]. \t  1.6690926371315682 \t 3.224923929499112\n",
      "8      \t [0.84562324 0.64942473 1.        ]. \t  1.9118030818962715 \t 3.224923929499112\n",
      "9      \t [ 0.00000000e+00  1.00000000e+00 -5.55111512e-17]. \t  0.0002735367680454458 \t 3.224923929499112\n",
      "10     \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.224923929499112\n",
      "11     \t [0.48895201 0.         1.        ]. \t  0.09166306784871898 \t 3.224923929499112\n",
      "12     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 3.224923929499112\n",
      "13     \t [0.         0.50168548 1.        ]. \t  1.9718988911462219 \t 3.224923929499112\n",
      "14     \t [1.         0.72327088 0.46650811]. \t  0.21653759089621594 \t 3.224923929499112\n",
      "15     \t [0.57195493 0.90329001 1.        ]. \t  0.6867298644155502 \t 3.224923929499112\n",
      "16     \t [0.        0.815422  0.7071965]. \t  2.2614840040033353 \t 3.224923929499112\n",
      "17     \t [0.         1.         0.37658369]. \t  0.9522401158850051 \t 3.224923929499112\n",
      "18     \t [1. 0. 1.]. \t  0.08848201872702738 \t 3.224923929499112\n",
      "19     \t [0. 0. 1.]. \t  0.0902894676548261 \t 3.224923929499112\n",
      "20     \t [0.         0.57497625 0.        ]. \t  0.009686158814567986 \t 3.224923929499112\n",
      "21     \t [1.         0.         0.51157327]. \t  0.06959041858898804 \t 3.224923929499112\n",
      "22     \t [0.22844337 0.70813944 1.        ]. \t  1.7174150160772692 \t 3.224923929499112\n",
      "23     \t [0.62835064 0.76867134 0.61981094]. \t  1.4774567243988659 \t 3.224923929499112\n",
      "24     \t [0.39445271 0.         0.        ]. \t  0.10204652770821096 \t 3.224923929499112\n",
      "25     \t [1.         0.64416199 0.83457883]. \t  \u001b[92m3.304276905748383\u001b[0m \t 3.304276905748383\n",
      "26     \t [0.6134682  0.         0.66771352]. \t  0.1699450386551747 \t 3.304276905748383\n",
      "27     \t [0.15457259 0.         0.24856293]. \t  0.751957126686994 \t 3.304276905748383\n",
      "28     \t [0.         0.38041915 0.3500251 ]. \t  0.3478201322215337 \t 3.304276905748383\n",
      "29     \t [0.54206991 1.         0.        ]. \t  0.00018666443579929808 \t 3.304276905748383\n",
      "30     \t [1.         0.34836103 0.99999999]. \t  1.2574171766154028 \t 3.304276905748383\n",
      "31     \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.304276905748383\n",
      "32     \t [0.82330938 0.77486885 0.        ]. \t  0.0008904501650408085 \t 3.304276905748383\n",
      "33     \t [0.20574834 0.28883858 0.88798219]. \t  1.88915569501773 \t 3.304276905748383\n",
      "34     \t [0.15908428 0.99999996 0.73022553]. \t  1.2866921577210888 \t 3.304276905748383\n",
      "35     \t [0.01196746 0.74764884 0.2408996 ]. \t  0.14242194654648035 \t 3.304276905748383\n",
      "36     \t [0.53689965 0.33561066 1.        ]. \t  1.231579691959532 \t 3.304276905748383\n",
      "37     \t [0.75858668 0.16561276 0.        ]. \t  0.0726087266988515 \t 3.304276905748383\n",
      "38     \t [0.09946307 0.20943184 0.00074717]. \t  0.08762442320411769 \t 3.304276905748383\n",
      "39     \t [2.16970381e-01 9.99999985e-01 5.69105035e-08]. \t  0.00028665945172427705 \t 3.304276905748383\n",
      "40     \t [0.26798306 0.81280099 0.68718298]. \t  2.318154892213493 \t 3.304276905748383\n",
      "41     \t [0.82797601 0.99064711 0.24992022]. \t  0.03443581674753675 \t 3.304276905748383\n",
      "42     \t [0.92387678 0.47317051 0.84795377]. \t  \u001b[92m3.507684307700127\u001b[0m \t 3.507684307700127\n",
      "43     \t [1.00000000e+00 7.73305353e-08 8.40632760e-01]. \t  0.23223322809942815 \t 3.507684307700127\n",
      "44     \t [0.01588451 0.49004362 0.8215483 ]. \t  \u001b[92m3.6013824861955737\u001b[0m \t 3.6013824861955737\n",
      "45     \t [0.87123255 0.55060868 0.87481428]. \t  \u001b[92m3.6963131929877973\u001b[0m \t 3.6963131929877973\n",
      "46     \t [0.8361864  0.53940006 0.84224717]. \t  \u001b[92m3.7189014549202817\u001b[0m \t 3.7189014549202817\n",
      "47     \t [0.01815901 0.62090423 0.78227839]. \t  3.304830069721738 \t 3.7189014549202817\n",
      "48     \t [0.98761736 0.27929007 0.30777228]. \t  0.23448272339638257 \t 3.7189014549202817\n",
      "49     \t [0.92714947 0.42897262 0.75680023]. \t  2.6293021132490355 \t 3.7189014549202817\n",
      "50     \t [0.7952016  0.76586564 0.86666667]. \t  2.441347445323982 \t 3.7189014549202817\n",
      "51     \t [0.82853964 0.         0.98288851]. \t  0.10627799052180631 \t 3.7189014549202817\n",
      "52     \t [0.40348727 0.99204337 0.1926006 ]. \t  0.03971673645181267 \t 3.7189014549202817\n",
      "53     \t [0.99541722 0.53602344 0.89285149]. \t  3.5331789313078694 \t 3.7189014549202817\n",
      "54     \t [0.58242741 0.56625754 0.85366338]. \t  \u001b[92m3.816589796700109\u001b[0m \t 3.816589796700109\n",
      "55     \t [0.55618287 0.39902937 0.06058198]. \t  0.11286264134719878 \t 3.816589796700109\n",
      "56     \t [0.5461439  0.53411354 0.85648867]. \t  \u001b[92m3.819588273867835\u001b[0m \t 3.819588273867835\n",
      "57     \t [0.94584982 0.61524349 0.84768623]. \t  3.5360288253036307 \t 3.819588273867835\n",
      "58     \t [0.5281055  0.57416255 0.81012047]. \t  3.6249709315829817 \t 3.819588273867835\n",
      "59     \t [0.928586   0.61095359 0.81296584]. \t  3.3603936635518004 \t 3.819588273867835\n",
      "60     \t [0.73626156 0.52116033 0.86187305]. \t  3.7423424698383734 \t 3.819588273867835\n",
      "61     \t [0.6513846  0.46533632 0.83333608]. \t  3.5273384442323064 \t 3.819588273867835\n",
      "62     \t [0.97325577 0.46511608 0.79133128]. \t  3.1421970769711045 \t 3.819588273867835\n",
      "63     \t [0.98713105 0.59111176 0.83968946]. \t  3.5794990500854587 \t 3.819588273867835\n",
      "64     \t [0.71706293 0.54424916 0.84340448]. \t  3.7680433289831328 \t 3.819588273867835\n",
      "65     \t [0.05587323 0.52904138 0.73218359]. \t  2.8350227176416545 \t 3.819588273867835\n",
      "66     \t [0.80089306 0.5870755  0.84047978]. \t  3.6768199366535996 \t 3.819588273867835\n",
      "67     \t [0.73994626 0.54521123 0.8252677 ]. \t  3.6825379086198486 \t 3.819588273867835\n",
      "68     \t [0.97894355 0.55771403 0.89692849]. \t  3.523198694897102 \t 3.819588273867835\n",
      "69     \t [0.78337055 0.49557621 0.87142031]. \t  3.6243107461026423 \t 3.819588273867835\n",
      "70     \t [0.46497253 0.52148905 0.86408992]. \t  3.799087317519076 \t 3.819588273867835\n",
      "71     \t [0.68197875 0.54844547 0.90083276]. \t  3.5877049193369097 \t 3.819588273867835\n",
      "72     \t [0.61759944 0.51343327 0.85055929]. \t  3.7622184911189693 \t 3.819588273867835\n",
      "73     \t [0.69891638 0.57070622 0.90357385]. \t  3.5561025555846926 \t 3.819588273867835\n",
      "74     \t [0.89150849 0.5597492  0.87848207]. \t  3.6727320460647235 \t 3.819588273867835\n",
      "75     \t [0.63085453 0.51267281 0.82464177]. \t  3.6779982459338947 \t 3.819588273867835\n",
      "76     \t [0.69529653 0.46106248 0.86000708]. \t  3.508729561868949 \t 3.819588273867835\n",
      "77     \t [0.84075966 0.51570029 0.87534613]. \t  3.6558146293964624 \t 3.819588273867835\n",
      "78     \t [0.70245019 0.56205058 0.85150731]. \t  3.783429509551843 \t 3.819588273867835\n",
      "79     \t [0.746152   0.51346196 0.8993909 ]. \t  3.5201687495545895 \t 3.819588273867835\n",
      "80     \t [0.81857874 0.52516917 0.83075646]. \t  3.667941201039601 \t 3.819588273867835\n",
      "81     \t [0.65026132 0.55540475 0.85970498]. \t  3.8040428833583277 \t 3.819588273867835\n",
      "82     \t [0.57497673 0.61307462 0.77590022]. \t  3.1473287555862437 \t 3.819588273867835\n",
      "83     \t [0.66949256 0.47639752 0.87941091]. \t  3.5304495741363726 \t 3.819588273867835\n",
      "84     \t [0.71244438 0.57346836 0.79628747]. \t  3.42054728092127 \t 3.819588273867835\n",
      "85     \t [0.63372877 0.53468743 0.85669786]. \t  3.799730917216035 \t 3.819588273867835\n",
      "86     \t [0.74297133 0.48442546 0.84055288]. \t  3.6126095565384064 \t 3.819588273867835\n",
      "87     \t [0.81841651 0.62643592 0.86114948]. \t  3.5551917563000153 \t 3.819588273867835\n",
      "88     \t [0.41122123 0.49290956 0.90662425]. \t  3.426281227444028 \t 3.819588273867835\n",
      "89     \t [0.66687858 0.57218669 0.84495884]. \t  3.771647198270471 \t 3.819588273867835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.76821153 0.51587517 0.81728031]. \t  3.5930061371538042 \t 3.819588273867835\n",
      "91     \t [0.50432738 0.45208135 0.8569793 ]. \t  3.493225429813474 \t 3.819588273867835\n",
      "92     \t [0.42088313 0.59786771 0.85320797]. \t  3.785688934480701 \t 3.819588273867835\n",
      "93     \t [0.32578215 0.64731621 0.86735065]. \t  3.557808661859921 \t 3.819588273867835\n",
      "94     \t [0.50581327 0.52864558 0.81672848]. \t  3.691973153921402 \t 3.819588273867835\n",
      "95     \t [0.42830048 0.60252064 0.85506938]. \t  3.7701095813706837 \t 3.819588273867835\n",
      "96     \t [0.56855353 0.52546251 0.87860766]. \t  3.738552839958162 \t 3.819588273867835\n",
      "97     \t [0.70751439 0.50370855 0.80882065]. \t  3.525439570632387 \t 3.819588273867835\n",
      "98     \t [0.64921217 0.53397061 0.90853479]. \t  3.5016991279832657 \t 3.819588273867835\n",
      "99     \t [0.55306673 0.65391424 0.78160353]. \t  3.0353314821060335 \t 3.819588273867835\n",
      "100    \t [0.38197263 0.47317488 0.8798675 ]. \t  3.547136917783297 \t 3.819588273867835\n"
     ]
    }
   ],
   "source": [
    "### 6(b). Bayesian optimization runs (x20): STP DF1 run number = 2\n",
    "\n",
    "np.random.seed(run_num_2)\n",
    "surrogate_stp_df1_2 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_2 = GPGO(surrogate_stp_df1_2, Acquisition_new(util_stp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_2.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.1222846725153683, -3.14210632679368)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(b). Training Regret Minimisation: run number = 2\n",
    "\n",
    "gp_output_2 = np.append(np.max(gpgo_gp_2.GP.y[0:n_init]),gpgo_gp_2.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_2 = np.append(np.max(gpgo_stp_df1_2.GP.y[0:n_init]),gpgo_stp_df1_2.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_gp_2 = np.log(y_global_orig - gp_output_2)\n",
    "regret_stp_df1_2 = np.log(y_global_orig - stp_df1_output_2)\n",
    "\n",
    "train_regret_gp_2 = min_max_array(regret_gp_2)\n",
    "train_regret_stp_df1_2 = min_max_array(regret_stp_df1_2)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 2\n",
    "min_train_regret_gp_2 = min(train_regret_gp_2)\n",
    "min_train_regret_stp_df1_2 = min(train_regret_stp_df1_2)\n",
    "\n",
    "min_train_regret_gp_2, min_train_regret_stp_df1_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.54329109 0.72895073 0.01688145]. \t  0.003524201143729458 \t 0.7966854416715844\n",
      "init   \t [0.3303388  0.36872182 0.04830367]. \t  0.1254131411976814 \t 0.7966854416715844\n",
      "init   \t [0.10453019 0.09743752 0.24540331]. \t  0.7966854416715844 \t 0.7966854416715844\n",
      "init   \t [0.87348935 0.05161164 0.06048134]. \t  0.12371149908893726 \t 0.7966854416715844\n",
      "init   \t [0.23874011 0.20369919 0.17453362]. \t  0.6814516737210936 \t 0.7966854416715844\n",
      "1      \t [0.58131256 0.44428472 1.        ]. \t  \u001b[92m1.7923481631128224\u001b[0m \t 1.7923481631128224\n",
      "2      \t [0.00168031 0.98640159 0.93345165]. \t  0.5794398689332044 \t 1.7923481631128224\n",
      "3      \t [0.91276762 0.99413854 0.79042039]. \t  0.5168155300754682 \t 1.7923481631128224\n",
      "4      \t [0.87462587 0.07463384 0.89216661]. \t  0.4064577125806287 \t 1.7923481631128224\n",
      "5      \t [0.25457824 0.1010065  0.77250547]. \t  0.5976449039333983 \t 1.7923481631128224\n",
      "6      \t [0.09233414 0.94854072 0.38769464]. \t  1.2282622201684914 \t 1.7923481631128224\n",
      "7      \t [0.88157782 0.51529473 0.52193939]. \t  0.3685802623610793 \t 1.7923481631128224\n",
      "8      \t [0.50687578 0.89099968 0.86870471]. \t  1.3526892112886904 \t 1.7923481631128224\n",
      "9      \t [0.04551299 0.9423127  0.02010334]. \t  0.000662673956798899 \t 1.7923481631128224\n",
      "10     \t [0.01016912 0.63915291 0.80228633]. \t  \u001b[92m3.3861839247519416\u001b[0m \t 3.3861839247519416\n",
      "11     \t [0.12454902 0.59654787 0.96395321]. \t  2.6992343242897117 \t 3.3861839247519416\n",
      "12     \t [0.01598455 0.44657728 0.7548834 ]. \t  2.827615056566966 \t 3.3861839247519416\n",
      "13     \t [0.06709494 0.59070326 0.57274322]. \t  1.768362012805887 \t 3.3861839247519416\n",
      "14     \t [0.91810051 0.80638717 0.99343641]. \t  1.2196168481493588 \t 3.3861839247519416\n",
      "15     \t [0.18298918 0.28739135 0.44389274]. \t  0.37024928247535455 \t 3.3861839247519416\n",
      "16     \t [0.02083702 0.89484403 0.4480259 ]. \t  2.081927991257122 \t 3.3861839247519416\n",
      "17     \t [0.7369536  0.50431006 0.74225228]. \t  2.729615663833928 \t 3.3861839247519416\n",
      "18     \t [0.92657607 0.02672549 0.99700991]. \t  0.12227511821960574 \t 3.3861839247519416\n",
      "19     \t [0.48710328 0.         0.        ]. \t  0.09804940545551795 \t 3.3861839247519416\n",
      "20     \t [0. 0. 1.]. \t  0.0902894676548261 \t 3.3861839247519416\n",
      "21     \t [0.00819027 0.59075112 0.89453881]. \t  \u001b[92m3.608301835883924\u001b[0m \t 3.608301835883924\n",
      "22     \t [0.68886838 0.43067036 0.01488505]. \t  0.04068796603733018 \t 3.608301835883924\n",
      "23     \t [0.06097656 0.74566813 0.20774061]. \t  0.08141239773806075 \t 3.608301835883924\n",
      "24     \t [0.46809105 0.15187007 0.86274051]. \t  0.8598006262254263 \t 3.608301835883924\n",
      "25     \t [0.43556045 0.62334053 0.25489399]. \t  0.15453693775167204 \t 3.608301835883924\n",
      "26     \t [0.63951474 0.40270985 0.16722734]. \t  0.26435021273588366 \t 3.608301835883924\n",
      "27     \t [0.22009739 0.4147994  0.2717027 ]. \t  0.4176871677689829 \t 3.608301835883924\n",
      "28     \t [0.540548   0.75709362 0.26408517]. \t  0.1343701389784544 \t 3.608301835883924\n",
      "29     \t [0.5042077  0.93752892 0.23539446]. \t  0.08570798338959185 \t 3.608301835883924\n",
      "30     \t [0.35621277 0.95742405 0.29551274]. \t  0.3110046780930744 \t 3.608301835883924\n",
      "31     \t [0.88528377 0.54402333 0.79458338]. \t  3.3590052672804833 \t 3.608301835883924\n",
      "32     \t [0.07240624 0.06073917 0.66521206]. \t  0.2764904513149654 \t 3.608301835883924\n",
      "33     \t [0.28441817 0.3116496  0.9177407 ]. \t  1.9057655917446787 \t 3.608301835883924\n",
      "34     \t [0.68803839 0.74271491 0.11611202]. \t  0.010284788186201222 \t 3.608301835883924\n",
      "35     \t [0.88414602 0.3199615  0.88687406]. \t  2.1503204324619722 \t 3.608301835883924\n",
      "36     \t [0.44243593 0.24507787 0.50826072]. \t  0.30544586563745313 \t 3.608301835883924\n",
      "37     \t [0.18855385 0.98096143 0.447161  ]. \t  1.8563327996857306 \t 3.608301835883924\n",
      "38     \t [0.2163858  0.31254142 0.43109419]. \t  0.39575738635037094 \t 3.608301835883924\n",
      "39     \t [1.         0.59026066 0.92171767]. \t  3.234034140948251 \t 3.608301835883924\n",
      "40     \t [0.58183244 0.19785212 0.66307775]. \t  0.6857080895357341 \t 3.608301835883924\n",
      "41     \t [0.77489638 0.28149615 0.8656607 ]. \t  1.8917406150227278 \t 3.608301835883924\n",
      "42     \t [0.58416554 0.58939555 0.98928157]. \t  2.2628118897302545 \t 3.608301835883924\n",
      "43     \t [0.57677231 0.65445536 0.62756979]. \t  1.5861104175495657 \t 3.608301835883924\n",
      "44     \t [0.71873253 0.38727243 0.33954317]. \t  0.31199361604988124 \t 3.608301835883924\n",
      "45     \t [0.41631698 0.77899579 0.85921632]. \t  2.4343929104737994 \t 3.608301835883924\n",
      "46     \t [0.60512328 0.28980512 0.05434369]. \t  0.16099697215601297 \t 3.608301835883924\n",
      "47     \t [0.56581492 0.27342047 0.841312  ]. \t  1.8863747896887912 \t 3.608301835883924\n",
      "48     \t [0.74566908 0.39079015 0.11789231]. \t  0.1582854699370522 \t 3.608301835883924\n",
      "49     \t [0.28822039 0.7453982  0.82539668]. \t  2.75932993641603 \t 3.608301835883924\n",
      "50     \t [0.05719169 0.4791313  0.14185822]. \t  0.12935583913496423 \t 3.608301835883924\n",
      "51     \t [0.57955164 0.23331699 0.399817  ]. \t  0.4758556349331539 \t 3.608301835883924\n",
      "52     \t [0.96937363 0.66731035 0.4521553 ]. \t  0.19881852484274248 \t 3.608301835883924\n",
      "53     \t [0.88209753 0.35873203 0.41844797]. \t  0.17410481258387447 \t 3.608301835883924\n",
      "54     \t [0.19782572 0.63860188 0.65838965]. \t  2.3565077684823375 \t 3.608301835883924\n",
      "55     \t [0.79304177 0.42681652 0.09715681]. \t  0.0938554048773821 \t 3.608301835883924\n",
      "56     \t [0.82690552 0.78390667 0.08485574]. \t  0.003083791965477053 \t 3.608301835883924\n",
      "57     \t [0.46144889 0.14536428 0.52143599]. \t  0.24007033636338132 \t 3.608301835883924\n",
      "58     \t [0.18195238 0.7338693  0.43582764]. \t  1.6190617802172047 \t 3.608301835883924\n",
      "59     \t [0.74053832 0.51122012 0.78208061]. \t  3.262909508341633 \t 3.608301835883924\n",
      "60     \t [0.10689202 0.08748939 0.19092893]. \t  0.6774326741207717 \t 3.608301835883924\n",
      "61     \t [0.87174517 0.00891929 0.96454407]. \t  0.13666273025902131 \t 3.608301835883924\n",
      "62     \t [0.12507173 0.11436994 0.9545812 ]. \t  0.4118521222881786 \t 3.608301835883924\n",
      "63     \t [0.60362661 0.87171872 0.95036888]. \t  1.1768898258032985 \t 3.608301835883924\n",
      "64     \t [0.84369341 0.27904102 0.14905424]. \t  0.2572204702244308 \t 3.608301835883924\n",
      "65     \t [0.42713967 0.02459249 0.65674807]. \t  0.19925454109576293 \t 3.608301835883924\n",
      "66     \t [0.36048986 0.62840864 0.16410704]. \t  0.06715560262134278 \t 3.608301835883924\n",
      "67     \t [0.94084764 0.25155484 0.45215663]. \t  0.1590584610750838 \t 3.608301835883924\n",
      "68     \t [0.5976383  0.77297909 0.74707751]. \t  1.9551632578123481 \t 3.608301835883924\n",
      "69     \t [0.47180141 0.76532015 0.04372231]. \t  0.003945767358207897 \t 3.608301835883924\n",
      "70     \t [0.9476665  0.21078247 0.50102273]. \t  0.15823259320432542 \t 3.608301835883924\n",
      "71     \t [0.16267848 0.34914529 0.52332069]. \t  0.4684143596808239 \t 3.608301835883924\n",
      "72     \t [0.02461752 0.37401346 0.80019727]. \t  2.727863591734349 \t 3.608301835883924\n",
      "73     \t [0.82329185 0.79910467 0.75349938]. \t  1.5651996899568172 \t 3.608301835883924\n",
      "74     \t [0.98626338 0.37870192 0.17740931]. \t  0.12647199031803344 \t 3.608301835883924\n",
      "75     \t [0.68681863 0.30869748 0.52940519]. \t  0.32189393946589445 \t 3.608301835883924\n",
      "76     \t [0.7078881  0.77355194 0.0117395 ]. \t  0.0014741684971952267 \t 3.608301835883924\n",
      "77     \t [0.11879399 0.89518607 0.59142876]. \t  2.98777717230509 \t 3.608301835883924\n",
      "78     \t [0.23077556 0.02963133 0.02222567]. \t  0.14436542786886752 \t 3.608301835883924\n",
      "79     \t [0.66000062 0.80101585 0.8791638 ]. \t  2.1214647230507193 \t 3.608301835883924\n",
      "80     \t [0.65365282 0.73853568 0.18572915]. \t  0.030805290675017825 \t 3.608301835883924\n",
      "81     \t [0.3539547  0.56513767 0.9937741 ]. \t  2.2017689433255665 \t 3.608301835883924\n",
      "82     \t [0.86602074 0.26833657 0.6300786 ]. \t  0.6977558627998486 \t 3.608301835883924\n",
      "83     \t [0.62090296 0.91244498 0.74647678]. \t  1.1027227488502724 \t 3.608301835883924\n",
      "84     \t [0.06391172 0.94824615 0.68196568]. \t  2.0003258426826793 \t 3.608301835883924\n",
      "85     \t [0.14751274 0.16122397 0.66390453]. \t  0.5605374769494726 \t 3.608301835883924\n",
      "86     \t [0.06880265 0.374662   0.07532075]. \t  0.13031302512177867 \t 3.608301835883924\n",
      "87     \t [0.37929378 0.99195217 0.11735952]. \t  0.00698301038607682 \t 3.608301835883924\n",
      "88     \t [0.73292381 0.40509627 0.88189523]. \t  3.001150205919309 \t 3.608301835883924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.07555548 0.26880522 0.6257711 ]. \t  0.7323313253399928 \t 3.608301835883924\n",
      "90     \t [0.27757916 0.48844877 0.35718201]. \t  0.393014260215152 \t 3.608301835883924\n",
      "91     \t [0.24779771 0.7020849  0.37180803]. \t  0.8017188234427736 \t 3.608301835883924\n",
      "92     \t [0.41147044 0.50186371 0.24538265]. \t  0.25562364040711605 \t 3.608301835883924\n",
      "93     \t [0.12363747 0.00604395 0.00947706]. \t  0.10048454145419308 \t 3.608301835883924\n",
      "94     \t [0.34233053 0.8561281  0.23545011]. \t  0.123547524559342 \t 3.608301835883924\n",
      "95     \t [0.66708453 0.12567109 0.05050743]. \t  0.18684930581019238 \t 3.608301835883924\n",
      "96     \t [0.27196874 0.4249447  0.04052478]. \t  0.08064974590850453 \t 3.608301835883924\n",
      "97     \t [0.66269814 0.80828927 0.4153153 ]. \t  0.6510516078803028 \t 3.608301835883924\n",
      "98     \t [0.80789932 0.70389513 0.99592796]. \t  1.7659592129198352 \t 3.608301835883924\n",
      "99     \t [0.40503689 0.40878657 0.23853599]. \t  0.42817250057984524 \t 3.608301835883924\n",
      "100    \t [0.61940483 0.67694345 0.61711839]. \t  1.4504777783102447 \t 3.608301835883924\n"
     ]
    }
   ],
   "source": [
    "### 6(c). Bayesian optimization runs (x20): GP run number = 3\n",
    "\n",
    "np.random.seed(run_num_3)\n",
    "surrogate_gp_3 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "gpgo_gp_3 = GPGO(surrogate_gp_3, Acquisition_new(util_gp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_3.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.54329109 0.72895073 0.01688145]. \t  0.003524201143729458 \t 0.7966854416715844\n",
      "init   \t [0.3303388  0.36872182 0.04830367]. \t  0.1254131411976814 \t 0.7966854416715844\n",
      "init   \t [0.10453019 0.09743752 0.24540331]. \t  0.7966854416715844 \t 0.7966854416715844\n",
      "init   \t [0.87348935 0.05161164 0.06048134]. \t  0.12371149908893726 \t 0.7966854416715844\n",
      "init   \t [0.23874011 0.20369919 0.17453362]. \t  0.6814516737210936 \t 0.7966854416715844\n",
      "1      \t [0.64681054 0.53437973 1.        ]. \t  \u001b[92m2.05083814853583\u001b[0m \t 2.05083814853583\n",
      "2      \t [0. 1. 1.]. \t  0.330219860606422 \t 2.05083814853583\n",
      "3      \t [0.42228444 0.         1.        ]. \t  0.09173257964910508 \t 2.05083814853583\n",
      "4      \t [1.         1.         0.94266076]. \t  0.45928899433621223 \t 2.05083814853583\n",
      "5      \t [1.         0.         0.93456932]. \t  0.15330770268481658 \t 2.05083814853583\n",
      "6      \t [0.         1.         0.14131942]. \t  0.014804220187581086 \t 2.05083814853583\n",
      "7      \t [1.         0.53674981 0.49886003]. \t  0.23210738908778578 \t 2.05083814853583\n",
      "8      \t [0.47909266 1.         0.68548542]. \t  1.1927504190114688 \t 2.05083814853583\n",
      "9      \t [0.         0.45894391 0.74230313]. \t  \u001b[92m2.7329671305569807\u001b[0m \t 2.7329671305569807\n",
      "10     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.7329671305569807\n",
      "11     \t [0.         0.37183775 1.        ]. \t  1.4143717562421383 \t 2.7329671305569807\n",
      "12     \t [1.         0.         0.47470943]. \t  0.0857916739409615 \t 2.7329671305569807\n",
      "13     \t [0.        0.        0.7478875]. \t  0.23395391649973374 \t 2.7329671305569807\n",
      "14     \t [0.         0.56322328 0.36555109]. \t  0.4529934560969075 \t 2.7329671305569807\n",
      "15     \t [0.38307023 0.         0.        ]. \t  0.10218504327661256 \t 2.7329671305569807\n",
      "16     \t [0.30133751 0.43783656 0.78546031]. \t  \u001b[92m3.123340525247833\u001b[0m \t 3.123340525247833\n",
      "17     \t [0.62681198 0.2356688  0.71058907]. \t  1.1276721967619248 \t 3.123340525247833\n",
      "18     \t [1.         0.48441388 0.        ]. \t  0.009208591380597682 \t 3.123340525247833\n",
      "19     \t [1.         0.47323059 1.        ]. \t  1.842167598835857 \t 3.123340525247833\n",
      "20     \t [0.         1.         0.61009276]. \t  2.312300806260275 \t 3.123340525247833\n",
      "21     \t [0.38500643 1.         0.        ]. \t  0.00024733136577236286 \t 3.123340525247833\n",
      "22     \t [0.14189183 0.70617165 0.82598824]. \t  3.1035948535463165 \t 3.123340525247833\n",
      "23     \t [1.         1.         0.44627705]. \t  0.16876546303560166 \t 3.123340525247833\n",
      "24     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.123340525247833\n",
      "25     \t [0.35005342 1.         1.        ]. \t  0.3341251519246544 \t 3.123340525247833\n",
      "26     \t [0.22055145 1.         0.38602854]. \t  1.0490673714127803 \t 3.123340525247833\n",
      "27     \t [0.44083658 0.         0.53965542]. \t  0.134612184299324 \t 3.123340525247833\n",
      "28     \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.123340525247833\n",
      "29     \t [1.         0.76473316 0.8294722 ]. \t  2.28317840494013 \t 3.123340525247833\n",
      "30     \t [0.73883365 1.         0.1505312 ]. \t  0.005966591714299601 \t 3.123340525247833\n",
      "31     \t [3.66414311e-12 6.08107546e-01 0.00000000e+00]. \t  0.007128142285976727 \t 3.123340525247833\n",
      "32     \t [0.2255994  0.51392522 1.        ]. \t  2.02522057447262 \t 3.123340525247833\n",
      "33     \t [1.         0.78249872 0.19554386]. \t  0.00843208167034371 \t 3.123340525247833\n",
      "34     \t [0.97835142 0.75747697 0.99088628]. \t  1.5204203333635409 \t 3.123340525247833\n",
      "35     \t [0.10272147 1.         0.81136496]. \t  0.8315110785881298 \t 3.123340525247833\n",
      "36     \t [1.50416407e-08 7.66906563e-01 7.64489872e-01]. \t  2.419351993637224 \t 3.123340525247833\n",
      "37     \t [0.         0.23759282 0.        ]. \t  0.0674011952184563 \t 3.123340525247833\n",
      "38     \t [0.81100155 0.85389596 0.80315273]. \t  1.4477614947378505 \t 3.123340525247833\n",
      "39     \t [0.38101995 0.67102852 0.69030122]. \t  2.341203651234767 \t 3.123340525247833\n",
      "40     \t [0.78197899 0.         1.        ]. \t  0.09040065575934668 \t 3.123340525247833\n",
      "41     \t [0.         0.         0.99999999]. \t  0.09028947416410901 \t 3.123340525247833\n",
      "42     \t [0.99999996 0.13668212 0.23476706]. \t  0.2922552675955511 \t 3.123340525247833\n",
      "43     \t [0.99605097 0.24933019 0.86762452]. \t  1.553489697633199 \t 3.123340525247833\n",
      "44     \t [0.15559039 0.24194933 0.64339679]. \t  0.7505498327245657 \t 3.123340525247833\n",
      "45     \t [0.38768884 0.51699942 0.86199831]. \t  \u001b[92m3.7993832234564366\u001b[0m \t 3.7993832234564366\n",
      "46     \t [0.34479166 0.61316419 0.84838794]. \t  3.7374667119632354 \t 3.7993832234564366\n",
      "47     \t [0.47223441 0.57543738 0.87005082]. \t  \u001b[92m3.8071926000937677\u001b[0m \t 3.8071926000937677\n",
      "48     \t [0.6032874  0.64073951 0.89898913]. \t  3.397694411531171 \t 3.8071926000937677\n",
      "49     \t [0.46684748 0.53682222 0.89705247]. \t  3.6458795936950255 \t 3.8071926000937677\n",
      "50     \t [1.50236489e-07 7.48232363e-01 1.00000000e+00]. \t  1.4954092353011388 \t 3.8071926000937677\n",
      "51     \t [0.41765952 0.57058062 0.90851599]. \t  3.551601713749879 \t 3.8071926000937677\n",
      "52     \t [0.40184998 0.69780311 0.93975458]. \t  2.6392782102213506 \t 3.8071926000937677\n",
      "53     \t [0.54510138 0.54557938 0.85419111]. \t  \u001b[92m3.8307306137822033\u001b[0m \t 3.8307306137822033\n",
      "54     \t [0.45310023 0.55284699 0.85707376]. \t  \u001b[92m3.84859242941921\u001b[0m \t 3.84859242941921\n",
      "55     \t [0.37932818 0.48225898 0.80850458]. \t  3.5298725509313256 \t 3.84859242941921\n",
      "56     \t [0.46635899 0.52012776 0.83431707]. \t  3.776289274008669 \t 3.84859242941921\n",
      "57     \t [0.44683123 0.54296644 0.86413126]. \t  3.8353473475152673 \t 3.84859242941921\n",
      "58     \t [0.48246489 0.54456488 0.8312119 ]. \t  3.7913814146745395 \t 3.84859242941921\n",
      "59     \t [0.31674487 0.55264045 0.80537248]. \t  3.657579779761803 \t 3.84859242941921\n",
      "60     \t [0.46765249 0.46801847 0.86674959]. \t  3.5719448219790064 \t 3.84859242941921\n",
      "61     \t [0.47915068 0.56805581 0.85186529]. \t  3.8366524965829365 \t 3.84859242941921\n",
      "62     \t [0.78020228 0.48639249 0.86985691]. \t  3.5919968292961406 \t 3.84859242941921\n",
      "63     \t [0.60426453 0.51837051 0.83956126]. \t  3.7592619920097423 \t 3.84859242941921\n",
      "64     \t [0.53218135 0.57002862 0.87685863]. \t  3.77873243017416 \t 3.84859242941921\n",
      "65     \t [0.30736322 0.43513064 0.85359805]. \t  3.388461852718523 \t 3.84859242941921\n",
      "66     \t [0.49817199 0.59140118 0.89457071]. \t  3.6412482399407677 \t 3.84859242941921\n",
      "67     \t [0.74806991 0.57031608 0.85421259]. \t  3.76121958315246 \t 3.84859242941921\n",
      "68     \t [0.57535775 0.56218878 0.87692701]. \t  3.7757857289501846 \t 3.84859242941921\n",
      "69     \t [0.94060643 0.5318179  0.85247217]. \t  3.6867386366733004 \t 3.84859242941921\n",
      "70     \t [0.48294346 0.50577755 0.86123429]. \t  3.757894204825861 \t 3.84859242941921\n",
      "71     \t [0.79738262 0.54936729 0.86608795]. \t  3.748335636060542 \t 3.84859242941921\n",
      "72     \t [0.12683975 0.55497485 0.86160443]. \t  3.8368548090030425 \t 3.84859242941921\n",
      "73     \t [0.90641055 0.56954152 0.86406031]. \t  3.698320766344933 \t 3.84859242941921\n",
      "74     \t [0.42833945 0.54611384 0.85589767]. \t  \u001b[92m3.8503389102407817\u001b[0m \t 3.8503389102407817\n",
      "75     \t [0.6281902  0.53831781 0.86833315]. \t  3.786925836987825 \t 3.8503389102407817\n",
      "76     \t [0.78972801 0.5279411  0.87257566]. \t  3.71139307038879 \t 3.8503389102407817\n",
      "77     \t [0.46042884 0.6110198  0.85333887]. \t  3.7327438240506474 \t 3.8503389102407817\n",
      "78     \t [0.69935302 0.5659377  0.85785739]. \t  3.783748638713403 \t 3.8503389102407817\n",
      "79     \t [0.81385544 0.51994826 0.83002566]. \t  3.6581136000746954 \t 3.8503389102407817\n",
      "80     \t [0.29925774 0.48591355 0.87480907]. \t  3.639315221428628 \t 3.8503389102407817\n",
      "81     \t [0.38021422 0.54646928 0.83863787]. \t  3.835178151459856 \t 3.8503389102407817\n",
      "82     \t [0.76133315 0.54645715 0.86472648]. \t  3.762437581219583 \t 3.8503389102407817\n",
      "83     \t [0.24845772 0.5402821  0.83376326]. \t  3.8206798729006106 \t 3.8503389102407817\n",
      "84     \t [0.57498586 0.55397694 0.86294373]. \t  3.8196009627806973 \t 3.8503389102407817\n",
      "85     \t [0.64576966 0.60410881 0.846925  ]. \t  3.6996698477696426 \t 3.8503389102407817\n",
      "86     \t [0.12766432 0.59283233 0.83516663]. \t  3.768149251062371 \t 3.8503389102407817\n",
      "87     \t [0.14993766 0.53213584 0.83557478]. \t  3.806683931237782 \t 3.8503389102407817\n",
      "88     \t [0.21152989 0.51493129 0.83944624]. \t  3.78889109865885 \t 3.8503389102407817\n",
      "89     \t [0.64200424 0.49695879 0.83539017]. \t  3.676315634513808 \t 3.8503389102407817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.43033865 0.5518736  0.88061621]. \t  3.7801078539392434 \t 3.8503389102407817\n",
      "91     \t [0.38686352 0.53047762 0.85186514]. \t  3.8378710804080014 \t 3.8503389102407817\n",
      "92     \t [0.41975174 0.49529405 0.83843001]. \t  3.7210508831162974 \t 3.8503389102407817\n",
      "93     \t [0.4751081  0.61043225 0.8461292 ]. \t  3.7228279318616817 \t 3.8503389102407817\n",
      "94     \t [0.10633441 0.54413372 0.89028113]. \t  3.692209427681085 \t 3.8503389102407817\n",
      "95     \t [0.42194945 0.53946853 0.82585848]. \t  3.7745730726343822 \t 3.8503389102407817\n",
      "96     \t [0.43781657 0.64469106 0.86353993]. \t  3.5666416785060386 \t 3.8503389102407817\n",
      "97     \t [0.50773052 0.54429454 0.85285952]. \t  3.837321293881975 \t 3.8503389102407817\n",
      "98     \t [0.48703905 0.5791936  0.84441782]. \t  3.808053205201958 \t 3.8503389102407817\n",
      "99     \t [0.71557179 0.52586525 0.86079281]. \t  3.7596229904726886 \t 3.8503389102407817\n",
      "100    \t [0.53008422 0.50545953 0.83314986]. \t  3.7227668727381595 \t 3.8503389102407817\n"
     ]
    }
   ],
   "source": [
    "### 6(c). Bayesian optimization runs (x20): STP DF1 run number = 3\n",
    "\n",
    "np.random.seed(run_num_3)\n",
    "surrogate_stp_df1_3 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_3 = GPGO(surrogate_stp_df1_3, Acquisition_new(util_stp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_3.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.3685402458234954, -4.386750594284538)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(c). Training Regret Minimisation: run number = 3\n",
    "\n",
    "gp_output_3 = np.append(np.max(gpgo_gp_3.GP.y[0:n_init]),gpgo_gp_3.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_3 = np.append(np.max(gpgo_stp_df1_3.GP.y[0:n_init]),gpgo_stp_df1_3.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_gp_3 = np.log(y_global_orig - gp_output_3)\n",
    "regret_stp_df1_3 = np.log(y_global_orig - stp_df1_output_3)\n",
    "\n",
    "train_regret_gp_3 = min_max_array(regret_gp_3)\n",
    "train_regret_stp_df1_3 = min_max_array(regret_stp_df1_3)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 3\n",
    "min_train_regret_gp_3 = min(train_regret_gp_3)\n",
    "min_train_regret_stp_df1_3 = min(train_regret_stp_df1_3)\n",
    "\n",
    "min_train_regret_gp_3, min_train_regret_stp_df1_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.83936105 0.82129497 0.64220716]. \t  0.8796727111154451 \t 1.9592421489197056\n",
      "init   \t [0.66722262 0.03913991 0.06567239]. \t  0.21283931850025994 \t 1.9592421489197056\n",
      "init   \t [0.27650064 0.3163605  0.19359241]. \t  0.5590312289203825 \t 1.9592421489197056\n",
      "init   \t [0.19709288 0.84920429 0.84235809]. \t  1.818649590001362 \t 1.9592421489197056\n",
      "init   \t [0.34736473 0.7397423  0.49340914]. \t  1.9592421489197056 \t 1.9592421489197056\n",
      "1      \t [0.47647277 0.92393606 0.15672912]. \t  0.017844770068977132 \t 1.9592421489197056\n",
      "2      \t [0.72176507 0.         0.9642597 ]. \t  0.12600563325695788 \t 1.9592421489197056\n",
      "3      \t [0.12891537 0.10450196 0.76439878]. \t  0.6009821728805325 \t 1.9592421489197056\n",
      "4      \t [0.07213246 0.95861543 0.39059898]. \t  1.2397133678239307 \t 1.9592421489197056\n",
      "5      \t [0.98597041 0.2856864  0.41065736]. \t  0.15284929019920618 \t 1.9592421489197056\n",
      "6      \t [0.35984016 0.39345196 0.95312626]. \t  \u001b[92m2.2080405720154364\u001b[0m \t 2.2080405720154364\n",
      "7      \t [0.80886206 0.5371506  0.97065275]. \t  \u001b[92m2.5535236020960204\u001b[0m \t 2.5535236020960204\n",
      "8      \t [0.78616262 0.98348255 0.89699467]. \t  0.6336596985860946 \t 2.5535236020960204\n",
      "9      \t [0.04681794 0.56517096 0.99093683]. \t  2.2288293321682677 \t 2.5535236020960204\n",
      "10     \t [0.92671631 0.30850458 0.73208369]. \t  1.6831956823568888 \t 2.5535236020960204\n",
      "11     \t [0.99251564 0.53200473 0.97139017]. \t  2.4848356404702816 \t 2.5535236020960204\n",
      "12     \t [0.46379372 0.38690137 0.37719231]. \t  0.4121445056585975 \t 2.5535236020960204\n",
      "13     \t [0.01989029 0.49631073 0.04396718]. \t  0.03714686443394807 \t 2.5535236020960204\n",
      "14     \t [0.004404   0.09896688 0.094363  ]. \t  0.27280620176167747 \t 2.5535236020960204\n",
      "15     \t [0.00680257 0.83486728 0.96935618]. \t  1.2909852498742977 \t 2.5535236020960204\n",
      "16     \t [0.45769784 0.80757882 0.94089557]. \t  1.7544268191834966 \t 2.5535236020960204\n",
      "17     \t [0.95230129 0.38134913 0.4858654 ]. \t  0.18305807391691747 \t 2.5535236020960204\n",
      "18     \t [0.6537332  0.60354243 0.75153684]. \t  \u001b[92m2.8201029650726914\u001b[0m \t 2.8201029650726914\n",
      "19     \t [0.969245   0.14050121 0.02213768]. \t  0.055581951405792945 \t 2.8201029650726914\n",
      "20     \t [0.61043042 0.76901763 0.23691638]. \t  0.07288343079196587 \t 2.8201029650726914\n",
      "21     \t [0.72091588 0.81569726 0.01051573]. \t  0.0008541583076826821 \t 2.8201029650726914\n",
      "22     \t [0.71083172 0.23505423 0.94873069]. \t  1.0498890486494339 \t 2.8201029650726914\n",
      "23     \t [0.72226889 0.20156952 0.09222259]. \t  0.25522171878427014 \t 2.8201029650726914\n",
      "24     \t [0.39269752 0.54762669 0.83297827]. \t  \u001b[92m3.8151893325258737\u001b[0m \t 3.8151893325258737\n",
      "25     \t [0.15506875 0.7398081  0.79992413]. \t  2.7493877272612255 \t 3.8151893325258737\n",
      "26     \t [0.47922685 0.39281578 0.00439121]. \t  0.056670893979183526 \t 3.8151893325258737\n",
      "27     \t [0.93341652 0.98195286 0.01379805]. \t  8.502821513842029e-05 \t 3.8151893325258737\n",
      "28     \t [0.58161244 0.85353531 0.4330298 ]. \t  0.9907374713535583 \t 3.8151893325258737\n",
      "29     \t [0.36698684 0.71903546 0.38399462]. \t  0.8319079231681563 \t 3.8151893325258737\n",
      "30     \t [0.96710442 0.0449519  0.9665492 ]. \t  0.19275441918756014 \t 3.8151893325258737\n",
      "31     \t [0.47944205 0.33257806 0.96390037]. \t  1.623099451158217 \t 3.8151893325258737\n",
      "32     \t [0.09745536 0.94251266 0.16993469]. \t  0.03433223941386708 \t 3.8151893325258737\n",
      "33     \t [0.71625866 0.87709437 0.30436503]. \t  0.1537142166541831 \t 3.8151893325258737\n",
      "34     \t [0.19012126 0.12058599 0.11795816]. \t  0.4653194865578239 \t 3.8151893325258737\n",
      "35     \t [0.74774734 0.04970054 0.42040843]. \t  0.31451243264143736 \t 3.8151893325258737\n",
      "36     \t [0.84958006 0.02224249 0.22693705]. \t  0.4352870206613002 \t 3.8151893325258737\n",
      "37     \t [0.73814083 0.75071711 0.80438852]. \t  2.3887537283639095 \t 3.8151893325258737\n",
      "38     \t [0.64913429 0.33518378 0.30617113]. \t  0.48114562793001553 \t 3.8151893325258737\n",
      "39     \t [0.79436457 0.7512464  0.20992399]. \t  0.02730621755927814 \t 3.8151893325258737\n",
      "40     \t [0.76449008 0.18849949 0.41987   ]. \t  0.3153807348262885 \t 3.8151893325258737\n",
      "41     \t [0.07141796 0.17239542 0.26121577]. \t  0.7446444819706365 \t 3.8151893325258737\n",
      "42     \t [0.38180059 0.43843081 0.22180373]. \t  0.3474035754294986 \t 3.8151893325258737\n",
      "43     \t [0.08482858 0.82164654 0.49044133]. \t  2.6127388005142196 \t 3.8151893325258737\n",
      "44     \t [0.         0.59979884 0.65921968]. \t  2.2245896209262392 \t 3.8151893325258737\n",
      "45     \t [0.32959084 0.36001994 0.87349593]. \t  2.667005212854527 \t 3.8151893325258737\n",
      "46     \t [0.99747198 0.15794026 0.84858456]. \t  0.8922482324898482 \t 3.8151893325258737\n",
      "47     \t [0.59205156 0.12922306 0.02366767]. \t  0.14491954357998088 \t 3.8151893325258737\n",
      "48     \t [0.99623678 0.57079417 0.8806454 ]. \t  3.6095994722521034 \t 3.8151893325258737\n",
      "49     \t [0.49316125 0.64947007 0.46837257]. \t  1.0091456887671393 \t 3.8151893325258737\n",
      "50     \t [0.77070132 0.78203712 0.49392271]. \t  0.7154307154874395 \t 3.8151893325258737\n",
      "51     \t [0.08966898 0.76237058 0.22546117]. \t  0.11425353032662568 \t 3.8151893325258737\n",
      "52     \t [0.85449185 0.45695142 0.28387514]. \t  0.16567414047062834 \t 3.8151893325258737\n",
      "53     \t [0.78613408 0.08167713 0.18378513]. \t  0.47525089404355164 \t 3.8151893325258737\n",
      "54     \t [0.29687815 0.87933742 0.87351696]. \t  1.4831575364258622 \t 3.8151893325258737\n",
      "55     \t [0.65080677 0.50894956 0.83584544]. \t  3.7124054670165427 \t 3.8151893325258737\n",
      "56     \t [0.25056999 0.23593882 0.47352493]. \t  0.33300666000299306 \t 3.8151893325258737\n",
      "57     \t [0.95419104 0.99391944 0.31777697]. \t  0.05673756586560111 \t 3.8151893325258737\n",
      "58     \t [0.68517702 0.04633665 0.7821367 ]. \t  0.3828038218861167 \t 3.8151893325258737\n",
      "59     \t [0.2365173  0.19077569 0.7414871 ]. \t  1.0200235283282992 \t 3.8151893325258737\n",
      "60     \t [0.6926002  0.01666735 0.52810415]. \t  0.12583141797854358 \t 3.8151893325258737\n",
      "61     \t [0.48323297 0.97953335 0.58107143]. \t  1.7743492083378223 \t 3.8151893325258737\n",
      "62     \t [0.29621598 0.79790783 0.02054715]. \t  0.002024858861289507 \t 3.8151893325258737\n",
      "63     \t [0.97243547 0.39776351 0.90290875]. \t  2.7231173646354363 \t 3.8151893325258737\n",
      "64     \t [0.67144269 0.06370543 0.88091737]. \t  0.3904208308121737 \t 3.8151893325258737\n",
      "65     \t [0.25392813 0.87188832 0.77817404]. \t  1.7048137935058674 \t 3.8151893325258737\n",
      "66     \t [0.43391942 0.96545355 0.6232254 ]. \t  1.8531866038798177 \t 3.8151893325258737\n",
      "67     \t [0.02004482 0.15447473 0.24703621]. \t  0.6771287708568413 \t 3.8151893325258737\n",
      "68     \t [0.03434399 0.61503807 0.84049126]. \t  3.6869648987519668 \t 3.8151893325258737\n",
      "69     \t [0.05458561 0.38780484 0.02342249]. \t  0.06002266829451044 \t 3.8151893325258737\n",
      "70     \t [0.93801884 0.55742704 0.78516778]. \t  3.2134915628582204 \t 3.8151893325258737\n",
      "71     \t [0.34682653 0.0185079  0.84163519]. \t  0.2873547834147019 \t 3.8151893325258737\n",
      "72     \t [0.04159497 0.06039998 0.78206446]. \t  0.4289808497990568 \t 3.8151893325258737\n",
      "73     \t [0.74001869 0.56057665 0.76221524]. \t  3.0080293794656545 \t 3.8151893325258737\n",
      "74     \t [0.78632777 0.4286754  0.5041643 ]. \t  0.3065319733912303 \t 3.8151893325258737\n",
      "75     \t [0.57431136 0.18655666 0.37108816]. \t  0.6173683076972666 \t 3.8151893325258737\n",
      "76     \t [0.26793312 0.20835569 0.39616386]. \t  0.5675650435515753 \t 3.8151893325258737\n",
      "77     \t [0.63036437 0.10444753 0.50685531]. \t  0.2032777599608422 \t 3.8151893325258737\n",
      "78     \t [0.94754388 0.93130913 0.02533518]. \t  0.00016201526987493124 \t 3.8151893325258737\n",
      "79     \t [0.72749109 0.29778109 0.07816875]. \t  0.16771807928609878 \t 3.8151893325258737\n",
      "80     \t [0.24104627 0.05784681 0.35761507]. \t  0.7223868418715442 \t 3.8151893325258737\n",
      "81     \t [0.22224128 0.58835294 0.54227105]. \t  1.5620019707976203 \t 3.8151893325258737\n",
      "82     \t [0.50645353 0.96469724 0.40230175]. \t  0.8577685518309991 \t 3.8151893325258737\n",
      "83     \t [0.47760278 0.03970655 0.12418723]. \t  0.4918340278072209 \t 3.8151893325258737\n",
      "84     \t [0.7042768  0.90279546 0.76947643]. \t  1.0802503344142702 \t 3.8151893325258737\n",
      "85     \t [0.93029315 0.72624329 0.18884353]. \t  0.013666408812849307 \t 3.8151893325258737\n",
      "86     \t [0.38226498 0.56576795 0.31580928]. \t  0.2940891935717929 \t 3.8151893325258737\n",
      "87     \t [0.36123269 0.05714129 0.24442103]. \t  0.9498213276855749 \t 3.8151893325258737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.42658279 0.36008033 0.44964381]. \t  0.37058270938451565 \t 3.8151893325258737\n",
      "89     \t [0.07821525 0.28395456 0.59400979]. \t  0.5962106261528902 \t 3.8151893325258737\n",
      "90     \t [0.35700609 0.85474401 0.67761676]. \t  2.0935319664822267 \t 3.8151893325258737\n",
      "91     \t [0.95577186 0.2331745  0.18906347]. \t  0.25888078497782724 \t 3.8151893325258737\n",
      "92     \t [0.89335318 0.48621021 0.25719843]. \t  0.11947728525669717 \t 3.8151893325258737\n",
      "93     \t [0.25767767 0.32707301 0.11423834]. \t  0.30731655934584096 \t 3.8151893325258737\n",
      "94     \t [0.08343446 0.38720798 0.21858599]. \t  0.3610006026372162 \t 3.8151893325258737\n",
      "95     \t [0.74268025 0.72842831 0.68601941]. \t  1.5289709164819008 \t 3.8151893325258737\n",
      "96     \t [0.02572599 0.03736815 0.77135148]. \t  0.34568347538666316 \t 3.8151893325258737\n",
      "97     \t [0.69612463 0.41825352 0.06053865]. \t  0.08125363518274566 \t 3.8151893325258737\n",
      "98     \t [0.17078721 0.27754479 0.64585559]. \t  0.9021637496207571 \t 3.8151893325258737\n",
      "99     \t [0.67232233 0.97100767 0.99615183]. \t  0.4304963204316718 \t 3.8151893325258737\n",
      "100    \t [0.33960335 0.24229867 0.9555809 ]. \t  1.0608480606353172 \t 3.8151893325258737\n"
     ]
    }
   ],
   "source": [
    "### 6(d). Bayesian optimization runs (x20): GP run number = 4\n",
    "\n",
    "np.random.seed(run_num_4)\n",
    "surrogate_gp_4 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "gpgo_gp_4 = GPGO(surrogate_gp_4, Acquisition_new(util_gp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_4.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.83936105 0.82129497 0.64220716]. \t  0.8796727111154451 \t 1.9592421489197056\n",
      "init   \t [0.66722262 0.03913991 0.06567239]. \t  0.21283931850025994 \t 1.9592421489197056\n",
      "init   \t [0.27650064 0.3163605  0.19359241]. \t  0.5590312289203825 \t 1.9592421489197056\n",
      "init   \t [0.19709288 0.84920429 0.84235809]. \t  1.818649590001362 \t 1.9592421489197056\n",
      "init   \t [0.34736473 0.7397423  0.49340914]. \t  1.9592421489197056 \t 1.9592421489197056\n",
      "1      \t [-2.77555756e-17  1.00000000e+00  0.00000000e+00]. \t  0.0002735367680454459 \t 1.9592421489197056\n",
      "2      \t [0.61652879 0.         1.        ]. \t  0.09130370140134696 \t 1.9592421489197056\n",
      "3      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 1.9592421489197056\n",
      "4      \t [-5.55111512e-17  0.00000000e+00  1.00000000e+00]. \t  0.0902894676548261 \t 1.9592421489197056\n",
      "5      \t [1.         0.         0.51422235]. \t  0.06905451217074736 \t 1.9592421489197056\n",
      "6      \t [1.         0.41630429 0.        ]. \t  0.0144951933932862 \t 1.9592421489197056\n",
      "7      \t [-1.38777878e-17  1.00000000e+00  5.87642173e-01]. \t  \u001b[92m2.4309670087277393\u001b[0m \t 2.4309670087277393\n",
      "8      \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.4309670087277393\n",
      "9      \t [1.         0.40445989 1.        ]. \t  1.549874769356735 \t 2.4309670087277393\n",
      "10     \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.4309670087277393\n",
      "11     \t [0.         0.55899111 0.64640904]. \t  1.9971358789761606 \t 2.4309670087277393\n",
      "12     \t [0.         0.         0.50476648]. \t  0.13085925348116942 \t 2.4309670087277393\n",
      "13     \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.4309670087277393\n",
      "14     \t [0.52931377 1.         0.        ]. \t  0.0001919637048726705 \t 2.4309670087277393\n",
      "15     \t [1. 0. 0.]. \t  0.03095471703300515 \t 2.4309670087277393\n",
      "16     \t [0. 1. 1.]. \t  0.330219860606422 \t 2.4309670087277393\n",
      "17     \t [0.5863495 1.        1.       ]. \t  0.33056294354275517 \t 2.4309670087277393\n",
      "18     \t [0.22811775 0.41112861 1.        ]. \t  1.6394608831948743 \t 2.4309670087277393\n",
      "19     \t [0.53004052 0.33597819 0.71931144]. \t  1.8057044605214523 \t 2.4309670087277393\n",
      "20     \t [0.36288621 1.         0.51006339]. \t  1.9947490796996332 \t 2.4309670087277393\n",
      "21     \t [0.         0.55499482 0.        ]. \t  0.011548813543552458 \t 2.4309670087277393\n",
      "22     \t [1.         1.         0.38270526]. \t  0.0978664704116668 \t 2.4309670087277393\n",
      "23     \t [0.30791768 0.         0.68940152]. \t  0.19230214344011476 \t 2.4309670087277393\n",
      "24     \t [1.         0.43860477 0.60322885]. \t  0.7748377619768868 \t 2.4309670087277393\n",
      "25     \t [0.         0.63623832 1.        ]. \t  1.9655715851338644 \t 2.4309670087277393\n",
      "26     \t [0.39025459 0.55640196 0.        ]. \t  0.017067233249107947 \t 2.4309670087277393\n",
      "27     \t [0.68314756 0.63903476 1.        ]. \t  1.9669553297141227 \t 2.4309670087277393\n",
      "28     \t [0.         0.87783955 0.313929  ]. \t  0.5105906315205772 \t 2.4309670087277393\n",
      "29     \t [0.7585421  0.53174837 0.25993879]. \t  0.13309222613456861 \t 2.4309670087277393\n",
      "30     \t [0.26118459 0.         0.        ]. \t  0.09874888498074068 \t 2.4309670087277393\n",
      "31     \t [1.         0.7547769  0.14091478]. \t  0.004627493691295758 \t 2.4309670087277393\n",
      "32     \t [0.62497035 0.         0.40563882]. \t  0.4067744927471494 \t 2.4309670087277393\n",
      "33     \t [0.01293933 0.18789153 0.84789256]. \t  1.1204329108164075 \t 2.4309670087277393\n",
      "34     \t [1.         0.12708208 0.32949755]. \t  0.2705296223374579 \t 2.4309670087277393\n",
      "35     \t [0.81940785 0.17216762 0.85890381]. \t  0.9868561887457612 \t 2.4309670087277393\n",
      "36     \t [1.         0.67362018 1.        ]. \t  1.7973515192652971 \t 2.4309670087277393\n",
      "37     \t [0.77010151 1.         0.28486013]. \t  0.07779222398884277 \t 2.4309670087277393\n",
      "38     \t [0.23219696 0.8839975  0.03544292]. \t  0.0014041393881706109 \t 2.4309670087277393\n",
      "39     \t [1.42550558e-08 1.56191858e-01 2.56073942e-01]. \t  0.6534832512727003 \t 2.4309670087277393\n",
      "40     \t [0.18353903 1.         0.20681916]. \t  0.06690916157436225 \t 2.4309670087277393\n",
      "41     \t [2.33418024e-08 7.88953007e-01 7.18894882e-01]. \t  2.3127383928190772 \t 2.4309670087277393\n",
      "42     \t [2.49839285e-01 8.66737600e-09 2.33037972e-01]. \t  0.8069163420052479 \t 2.4309670087277393\n",
      "43     \t [0.49714441 0.62382651 0.81382191]. \t  \u001b[92m3.5052169973807152\u001b[0m \t 3.5052169973807152\n",
      "44     \t [1.         1.         0.76867623]. \t  0.4277390276651257 \t 3.5052169973807152\n",
      "45     \t [0.61107312 0.99933032 0.73065139]. \t  0.7583467670773724 \t 3.5052169973807152\n",
      "46     \t [0.40631879 0.60323116 0.93078331]. \t  3.2239638208011203 \t 3.5052169973807152\n",
      "47     \t [0.         0.35108951 0.99999999]. \t  1.301077306443707 \t 3.5052169973807152\n",
      "48     \t [0.44387672 0.68040196 1.        ]. \t  1.8423065851815386 \t 3.5052169973807152\n",
      "49     \t [0.30677354 0.58343021 0.76965817]. \t  3.2830824812548096 \t 3.5052169973807152\n",
      "50     \t [0.80809396 0.7812868  0.        ]. \t  0.0008591600764114642 \t 3.5052169973807152\n",
      "51     \t [0.44660816 0.53723506 0.85395988]. \t  \u001b[92m3.8410348852064273\u001b[0m \t 3.8410348852064273\n",
      "52     \t [0.22159922 0.56766273 0.74955434]. \t  3.0871801177913363 \t 3.8410348852064273\n",
      "53     \t [0.53047495 0.65801925 0.78825024]. \t  3.0914313092793253 \t 3.8410348852064273\n",
      "54     \t [0.45447189 0.59570379 0.80339644]. \t  3.544124297044273 \t 3.8410348852064273\n",
      "55     \t [0.52949501 0.53493472 0.89436705]. \t  3.658146594407193 \t 3.8410348852064273\n",
      "56     \t [3.47932220e-08 2.43610389e-01 2.39644008e-07]. \t  0.0664067106580491 \t 3.8410348852064273\n",
      "57     \t [0.36791987 0.58306459 0.89876105]. \t  3.6340830387913625 \t 3.8410348852064273\n",
      "58     \t [0.43775755 0.56235598 0.84066885]. \t  3.8304699163891067 \t 3.8410348852064273\n",
      "59     \t [0.52390434 0.57116564 0.84955576]. \t  3.8215643878026837 \t 3.8410348852064273\n",
      "60     \t [0.83729587 0.51642097 0.8686788 ]. \t  3.6824209931534146 \t 3.8410348852064273\n",
      "61     \t [0.48633887 0.62352755 0.80665649]. \t  3.4560608492489853 \t 3.8410348852064273\n",
      "62     \t [0.99629373 0.51326531 0.85306274]. \t  3.6280885511992906 \t 3.8410348852064273\n",
      "63     \t [0.66323424 0.52036487 0.79831626]. \t  3.479868372580032 \t 3.8410348852064273\n",
      "64     \t [0.76222523 0.51235815 0.88446674]. \t  3.6251675128135075 \t 3.8410348852064273\n",
      "65     \t [0.29638109 0.46240439 0.86898805]. \t  3.5351171366869285 \t 3.8410348852064273\n",
      "66     \t [0.35955485 0.51797221 0.82401753]. \t  3.741218642778799 \t 3.8410348852064273\n",
      "67     \t [0.45618312 0.50450913 0.85197971]. \t  3.765776905597936 \t 3.8410348852064273\n",
      "68     \t [0.66279711 0.53231846 0.84669036]. \t  3.782530524721641 \t 3.8410348852064273\n",
      "69     \t [0.50276975 0.7282784  0.16642491]. \t  0.03323705851535364 \t 3.8410348852064273\n",
      "70     \t [0.49550672 0.4808993  0.82965343]. \t  3.6238019484476287 \t 3.8410348852064273\n",
      "71     \t [0.51524657 0.51060444 0.88154452]. \t  3.6916091683005123 \t 3.8410348852064273\n",
      "72     \t [0.30833887 0.50780318 0.87189821]. \t  3.742503876868913 \t 3.8410348852064273\n",
      "73     \t [0.47435515 0.48014884 0.881544  ]. \t  3.569218790854325 \t 3.8410348852064273\n",
      "74     \t [0.36679962 0.57648386 0.82818903]. \t  3.777893355781951 \t 3.8410348852064273\n",
      "75     \t [0.41148259 0.54683101 0.85695995]. \t  \u001b[92m3.851987105943082\u001b[0m \t 3.851987105943082\n",
      "76     \t [0.45203417 0.53567756 0.8016389 ]. \t  3.5944493182566726 \t 3.851987105943082\n",
      "77     \t [0.08338412 0.55633836 0.78803786]. \t  3.4936430318509544 \t 3.851987105943082\n",
      "78     \t [0.31708664 0.59040173 0.839278  ]. \t  3.7960391732963865 \t 3.851987105943082\n",
      "79     \t [0.42436976 0.49208826 0.80878714]. \t  3.563931336580688 \t 3.851987105943082\n",
      "80     \t [0.54544629 0.49027786 0.93236976]. \t  3.084989589259835 \t 3.851987105943082\n",
      "81     \t [0.30980616 0.57104439 0.88094943]. \t  3.77757711883093 \t 3.851987105943082\n",
      "82     \t [0.33132233 0.5097567  0.82340787]. \t  3.71970439598168 \t 3.851987105943082\n",
      "83     \t [0.23978321 0.58549462 0.83966553]. \t  3.810007924726116 \t 3.851987105943082\n",
      "84     \t [0.72757761 0.56533101 0.84228593]. \t  3.7527504284876114 \t 3.851987105943082\n",
      "85     \t [0.37562798 0.52557347 0.84284278]. \t  3.821172373145294 \t 3.851987105943082\n",
      "86     \t [0.99999833 0.         0.19899131]. \t  0.22953629927639552 \t 3.851987105943082\n",
      "87     \t [0.46198785 0.55071717 0.83650748]. \t  3.8174136224564457 \t 3.851987105943082\n",
      "88     \t [0.47147661 0.5594117  0.93361274]. \t  3.2314163476347626 \t 3.851987105943082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.33554947 0.58795392 0.7916314 ]. \t  3.493034350901529 \t 3.851987105943082\n",
      "90     \t [0.07221487 0.56387826 0.80832072]. \t  3.6594412184362977 \t 3.851987105943082\n",
      "91     \t [0.49112966 0.52910212 0.78376584]. \t  3.399139966543606 \t 3.851987105943082\n",
      "92     \t [0.62228298 0.56596858 0.84842287]. \t  3.8005613905024203 \t 3.851987105943082\n",
      "93     \t [0.57528114 0.59620818 0.79440127]. \t  3.407557838729203 \t 3.851987105943082\n",
      "94     \t [0.35697267 0.49298132 0.81787027]. \t  3.6338928690176866 \t 3.851987105943082\n",
      "95     \t [0.36221238 0.52669314 0.8282026 ]. \t  3.7779617693726424 \t 3.851987105943082\n",
      "96     \t [0.31736953 0.61633474 0.74698496]. \t  2.970275328203936 \t 3.851987105943082\n",
      "97     \t [0.7031446  0.47190759 0.88263208]. \t  3.480019288040118 \t 3.851987105943082\n",
      "98     \t [0.94883499 0.47504659 0.86131674]. \t  3.499762991873669 \t 3.851987105943082\n",
      "99     \t [0.05970461 0.53394763 0.87106289]. \t  3.7765887954417665 \t 3.851987105943082\n",
      "100    \t [0.40073814 0.47767456 0.81693613]. \t  3.5613643399533594 \t 3.851987105943082\n"
     ]
    }
   ],
   "source": [
    "### 6(d). Bayesian optimization runs (x20): STP DF1 run number = 4\n",
    "\n",
    "np.random.seed(run_num_4)\n",
    "surrogate_stp_df1_4 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_4 = GPGO(surrogate_stp_df1_4, Acquisition_new(util_stp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_4.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.045118598435262, -4.528867319093919)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(d). Training Regret Minimisation: run number = 4\n",
    "\n",
    "gp_output_4 = np.append(np.max(gpgo_gp_4.GP.y[0:n_init]),gpgo_gp_4.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_4 = np.append(np.max(gpgo_stp_df1_4.GP.y[0:n_init]),gpgo_stp_df1_4.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_4 = np.log(y_global_orig - gp_output_4)\n",
    "regret_stp_df1_4 = np.log(y_global_orig - stp_df1_output_4)\n",
    "\n",
    "train_regret_gp_4 = min_max_array(regret_gp_4)\n",
    "train_regret_stp_df1_4 = min_max_array(regret_stp_df1_4)\n",
    "\n",
    "# GP, STP df1- training regret minimization: run number = 4\n",
    "min_train_regret_gp_4 = min(train_regret_gp_4)\n",
    "min_train_regret_stp_df1_4 = min(train_regret_stp_df1_4)\n",
    "\n",
    "min_train_regret_gp_4, min_train_regret_stp_df1_4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.71783409 0.04785513 0.94447198]. \t  0.23942957472217685 \t 2.653445131690671\n",
      "init   \t [0.68638004 0.58120733 0.14267862]. \t  0.05665494655696168 \t 2.653445131690671\n",
      "init   \t [0.94591918 0.33999059 0.62043546]. \t  0.7924779233128625 \t 2.653445131690671\n",
      "init   \t [0.45295964 0.23976742 0.03450459]. \t  0.16568631294324107 \t 2.653445131690671\n",
      "init   \t [0.14859311 0.81086617 0.65310538]. \t  2.653445131690671 \t 2.653445131690671\n",
      "1      \t [0. 1. 1.]. \t  0.330219860606422 \t 2.653445131690671\n",
      "2      \t [0.         0.75325522 0.27055349]. \t  0.23418929122511511 \t 2.653445131690671\n",
      "3      \t [0.53946488 0.78293522 0.80108436]. \t  2.222787207159752 \t 2.653445131690671\n",
      "4      \t [0.10511628 0.29880407 0.69910642]. \t  1.416313449816539 \t 2.653445131690671\n",
      "5      \t [0.47450161 0.98807145 0.56284889]. \t  1.7918639174776751 \t 2.653445131690671\n",
      "6      \t [0.9750877  0.07772881 0.04431082]. \t  0.0735687544157988 \t 2.653445131690671\n",
      "7      \t [0.51080801 0.14765344 0.53842891]. \t  0.23262144137384014 \t 2.653445131690671\n",
      "8      \t [0.99147253 0.84606207 0.874144  ]. \t  1.608855547016812 \t 2.653445131690671\n",
      "9      \t [0.17498715 0.44088599 0.9504134 ]. \t  2.556266712015414 \t 2.653445131690671\n",
      "10     \t [0.9689139  0.85342743 0.50743451]. \t  0.34279320195861607 \t 2.653445131690671\n",
      "11     \t [0.99091283 0.30170987 0.95905781]. \t  1.3971498280410732 \t 2.653445131690671\n",
      "12     \t [0.17442592 0.79251669 0.02707133]. \t  0.002303237184459098 \t 2.653445131690671\n",
      "13     \t [0.69490127 0.67220377 0.81143804]. \t  \u001b[92m3.1065380606704998\u001b[0m \t 3.1065380606704998\n",
      "14     \t [0.0684289  0.97784387 0.26045644]. \t  0.19972046503569793 \t 3.1065380606704998\n",
      "15     \t [0.01735697 0.4182733  0.90778066]. \t  2.904632758505507 \t 3.1065380606704998\n",
      "16     \t [0.83815407 0.69032379 0.27009983]. \t  0.05789803650399851 \t 3.1065380606704998\n",
      "17     \t [0.9901895  0.2085453  0.21826616]. \t  0.2689169207283076 \t 3.1065380606704998\n",
      "18     \t [0.74310225 0.03563107 0.08843026]. \t  0.2354828289635878 \t 3.1065380606704998\n",
      "19     \t [0.78190639 0.49542566 1.        ]. \t  1.952578164760286 \t 3.1065380606704998\n",
      "20     \t [0.54929036 0.79714298 0.56940339]. \t  1.7460080298718867 \t 3.1065380606704998\n",
      "21     \t [0.04082228 0.58331118 0.67370057]. \t  2.3115146383210643 \t 3.1065380606704998\n",
      "22     \t [0.16569241 0.01199439 0.95103484]. \t  0.15945013184183512 \t 3.1065380606704998\n",
      "23     \t [0.10227433 0.80378519 0.2233223 ]. \t  0.11286928348084434 \t 3.1065380606704998\n",
      "24     \t [0.54460759 0.10299779 0.05328168]. \t  0.23022806548399133 \t 3.1065380606704998\n",
      "25     \t [0.53314735 0.21417554 0.84537225]. \t  1.3492611735643345 \t 3.1065380606704998\n",
      "26     \t [0.86788925 0.8459283  0.97497186]. \t  1.1490386699071933 \t 3.1065380606704998\n",
      "27     \t [0.74824482 0.93233579 0.45523959]. \t  0.6391802511196352 \t 3.1065380606704998\n",
      "28     \t [0.942744   0.33680993 0.90873704]. \t  2.1468439999759186 \t 3.1065380606704998\n",
      "29     \t [0.56323109 0.6639674  0.61047253]. \t  1.5468523157347431 \t 3.1065380606704998\n",
      "30     \t [0.03265563 0.12971526 0.0244662 ]. \t  0.12125927618505171 \t 3.1065380606704998\n",
      "31     \t [0.18675334 0.51938307 0.27672425]. \t  0.26227830839855387 \t 3.1065380606704998\n",
      "32     \t [0.01186288 0.94231969 0.66240129]. \t  2.184255099878007 \t 3.1065380606704998\n",
      "33     \t [0.51466765 0.65551067 0.80627252]. \t  \u001b[92m3.270025239379082\u001b[0m \t 3.270025239379082\n",
      "34     \t [0.13028546 0.44556246 0.86488255]. \t  \u001b[92m3.4200437726325505\u001b[0m \t 3.4200437726325505\n",
      "35     \t [0.93923327 0.08403978 0.98450883]. \t  0.24155591796552575 \t 3.4200437726325505\n",
      "36     \t [1.         0.         0.72057481]. \t  0.212075429408697 \t 3.4200437726325505\n",
      "37     \t [0.38064389 0.446977   0.52112375]. \t  0.6554873364154958 \t 3.4200437726325505\n",
      "38     \t [0.86605745 0.87815447 0.2113348 ]. \t  0.01697450448207129 \t 3.4200437726325505\n",
      "39     \t [0.05957027 0.82361044 0.57524157]. \t  3.02698225922307 \t 3.4200437726325505\n",
      "40     \t [0.37256499 0.26727281 0.95191111]. \t  1.26085059502742 \t 3.4200437726325505\n",
      "41     \t [0.07080758 0.37422851 0.88105394]. \t  2.737826341580066 \t 3.4200437726325505\n",
      "42     \t [0.05135019 0.61636821 0.86097973]. \t  \u001b[92m3.696332904798286\u001b[0m \t 3.696332904798286\n",
      "43     \t [0.45958997 0.31064782 0.91327634]. \t  1.9336603473313412 \t 3.696332904798286\n",
      "44     \t [0.92670244 0.53544798 0.17129372]. \t  0.05334898818055021 \t 3.696332904798286\n",
      "45     \t [0.75926608 0.39943736 0.45591275]. \t  0.2291427202811579 \t 3.696332904798286\n",
      "46     \t [0.12490652 0.34024622 0.19529867]. \t  0.43861373617762134 \t 3.696332904798286\n",
      "47     \t [0.06146863 0.44642818 0.47088995]. \t  0.5528061733017825 \t 3.696332904798286\n",
      "48     \t [0.36880955 0.14859939 0.23598755]. \t  0.9620327322290172 \t 3.696332904798286\n",
      "49     \t [0.08862536 0.31669705 0.21431672]. \t  0.49163953577434505 \t 3.696332904798286\n",
      "50     \t [0.64928073 0.17863741 0.95167966]. \t  0.7028283463429337 \t 3.696332904798286\n",
      "51     \t [0.1821931  0.41363313 0.97942346]. \t  1.956012215021707 \t 3.696332904798286\n",
      "52     \t [0.97670192 0.60277487 0.51456542]. \t  0.3102779781254208 \t 3.696332904798286\n",
      "53     \t [0.50662622 0.76305035 0.37216274]. \t  0.6224866903904591 \t 3.696332904798286\n",
      "54     \t [0.16169722 0.89639586 0.64118007]. \t  2.6096560936025566 \t 3.696332904798286\n",
      "55     \t [0.43159644 0.93698497 0.04977396]. \t  0.001292452907674791 \t 3.696332904798286\n",
      "56     \t [0.69432931 0.05159973 0.76908267]. \t  0.3941346022445673 \t 3.696332904798286\n",
      "57     \t [0.01402908 0.3502412  0.26791591]. \t  0.41421946345271343 \t 3.696332904798286\n",
      "58     \t [0.44966131 0.84991615 0.60331068]. \t  2.1675072146281726 \t 3.696332904798286\n",
      "59     \t [0.1105219  0.94360722 0.42594413]. \t  1.7396587756900441 \t 3.696332904798286\n",
      "60     \t [0.02683116 0.38127875 0.63221084]. \t  1.1844055515237488 \t 3.696332904798286\n",
      "61     \t [0.85921402 0.5618037  0.86866038]. \t  \u001b[92m3.7169884282337264\u001b[0m \t 3.7169884282337264\n",
      "62     \t [0.2311224  0.95854223 0.90769924]. \t  0.8096281880376694 \t 3.7169884282337264\n",
      "63     \t [0.1804075 0.5240906 0.5425633]. \t  1.197415743994161 \t 3.7169884282337264\n",
      "64     \t [0.87248355 0.90149869 0.34945081]. \t  0.1478012610767934 \t 3.7169884282337264\n",
      "65     \t [0.21472163 0.90045944 0.86731212]. \t  1.3308691300125357 \t 3.7169884282337264\n",
      "66     \t [0.38177315 0.02835318 0.88536658]. \t  0.2763924648772922 \t 3.7169884282337264\n",
      "67     \t [0.15861111 0.18520739 0.24564524]. \t  0.8258591890051944 \t 3.7169884282337264\n",
      "68     \t [0.61021866 0.47757434 0.71395635]. \t  2.3427929360524082 \t 3.7169884282337264\n",
      "69     \t [0.4666647  0.40686551 0.1626586 ]. \t  0.3042994435249203 \t 3.7169884282337264\n",
      "70     \t [0.55776241 0.81016548 0.3718018 ]. \t  0.5854631611468374 \t 3.7169884282337264\n",
      "71     \t [0.51318579 0.10136964 0.87726569]. \t  0.5533882883899908 \t 3.7169884282337264\n",
      "72     \t [0.12754913 0.5649042  0.25773108]. \t  0.1950633929524088 \t 3.7169884282337264\n",
      "73     \t [0.62127425 0.79834131 0.21497893]. \t  0.04783879736338409 \t 3.7169884282337264\n",
      "74     \t [0.27175692 0.19949637 0.42863465]. \t  0.45431382625528927 \t 3.7169884282337264\n",
      "75     \t [0.55836213 0.76849103 0.54108996]. \t  1.5889504297336576 \t 3.7169884282337264\n",
      "76     \t [0.01162267 0.77563341 0.82705021]. \t  2.47587666857222 \t 3.7169884282337264\n",
      "77     \t [0.39552228 0.86980134 0.37932455]. \t  0.9363055865191319 \t 3.7169884282337264\n",
      "78     \t [0.3501268  0.2846168  0.37963161]. \t  0.5565989044037212 \t 3.7169884282337264\n",
      "79     \t [0.50079185 0.89201505 0.90844093]. \t  1.24393151205296 \t 3.7169884282337264\n",
      "80     \t [0.11810232 0.87645159 0.94729653]. \t  1.174884478022486 \t 3.7169884282337264\n",
      "81     \t [0.91891812 0.66423074 0.49006052]. \t  0.32678733806794863 \t 3.7169884282337264\n",
      "82     \t [0.07640205 0.57596116 0.49842286]. \t  1.2735081270053945 \t 3.7169884282337264\n",
      "83     \t [0.68074186 0.48429532 0.10419244]. \t  0.0878185562861984 \t 3.7169884282337264\n",
      "84     \t [0.38241431 0.01208905 0.77996516]. \t  0.281377207461615 \t 3.7169884282337264\n",
      "85     \t [0.77240272 0.1130135  0.94165953]. \t  0.44176186148207486 \t 3.7169884282337264\n",
      "86     \t [0.83322132 0.15248355 0.67012288]. \t  0.5393964745684465 \t 3.7169884282337264\n",
      "87     \t [0.70576803 0.65660646 0.14532371]. \t  0.02898922145235575 \t 3.7169884282337264\n",
      "88     \t [0.44978954 0.80119302 0.11776033]. \t  0.011192349860034865 \t 3.7169884282337264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.49207924 0.18659491 0.54731336]. \t  0.2751628713079586 \t 3.7169884282337264\n",
      "90     \t [0.73660985 0.19164309 0.98078296]. \t  0.6177517269426429 \t 3.7169884282337264\n",
      "91     \t [0.27854818 0.01255401 0.77187486]. \t  0.27971096302364934 \t 3.7169884282337264\n",
      "92     \t [0.23299495 0.51536524 0.86203195]. \t  \u001b[92m3.792695710694982\u001b[0m \t 3.792695710694982\n",
      "93     \t [0.87823746 0.39927767 0.2915064 ]. \t  0.21094708658993638 \t 3.792695710694982\n",
      "94     \t [0.7816664  0.46364056 0.87010867]. \t  3.4725796280725243 \t 3.792695710694982\n",
      "95     \t [0.89014707 0.33122848 0.38813943]. \t  0.20349648159430925 \t 3.792695710694982\n",
      "96     \t [0.13001166 0.66817201 0.22925082]. \t  0.12077318465871603 \t 3.792695710694982\n",
      "97     \t [0.03382991 0.85942112 0.50037557]. \t  2.7272350214576333 \t 3.792695710694982\n",
      "98     \t [0.20597371 0.04876477 0.00068067]. \t  0.10447910129532237 \t 3.792695710694982\n",
      "99     \t [0.65688467 0.18672101 0.74281171]. \t  0.9940940882459904 \t 3.792695710694982\n",
      "100    \t [0.1621024  0.04890557 0.62449311]. \t  0.19449508687840614 \t 3.792695710694982\n"
     ]
    }
   ],
   "source": [
    "### 6(e). Bayesian optimization runs (x20): GP run number = 5\n",
    "\n",
    "np.random.seed(run_num_5)\n",
    "surrogate_gp_5 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "gpgo_gp_5 = GPGO(surrogate_gp_5, Acquisition_new(util_gp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_5.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.71783409 0.04785513 0.94447198]. \t  0.23942957472217685 \t 2.653445131690671\n",
      "init   \t [0.68638004 0.58120733 0.14267862]. \t  0.05665494655696168 \t 2.653445131690671\n",
      "init   \t [0.94591918 0.33999059 0.62043546]. \t  0.7924779233128625 \t 2.653445131690671\n",
      "init   \t [0.45295964 0.23976742 0.03450459]. \t  0.16568631294324107 \t 2.653445131690671\n",
      "init   \t [0.14859311 0.81086617 0.65310538]. \t  2.653445131690671 \t 2.653445131690671\n",
      "1      \t [0. 1. 1.]. \t  0.330219860606422 \t 2.653445131690671\n",
      "2      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 2.653445131690671\n",
      "3      \t [0.         0.14421256 0.605396  ]. \t  0.3125715870387812 \t 2.653445131690671\n",
      "4      \t [1. 1. 1.]. \t  0.3168836207041561 \t 2.653445131690671\n",
      "5      \t [1. 0. 0.]. \t  0.03095471703300515 \t 2.653445131690671\n",
      "6      \t [0.44652648 1.         0.58132645]. \t  1.8184978148995572 \t 2.653445131690671\n",
      "7      \t [0.45140706 0.60773778 1.        ]. \t  2.0543322271641875 \t 2.653445131690671\n",
      "8      \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.653445131690671\n",
      "9      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.653445131690671\n",
      "10     \t [1.         0.35219893 1.        ]. \t  1.2780213362060437 \t 2.653445131690671\n",
      "11     \t [0.         0.42703282 1.        ]. \t  1.695670496780576 \t 2.653445131690671\n",
      "12     \t [0.         0.48200774 0.        ]. \t  0.020628823298673046 \t 2.653445131690671\n",
      "13     \t [1.         1.         0.49264761]. \t  0.218050389437231 \t 2.653445131690671\n",
      "14     \t [0. 0. 1.]. \t  0.0902894676548261 \t 2.653445131690671\n",
      "15     \t [0.         1.         0.48240234]. \t  2.1133884395284714 \t 2.653445131690671\n",
      "16     \t [1.         0.         0.55678867]. \t  0.072255771989955 \t 2.653445131690671\n",
      "17     \t [1.         0.47186494 0.        ]. \t  0.010081102725802654 \t 2.653445131690671\n",
      "18     \t [0.41675562 1.         0.        ]. \t  0.00023627001244381227 \t 2.653445131690671\n",
      "19     \t [0.39708502 0.         0.41015402]. \t  0.4756195689762221 \t 2.653445131690671\n",
      "20     \t [0.27581216 0.25593113 1.        ]. \t  0.8085327955850942 \t 2.653445131690671\n",
      "21     \t [1.         0.71670486 0.84191596]. \t  \u001b[92m2.782811808825258\u001b[0m \t 2.782811808825258\n",
      "22     \t [0.61039712 0.         0.        ]. \t  0.08583482346107348 \t 2.782811808825258\n",
      "23     \t [0.71352299 0.71480923 0.79623862]. \t  2.645365037631002 \t 2.782811808825258\n",
      "24     \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.782811808825258\n",
      "25     \t [0.         0.64700712 0.35846089]. \t  0.5818873279978735 \t 2.782811808825258\n",
      "26     \t [0.51834715 1.         1.        ]. \t  0.33198685504225567 \t 2.782811808825258\n",
      "27     \t [0.         0.74328574 0.82540371]. \t  2.757977593934127 \t 2.782811808825258\n",
      "28     \t [1.         0.68642526 0.33650291]. \t  0.05896237031330917 \t 2.782811808825258\n",
      "29     \t [0.28027877 0.         0.77680871]. \t  0.24952943359252486 \t 2.782811808825258\n",
      "30     \t [0.29791845 0.72227312 0.        ]. \t  0.0031715519878030263 \t 2.782811808825258\n",
      "31     \t [0.        0.        0.3198057]. \t  0.5342943451409007 \t 2.782811808825258\n",
      "32     \t [0.91646153 0.6975659  1.        ]. \t  1.7165242023168308 \t 2.782811808825258\n",
      "33     \t [8.36549792e-01 5.99571158e-04 2.17026697e-01]. \t  0.42006076783922247 \t 2.782811808825258\n",
      "34     \t [0.         1.         0.74444246]. \t  1.1507833158388754 \t 2.782811808825258\n",
      "35     \t [0.66683943 0.85328708 0.        ]. \t  0.0005123840928527548 \t 2.782811808825258\n",
      "36     \t [0.41453261 0.54188025 0.71634251]. \t  2.5899499421773093 \t 2.782811808825258\n",
      "37     \t [0.18062906 0.67672083 0.92992469]. \t  \u001b[92m2.8939310436341046\u001b[0m \t 2.8939310436341046\n",
      "38     \t [0.16600121 0.96404379 0.34519825]. \t  0.7340424334377929 \t 2.8939310436341046\n",
      "39     \t [3.93738686e-01 1.35431736e-08 1.00000000e+00]. \t  0.09173743404230622 \t 2.8939310436341046\n",
      "40     \t [0.00750032 0.73180422 0.99172234]. \t  1.6945030735473703 \t 2.8939310436341046\n",
      "41     \t [0.24901191 0.         0.        ]. \t  0.09793152450034166 \t 2.8939310436341046\n",
      "42     \t [1.         0.15075965 0.24309356]. \t  0.29425161609917355 \t 2.8939310436341046\n",
      "43     \t [0.48830362 0.79018121 0.69536188]. \t  1.9089832992505071 \t 2.8939310436341046\n",
      "44     \t [0.8396906  1.         0.79088986]. \t  0.5246996431705817 \t 2.8939310436341046\n",
      "45     \t [0.00000000e+00 5.04492513e-08 6.52251915e-01]. \t  0.1511107202952928 \t 2.8939310436341046\n",
      "46     \t [0.22785945 1.         0.97168865]. \t  0.41738442223727795 \t 2.8939310436341046\n",
      "47     \t [0.67358198 1.         0.24958788]. \t  0.06046880157608349 \t 2.8939310436341046\n",
      "48     \t [0.0930859  0.59269887 0.7290922 ]. \t  2.8545161830911177 \t 2.8939310436341046\n",
      "49     \t [0.75005728 0.49202183 0.90363891]. \t  \u001b[92m3.4012006836143707\u001b[0m \t 3.4012006836143707\n",
      "50     \t [0.99084298 0.59751262 0.89999019]. \t  \u001b[92m3.4388819486597617\u001b[0m \t 3.4388819486597617\n",
      "51     \t [0.97039282 0.50572725 0.89163438]. \t  \u001b[92m3.481559017977047\u001b[0m \t 3.481559017977047\n",
      "52     \t [0.63072631 0.52064937 0.85479633]. \t  \u001b[92m3.7770819717651856\u001b[0m \t 3.7770819717651856\n",
      "53     \t [0.5434711  0.51392263 0.87934777]. \t  3.709808882743553 \t 3.7770819717651856\n",
      "54     \t [0.61212713 0.49533848 0.86930819]. \t  3.6759029995362513 \t 3.7770819717651856\n",
      "55     \t [0.9631663  0.52095042 0.85830527]. \t  3.6596538864358297 \t 3.7770819717651856\n",
      "56     \t [0.6524437  0.46943681 0.85863662]. \t  3.5692783696980914 \t 3.7770819717651856\n",
      "57     \t [0.78826637 0.54335973 0.84019004]. \t  3.7330134935788655 \t 3.7770819717651856\n",
      "58     \t [0.57035754 0.68628059 0.84756518]. \t  3.2342544465394414 \t 3.7770819717651856\n",
      "59     \t [0.50952084 0.61856818 0.82787229]. \t  3.6127033853910864 \t 3.7770819717651856\n",
      "60     \t [0.98886699 0.35214384 0.88368314]. \t  2.446933068155871 \t 3.7770819717651856\n",
      "61     \t [0.57662347 0.55470529 0.83726677]. \t  \u001b[92m3.79251502869015\u001b[0m \t 3.79251502869015\n",
      "62     \t [0.31895894 0.59794229 0.80767738]. \t  3.6082514653622955 \t 3.79251502869015\n",
      "63     \t [0.68374972 0.54460282 0.87484757]. \t  3.756108786940721 \t 3.79251502869015\n",
      "64     \t [0.61833676 0.53162701 0.87445814]. \t  3.7589690065656365 \t 3.79251502869015\n",
      "65     \t [0.55118074 0.50519669 0.83333666]. \t  3.718150943460705 \t 3.79251502869015\n",
      "66     \t [0.60192072 0.51354417 0.85185086]. \t  3.766811200249771 \t 3.79251502869015\n",
      "67     \t [0.95916693 0.53547483 0.84648703]. \t  3.6740436321434027 \t 3.79251502869015\n",
      "68     \t [0.79576241 0.53914262 0.83864214]. \t  3.723116178282974 \t 3.79251502869015\n",
      "69     \t [0.49881258 0.63708561 0.88441399]. \t  3.5315428554406716 \t 3.79251502869015\n",
      "70     \t [0.50494332 0.54772335 0.84354263]. \t  \u001b[92m3.8280445599709836\u001b[0m \t 3.8280445599709836\n",
      "71     \t [0.54655616 0.51529183 0.89574174]. \t  3.5997291957214275 \t 3.8280445599709836\n",
      "72     \t [0.47304383 0.6187887  0.83442623]. \t  3.6530320153688733 \t 3.8280445599709836\n",
      "73     \t [0.72262241 0.50827583 0.87591455]. \t  3.669524605287596 \t 3.8280445599709836\n",
      "74     \t [7.18844824e-08 7.51387434e-01 0.00000000e+00]. \t  0.0016376277176384563 \t 3.8280445599709836\n",
      "75     \t [0.86514482 0.56617247 0.88141155]. \t  3.6657423589158915 \t 3.8280445599709836\n",
      "76     \t [0.60382578 0.44192854 0.87813049]. \t  3.336169468280636 \t 3.8280445599709836\n",
      "77     \t [0.24565148 0.53860515 0.86062778]. \t  \u001b[92m3.843478576732947\u001b[0m \t 3.843478576732947\n",
      "78     \t [0.03734491 0.66651142 0.8053755 ]. \t  3.2695565220658334 \t 3.843478576732947\n",
      "79     \t [0.65038741 0.55605099 0.91857304]. \t  3.4081564692769293 \t 3.843478576732947\n",
      "80     \t [0.64781726 0.50555479 0.83272852]. \t  3.6924915517953667 \t 3.843478576732947\n",
      "81     \t [0.46610483 0.58942423 0.84967888]. \t  3.7985453772060787 \t 3.843478576732947\n",
      "82     \t [0.17796339 0.64860066 0.80008305]. \t  3.3607182656509647 \t 3.843478576732947\n",
      "83     \t [0.664317   0.55794445 0.88999799]. \t  3.682254408827846 \t 3.843478576732947\n",
      "84     \t [0.36250881 0.55604314 0.78439211]. \t  3.4528838331045066 \t 3.843478576732947\n",
      "85     \t [0.22507535 0.61908698 0.82147247]. \t  3.631056946163421 \t 3.843478576732947\n",
      "86     \t [0.45263253 0.64281533 0.85451876]. \t  3.578787892695947 \t 3.843478576732947\n",
      "87     \t [0.83322825 0.52867207 0.83691515]. \t  3.692903635208783 \t 3.843478576732947\n",
      "88     \t [0.65627752 0.59386935 0.83874339]. \t  3.705805601714844 \t 3.843478576732947\n",
      "89     \t [0.83475437 0.51546386 0.93813636]. \t  3.0332748154840057 \t 3.843478576732947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.43229765 0.63475502 0.85278179]. \t  3.627183459424452 \t 3.843478576732947\n",
      "91     \t [0.95503929 0.58730325 0.79437499]. \t  3.247482904619692 \t 3.843478576732947\n",
      "92     \t [0.77167449 0.56127307 0.87083914]. \t  3.743847965642183 \t 3.843478576732947\n",
      "93     \t [0.38879838 0.51397212 0.83971091]. \t  3.7873165136814286 \t 3.843478576732947\n",
      "94     \t [0.59944541 0.57451079 0.85999768]. \t  3.8009955735329406 \t 3.843478576732947\n",
      "95     \t [0.38008394 0.48464751 0.86417086]. \t  3.673345076610463 \t 3.843478576732947\n",
      "96     \t [0.30636817 0.54176907 0.8578429 ]. \t  \u001b[92m3.852870163398532\u001b[0m \t 3.852870163398532\n",
      "97     \t [0.71345481 0.52699276 0.84586713]. \t  3.7580267540320804 \t 3.852870163398532\n",
      "98     \t [0.79685936 0.51588349 0.85318018]. \t  3.715554425180999 \t 3.852870163398532\n",
      "99     \t [0.82080062 0.5420274  0.83111804]. \t  3.6827048681412515 \t 3.852870163398532\n",
      "100    \t [0.4220209  0.58512838 0.85147439]. \t  3.818040012683383 \t 3.852870163398532\n"
     ]
    }
   ],
   "source": [
    "### 6(e). Bayesian optimization runs (x20): STP DF1 run number = 5\n",
    "\n",
    "np.random.seed(run_num_5)\n",
    "surrogate_stp_df1_5 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_5 = GPGO(surrogate_stp_df1_3, Acquisition_new(util_stp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_5.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.658056628390561, -4.614227419023595)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(e). Training Regret Minimisation: run number = 5\n",
    "\n",
    "gp_output_5 = np.append(np.max(gpgo_gp_5.GP.y[0:n_init]),gpgo_gp_5.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_5 = np.append(np.max(gpgo_stp_df1_5.GP.y[0:n_init]),gpgo_stp_df1_5.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_5 = np.log(y_global_orig - gp_output_5)\n",
    "regret_stp_df1_5 = np.log(y_global_orig - stp_df1_output_5)\n",
    "\n",
    "train_regret_gp_5 = min_max_array(regret_gp_5)\n",
    "train_regret_stp_df1_5 = min_max_array(regret_stp_df1_5)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 5\n",
    "min_train_regret_gp_5 = min(train_regret_gp_5)\n",
    "min_train_regret_stp_df1_5 = min(train_regret_stp_df1_5)\n",
    "\n",
    "min_train_regret_gp_5, min_train_regret_stp_df1_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.70043712 0.84418664 0.67651434]. \t  1.2062241992418108 \t 1.2062241992418108\n",
      "init   \t [0.72785806 0.95145796 0.0127032 ]. \t  0.00022509413622777657 \t 1.2062241992418108\n",
      "init   \t [0.4135877  0.04881279 0.09992856]. \t  0.40947924176415684 \t 1.2062241992418108\n",
      "init   \t [0.50806631 0.20024754 0.74415417]. \t  1.089455749271998 \t 1.2062241992418108\n",
      "init   \t [0.192892   0.70084475 0.29322811]. \t  0.3106827413404766 \t 1.2062241992418108\n",
      "1      \t [0.99199232 0.2122575  0.94737066]. \t  0.8898926780623154 \t 1.2062241992418108\n",
      "2      \t [0.18831673 0.95207131 0.97450331]. \t  0.5979987320647504 \t 1.2062241992418108\n",
      "3      \t [0.92419115 0.09815225 0.03236548]. \t  0.07543980589730874 \t 1.2062241992418108\n",
      "4      \t [0.1353476  0.02231523 0.72994998]. \t  0.2762845548602274 \t 1.2062241992418108\n",
      "5      \t [0.5320398  0.71357229 0.99914264]. \t  \u001b[92m1.7019893771753856\u001b[0m \t 1.7019893771753856\n",
      "6      \t [0.96967939 0.91981133 0.88837821]. \t  1.0045027062071255 \t 1.7019893771753856\n",
      "7      \t [0.02249897 0.57634127 0.96563272]. \t  \u001b[92m2.6749226727400632\u001b[0m \t 2.6749226727400632\n",
      "8      \t [0.96280668 0.42546281 0.42732808]. \t  0.12450505366471323 \t 2.6749226727400632\n",
      "9      \t [0.03142085 0.00353406 0.98828589]. \t  0.10549017517439943 \t 2.6749226727400632\n",
      "10     \t [0.0648346  0.08127815 0.04970036]. \t  0.1807571184471449 \t 2.6749226727400632\n",
      "11     \t [0.62557271 0.86027503 0.20650202]. \t  0.038354080845226526 \t 2.6749226727400632\n",
      "12     \t [0.35357179 0.8359121  0.95966125]. \t  1.381317354705555 \t 2.6749226727400632\n",
      "13     \t [0.00737672 0.98557163 0.08685918]. \t  0.003741361176768445 \t 2.6749226727400632\n",
      "14     \t [0.98743524 0.14535486 0.07341882]. \t  0.10192884887965428 \t 2.6749226727400632\n",
      "15     \t [0.25167071 0.4501802  0.86025165]. \t  \u001b[92m3.4802307312461145\u001b[0m \t 3.4802307312461145\n",
      "16     \t [0.44392798 0.61687422 0.67463302]. \t  2.138734663455632 \t 3.4802307312461145\n",
      "17     \t [0.17954273 0.372267   0.9973969 ]. \t  1.4656729271260875 \t 3.4802307312461145\n",
      "18     \t [0.         0.52748265 0.74210201]. \t  2.932731699327913 \t 3.4802307312461145\n",
      "19     \t [0.26658556 0.42072028 0.        ]. \t  0.04520425571965342 \t 3.4802307312461145\n",
      "20     \t [1.         1.         0.26461147]. \t  0.019054330766772126 \t 3.4802307312461145\n",
      "21     \t [0.23136664 0.63002485 0.82945864]. \t  \u001b[92m3.618951757309445\u001b[0m \t 3.618951757309445\n",
      "22     \t [0.99842533 0.41069338 0.84161296]. \t  3.0702553369570653 \t 3.618951757309445\n",
      "23     \t [0.46521953 0.17228426 0.76268939]. \t  0.9731679617437283 \t 3.618951757309445\n",
      "24     \t [0.45177568 0.6113685  0.51802111]. \t  1.2056117170432679 \t 3.618951757309445\n",
      "25     \t [0.50328841 0.17750799 0.77655938]. \t  1.0378665692480444 \t 3.618951757309445\n",
      "26     \t [0.25562751 0.41536701 0.08138154]. \t  0.1404761401233126 \t 3.618951757309445\n",
      "27     \t [0.30966918 0.45759835 0.06930978]. \t  0.09609664345560852 \t 3.618951757309445\n",
      "28     \t [0.5698999  0.31114695 0.62591035]. \t  0.8389278785809439 \t 3.618951757309445\n",
      "29     \t [0.60065506 0.77222123 0.35299121]. \t  0.39836124420232405 \t 3.618951757309445\n",
      "30     \t [0.45646851 0.38386258 0.78930417]. \t  2.7692183972653472 \t 3.618951757309445\n",
      "31     \t [0.92011301 0.63097443 0.89763353]. \t  3.3610121386285896 \t 3.618951757309445\n",
      "32     \t [0.02491158 0.62582456 0.83780949]. \t  \u001b[92m3.631395910951411\u001b[0m \t 3.631395910951411\n",
      "33     \t [0.86663359 0.30828141 0.90733757]. \t  1.9138393167803975 \t 3.631395910951411\n",
      "34     \t [0.42975177 0.68042721 0.66818432]. \t  2.1413886767991763 \t 3.631395910951411\n",
      "35     \t [0.76956454 0.53412624 0.1728041 ]. \t  0.08619906471193466 \t 3.631395910951411\n",
      "36     \t [0.31107226 0.79623301 0.82484784]. \t  2.2875337035832195 \t 3.631395910951411\n",
      "37     \t [0.48868579 0.99151604 0.59512537]. \t  1.6679563447512133 \t 3.631395910951411\n",
      "38     \t [0.5801927  0.49920576 0.3535595 ]. \t  0.2807334721603921 \t 3.631395910951411\n",
      "39     \t [0.91515911 0.78037218 0.01156124]. \t  0.0007607848756998799 \t 3.631395910951411\n",
      "40     \t [0.47245819 0.41007766 0.00344366]. \t  0.0508328742252886 \t 3.631395910951411\n",
      "41     \t [0.84779045 0.74309193 0.16877549]. \t  0.013108713841932374 \t 3.631395910951411\n",
      "42     \t [0.99868682 0.34453368 0.56645692]. \t  0.4253343450248834 \t 3.631395910951411\n",
      "43     \t [0.94464404 0.38952174 0.60476453]. \t  0.750169742774895 \t 3.631395910951411\n",
      "44     \t [0.78354625 0.02688303 0.14376768]. \t  0.3482621911016577 \t 3.631395910951411\n",
      "45     \t [0.70126879 0.5671906  0.58386907]. \t  0.9425030595600892 \t 3.631395910951411\n",
      "46     \t [0.87539429 0.44936701 0.00366957]. \t  0.019085587890249934 \t 3.631395910951411\n",
      "47     \t [1.         0.54238994 1.        ]. \t  1.9966990819268216 \t 3.631395910951411\n",
      "48     \t [0.19986681 0.79746444 0.22686557]. \t  0.11849865787879467 \t 3.631395910951411\n",
      "49     \t [0.64139538 0.48348759 0.75209892]. \t  2.8686938108725117 \t 3.631395910951411\n",
      "50     \t [0.74859362 0.4747356  0.99417511]. \t  1.9912237318464052 \t 3.631395910951411\n",
      "51     \t [0.31250508 0.10400265 0.07861129]. \t  0.3398300075318897 \t 3.631395910951411\n",
      "52     \t [0.8405843  0.55480266 0.91902122]. \t  3.352401949813673 \t 3.631395910951411\n",
      "53     \t [0.97519071 0.22251482 0.5018128 ]. \t  0.15622412474763067 \t 3.631395910951411\n",
      "54     \t [0.61025189 0.14745057 0.79563995]. \t  0.8680512060592391 \t 3.631395910951411\n",
      "55     \t [0.14800953 0.02150931 0.45399279]. \t  0.28927406725164967 \t 3.631395910951411\n",
      "56     \t [0.66457175 0.36404906 0.55954109]. \t  0.5042935717545962 \t 3.631395910951411\n",
      "57     \t [0.13865804 0.23212003 0.3023823 ]. \t  0.7280600263738355 \t 3.631395910951411\n",
      "58     \t [0.7971024  0.23188818 0.45966989]. \t  0.22052170061046178 \t 3.631395910951411\n",
      "59     \t [0.96627101 0.02116893 0.79871835]. \t  0.2998754490863392 \t 3.631395910951411\n",
      "60     \t [0.15397015 0.77395656 0.36659253]. \t  0.9460495431428722 \t 3.631395910951411\n",
      "61     \t [0.91040329 0.77031951 0.26875232]. \t  0.039773410300865286 \t 3.631395910951411\n",
      "62     \t [0.14265588 0.47810206 0.80292986]. \t  3.466786518373034 \t 3.631395910951411\n",
      "63     \t [0.16751845 0.04494349 0.86422278]. \t  0.3471116130338163 \t 3.631395910951411\n",
      "64     \t [0.11908225 0.37692245 0.16105509]. \t  0.30320683659110514 \t 3.631395910951411\n",
      "65     \t [0.84209823 0.59777435 0.61223325]. \t  0.9821792659747005 \t 3.631395910951411\n",
      "66     \t [0.14423902 0.90849685 0.05056491]. \t  0.001840933922314098 \t 3.631395910951411\n",
      "67     \t [0.51304798 0.9750974  0.02710961]. \t  0.0004971364831303624 \t 3.631395910951411\n",
      "68     \t [0.85583446 0.84741916 0.79197645]. \t  1.422331338451765 \t 3.631395910951411\n",
      "69     \t [0.32022152 0.56586823 0.47474941]. \t  0.9643977766692223 \t 3.631395910951411\n",
      "70     \t [0.17569456 0.32155976 0.72713279]. \t  1.8021683463164861 \t 3.631395910951411\n",
      "71     \t [0.47337045 0.81134363 0.66226146]. \t  1.9356129641772624 \t 3.631395910951411\n",
      "72     \t [0.47140452 0.80047927 0.16305659]. \t  0.025810136828398656 \t 3.631395910951411\n",
      "73     \t [0.15373939 0.38265696 0.22692969]. \t  0.42001486203907246 \t 3.631395910951411\n",
      "74     \t [0.66832469 0.73309849 0.60309759]. \t  1.3028137449723738 \t 3.631395910951411\n",
      "75     \t [0.5787669  0.25205005 0.87834777]. \t  1.5915411128199615 \t 3.631395910951411\n",
      "76     \t [0.07789003 0.3933394  0.70706182]. \t  2.0202816906050076 \t 3.631395910951411\n",
      "77     \t [0.23631666 0.54709394 0.42989696]. \t  0.7197134317844194 \t 3.631395910951411\n",
      "78     \t [0.69852897 0.86559088 0.16347082]. \t  0.012641072204148 \t 3.631395910951411\n",
      "79     \t [0.47671872 0.89088123 0.74104059]. \t  1.4394541667887728 \t 3.631395910951411\n",
      "80     \t [0.43261612 0.69083174 0.04334452]. \t  0.008766268230116813 \t 3.631395910951411\n",
      "81     \t [0.08837282 0.1829417  0.85135761]. \t  1.0844105423554566 \t 3.631395910951411\n",
      "82     \t [0.0205107  0.98119877 0.7969542 ]. \t  0.9712061280277842 \t 3.631395910951411\n",
      "83     \t [0.41888587 0.39371565 0.0348003 ]. \t  0.09125023015021681 \t 3.631395910951411\n",
      "84     \t [0.74644793 0.36422059 0.46081572]. \t  0.2341971764235136 \t 3.631395910951411\n",
      "85     \t [0.63837092 0.27058218 0.74493206]. \t  1.5638462605070549 \t 3.631395910951411\n",
      "86     \t [0.86332783 0.7429181  0.47475537]. \t  0.42384142002844566 \t 3.631395910951411\n",
      "87     \t [0.17529514 0.46584793 0.80065333]. \t  3.396476016905371 \t 3.631395910951411\n",
      "88     \t [0.31663395 0.05878803 0.34986774]. \t  0.7835788856992688 \t 3.631395910951411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.96693731 0.04089108 0.2218996 ]. \t  0.3034259080999027 \t 3.631395910951411\n",
      "90     \t [0.63194619 0.85029628 0.99538087]. \t  0.9819249388532151 \t 3.631395910951411\n",
      "91     \t [0.16676123 0.73189205 0.82632016]. \t  2.8921679168004295 \t 3.631395910951411\n",
      "92     \t [0.3483706  0.19890713 0.59694574]. \t  0.41261201587627494 \t 3.631395910951411\n",
      "93     \t [0.65499867 0.07044948 0.14209422]. \t  0.4783105637389237 \t 3.631395910951411\n",
      "94     \t [0.53240164 0.55647344 0.52951779]. \t  0.8873554071189568 \t 3.631395910951411\n",
      "95     \t [0.30651514 0.99513644 0.86322508]. \t  0.7029896029366762 \t 3.631395910951411\n",
      "96     \t [0.78305025 0.29481564 0.58385283]. \t  0.491672717461569 \t 3.631395910951411\n",
      "97     \t [0.01972056 0.20133133 0.70928067]. \t  0.9283438729774103 \t 3.631395910951411\n",
      "98     \t [0.79695044 0.81877152 0.01329687]. \t  0.00071201189510698 \t 3.631395910951411\n",
      "99     \t [0.52215673 0.86370663 0.74815905]. \t  1.5143871972076686 \t 3.631395910951411\n",
      "100    \t [0.30436324 0.66676446 0.15994862]. \t  0.05032680391633405 \t 3.631395910951411\n"
     ]
    }
   ],
   "source": [
    "### 6(f). Bayesian optimization runs (x20): GP run number = 6\n",
    "\n",
    "np.random.seed(run_num_6)\n",
    "surrogate_gp_6 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "gpgo_gp_6 = GPGO(surrogate_gp_6, Acquisition_new(util_gp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_6.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.70043712 0.84418664 0.67651434]. \t  1.2062241992418108 \t 1.2062241992418108\n",
      "init   \t [0.72785806 0.95145796 0.0127032 ]. \t  0.00022509413622777657 \t 1.2062241992418108\n",
      "init   \t [0.4135877  0.04881279 0.09992856]. \t  0.40947924176415684 \t 1.2062241992418108\n",
      "init   \t [0.50806631 0.20024754 0.74415417]. \t  1.089455749271998 \t 1.2062241992418108\n",
      "init   \t [0.192892   0.70084475 0.29322811]. \t  0.3106827413404766 \t 1.2062241992418108\n",
      "1      \t [0. 1. 1.]. \t  0.330219860606422 \t 1.2062241992418108\n",
      "2      \t [1. 0. 1.]. \t  0.08848201872702738 \t 1.2062241992418108\n",
      "3      \t [0. 0. 1.]. \t  0.0902894676548261 \t 1.2062241992418108\n",
      "4      \t [ 1.00000000e+00  0.00000000e+00 -1.11022302e-16]. \t  0.030954717033005095 \t 1.2062241992418108\n",
      "5      \t [1. 1. 1.]. \t  0.31688362070415665 \t 1.2062241992418108\n",
      "6      \t [1.        0.5428576 0.4121276]. \t  0.09890311393321821 \t 1.2062241992418108\n",
      "7      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 1.2062241992418108\n",
      "8      \t [0.54187671 0.56256648 1.        ]. \t  \u001b[92m2.081972432637626\u001b[0m \t 2.081972432637626\n",
      "9      \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.081972432637626\n",
      "10     \t [0.48107725 1.         1.        ]. \t  0.33264162663119534 \t 2.081972432637626\n",
      "11     \t [0.         0.49756698 1.        ]. \t  1.9611015454101317 \t 2.081972432637626\n",
      "12     \t [1.         1.         0.28750319]. \t  0.027889662105655504 \t 2.081972432637626\n",
      "13     \t [0.         0.         0.48674894]. \t  0.15442652398949674 \t 2.081972432637626\n",
      "14     \t [0.         1.         0.52247596]. \t  \u001b[92m2.403350520968383\u001b[0m \t 2.403350520968383\n",
      "15     \t [1.         0.44192308 1.        ]. \t  1.7224163534435915 \t 2.403350520968383\n",
      "16     \t [1.         0.         0.47232761]. \t  0.08737420172042377 \t 2.403350520968383\n",
      "17     \t [0.34567637 1.         0.31444072]. \t  0.3826639373456029 \t 2.403350520968383\n",
      "18     \t [0.176854   0.72593085 0.79296432]. \t  \u001b[92m2.8270707222048257\u001b[0m \t 2.8270707222048257\n",
      "19     \t [1.         0.74174965 0.        ]. \t  0.000739127262821894 \t 2.8270707222048257\n",
      "20     \t [0.45734582 0.         1.        ]. \t  0.09170617769488082 \t 2.8270707222048257\n",
      "21     \t [0.         0.37646373 0.        ]. \t  0.03978164196860876 \t 2.8270707222048257\n",
      "22     \t [0.         0.62402587 0.71186321]. \t  2.645616688331641 \t 2.8270707222048257\n",
      "23     \t [0.69695464 0.34962877 0.        ]. \t  0.04942180301825342 \t 2.8270707222048257\n",
      "24     \t [0.16675697 0.32702711 0.85167415]. \t  2.3966373546409563 \t 2.8270707222048257\n",
      "25     \t [1.         0.72630603 0.7931628 ]. \t  2.3592685780611697 \t 2.8270707222048257\n",
      "26     \t [0.29644823 0.73013855 0.        ]. \t  0.0029045163770924105 \t 2.8270707222048257\n",
      "27     \t [0.68109167 0.         0.        ]. \t  0.07632468236235812 \t 2.8270707222048257\n",
      "28     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.8270707222048257\n",
      "29     \t [1.         1.         0.72084502]. \t  0.35276344708837837 \t 2.8270707222048257\n",
      "30     \t [0.22677495 0.         0.76216547]. \t  0.2444918338158073 \t 2.8270707222048257\n",
      "31     \t [1.         0.22575721 0.77607778]. \t  1.3343220171617616 \t 2.8270707222048257\n",
      "32     \t [0.27169908 0.15471403 0.        ]. \t  0.1123616744584282 \t 2.8270707222048257\n",
      "33     \t [0.79955914 0.44363198 0.90151492]. \t  \u001b[92m3.136229521813524\u001b[0m \t 3.136229521813524\n",
      "34     \t [0.22613488 1.         0.776285  ]. \t  0.9620227281651821 \t 3.136229521813524\n",
      "35     \t [0.81070279 0.76640508 0.93776331]. \t  2.0749442498936177 \t 3.136229521813524\n",
      "36     \t [3.90042409e-09 8.83894896e-01 3.12941706e-01]. \t  0.5027265990668596 \t 3.136229521813524\n",
      "37     \t [0.72683004 0.         0.54205661]. \t  0.10413937561961004 \t 3.136229521813524\n",
      "38     \t [0.52180674 0.54942136 0.79920115]. \t  \u001b[92m3.554965583996025\u001b[0m \t 3.554965583996025\n",
      "39     \t [0.79378772 0.54645529 0.81312998]. \t  \u001b[92m3.5752548036537846\u001b[0m \t 3.5752548036537846\n",
      "40     \t [0.         0.13440476 0.16181682]. \t  0.47481381967449837 \t 3.5752548036537846\n",
      "41     \t [0.64639615 0.63262606 0.18243734]. \t  0.05587126595781916 \t 3.5752548036537846\n",
      "42     \t [0.32393453 0.52730818 0.65944055]. \t  1.962329710622442 \t 3.5752548036537846\n",
      "43     \t [0.31645573 1.         0.        ]. \t  0.0002677851398950019 \t 3.5752548036537846\n",
      "44     \t [0.         0.36158707 0.76489725]. \t  2.3832695194347786 \t 3.5752548036537846\n",
      "45     \t [0.57842576 0.54189794 0.84854498]. \t  \u001b[92m3.8171889097993197\u001b[0m \t 3.8171889097993197\n",
      "46     \t [0.61961755 0.5734313  0.8787922 ]. \t  3.7476377003473234 \t 3.8171889097993197\n",
      "47     \t [0.36239325 0.54073067 0.86061986]. \t  \u001b[92m3.8469195211913734\u001b[0m \t 3.8469195211913734\n",
      "48     \t [0.         1.         0.81344113]. \t  0.8090613171216462 \t 3.8469195211913734\n",
      "49     \t [0.90079921 0.00151781 0.18093451]. \t  0.2994337382769649 \t 3.8469195211913734\n",
      "50     \t [0.21333046 0.54373475 0.92049326]. \t  3.3979844732177438 \t 3.8469195211913734\n",
      "51     \t [0.61239304 0.5488865  0.86532115]. \t  3.8056882674176604 \t 3.8469195211913734\n",
      "52     \t [0.61390982 0.50933368 0.84360685]. \t  3.7436059120641016 \t 3.8469195211913734\n",
      "53     \t [0.7778354  0.53629267 0.82634649]. \t  3.671633872959347 \t 3.8469195211913734\n",
      "54     \t [0.40340924 0.60815301 0.89184347]. \t  3.6241064102623044 \t 3.8469195211913734\n",
      "55     \t [0.45553787 0.55900414 0.8683689 ]. \t  3.8283551348471807 \t 3.8469195211913734\n",
      "56     \t [0.17478475 0.53863758 0.88771689]. \t  3.7165229331077567 \t 3.8469195211913734\n",
      "57     \t [0.43825691 0.55175266 0.86392542]. \t  3.841449841654266 \t 3.8469195211913734\n",
      "58     \t [0.75156794 0.53790401 0.84810406]. \t  3.762096056488323 \t 3.8469195211913734\n",
      "59     \t [0.61020215 0.57519594 0.8161589 ]. \t  3.6405556214947232 \t 3.8469195211913734\n",
      "60     \t [0.161784   0.59160036 0.89894793]. \t  3.604117944286168 \t 3.8469195211913734\n",
      "61     \t [0.38141016 0.51293985 0.82858929]. \t  3.7485654407089806 \t 3.8469195211913734\n",
      "62     \t [0.35645951 0.60225067 0.81221705]. \t  3.621108266259831 \t 3.8469195211913734\n",
      "63     \t [0.40499589 0.56702482 0.84868487]. \t  3.846518635977901 \t 3.8469195211913734\n",
      "64     \t [0.74943966 0.50886168 0.83521186]. \t  3.6792342037441443 \t 3.8469195211913734\n",
      "65     \t [0.50043009 0.56153794 0.846133  ]. \t  3.831255242574026 \t 3.8469195211913734\n",
      "66     \t [0.66458832 0.52963896 0.88768013]. \t  3.6727502427938497 \t 3.8469195211913734\n",
      "67     \t [0.44300697 0.52493768 0.90787591]. \t  3.519199662948913 \t 3.8469195211913734\n",
      "68     \t [0.38122007 0.5385906  0.84311532]. \t  3.8400645767844916 \t 3.8469195211913734\n",
      "69     \t [0.26991031 0.50552085 0.82727683]. \t  3.726140921292502 \t 3.8469195211913734\n",
      "70     \t [0.26532202 0.57771876 0.85357292]. \t  3.842989117532316 \t 3.8469195211913734\n",
      "71     \t [0.53866622 0.53388384 0.85869318]. \t  3.8191507143518195 \t 3.8469195211913734\n",
      "72     \t [0.17255986 0.58687433 0.85725994]. \t  3.817088601859581 \t 3.8469195211913734\n",
      "73     \t [0.34720434 0.54131917 0.84812635]. \t  \u001b[92m3.8521177652615153\u001b[0m \t 3.8521177652615153\n",
      "74     \t [0.58807402 0.54682516 0.86853762]. \t  3.8027033508191384 \t 3.8521177652615153\n",
      "75     \t [0.18693955 0.65506574 0.85110951]. \t  3.5247653774623635 \t 3.8521177652615153\n",
      "76     \t [0.29364755 0.50641145 0.86287836]. \t  3.7664451473982323 \t 3.8521177652615153\n",
      "77     \t [0.16738454 0.57784604 0.85234423]. \t  3.8347193424694495 \t 3.8521177652615153\n",
      "78     \t [0.29891625 0.55420426 0.86589217]. \t  3.8449513809469154 \t 3.8521177652615153\n",
      "79     \t [0.36145262 0.63147896 0.82887896]. \t  3.5958564192140674 \t 3.8521177652615153\n",
      "80     \t [0.3041068  0.63400232 0.82144375]. \t  3.5569510698498066 \t 3.8521177652615153\n",
      "81     \t [0.63788919 0.54617193 0.81516412]. \t  3.651511405873558 \t 3.8521177652615153\n",
      "82     \t [0.90014033 0.00845993 0.77269552]. \t  0.26396366609173655 \t 3.8521177652615153\n",
      "83     \t [0.4531353  0.57344589 0.83734874]. \t  3.80521561007876 \t 3.8521177652615153\n",
      "84     \t [0.         1.         0.25322506]. \t  0.161581440701967 \t 3.8521177652615153\n",
      "85     \t [0.99072357 0.21056183 0.03355631]. \t  0.055765698674678794 \t 3.8521177652615153\n",
      "86     \t [0.41413086 0.63275058 0.89457693]. \t  3.5011118093738176 \t 3.8521177652615153\n",
      "87     \t [0.34034549 0.54391568 0.86473541]. \t  3.8425964040294787 \t 3.8521177652615153\n",
      "88     \t [0.5752533  0.49978464 0.83147782]. \t  3.688654267905017 \t 3.8521177652615153\n",
      "89     \t [0.59770084 0.56300667 0.82179677]. \t  3.7023343453482305 \t 3.8521177652615153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.36108393 0.59421724 0.87186159]. \t  3.7748978146620287 \t 3.8521177652615153\n",
      "91     \t [0.69504787 0.63143765 0.80353415]. \t  3.2908978426176168 \t 3.8521177652615153\n",
      "92     \t [0.42033045 0.55475307 0.8108826 ]. \t  3.6821407260021415 \t 3.8521177652615153\n",
      "93     \t [0.55685487 0.62004649 0.8040618 ]. \t  3.4188640656351557 \t 3.8521177652615153\n",
      "94     \t [0.63820147 0.53412636 0.86959706]. \t  3.775499845514615 \t 3.8521177652615153\n",
      "95     \t [0.72643942 0.53050076 0.81046559]. \t  3.576111828623586 \t 3.8521177652615153\n",
      "96     \t [0.25738024 0.58024808 0.91419564]. \t  3.4778586846257467 \t 3.8521177652615153\n",
      "97     \t [0.22318434 0.4951447  0.82997435]. \t  3.6982870979713045 \t 3.8521177652615153\n",
      "98     \t [0.28225384 0.51882015 0.87024726]. \t  3.781318191194296 \t 3.8521177652615153\n",
      "99     \t [0.22248359 0.5533743  0.9079783 ]. \t  3.5550294918183623 \t 3.8521177652615153\n",
      "100    \t [0.19634479 0.4866219  0.84371076]. \t  3.692108997861765 \t 3.8521177652615153\n"
     ]
    }
   ],
   "source": [
    "### 6(f). Bayesian optimization runs (x20): STP DF1 run number = 6\n",
    "\n",
    "np.random.seed(run_num_6)\n",
    "surrogate_stp_df1_6 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_6 = GPGO(surrogate_stp_df1_6, Acquisition_new(util_stp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_6.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.4636762261000498, -4.5410472444588335)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(f). Training Regret Minimisation: run number = 6\n",
    "\n",
    "gp_output_6 = np.append(np.max(gpgo_gp_6.GP.y[0:n_init]),gpgo_gp_6.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_6 = np.append(np.max(gpgo_stp_df1_6.GP.y[0:n_init]),gpgo_stp_df1_6.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_6 = np.log(y_global_orig - gp_output_6)\n",
    "regret_stp_df1_6 = np.log(y_global_orig - stp_df1_output_6)\n",
    "\n",
    "train_regret_gp_6 = min_max_array(regret_gp_6)\n",
    "train_regret_stp_df1_6 = min_max_array(regret_stp_df1_6)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 6\n",
    "min_train_regret_gp_6 = min(train_regret_gp_6)\n",
    "min_train_regret_stp_df1_6 = min(train_regret_stp_df1_6)\n",
    "\n",
    "min_train_regret_gp_6, min_train_regret_stp_df1_6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.15266373 0.30235661 0.06203641]. \t  0.1742288077468587 \t 1.6237282255098657\n",
      "init   \t [0.45986034 0.83525338 0.92699705]. \t  1.6237282255098657 \t 1.6237282255098657\n",
      "init   \t [0.72698898 0.76849622 0.26920507]. \t  0.08405715787489784 \t 1.6237282255098657\n",
      "init   \t [0.64402929 0.09337326 0.07968589]. \t  0.2756481743251193 \t 1.6237282255098657\n",
      "init   \t [0.58961375 0.34334054 0.98887615]. \t  1.4020548914621052 \t 1.6237282255098657\n",
      "1      \t [0.0095305  0.06847696 0.97789675]. \t  0.22422993094430546 \t 1.6237282255098657\n",
      "2      \t [0.00962764 0.96956788 0.65396657]. \t  \u001b[92m2.132425580713128\u001b[0m \t 2.132425580713128\n",
      "3      \t [0.97409469 0.03143273 0.64953327]. \t  0.1879512798009964 \t 2.132425580713128\n",
      "4      \t [0.22595933 0.95348785 0.12294433]. \t  0.010520248665944764 \t 2.132425580713128\n",
      "5      \t [0.97122775 0.93199907 0.87171651]. \t  0.9308657054768225 \t 2.132425580713128\n",
      "6      \t [0.3714494  0.05992124 0.47930973]. \t  0.2780879042129226 \t 2.132425580713128\n",
      "7      \t [0.08978758 0.58671321 0.7206821 ]. \t  \u001b[92m2.7673013179295816\u001b[0m \t 2.7673013179295816\n",
      "8      \t [0.03685289 0.49116928 0.9040006 ]. \t  \u001b[92m3.4062663378427063\u001b[0m \t 3.4062663378427063\n",
      "9      \t [9.85660232e-01 7.88246377e-01 8.84621003e-04]. \t  0.00044769353083181577 \t 3.4062663378427063\n",
      "10     \t [0.9465703  0.61593804 0.8230332 ]. \t  \u001b[92m3.409651114729523\u001b[0m \t 3.409651114729523\n",
      "11     \t [0.         0.72985365 1.        ]. \t  1.590649948255382 \t 3.409651114729523\n",
      "12     \t [0.98382715 0.27993618 0.29751896]. \t  0.24196930797294067 \t 3.409651114729523\n",
      "13     \t [0.00580799 0.3712785  0.82623368]. \t  2.794577363798754 \t 3.409651114729523\n",
      "14     \t [0.32910241 0.02426588 0.07541194]. \t  0.3025913601388581 \t 3.409651114729523\n",
      "15     \t [0.48586458 0.50925965 0.78303676]. \t  3.362885414060049 \t 3.409651114729523\n",
      "16     \t [0.02599962 0.80037721 0.76595196]. \t  2.2200012557864595 \t 3.409651114729523\n",
      "17     \t [0.20125009 0.47131934 0.88759595]. \t  \u001b[92m3.4774994382775026\u001b[0m \t 3.4774994382775026\n",
      "18     \t [0.98211039 0.65919527 0.59261324]. \t  0.6482813411459536 \t 3.4774994382775026\n",
      "19     \t [0.98410255 0.31170284 0.82693474]. \t  2.184864208659012 \t 3.4774994382775026\n",
      "20     \t [0.4130355  0.00785469 0.12824955]. \t  0.4941035975615608 \t 3.4774994382775026\n",
      "21     \t [0.30533773 0.01277795 0.22343655]. \t  0.8366289500550946 \t 3.4774994382775026\n",
      "22     \t [0.78705306 0.66304608 0.9348571 ]. \t  2.8612161320195604 \t 3.4774994382775026\n",
      "23     \t [0.75381232 0.99392978 0.70725623]. \t  0.6068270173563677 \t 3.4774994382775026\n",
      "24     \t [0.02404341 0.0942877  0.62108141]. \t  0.25719593506293176 \t 3.4774994382775026\n",
      "25     \t [0.6520758  0.26029302 0.04619241]. \t  0.1477154540736463 \t 3.4774994382775026\n",
      "26     \t [0.43665953 0.03481133 0.11106203]. \t  0.4432502741230613 \t 3.4774994382775026\n",
      "27     \t [0.21798613 0.03979611 0.46539245]. \t  0.28887198045196705 \t 3.4774994382775026\n",
      "28     \t [0.98696718 0.46573214 0.96253201]. \t  2.4232975691625884 \t 3.4774994382775026\n",
      "29     \t [0.29933128 0.32524601 0.26997656]. \t  0.6507621925232626 \t 3.4774994382775026\n",
      "30     \t [0.04246869 0.2875836  0.97840552]. \t  1.1541184522251169 \t 3.4774994382775026\n",
      "31     \t [0.41417838 0.19311361 0.76261502]. \t  1.1087495702662022 \t 3.4774994382775026\n",
      "32     \t [0.87203038 0.42640354 0.3821151 ]. \t  0.16160955464686083 \t 3.4774994382775026\n",
      "33     \t [0.72418083 1.         0.        ]. \t  0.00011344635306565488 \t 3.4774994382775026\n",
      "34     \t [0.81241931 0.36064909 0.72616583]. \t  1.963127413588715 \t 3.4774994382775026\n",
      "35     \t [0.34502756 0.00681989 0.10873278]. \t  0.41586096294873914 \t 3.4774994382775026\n",
      "36     \t [0.43447726 0.04955604 0.96719786]. \t  0.20718758555534678 \t 3.4774994382775026\n",
      "37     \t [0.25432862 0.76856548 0.28473518]. \t  0.29737793386262007 \t 3.4774994382775026\n",
      "38     \t [0.04794507 0.26936512 0.54927872]. \t  0.3816210624341058 \t 3.4774994382775026\n",
      "39     \t [0.58522174 0.58869764 0.78062516]. \t  3.2728937092037884 \t 3.4774994382775026\n",
      "40     \t [0.93189822 0.24939339 0.75233946]. \t  1.4182079501843896 \t 3.4774994382775026\n",
      "41     \t [0.17802938 0.0294758  0.79506928]. \t  0.3313348891871022 \t 3.4774994382775026\n",
      "42     \t [0.97651534 0.4841796  0.21944172]. \t  0.08253771839622574 \t 3.4774994382775026\n",
      "43     \t [0.71300613 0.53677098 0.93514556]. \t  3.1533828886945177 \t 3.4774994382775026\n",
      "44     \t [0.62700145 0.8012262  0.97846886]. \t  1.4347536734805706 \t 3.4774994382775026\n",
      "45     \t [0.22934906 0.18684188 0.3254738 ]. \t  0.8182322861277761 \t 3.4774994382775026\n",
      "46     \t [0.87761517 0.03556698 0.76435623]. \t  0.3351948214917791 \t 3.4774994382775026\n",
      "47     \t [0.0643714  0.08522135 0.27871535]. \t  0.7473716413693425 \t 3.4774994382775026\n",
      "48     \t [0.46313356 0.47927302 0.88073863]. \t  \u001b[92m3.5704663420411387\u001b[0m \t 3.5704663420411387\n",
      "49     \t [0.23583495 0.52125632 0.26348166]. \t  0.25030329997336537 \t 3.5704663420411387\n",
      "50     \t [0.77135093 0.65990973 0.91322584]. \t  3.1234636405375196 \t 3.5704663420411387\n",
      "51     \t [0.59853585 0.3089528  0.64636139]. \t  0.9878666569123074 \t 3.5704663420411387\n",
      "52     \t [0.65619053 0.28700976 0.08111451]. \t  0.20672553862945914 \t 3.5704663420411387\n",
      "53     \t [0.11224669 0.42759766 0.89438893]. \t  3.1159234782761778 \t 3.5704663420411387\n",
      "54     \t [0.56877483 0.15510009 0.37961578]. \t  0.6073058194865852 \t 3.5704663420411387\n",
      "55     \t [0.88504033 0.87772647 0.67648178]. \t  0.7400437002684781 \t 3.5704663420411387\n",
      "56     \t [0.88778347 0.032233   0.87823258]. \t  0.2887325615180171 \t 3.5704663420411387\n",
      "57     \t [0.63252512 0.39155287 0.29382626]. \t  0.3917616767011882 \t 3.5704663420411387\n",
      "58     \t [0.78302483 0.84043631 0.11391371]. \t  0.0038097353465152486 \t 3.5704663420411387\n",
      "59     \t [0.30983316 0.54263777 0.86647888]. \t  \u001b[92m3.83722875791526\u001b[0m \t 3.83722875791526\n",
      "60     \t [0.93884062 0.6907784  0.40388567]. \t  0.15680004897093414 \t 3.83722875791526\n",
      "61     \t [0.45611335 0.03406024 0.61887374]. \t  0.1704663937816772 \t 3.83722875791526\n",
      "62     \t [0.39141075 0.6682249  0.32772495]. \t  0.37526470986621746 \t 3.83722875791526\n",
      "63     \t [0.93984625 0.87229769 0.12249176]. \t  0.0020613546464448135 \t 3.83722875791526\n",
      "64     \t [0.10799289 0.49298973 0.43224513]. \t  0.5767401050317421 \t 3.83722875791526\n",
      "65     \t [0.3998278  0.62664926 0.90508839]. \t  3.44288732608451 \t 3.83722875791526\n",
      "66     \t [0.83943545 0.77914392 0.08044774]. \t  0.0029026804888624066 \t 3.83722875791526\n",
      "67     \t [0.61241343 0.97717877 0.79372164]. \t  0.7635983690255415 \t 3.83722875791526\n",
      "68     \t [0.44986045 0.04096689 0.43530225]. \t  0.40659493777246697 \t 3.83722875791526\n",
      "69     \t [0.39279359 0.18140795 0.97664482]. \t  0.5999814463912994 \t 3.83722875791526\n",
      "70     \t [0.94246849 0.08998696 0.21367445]. \t  0.33946349176507806 \t 3.83722875791526\n",
      "71     \t [0.38151606 0.64937266 0.67849598]. \t  2.2736534531887607 \t 3.83722875791526\n",
      "72     \t [0.93543722 0.25526505 0.61043679]. \t  0.5372273868015285 \t 3.83722875791526\n",
      "73     \t [0.93182818 0.58102934 0.27465297]. \t  0.061145639167543266 \t 3.83722875791526\n",
      "74     \t [0.75148069 0.37839204 0.86510762]. \t  2.8356358916260005 \t 3.83722875791526\n",
      "75     \t [0.53842689 0.20209545 0.03517025]. \t  0.16946266418547523 \t 3.83722875791526\n",
      "76     \t [0.66666872 0.9879886  0.97856614]. \t  0.4295463085574229 \t 3.83722875791526\n",
      "77     \t [0.81938583 0.15484641 0.33275164]. \t  0.47376936237519857 \t 3.83722875791526\n",
      "78     \t [0.12984349 0.95836702 0.14229128]. \t  0.017391810552035626 \t 3.83722875791526\n",
      "79     \t [0.60561798 0.49346954 0.56466118]. \t  0.8014164770549999 \t 3.83722875791526\n",
      "80     \t [0.41979638 0.58009508 0.62413737]. \t  1.712321970347566 \t 3.83722875791526\n",
      "81     \t [0.44280206 0.63711349 0.86091196]. \t  3.611710810406915 \t 3.83722875791526\n",
      "82     \t [0.9833747  0.07992838 0.06819232]. \t  0.09673389388243601 \t 3.83722875791526\n",
      "83     \t [0.68086621 0.0946363  0.20945249]. \t  0.6721802110522616 \t 3.83722875791526\n",
      "84     \t [0.01984272 0.53509364 0.47590447]. \t  0.8991836318454244 \t 3.83722875791526\n",
      "85     \t [0.18697394 0.06951492 0.33744015]. \t  0.7658020018088214 \t 3.83722875791526\n",
      "86     \t [0.29234884 0.38878995 0.02698622]. \t  0.08307067463903102 \t 3.83722875791526\n",
      "87     \t [0.9501523  0.45799694 0.58188769]. \t  0.6322108433550255 \t 3.83722875791526\n",
      "88     \t [0.99377564 0.68733491 0.46147929]. \t  0.20195144177128935 \t 3.83722875791526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.12129095 0.65963091 0.57861225]. \t  2.2560765840740222 \t 3.83722875791526\n",
      "90     \t [0.61533137 0.16596426 0.11691732]. \t  0.41290596966549614 \t 3.83722875791526\n",
      "91     \t [0.1908268  0.52539051 0.95233098]. \t  2.8886375190571583 \t 3.83722875791526\n",
      "92     \t [0.71387159 0.68811182 0.42912745]. \t  0.4759020563078621 \t 3.83722875791526\n",
      "93     \t [0.25601229 0.57490552 0.53964237]. \t  1.4320519550088604 \t 3.83722875791526\n",
      "94     \t [0.66192095 0.37186563 0.85521659]. \t  2.8173963307276444 \t 3.83722875791526\n",
      "95     \t [0.3834012  0.00784596 0.41197929]. \t  0.47790389594330634 \t 3.83722875791526\n",
      "96     \t [0.28167736 0.09390389 0.82019296]. \t  0.5800442303293563 \t 3.83722875791526\n",
      "97     \t [0.53381469 0.51381329 0.15333844]. \t  0.1331021987449858 \t 3.83722875791526\n",
      "98     \t [0.43305606 0.25897579 0.04973795]. \t  0.19518771727017228 \t 3.83722875791526\n",
      "99     \t [0.06620756 0.07810312 0.39393038]. \t  0.4691539503144004 \t 3.83722875791526\n",
      "100    \t [0.51140729 0.95279908 0.47172793]. \t  1.417731105179221 \t 3.83722875791526\n"
     ]
    }
   ],
   "source": [
    "### 6(g). Bayesian optimization runs (x20): GP run number = 7\n",
    "\n",
    "np.random.seed(run_num_7)\n",
    "surrogate_gp_7 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "gpgo_gp_7 = GPGO(surrogate_gp_7, Acquisition_new(util_gp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_7.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.15266373 0.30235661 0.06203641]. \t  0.1742288077468587 \t 1.6237282255098657\n",
      "init   \t [0.45986034 0.83525338 0.92699705]. \t  1.6237282255098657 \t 1.6237282255098657\n",
      "init   \t [0.72698898 0.76849622 0.26920507]. \t  0.08405715787489784 \t 1.6237282255098657\n",
      "init   \t [0.64402929 0.09337326 0.07968589]. \t  0.2756481743251193 \t 1.6237282255098657\n",
      "init   \t [0.58961375 0.34334054 0.98887615]. \t  1.4020548914621052 \t 1.6237282255098657\n",
      "1      \t [0. 0. 1.]. \t  0.0902894676548261 \t 1.6237282255098657\n",
      "2      \t [1. 1. 1.]. \t  0.31688362070415665 \t 1.6237282255098657\n",
      "3      \t [0. 1. 0.]. \t  0.00027353676804544577 \t 1.6237282255098657\n",
      "4      \t [-5.55111512e-17  1.00000000e+00  1.00000000e+00]. \t  0.330219860606422 \t 1.6237282255098657\n",
      "5      \t [1.         0.         0.72249673]. \t  0.2136096993375281 \t 1.6237282255098657\n",
      "6      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 1.6237282255098657\n",
      "7      \t [1.         0.27250893 0.        ]. \t  0.027871682239082582 \t 1.6237282255098657\n",
      "8      \t [0. 0. 0.]. \t  0.06797411659013229 \t 1.6237282255098657\n",
      "9      \t [0.         0.49621876 0.61811442]. \t  1.5138719957652 \t 1.6237282255098657\n",
      "10     \t [0.29457012 0.         0.58485126]. \t  0.1177738330344003 \t 1.6237282255098657\n",
      "11     \t [0.12651722 1.         0.52128942]. \t  \u001b[92m2.4820023372272644\u001b[0m \t 2.4820023372272644\n",
      "12     \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.4820023372272644\n",
      "13     \t [1.         1.         0.52616622]. \t  0.24540065170734937 \t 2.4820023372272644\n",
      "14     \t [1.        0.5102423 1.       ]. \t  1.9458476646192588 \t 2.4820023372272644\n",
      "15     \t [1. 0. 0.]. \t  0.03095471703300515 \t 2.4820023372272644\n",
      "16     \t [0.44751062 1.         0.        ]. \t  0.00022481199106825838 \t 2.4820023372272644\n",
      "17     \t [0.10195241 0.53149452 1.        ]. \t  2.0456532268767207 \t 2.4820023372272644\n",
      "18     \t [0.         0.         0.35942579]. \t  0.4507843016488767 \t 2.4820023372272644\n",
      "19     \t [1.         0.27746061 0.37303647]. \t  0.1771781572814561 \t 2.4820023372272644\n",
      "20     \t [0.55241978 1.         0.60101871]. \t  1.3849874933537125 \t 2.4820023372272644\n",
      "21     \t [0.37297486 0.52275043 0.66566124]. \t  1.9788749523467613 \t 2.4820023372272644\n",
      "22     \t [0.31751626 0.         0.        ]. \t  0.10143994620081911 \t 2.4820023372272644\n",
      "23     \t [0.         0.76029765 0.29457515]. \t  0.34540692780100546 \t 2.4820023372272644\n",
      "24     \t [0.         1.         0.67685782]. \t  1.7343374956948148 \t 2.4820023372272644\n",
      "25     \t [1.         0.69144415 0.74985555]. \t  2.1256767631675997 \t 2.4820023372272644\n",
      "26     \t [0.36021291 0.         1.        ]. \t  0.09172401163269758 \t 2.4820023372272644\n",
      "27     \t [0.30986466 1.         1.        ]. \t  0.3342927836557406 \t 2.4820023372272644\n",
      "28     \t [0.22645171 0.72634275 0.        ]. \t  0.002919680263212792 \t 2.4820023372272644\n",
      "29     \t [1.         0.69540126 0.        ]. \t  0.0012708002359292808 \t 2.4820023372272644\n",
      "30     \t [0.74093997 0.         0.86603957]. \t  0.22147829319442613 \t 2.4820023372272644\n",
      "31     \t [0.13697145 0.79569963 0.82066778]. \t  2.3141451980908734 \t 2.4820023372272644\n",
      "32     \t [2.68228750e-05 1.72543844e-01 8.01779624e-01]. \t  1.0229118493602614 \t 2.4820023372272644\n",
      "33     \t [0.93164404 0.36382834 0.71818396]. \t  1.8646386873318423 \t 2.4820023372272644\n",
      "34     \t [0.66077859 0.38392407 0.        ]. \t  0.04454297444761519 \t 2.4820023372272644\n",
      "35     \t [0.8145265  0.80077753 1.        ]. \t  1.1983128001224255 \t 2.4820023372272644\n",
      "36     \t [0.00710675 0.00526163 0.71779136]. \t  0.2239831879857825 \t 2.4820023372272644\n",
      "37     \t [0.         0.75946921 0.        ]. \t  0.001505736185066995 \t 2.4820023372272644\n",
      "38     \t [0.32055429 1.         0.25819299]. \t  0.16022470356172083 \t 2.4820023372272644\n",
      "39     \t [1.00000000e+00 7.61053206e-08 3.11726713e-01]. \t  0.2490842638208009 \t 2.4820023372272644\n",
      "40     \t [1.79413969e-07 7.55480823e-01 1.00000000e+00]. \t  1.4567779160780236 \t 2.4820023372272644\n",
      "41     \t [0.78259744 0.59349898 0.70843918]. \t  2.1435327178864636 \t 2.4820023372272644\n",
      "42     \t [0.42514014 0.78076102 0.56453586]. \t  2.2125049198176034 \t 2.4820023372272644\n",
      "43     \t [0.99999997 0.99569463 0.79671702]. \t  0.49169160858958993 \t 2.4820023372272644\n",
      "44     \t [0.         0.28669839 0.        ]. \t  0.05845137458109064 \t 2.4820023372272644\n",
      "45     \t [0.75490835 0.15274293 0.49116422]. \t  0.20029271047823086 \t 2.4820023372272644\n",
      "46     \t [1.         0.27137551 1.        ]. \t  0.8545409600251975 \t 2.4820023372272644\n",
      "47     \t [0.79051594 0.9999999  0.        ]. \t  9.042467438379112e-05 \t 2.4820023372272644\n",
      "48     \t [0.7855684 0.        0.       ]. \t  0.06073678426884857 \t 2.4820023372272644\n",
      "49     \t [0.20560785 0.98557651 0.72266092]. \t  1.4012380288260553 \t 2.4820023372272644\n",
      "50     \t [0.22553495 0.76392236 0.99999999]. \t  1.4284500043809611 \t 2.4820023372272644\n",
      "51     \t [0.7426482 1.        1.       ]. \t  0.326324899087415 \t 2.4820023372272644\n",
      "52     \t [0.61024419 0.         0.25068119]. \t  0.7262842705980516 \t 2.4820023372272644\n",
      "53     \t [0.70563162 0.         1.        ]. \t  0.0908780102020433 \t 2.4820023372272644\n",
      "54     \t [0.0343035  0.86908872 0.54935472]. \t  \u001b[92m3.0214072260016223\u001b[0m \t 3.0214072260016223\n",
      "55     \t [0.         0.65072294 0.78273562]. \t  \u001b[92m3.182355930797319\u001b[0m \t 3.182355930797319\n",
      "56     \t [0.01344597 0.28014046 0.99371564]. \t  0.9727938444415382 \t 3.182355930797319\n",
      "57     \t [0.0490467  0.75873318 0.60921374]. \t  2.7634251848841727 \t 3.182355930797319\n",
      "58     \t [0.05911271 0.55455605 0.78651416]. \t  \u001b[92m3.472572086525786\u001b[0m \t 3.472572086525786\n",
      "59     \t [0.0587215  0.62038844 0.85728807]. \t  \u001b[92m3.6854155463362854\u001b[0m \t 3.6854155463362854\n",
      "60     \t [0.27358529 0.60648725 0.82194975]. \t  3.6792920873113713 \t 3.6854155463362854\n",
      "61     \t [0.13008596 0.55398109 0.90854614]. \t  3.536109800999928 \t 3.6854155463362854\n",
      "62     \t [0.04493447 0.62671624 0.72457891]. \t  2.774026477763833 \t 3.6854155463362854\n",
      "63     \t [0.07906518 0.71007162 0.73519826]. \t  2.6561391933420175 \t 3.6854155463362854\n",
      "64     \t [0.15403035 0.36097905 0.85606727]. \t  2.72154812079677 \t 3.6854155463362854\n",
      "65     \t [0.01959379 0.62550973 0.85035024]. \t  3.6513486633344003 \t 3.6854155463362854\n",
      "66     \t [0.00445148 0.630332   0.86583621]. \t  3.6111525386865604 \t 3.6854155463362854\n",
      "67     \t [0.17555335 0.62157021 0.8381856 ]. \t  3.683950582762198 \t 3.6854155463362854\n",
      "68     \t [0.17642024 0.48462474 0.81569533]. \t  3.5839306685037124 \t 3.6854155463362854\n",
      "69     \t [0.18958662 0.6281537  0.76305284]. \t  3.1379593657653118 \t 3.6854155463362854\n",
      "70     \t [0.04259872 0.63979944 0.88563176]. \t  3.4954342120269404 \t 3.6854155463362854\n",
      "71     \t [0.31412065 0.55227624 0.90145612]. \t  3.626703943174899 \t 3.6854155463362854\n",
      "72     \t [0.20636511 0.59102788 0.76155634]. \t  3.204139200028231 \t 3.6854155463362854\n",
      "73     \t [0.0192442  0.56712411 0.80209488]. \t  3.592820503827256 \t 3.6854155463362854\n",
      "74     \t [0.09340515 0.5386708  0.87949327]. \t  \u001b[92m3.752896450198091\u001b[0m \t 3.752896450198091\n",
      "75     \t [0.29988137 0.59036135 0.94338964]. \t  3.0645327055242273 \t 3.752896450198091\n",
      "76     \t [0.0619409  0.5948806  0.84954251]. \t  \u001b[92m3.775644954151162\u001b[0m \t 3.775644954151162\n",
      "77     \t [0.04072258 0.55197287 0.91573939]. \t  3.4337902167112238 \t 3.775644954151162\n",
      "78     \t [0.3862625  0.65368053 0.82139991]. \t  3.429694103224837 \t 3.775644954151162\n",
      "79     \t [0.20404984 0.56419343 0.89716421]. \t  3.6594639939013054 \t 3.775644954151162\n",
      "80     \t [0.45000358 0.47408458 0.88754588]. \t  3.4996528914979375 \t 3.775644954151162\n",
      "81     \t [0.44280206 0.63711349 0.86091196]. \t  3.611710810406915 \t 3.775644954151162\n",
      "82     \t [0.08212368 0.61876487 0.82552276]. \t  3.634539855655828 \t 3.775644954151162\n",
      "83     \t [0.23855087 0.54897999 0.90258909]. \t  3.609302021752424 \t 3.775644954151162\n",
      "84     \t [0.22343934 0.49960566 0.82614201]. \t  3.6994351844813487 \t 3.775644954151162\n",
      "85     \t [0.20601977 0.66581318 0.83807222]. \t  3.4382841043578454 \t 3.775644954151162\n",
      "86     \t [0.41685819 0.44944435 0.8328667 ]. \t  3.4714975531670955 \t 3.775644954151162\n",
      "87     \t [0.5126533  0.51873554 0.88156769]. \t  3.7154949835122375 \t 3.775644954151162\n",
      "88     \t [0.3552446  0.59906634 0.85134439]. \t  \u001b[92m3.789204011969339\u001b[0m \t 3.789204011969339\n",
      "89     \t [0.3600519  0.58110092 0.79378462]. \t  3.5206209014183907 \t 3.789204011969339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.1416465  0.64555345 0.80692456]. \t  3.4189312410176798 \t 3.789204011969339\n",
      "91     \t [0.31999922 0.64126387 0.84543485]. \t  3.599966954282416 \t 3.789204011969339\n",
      "92     \t [0.444207   0.54416123 0.91724388]. \t  3.4442517199842815 \t 3.789204011969339\n",
      "93     \t [0.24190681 0.47435571 0.94809099]. \t  2.781438865751517 \t 3.789204011969339\n",
      "94     \t [0.0902049  0.52531857 0.85632568]. \t  \u001b[92m3.803281368175924\u001b[0m \t 3.803281368175924\n",
      "95     \t [0.15981585 0.55222198 0.86913541]. \t  \u001b[92m3.8228420623232893\u001b[0m \t 3.8228420623232893\n",
      "96     \t [0.38164947 0.6227622  0.88162782]. \t  3.630615361188895 \t 3.8228420623232893\n",
      "97     \t [0.29531631 0.59319113 0.81554321]. \t  3.679480612979479 \t 3.8228420623232893\n",
      "98     \t [0.04744738 0.73049493 0.78792958]. \t  2.7571008894478 \t 3.8228420623232893\n",
      "99     \t [0.37279213 0.56240402 0.82791702]. \t  3.7925300356224994 \t 3.8228420623232893\n",
      "100    \t [0.30798824 0.50163185 0.88752958]. \t  3.631864959904331 \t 3.8228420623232893\n"
     ]
    }
   ],
   "source": [
    "### 6(g). Bayesian optimization runs (x20): STP DF1 run number = 7\n",
    "\n",
    "np.random.seed(run_num_7)\n",
    "surrogate_stp_df1_7 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_7 = GPGO(surrogate_stp_df1_7, Acquisition_new(util_stp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_7.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.667069349629895, -3.220428587863166)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(g). Training Regret Minimisation: run number = 7\n",
    "\n",
    "gp_output_7 = np.append(np.max(gpgo_gp_7.GP.y[0:n_init]),gpgo_gp_7.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_7 = np.append(np.max(gpgo_stp_df1_7.GP.y[0:n_init]),gpgo_stp_df1_7.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_7 = np.log(y_global_orig - gp_output_7)\n",
    "regret_stp_df1_7 = np.log(y_global_orig - stp_df1_output_7)\n",
    "\n",
    "train_regret_gp_7 = min_max_array(regret_gp_7)\n",
    "train_regret_stp_df1_7 = min_max_array(regret_stp_df1_7)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 7\n",
    "min_train_regret_gp_7 = min(train_regret_gp_7)\n",
    "min_train_regret_stp_df1_7 = min(train_regret_stp_df1_7)\n",
    "\n",
    "min_train_regret_gp_7, min_train_regret_stp_df1_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.85956061 0.1645695  0.48347596]. \t  0.17123941582426935 \t 0.42900944485173353\n",
      "init   \t [0.92102727 0.42855644 0.05746009]. \t  0.04054521544875858 \t 0.42900944485173353\n",
      "init   \t [0.92500743 0.65760154 0.13295284]. \t  0.013612101711497782 \t 0.42900944485173353\n",
      "init   \t [0.53344893 0.8994776  0.24836496]. \t  0.10598509010979217 \t 0.42900944485173353\n",
      "init   \t [0.03017182 0.07244715 0.87416449]. \t  0.42900944485173353 \t 0.42900944485173353\n",
      "1      \t [0.16461217 0.8901079  0.9866158 ]. \t  \u001b[92m0.8374245166267126\u001b[0m \t 0.8374245166267126\n",
      "2      \t [0.13138131 0.16657944 0.03118204]. \t  0.15468848856055692 \t 0.8374245166267126\n",
      "3      \t [0.92428274 0.85996239 0.96078895]. \t  \u001b[92m1.1535531467869156\u001b[0m \t 1.1535531467869156\n",
      "4      \t [0.04705027 0.99617972 0.34236676]. \t  0.6596544344375036 \t 1.1535531467869156\n",
      "5      \t [0.73718979 0.03046075 0.89331261]. \t  0.2699038783099723 \t 1.1535531467869156\n",
      "6      \t [0.04217843 0.70536077 0.01097488]. \t  0.0034873959328945133 \t 1.1535531467869156\n",
      "7      \t [0.49342002 0.50344076 0.87707862]. \t  \u001b[92m3.6941337135407193\u001b[0m \t 3.6941337135407193\n",
      "8      \t [0.36323898 0.49440518 0.57528322]. \t  1.1187957671081112 \t 3.6941337135407193\n",
      "9      \t [0.57712859 0.4778289  1.        ]. \t  1.9223330856697787 \t 3.6941337135407193\n",
      "10     \t [0.68245039 1.         0.73133027]. \t  0.6563906047143845 \t 3.6941337135407193\n",
      "11     \t [0.43370592 0.52565185 0.85682426]. \t  \u001b[92m3.823019745236506\u001b[0m \t 3.823019745236506\n",
      "12     \t [0.7032776  0.02400766 0.07441756]. \t  0.21480893368815296 \t 3.823019745236506\n",
      "13     \t [1.         0.64233769 0.75565557]. \t  2.487021193779122 \t 3.823019745236506\n",
      "14     \t [0.45672475 0.57204174 0.81910701]. \t  3.7155802154139 \t 3.823019745236506\n",
      "15     \t [0.32055318 0.91398521 0.03242211]. \t  0.0010514653829984412 \t 3.823019745236506\n",
      "16     \t [0.         0.56886143 0.99232082]. \t  2.1962447540854964 \t 3.823019745236506\n",
      "17     \t [0.29566951 0.44448217 0.97003261]. \t  2.2821624252902035 \t 3.823019745236506\n",
      "18     \t [0.04079787 0.72756005 0.89361297]. \t  2.808096616683356 \t 3.823019745236506\n",
      "19     \t [0.82162247 0.96045288 0.05856192]. \t  0.0004953148167878036 \t 3.823019745236506\n",
      "20     \t [0.51956953 0.60052947 0.83346124]. \t  3.707253883123973 \t 3.823019745236506\n",
      "21     \t [0.96918879 0.61448157 0.61371163]. \t  0.8538765583967243 \t 3.823019745236506\n",
      "22     \t [0.38389769 0.2292826  0.4644193 ]. \t  0.3536394211641349 \t 3.823019745236506\n",
      "23     \t [0.09688337 0.56040261 0.78374623]. \t  3.454357784257132 \t 3.823019745236506\n",
      "24     \t [0.99683873 0.45113939 0.88591736]. \t  3.2412484880462418 \t 3.823019745236506\n",
      "25     \t [0.06469056 0.0353045  0.06457608]. \t  0.2065378431693179 \t 3.823019745236506\n",
      "26     \t [0.52350953 0.04325044 0.03341004]. \t  0.17080469089986514 \t 3.823019745236506\n",
      "27     \t [0.01566279 0.44335379 0.91133016]. \t  3.052177225461454 \t 3.823019745236506\n",
      "28     \t [0.07330303 0.5085171  0.16672105]. \t  0.13129525551066568 \t 3.823019745236506\n",
      "29     \t [0.90724415 0.49092864 0.70132134]. \t  2.036171392928326 \t 3.823019745236506\n",
      "30     \t [0.12740435 0.21589902 0.351003  ]. \t  0.6314602531824375 \t 3.823019745236506\n",
      "31     \t [0.65665145 0.38287979 0.65646959]. \t  1.3214744636768616 \t 3.823019745236506\n",
      "32     \t [0.66890429 0.27952424 0.10147384]. \t  0.2569654603820211 \t 3.823019745236506\n",
      "33     \t [0.26804496 0.67289349 0.03320225]. \t  0.009057973009984253 \t 3.823019745236506\n",
      "34     \t [0.44339888 0.59889489 0.82680872]. \t  3.7015384349873917 \t 3.823019745236506\n",
      "35     \t [0.34268898 0.23504387 0.0095214 ]. \t  0.11826238705499055 \t 3.823019745236506\n",
      "36     \t [0.06716444 0.62030615 0.015657  ]. \t  0.009298429309766706 \t 3.823019745236506\n",
      "37     \t [0.98132899 0.25906558 0.72546769]. \t  1.3168762722102685 \t 3.823019745236506\n",
      "38     \t [0.47653682 0.98569407 0.92792568]. \t  0.5941494983300468 \t 3.823019745236506\n",
      "39     \t [0.55103553 0.19331258 0.30403077]. \t  0.8233618497842383 \t 3.823019745236506\n",
      "40     \t [0.22140167 0.6487773  0.81732127]. \t  3.464502875674259 \t 3.823019745236506\n",
      "41     \t [0.93842423 0.93899493 0.99277393]. \t  0.5500284674529098 \t 3.823019745236506\n",
      "42     \t [0.21985224 0.94747241 0.81026273]. \t  1.1231389016121418 \t 3.823019745236506\n",
      "43     \t [0.73051148 0.9821618  0.36078234]. \t  0.27138471094694433 \t 3.823019745236506\n",
      "44     \t [0.36962901 0.49800321 0.89462842]. \t  3.5627046681521097 \t 3.823019745236506\n",
      "45     \t [0.09759747 0.18559209 0.71091266]. \t  0.8597975733984988 \t 3.823019745236506\n",
      "46     \t [0.51066009 0.1498453  0.00941683]. \t  0.1266690677669085 \t 3.823019745236506\n",
      "47     \t [0.7815017  0.84140557 0.20235004]. \t  0.021244181415123763 \t 3.823019745236506\n",
      "48     \t [0.64526001 0.60548293 0.35752472]. \t  0.2546335238306623 \t 3.823019745236506\n",
      "49     \t [0.0702375  0.52385998 0.36482273]. \t  0.4155876302732004 \t 3.823019745236506\n",
      "50     \t [0.6893243  0.45214291 0.34490614]. \t  0.2533285029450272 \t 3.823019745236506\n",
      "51     \t [0.91879431 0.55135306 0.51880509]. \t  0.3498509328588184 \t 3.823019745236506\n",
      "52     \t [0.84098763 0.93893627 0.7671636 ]. \t  0.7633816815081917 \t 3.823019745236506\n",
      "53     \t [0.58745342 0.43099619 0.16669467]. \t  0.24096586633626849 \t 3.823019745236506\n",
      "54     \t [0.38653507 0.27325332 0.17290916]. \t  0.5999026158374239 \t 3.823019745236506\n",
      "55     \t [0.35022831 0.61491273 0.84095179]. \t  3.7158757767330313 \t 3.823019745236506\n",
      "56     \t [0.25944777 0.26415109 0.99296971]. \t  0.9046855789233754 \t 3.823019745236506\n",
      "57     \t [0.92853688 0.01021371 0.4154357 ]. \t  0.1847035623204289 \t 3.823019745236506\n",
      "58     \t [0.35818285 0.01576635 0.28468104]. \t  0.8944352322888152 \t 3.823019745236506\n",
      "59     \t [0.3740817  0.5986984  0.87990964]. \t  3.7281167259786794 \t 3.823019745236506\n",
      "60     \t [0.97178863 0.74896008 0.09820289]. \t  0.0031583479810728455 \t 3.823019745236506\n",
      "61     \t [0.91226761 0.07101596 0.60712953]. \t  0.18408467261572897 \t 3.823019745236506\n",
      "62     \t [0.29479338 0.57453258 0.03766525]. \t  0.025291762837263233 \t 3.823019745236506\n",
      "63     \t [0.64407177 0.15556675 0.06955516]. \t  0.24290208626530796 \t 3.823019745236506\n",
      "64     \t [0.16533643 0.40630726 0.10646459]. \t  0.17679841658351403 \t 3.823019745236506\n",
      "65     \t [0.30191224 0.62842482 0.48959687]. \t  1.4103282163723403 \t 3.823019745236506\n",
      "66     \t [0.95099978 0.27087665 0.76691431]. \t  1.646187466585594 \t 3.823019745236506\n",
      "67     \t [0.53415418 0.42180329 0.34055638]. \t  0.3709859122544758 \t 3.823019745236506\n",
      "68     \t [0.68871904 0.21787991 0.43365268]. \t  0.323714217593499 \t 3.823019745236506\n",
      "69     \t [0.69782465 0.58842892 0.76740467]. \t  3.045457745801297 \t 3.823019745236506\n",
      "70     \t [0.29293767 0.97835757 0.01638401]. \t  0.0004960531559445393 \t 3.823019745236506\n",
      "71     \t [0.16113514 0.25043939 0.20789561]. \t  0.6630807489541694 \t 3.823019745236506\n",
      "72     \t [0.03481147 0.93524728 0.82748392]. \t  1.1563488257373582 \t 3.823019745236506\n",
      "73     \t [0.59507347 0.38986522 0.62400819]. \t  1.0390903015937916 \t 3.823019745236506\n",
      "74     \t [0.31807489 0.2146795  0.82814986]. \t  1.36987158385924 \t 3.823019745236506\n",
      "75     \t [0.60885343 0.38947062 0.06250908]. \t  0.11389517098113625 \t 3.823019745236506\n",
      "76     \t [0.62498705 0.00756349 0.48282604]. \t  0.198195889029941 \t 3.823019745236506\n",
      "77     \t [0.90192992 0.64285562 0.32243434]. \t  0.07904065461643665 \t 3.823019745236506\n",
      "78     \t [0.31076692 0.45156646 0.73872273]. \t  2.685439185532016 \t 3.823019745236506\n",
      "79     \t [0.81799126 0.44992463 0.80169135]. \t  3.2205356238707825 \t 3.823019745236506\n",
      "80     \t [0.20163648 0.45932862 0.99632994]. \t  1.9162225972522726 \t 3.823019745236506\n",
      "81     \t [0.18712544 0.24898045 0.16635692]. \t  0.5611138217946533 \t 3.823019745236506\n",
      "82     \t [0.13555126 0.29678279 0.72699289]. \t  1.631675975758462 \t 3.823019745236506\n",
      "83     \t [0.98155883 0.81801641 0.71736406]. \t  1.0569002220484887 \t 3.823019745236506\n",
      "84     \t [0.1316172  0.65237153 0.29425874]. \t  0.28802360566697366 \t 3.823019745236506\n",
      "85     \t [0.39778464 0.27753012 0.5353789 ]. \t  0.3680886723034062 \t 3.823019745236506\n",
      "86     \t [0.72037737 0.50873056 0.84198157]. \t  3.7083636711986636 \t 3.823019745236506\n",
      "87     \t [0.27832997 0.83709948 0.6739576 ]. \t  2.3246199923577295 \t 3.823019745236506\n",
      "88     \t [0.52520702 0.84582879 0.13483506]. \t  0.01165101880210246 \t 3.823019745236506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.50791034 0.60891927 0.28761631]. \t  0.19239940567509028 \t 3.823019745236506\n",
      "90     \t [0.16144009 0.17118487 0.35247701]. \t  0.69536288809979 \t 3.823019745236506\n",
      "91     \t [0.9058277  0.18713444 0.79745337]. \t  1.111625436069498 \t 3.823019745236506\n",
      "92     \t [0.92567543 0.98495109 0.02881038]. \t  0.0001276193599449713 \t 3.823019745236506\n",
      "93     \t [0.5825282  0.7708061  0.44244127]. \t  0.9676382349402387 \t 3.823019745236506\n",
      "94     \t [0.24180664 0.77918959 0.20204415]. \t  0.07289937495987775 \t 3.823019745236506\n",
      "95     \t [0.65977955 0.82987214 0.85098239]. \t  1.857294933830517 \t 3.823019745236506\n",
      "96     \t [0.27119447 0.102766   0.17791954]. \t  0.7632349650684725 \t 3.823019745236506\n",
      "97     \t [0.60768369 0.55129249 0.29980179]. \t  0.19670702362012804 \t 3.823019745236506\n",
      "98     \t [0.8524689  0.29413459 0.45924925]. \t  0.1907484684402625 \t 3.823019745236506\n",
      "99     \t [0.07907532 0.24638832 0.51360488]. \t  0.2929369610095534 \t 3.823019745236506\n",
      "100    \t [0.20984065 0.72705831 0.15723413]. \t  0.03614627268921907 \t 3.823019745236506\n"
     ]
    }
   ],
   "source": [
    "### 6(h). Bayesian optimization runs (x20): GP run number = 8\n",
    "\n",
    "np.random.seed(run_num_8)\n",
    "surrogate_gp_8 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "gpgo_gp_8 = GPGO(surrogate_gp_8, Acquisition_new(util_gp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_8.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.85956061 0.1645695  0.48347596]. \t  0.17123941582426935 \t 0.42900944485173353\n",
      "init   \t [0.92102727 0.42855644 0.05746009]. \t  0.04054521544875858 \t 0.42900944485173353\n",
      "init   \t [0.92500743 0.65760154 0.13295284]. \t  0.013612101711497782 \t 0.42900944485173353\n",
      "init   \t [0.53344893 0.8994776  0.24836496]. \t  0.10598509010979217 \t 0.42900944485173353\n",
      "init   \t [0.03017182 0.07244715 0.87416449]. \t  0.42900944485173353 \t 0.42900944485173353\n",
      "1      \t [0. 1. 1.]. \t  0.330219860606422 \t 0.42900944485173353\n",
      "2      \t [0. 0. 0.]. \t  0.06797411659013229 \t 0.42900944485173353\n",
      "3      \t [1. 1. 1.]. \t  0.31688362070415665 \t 0.42900944485173353\n",
      "4      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 0.42900944485173353\n",
      "5      \t [1. 0. 1.]. \t  0.08848201872702738 \t 0.42900944485173353\n",
      "6      \t [0.51902831 0.50343343 1.        ]. \t  \u001b[92m2.001762678272167\u001b[0m \t 2.001762678272167\n",
      "7      \t [0.         0.54098981 0.45735671]. \t  0.8214223483044264 \t 2.001762678272167\n",
      "8      \t [1. 0. 0.]. \t  0.03095471703300515 \t 2.001762678272167\n",
      "9      \t [1.         0.50922358 1.        ]. \t  1.9436143332765103 \t 2.001762678272167\n",
      "10     \t [0.         0.48922657 1.        ]. \t  1.9374487070196031 \t 2.001762678272167\n",
      "11     \t [0.492905 0.       0.      ]. \t  0.09763693040932148 \t 2.001762678272167\n",
      "12     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.001762678272167\n",
      "13     \t [0.         0.49977327 0.        ]. \t  0.01807914394307387 \t 2.001762678272167\n",
      "14     \t [1.         1.         0.55214505]. \t  0.26030153351629165 \t 2.001762678272167\n",
      "15     \t [0.47233438 0.         1.        ]. \t  0.09168801475150219 \t 2.001762678272167\n",
      "16     \t [0.         1.         0.53752651]. \t  \u001b[92m2.4619059732496513\u001b[0m \t 2.4619059732496513\n",
      "17     \t [0.37531321 1.         0.78500847]. \t  0.8433293092877988 \t 2.4619059732496513\n",
      "18     \t [0.27391723 0.56280422 0.74270668]. \t  \u001b[92m2.9956992581027437\u001b[0m \t 2.9956992581027437\n",
      "19     \t [0.73508757 0.62646505 0.76251822]. \t  2.821850837724074 \t 2.9956992581027437\n",
      "20     \t [0.         0.77680036 0.76868351]. \t  2.362792247332116 \t 2.9956992581027437\n",
      "21     \t [1.         0.46738879 0.72229937]. \t  2.2709920247375486 \t 2.9956992581027437\n",
      "22     \t [0.60143794 0.31206763 0.82222512]. \t  2.2524650396776917 \t 2.9956992581027437\n",
      "23     \t [0.27480016 0.         0.51583739]. \t  0.16234044944206238 \t 2.9956992581027437\n",
      "24     \t [0.40650452 0.64003916 0.        ]. \t  0.007701966335434231 \t 2.9956992581027437\n",
      "25     \t [1.         0.         0.70542909]. \t  0.19901656884176377 \t 2.9956992581027437\n",
      "26     \t [0.67303724 1.         0.        ]. \t  0.0001329118685611352 \t 2.9956992581027437\n",
      "27     \t [ 2.13519872e-01  2.07799058e-01 -2.77555756e-17]. \t  0.10042757203337639 \t 2.9956992581027437\n",
      "28     \t [0. 0. 1.]. \t  0.0902894676548261 \t 2.9956992581027437\n",
      "29     \t [1.         0.76318037 0.78798787]. \t  2.0056175117272668 \t 2.9956992581027437\n",
      "30     \t [0.17557216 0.75085051 1.        ]. \t  1.4970767162393523 \t 2.9956992581027437\n",
      "31     \t [0.21720352 0.25392589 1.        ]. \t  0.7973872020225438 \t 2.9956992581027437\n",
      "32     \t [0.         0.         0.35205523]. \t  0.4685076434293785 \t 2.9956992581027437\n",
      "33     \t [0.32004345 1.         0.12944735]. \t  0.010075202987497447 \t 2.9956992581027437\n",
      "34     \t [0.58078123 0.35694565 0.26955726]. \t  0.5010929058988967 \t 2.9956992581027437\n",
      "35     \t [0.80041221 0.78567407 1.        ]. \t  1.28323433414943 \t 2.9956992581027437\n",
      "36     \t [0.00623815 0.92946366 0.31776213]. \t  0.5236110720143617 \t 2.9956992581027437\n",
      "37     \t [7.53131957e-01 1.26156990e-08 8.02113976e-01]. \t  0.24922739952675987 \t 2.9956992581027437\n",
      "38     \t [1.         0.66517091 0.        ]. \t  0.0017762012123035975 \t 2.9956992581027437\n",
      "39     \t [0.22895784 0.80961897 0.52883887]. \t  2.785638215926358 \t 2.9956992581027437\n",
      "40     \t [0.50200498 0.55056998 0.64076712]. \t  1.658225129068545 \t 2.9956992581027437\n",
      "41     \t [0.83508972 0.46687831 0.89980935]. \t  \u001b[92m3.287046270400024\u001b[0m \t 3.287046270400024\n",
      "42     \t [0.13890868 0.99308723 0.61061195]. \t  2.4292959292345384 \t 3.287046270400024\n",
      "43     \t [0.98726844 0.25622202 0.89120949]. \t  1.5233612081367287 \t 3.287046270400024\n",
      "44     \t [1.         0.         0.20234113]. \t  0.23263035814303787 \t 3.287046270400024\n",
      "45     \t [0.19701944 0.00240658 0.11697058]. \t  0.40742385964759836 \t 3.287046270400024\n",
      "46     \t [0.82599511 0.96860005 0.71468571]. \t  0.587343009767801 \t 3.287046270400024\n",
      "47     \t [0.07387342 0.15641714 0.21097114]. \t  0.6899891871070227 \t 3.287046270400024\n",
      "48     \t [0.77052164 0.06025532 0.07970274]. \t  0.20765544021325516 \t 3.287046270400024\n",
      "49     \t [0.82599379 0.31251473 1.        ]. \t  1.0851466963777958 \t 3.287046270400024\n",
      "50     \t [0.01959032 0.94803295 0.7610921 ]. \t  1.3240656858864752 \t 3.287046270400024\n",
      "51     \t [0.99691604 0.32647458 0.22845664]. \t  0.1894283985345512 \t 3.287046270400024\n",
      "52     \t [0.1943526  0.61817199 0.71791181]. \t  2.7346531262045692 \t 3.287046270400024\n",
      "53     \t [0.40145599 1.         1.        ]. \t  0.3337057977712853 \t 3.287046270400024\n",
      "54     \t [0.00612957 0.31030236 0.72605408]. \t  1.6974148924124495 \t 3.287046270400024\n",
      "55     \t [0.72795076 0.53048514 0.82550312]. \t  \u001b[92m3.6806220539920904\u001b[0m \t 3.6806220539920904\n",
      "56     \t [0.90239235 0.44304327 0.86230314]. \t  3.326668265194453 \t 3.6806220539920904\n",
      "57     \t [0.65010126 0.55076046 0.87648848]. \t  \u001b[92m3.7613204608177986\u001b[0m \t 3.7613204608177986\n",
      "58     \t [0.49153731 0.53744275 0.86059788]. \t  \u001b[92m3.8297609028012825\u001b[0m \t 3.8297609028012825\n",
      "59     \t [0.83245381 0.50451523 0.90493289]. \t  3.41340584932926 \t 3.8297609028012825\n",
      "60     \t [0.77352738 0.55644448 0.81263187]. \t  3.573810919588476 \t 3.8297609028012825\n",
      "61     \t [0.81876846 0.46580948 0.85078333]. \t  3.50861575197697 \t 3.8297609028012825\n",
      "62     \t [0.77266943 0.56893104 0.89120257]. \t  3.637908432667203 \t 3.8297609028012825\n",
      "63     \t [0.81316086 0.49751535 0.91047046]. \t  3.336005335709788 \t 3.8297609028012825\n",
      "64     \t [0.66327345 0.52169999 0.86562644]. \t  3.757282472477672 \t 3.8297609028012825\n",
      "65     \t [0.84346801 0.58101367 0.86502099]. \t  3.70404679108428 \t 3.8297609028012825\n",
      "66     \t [0.89651467 0.58461337 0.91639188]. \t  3.3451029269401444 \t 3.8297609028012825\n",
      "67     \t [0.69290004 0.54807946 0.88557515]. \t  3.7014761656956128 \t 3.8297609028012825\n",
      "68     \t [0.80525667 0.5643814  0.88396334]. \t  3.675604560143718 \t 3.8297609028012825\n",
      "69     \t [0.69782465 0.58842892 0.76740467]. \t  3.045457745801297 \t 3.8297609028012825\n",
      "70     \t [0.77966488 0.54829397 0.80893882]. \t  3.545119510764989 \t 3.8297609028012825\n",
      "71     \t [0.73876897 0.59603075 0.82559315]. \t  3.6002700716658325 \t 3.8297609028012825\n",
      "72     \t [0.612314   0.61123068 0.87574936]. \t  3.6642342508978363 \t 3.8297609028012825\n",
      "73     \t [0.81338821 0.56500627 0.89752893]. \t  3.5794477167705216 \t 3.8297609028012825\n",
      "74     \t [0.54164219 0.65361816 0.84941427]. \t  3.484018309132155 \t 3.8297609028012825\n",
      "75     \t [0.86632658 0.52600149 0.80134378]. \t  3.4337262136026014 \t 3.8297609028012825\n",
      "76     \t [0.87158515 0.46672987 0.81816086]. \t  3.3978080584150403 \t 3.8297609028012825\n",
      "77     \t [0.59023898 0.56869788 0.78169902]. \t  3.3236713264181685 \t 3.8297609028012825\n",
      "78     \t [0.40657977 0.53050401 0.8196715 ]. \t  3.7333461076880305 \t 3.8297609028012825\n",
      "79     \t [0.81799126 0.44992463 0.80169135]. \t  3.2205356238707825 \t 3.8297609028012825\n",
      "80     \t [0.59047842 0.65197251 0.75929155]. \t  2.7763893042125183 \t 3.8297609028012825\n",
      "81     \t [0.70836984 0.54309307 0.90839457]. \t  3.502169602984023 \t 3.8297609028012825\n",
      "82     \t [0.59393934 0.52394842 0.83345951]. \t  3.7523238713572464 \t 3.8297609028012825\n",
      "83     \t [0.81289957 0.59181585 0.77521931]. \t  3.072157615762784 \t 3.8297609028012825\n",
      "84     \t [0.66409718 0.46347315 0.89041455]. \t  3.387830612221129 \t 3.8297609028012825\n",
      "85     \t [0.43459778 0.50138904 0.88017803]. \t  3.6756594781567022 \t 3.8297609028012825\n",
      "86     \t [0.72037728 0.50873063 0.84198146]. \t  3.708363624887971 \t 3.8297609028012825\n",
      "87     \t [0.62525271 0.61201285 0.85991426]. \t  3.688165331628105 \t 3.8297609028012825\n",
      "88     \t [0.75034354 0.54605525 0.83171467]. \t  3.7135112634754557 \t 3.8297609028012825\n",
      "89     \t [0.66390855 0.64019667 0.82367916]. \t  3.417005645773149 \t 3.8297609028012825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.54320664 0.57948173 0.81942506]. \t  3.678921034033856 \t 3.8297609028012825\n",
      "91     \t [0.10273305 0.56164409 0.83993727]. \t  3.8240692117144075 \t 3.8297609028012825\n",
      "92     \t [0.17642915 0.52666054 0.88434105]. \t  3.7180772182473563 \t 3.8297609028012825\n",
      "93     \t [0.70356162 0.55031103 0.93005956]. \t  3.243378953996766 \t 3.8297609028012825\n",
      "94     \t [0.60557016 0.46527021 0.87223848]. \t  3.516122028879703 \t 3.8297609028012825\n",
      "95     \t [0.60818402 0.6057925  0.87763032]. \t  3.6781518994431632 \t 3.8297609028012825\n",
      "96     \t [0.33350555 0.60101872 0.8482901 ]. \t  3.7821542849597805 \t 3.8297609028012825\n",
      "97     \t [0.06323869 0.46502391 0.8828575 ]. \t  3.4479331186078572 \t 3.8297609028012825\n",
      "98     \t [0.10333111 0.57573331 0.77094186]. \t  3.315599698552601 \t 3.8297609028012825\n",
      "99     \t [0.59240398 0.61087306 0.7819172 ]. \t  3.214000278028756 \t 3.8297609028012825\n",
      "100    \t [0.31452875 0.5962184  0.80877325]. \t  3.621960306848141 \t 3.8297609028012825\n"
     ]
    }
   ],
   "source": [
    "### 6(h). Bayesian optimization runs (x20): STP DF1 run number = 8\n",
    "\n",
    "np.random.seed(run_num_8)\n",
    "surrogate_stp_df1_8 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_8 = GPGO(surrogate_stp_df1_8, Acquisition_new(util_stp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_8.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.2248874896817443, -3.4106691819082218)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(h). Training Regret Minimisation: run number = 8\n",
    "\n",
    "gp_output_8 = np.append(np.max(gpgo_gp_8.GP.y[0:n_init]),gpgo_gp_8.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_8 = np.append(np.max(gpgo_stp_df1_8.GP.y[0:n_init]),gpgo_stp_df1_8.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_8 = np.log(y_global_orig - gp_output_8)\n",
    "regret_stp_df1_8 = np.log(y_global_orig - stp_df1_output_8)\n",
    "\n",
    "train_regret_gp_8 = min_max_array(regret_gp_8)\n",
    "train_regret_stp_df1_8 = min_max_array(regret_stp_df1_8)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 8\n",
    "min_train_regret_gp_8 = min(train_regret_gp_8)\n",
    "min_train_regret_stp_df1_8 = min(train_regret_stp_df1_8)\n",
    "\n",
    "min_train_regret_gp_8, min_train_regret_stp_df1_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.80342804 0.5275223  0.11911147]. \t  0.05516938924925502 \t 1.6536488994056173\n",
      "init   \t [0.63968144 0.09092526 0.33222568]. \t  0.7039339223433662 \t 1.6536488994056173\n",
      "init   \t [0.42738095 0.55438581 0.62812652]. \t  1.6536488994056173 \t 1.6536488994056173\n",
      "init   \t [0.69739294 0.78994969 0.13189035]. \t  0.009151170523361311 \t 1.6536488994056173\n",
      "init   \t [0.34277045 0.20155961 0.70732423]. \t  0.9342632521680911 \t 1.6536488994056173\n",
      "1      \t [0.96095706 1.         1.        ]. \t  0.318503383804133 \t 1.6536488994056173\n",
      "2      \t [0. 1. 1.]. \t  0.330219860606422 \t 1.6536488994056173\n",
      "3      \t [0.09877624 0.37453955 0.03083124]. \t  0.07739494328334615 \t 1.6536488994056173\n",
      "4      \t [0.7655898  0.09954979 0.95324089]. \t  0.3632581794305381 \t 1.6536488994056173\n",
      "5      \t [0.07685589 0.9986512  0.2070639 ]. \t  0.06835104660957286 \t 1.6536488994056173\n",
      "6      \t [0.99295938 0.58260624 0.68139351]. \t  1.6378476185605113 \t 1.6536488994056173\n",
      "7      \t [0.99717034 0.0315813  0.09580822]. \t  0.1177239192626055 \t 1.6536488994056173\n",
      "8      \t [0.01965361 0.61193923 0.81444289]. \t  \u001b[92m3.582636038021768\u001b[0m \t 3.582636038021768\n",
      "9      \t [0.05386941 0.43214999 0.47398938]. \t  0.5201086730415562 \t 3.582636038021768\n",
      "10     \t [0.57153349 0.47351466 0.97080999]. \t  2.4033727930289723 \t 3.582636038021768\n",
      "11     \t [0.         0.48097697 1.        ]. \t  1.9117769678665244 \t 3.582636038021768\n",
      "12     \t [0.00950299 0.85804588 0.62993419]. \t  2.7274940693773053 \t 3.582636038021768\n",
      "13     \t [0.86070331 0.94279037 0.03129544]. \t  0.00024096625191581512 \t 3.582636038021768\n",
      "14     \t [0.58736274 0.14253566 0.01726089]. \t  0.1319574387907576 \t 3.582636038021768\n",
      "15     \t [0.51230976 0.46311366 0.81907924]. \t  3.4855149583050467 \t 3.582636038021768\n",
      "16     \t [0.85692273 0.35361895 0.80452699]. \t  2.5364429386659753 \t 3.582636038021768\n",
      "17     \t [0.69202224 0.46887113 0.75475824]. \t  2.8420927664754396 \t 3.582636038021768\n",
      "18     \t [0.87170206 0.0674392  0.46786281]. \t  0.15740205210010505 \t 3.582636038021768\n",
      "19     \t [0.54758351 0.32871193 0.84260075]. \t  2.428820229437301 \t 3.582636038021768\n",
      "20     \t [0.         0.         0.02452379]. \t  0.09892857177007897 \t 3.582636038021768\n",
      "21     \t [0.42887758 0.6808845  0.86162288]. \t  3.3229607728332144 \t 3.582636038021768\n",
      "22     \t [0.23223    0.81466463 0.81312518]. \t  2.1344458838231484 \t 3.582636038021768\n",
      "23     \t [1.         0.62576793 0.91090117]. \t  3.241899763255023 \t 3.582636038021768\n",
      "24     \t [0.01861726 0.28536687 0.92691861]. \t  1.5810703920620306 \t 3.582636038021768\n",
      "25     \t [0.73127362 0.26076201 0.10774162]. \t  0.2555933057228006 \t 3.582636038021768\n",
      "26     \t [0.22009372 0.49903796 0.58530644]. \t  1.3003922597466797 \t 3.582636038021768\n",
      "27     \t [0.01323101 0.62938705 0.02687964]. \t  0.009125474754125604 \t 3.582636038021768\n",
      "28     \t [0.83758935 0.80341719 0.55107413]. \t  0.7027451088576724 \t 3.582636038021768\n",
      "29     \t [0.90501671 0.69059964 0.83548738]. \t  3.019360583933946 \t 3.582636038021768\n",
      "30     \t [0.99732881 0.1297632  0.90248715]. \t  0.6140839842939169 \t 3.582636038021768\n",
      "31     \t [0.64995457 0.88235472 0.88016795]. \t  1.3713747637575866 \t 3.582636038021768\n",
      "32     \t [0.50413014 0.72337841 0.56447151]. \t  1.7381843749720098 \t 3.582636038021768\n",
      "33     \t [0.52100745 0.59714438 0.7324335 ]. \t  2.692776766848315 \t 3.582636038021768\n",
      "34     \t [0.24575224 0.14879951 0.61626155]. \t  0.3645610975736748 \t 3.582636038021768\n",
      "35     \t [0.25017495 0.28178134 0.39352209]. \t  0.5043593662010872 \t 3.582636038021768\n",
      "36     \t [0.694958   0.61515382 0.13494162]. \t  0.03872239456729957 \t 3.582636038021768\n",
      "37     \t [0.71958243 0.76891975 0.65572309]. \t  1.303541953523637 \t 3.582636038021768\n",
      "38     \t [0.83658185 0.28999234 0.17831345]. \t  0.30361285931803395 \t 3.582636038021768\n",
      "39     \t [0.62471784 0.01689354 0.17887161]. \t  0.5879562422219591 \t 3.582636038021768\n",
      "40     \t [0.65993137 0.47075859 0.54666303]. \t  0.6050314082021813 \t 3.582636038021768\n",
      "41     \t [0.57760568 0.59807284 0.37461947]. \t  0.34437378999251556 \t 3.582636038021768\n",
      "42     \t [0.2330043  0.94932489 0.63231992]. \t  2.399180912517615 \t 3.582636038021768\n",
      "43     \t [0.54134831 0.51655529 0.12827142]. \t  0.10582862611495988 \t 3.582636038021768\n",
      "44     \t [0.02454027 0.11229068 0.87408474]. \t  0.6035591048625235 \t 3.582636038021768\n",
      "45     \t [0.11322736 0.83179594 0.89396408]. \t  1.8419758353895992 \t 3.582636038021768\n",
      "46     \t [0.13965536 0.24093939 0.65817463]. \t  0.8354675500972123 \t 3.582636038021768\n",
      "47     \t [0.67247276 0.11995418 0.59946499]. \t  0.25766334814393144 \t 3.582636038021768\n",
      "48     \t [0.58300914 0.93192586 0.21282601]. \t  0.045381024986049685 \t 3.582636038021768\n",
      "49     \t [0.62218283 0.90900293 0.1277241 ]. \t  0.006534253992439146 \t 3.582636038021768\n",
      "50     \t [0.74256556 0.43381153 0.19264891]. \t  0.2065593867545645 \t 3.582636038021768\n",
      "51     \t [0.69338885 0.08627401 0.11065029]. \t  0.3459448233643284 \t 3.582636038021768\n",
      "52     \t [0.37136239 0.91648361 0.59464345]. \t  2.392000869413017 \t 3.582636038021768\n",
      "53     \t [0.11736069 0.75316754 0.36771477]. \t  0.9235219652528445 \t 3.582636038021768\n",
      "54     \t [0.7043113  0.26137456 0.97964802]. \t  0.9877612750800018 \t 3.582636038021768\n",
      "55     \t [0.58822533 0.35373533 0.98274796]. \t  1.540692159340522 \t 3.582636038021768\n",
      "56     \t [0.35292396 0.7284181  0.45097298]. \t  1.5102668359383205 \t 3.582636038021768\n",
      "57     \t [0.51468179 0.71326422 0.35311863]. \t  0.44317841096501803 \t 3.582636038021768\n",
      "58     \t [0.38990676 0.53132425 0.22409725]. \t  0.19745453644596042 \t 3.582636038021768\n",
      "59     \t [0.88432717 0.1576333  0.25483134]. \t  0.44154917587181813 \t 3.582636038021768\n",
      "60     \t [0.25417215 0.85139669 0.08313939]. \t  0.005131634668135197 \t 3.582636038021768\n",
      "61     \t [0.26827285 0.84672031 0.82024833]. \t  1.8471915059924302 \t 3.582636038021768\n",
      "62     \t [0.20044985 0.38313224 0.42858053]. \t  0.4065415917447023 \t 3.582636038021768\n",
      "63     \t [0.00894797 0.86311035 0.32490426]. \t  0.5992491399772332 \t 3.582636038021768\n",
      "64     \t [0.86282608 0.21285425 0.10259378]. \t  0.19445902589887212 \t 3.582636038021768\n",
      "65     \t [0.47916406 0.07391472 0.40147271]. \t  0.5580936784695917 \t 3.582636038021768\n",
      "66     \t [0.91451142 0.6909301  0.81795158]. \t  2.9036696684743233 \t 3.582636038021768\n",
      "67     \t [0.43468799 0.03923949 0.33883625]. \t  0.7981758545610268 \t 3.582636038021768\n",
      "68     \t [0.56619891 0.07736869 0.7547747 ]. \t  0.4778607290860891 \t 3.582636038021768\n",
      "69     \t [0.97029792 0.92910227 0.59798756]. \t  0.3969645275729967 \t 3.582636038021768\n",
      "70     \t [0.16982173 0.06557014 0.73734615]. \t  0.4115204929868468 \t 3.582636038021768\n",
      "71     \t [0.43860812 0.29570706 0.13229548]. \t  0.41484876224588224 \t 3.582636038021768\n",
      "72     \t [0.24463422 0.71318517 0.79012456]. \t  2.8933086874032927 \t 3.582636038021768\n",
      "73     \t [0.92307882 0.26724234 0.62293382]. \t  0.6413453578329811 \t 3.582636038021768\n",
      "74     \t [0.13369466 0.45324564 0.56827039]. \t  1.0017610441141187 \t 3.582636038021768\n",
      "75     \t [0.17403528 0.45591836 0.68597959]. \t  2.066053953610975 \t 3.582636038021768\n",
      "76     \t [0.17228086 0.74038486 0.67007788]. \t  2.552227852029529 \t 3.582636038021768\n",
      "77     \t [0.24614794 0.29720669 0.65786637]. \t  1.0741448359785744 \t 3.582636038021768\n",
      "78     \t [0.79929807 0.92451776 0.74737308]. \t  0.8272617348032159 \t 3.582636038021768\n",
      "79     \t [0.59337992 0.13917599 0.07763026]. \t  0.2907462934900245 \t 3.582636038021768\n",
      "80     \t [0.95843043 0.94200319 0.35650309]. \t  0.10194176178359038 \t 3.582636038021768\n",
      "81     \t [0.52240711 0.30817363 0.33234368]. \t  0.5886953591025507 \t 3.582636038021768\n",
      "82     \t [0.69352928 0.66088913 0.66191433]. \t  1.5849515268590912 \t 3.582636038021768\n",
      "83     \t [0.27146486 0.74267653 0.18100121]. \t  0.05093524582366787 \t 3.582636038021768\n",
      "84     \t [0.52519573 0.5521018  0.84112446]. \t  \u001b[92m3.818189709401497\u001b[0m \t 3.818189709401497\n",
      "85     \t [0.64144854 0.01569875 0.07184981]. \t  0.2295837295346077 \t 3.818189709401497\n",
      "86     \t [0.48284383 0.60851186 0.30178257]. \t  0.2274639721561767 \t 3.818189709401497\n",
      "87     \t [0.67793529 0.48675574 0.48690062]. \t  0.38896239080246947 \t 3.818189709401497\n",
      "88     \t [0.64175135 0.90497947 0.69370479]. \t  1.1379403889038424 \t 3.818189709401497\n",
      "89     \t [0.6791164  0.55313926 0.8965976 ]. \t  3.627618240345153 \t 3.818189709401497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.78575744 0.42016254 0.21131259]. \t  0.2184692038879679 \t 3.818189709401497\n",
      "91     \t [0.02936286 0.50790832 0.37360011]. \t  0.40799229236381035 \t 3.818189709401497\n",
      "92     \t [0.22109237 0.04755993 0.20649857]. \t  0.7988764965553357 \t 3.818189709401497\n",
      "93     \t [0.86752059 0.35948895 0.18470936]. \t  0.2153457044236414 \t 3.818189709401497\n",
      "94     \t [0.86645362 0.73534995 0.44053744]. \t  0.32533968898162047 \t 3.818189709401497\n",
      "95     \t [0.18181159 0.57905266 0.39107203]. \t  0.6362255092541039 \t 3.818189709401497\n",
      "96     \t [0.34508795 0.46351135 0.42728305]. \t  0.4658177758162148 \t 3.818189709401497\n",
      "97     \t [0.31903146 0.71537505 0.32247926]. \t  0.4326724181997435 \t 3.818189709401497\n",
      "98     \t [0.34449026 0.69289775 0.33003677]. \t  0.43744149448894865 \t 3.818189709401497\n",
      "99     \t [0.32246996 0.71313853 0.51126178]. \t  2.020487917978038 \t 3.818189709401497\n",
      "100    \t [0.92031871 0.92553402 0.12703466]. \t  0.0020009372160628078 \t 3.818189709401497\n"
     ]
    }
   ],
   "source": [
    "### 6(i). Bayesian optimization runs (x20): GP run number = 9\n",
    "\n",
    "np.random.seed(run_num_9)\n",
    "surrogate_gp_9 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "gpgo_gp_9 = GPGO(surrogate_gp_9, Acquisition_new(util_gp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_9.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.80342804 0.5275223  0.11911147]. \t  0.05516938924925502 \t 1.6536488994056173\n",
      "init   \t [0.63968144 0.09092526 0.33222568]. \t  0.7039339223433662 \t 1.6536488994056173\n",
      "init   \t [0.42738095 0.55438581 0.62812652]. \t  1.6536488994056173 \t 1.6536488994056173\n",
      "init   \t [0.69739294 0.78994969 0.13189035]. \t  0.009151170523361311 \t 1.6536488994056173\n",
      "init   \t [0.34277045 0.20155961 0.70732423]. \t  0.9342632521680911 \t 1.6536488994056173\n",
      "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 1.6536488994056173\n",
      "2      \t [0. 1. 1.]. \t  0.330219860606422 \t 1.6536488994056173\n",
      "3      \t [0.         0.28855326 0.        ]. \t  0.058082815240340945 \t 1.6536488994056173\n",
      "4      \t [1. 0. 1.]. \t  0.08848201872702738 \t 1.6536488994056173\n",
      "5      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 1.6536488994056173\n",
      "6      \t [1. 0. 0.]. \t  0.03095471703300515 \t 1.6536488994056173\n",
      "7      \t [0.55104764 0.5861684  1.        ]. \t  \u001b[92m2.0747164618610654\u001b[0m \t 2.0747164618610654\n",
      "8      \t [0.         0.44043538 1.        ]. \t  1.7562457277978405 \t 2.0747164618610654\n",
      "9      \t [1.         0.54019842 0.63933551]. \t  1.1584722977067303 \t 2.0747164618610654\n",
      "10     \t [5.55111512e-17 0.00000000e+00 1.00000000e+00]. \t  0.0902894676548261 \t 2.0747164618610654\n",
      "11     \t [0.         0.         0.36629136]. \t  0.43365671175334647 \t 2.0747164618610654\n",
      "12     \t [0.36849723 0.         0.        ]. \t  0.10224656956611179 \t 2.0747164618610654\n",
      "13     \t [1.         1.         0.35510473]. \t  0.0718331116227256 \t 2.0747164618610654\n",
      "14     \t [0.        1.        0.4752432]. \t  2.044934958142513 \t 2.0747164618610654\n",
      "15     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.0747164618610654\n",
      "16     \t [1.         0.48407027 1.        ]. \t  1.877146985704256 \t 2.0747164618610654\n",
      "17     \t [0.         0.65964311 0.43027902]. \t  1.1926113240617389 \t 2.0747164618610654\n",
      "18     \t [0.46280393 1.         0.78465287]. \t  0.7808583576689232 \t 2.0747164618610654\n",
      "19     \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.0747164618610654\n",
      "20     \t [1.         0.         0.47338381]. \t  0.08666489273122532 \t 2.0747164618610654\n",
      "21     \t [0.25822054 1.         0.25028357]. \t  0.1486364927898805 \t 2.0747164618610654\n",
      "22     \t [0.77942699 0.34027469 0.84981116]. \t  \u001b[92m2.501878169900019\u001b[0m \t 2.501878169900019\n",
      "23     \t [0.59990209 0.         1.        ]. \t  0.09136730293094905 \t 2.501878169900019\n",
      "24     \t [0.         0.75500528 0.79500112]. \t  \u001b[92m2.580632645001071\u001b[0m \t 2.580632645001071\n",
      "25     \t [0.         0.32363895 0.69123434]. \t  1.4706765650885474 \t 2.580632645001071\n",
      "26     \t [0.81887047 0.73639634 0.86503304]. \t  \u001b[92m2.7179454864988153\u001b[0m \t 2.7179454864988153\n",
      "27     \t [0.71668765 0.         0.68805194]. \t  0.18840815930546906 \t 2.7179454864988153\n",
      "28     \t [0.31702576 0.65709426 0.        ]. \t  0.006453261382731076 \t 2.7179454864988153\n",
      "29     \t [0.63161918 1.         0.        ]. \t  0.0001495092329521808 \t 2.7179454864988153\n",
      "30     \t [1.         1.         0.74609371]. \t  0.39011926609630176 \t 2.7179454864988153\n",
      "31     \t [1.         0.40083753 0.04503781]. \t  0.030742404606151668 \t 2.7179454864988153\n",
      "32     \t [0.74999997 1.         0.59642799]. \t  0.7593308420705019 \t 2.7179454864988153\n",
      "33     \t [0.31989531 0.23504936 0.14526444]. \t  0.5527734812513255 \t 2.7179454864988153\n",
      "34     \t [0.81317317 0.36258052 1.        ]. \t  1.3596810280856293 \t 2.7179454864988153\n",
      "35     \t [0.62465269 0.09789247 0.        ]. \t  0.09600503930609038 \t 2.7179454864988153\n",
      "36     \t [0.25564187 0.77216618 0.94710905]. \t  1.9906160585360246 \t 2.7179454864988153\n",
      "37     \t [1.13270378e-10 7.32119520e-01 1.00000000e+00]. \t  1.5791531003657922 \t 2.7179454864988153\n",
      "38     \t [1.         0.18514946 0.2807468 ]. \t  0.2880063054989693 \t 2.7179454864988153\n",
      "39     \t [0.611464   1.         0.99756121]. \t  0.33684431752841265 \t 2.7179454864988153\n",
      "40     \t [0.97590145 0.17509049 0.826948  ]. \t  1.0297171506532727 \t 2.7179454864988153\n",
      "41     \t [1.97259882e-09 6.31284467e-01 2.54223184e-10]. \t  0.0056931868842246235 \t 2.7179454864988153\n",
      "42     \t [0.72283044 0.         0.16560197]. \t  0.43913404730703326 \t 2.7179454864988153\n",
      "43     \t [1.         0.75176334 1.        ]. \t  1.4370066094817826 \t 2.7179454864988153\n",
      "44     \t [0.28281308 0.         1.        ]. \t  0.09161433822197723 \t 2.7179454864988153\n",
      "45     \t [7.38648052e-08 0.00000000e+00 7.91279837e-01]. \t  0.24727987406896534 \t 2.7179454864988153\n",
      "46     \t [0.17185191 0.54087292 0.86522883]. \t  \u001b[92m3.828146585752626\u001b[0m \t 3.828146585752626\n",
      "47     \t [0.11676066 0.53537313 0.82369668]. \t  3.7576025387046688 \t 3.828146585752626\n",
      "48     \t [0.25471943 0.50534912 0.89136225]. \t  3.613961004185694 \t 3.828146585752626\n",
      "49     \t [8.05586765e-06 1.00000000e+00 2.03377268e-01]. \t  0.06101071168237301 \t 3.828146585752626\n",
      "50     \t [0.25830329 0.40782581 0.95220635]. \t  2.319879503587255 \t 3.828146585752626\n",
      "51     \t [0.04465363 0.99400235 0.80885091]. \t  0.864836062514363 \t 3.828146585752626\n",
      "52     \t [0.10202334 0.48877158 0.82449513]. \t  3.632932787662144 \t 3.828146585752626\n",
      "53     \t [1.         0.78620555 0.        ]. \t  0.00042814309867822463 \t 3.828146585752626\n",
      "54     \t [0.20453338 0.59281837 0.76891855]. \t  3.2789183920449876 \t 3.828146585752626\n",
      "55     \t [0.27868981 0.60952188 0.88555826]. \t  3.6639440753470276 \t 3.828146585752626\n",
      "56     \t [0.05220531 0.52997813 0.76227077]. \t  3.194226891849147 \t 3.828146585752626\n",
      "57     \t [0.20089161 0.55400885 0.79343603]. \t  3.5602170387791165 \t 3.828146585752626\n",
      "58     \t [0.84750393 0.54278334 0.88915468]. \t  3.625955971537727 \t 3.828146585752626\n",
      "59     \t [0.17350023 0.60424328 0.77647895]. \t  3.3319423595083277 \t 3.828146585752626\n",
      "60     \t [0.20789946 0.59778164 0.85934813]. \t  3.7915640789819727 \t 3.828146585752626\n",
      "61     \t [0.64283459 0.58228316 0.84638177]. \t  3.7637007602518597 \t 3.828146585752626\n",
      "62     \t [0.55441972 0.53866256 0.86694897]. \t  3.8073574313306757 \t 3.828146585752626\n",
      "63     \t [0.7492382  0.57784443 0.87962905]. \t  3.7009033854016 \t 3.828146585752626\n",
      "64     \t [0.60583355 0.5271356  0.79519834]. \t  3.477791962997869 \t 3.828146585752626\n",
      "65     \t [0.60811414 0.52692634 0.84306247]. \t  3.7830746353569484 \t 3.828146585752626\n",
      "66     \t [0.46404727 0.51926532 0.90629246]. \t  3.520136212508339 \t 3.828146585752626\n",
      "67     \t [0.21665482 0.00962971 0.32024355]. \t  0.7648578803330891 \t 3.828146585752626\n",
      "68     \t [0.77134793 0.49594074 0.88263435]. \t  3.5760794927682316 \t 3.828146585752626\n",
      "69     \t [0.24073626 0.57266985 0.87987588]. \t  3.7787941678699477 \t 3.828146585752626\n",
      "70     \t [0.54462027 0.46430426 0.86094117]. \t  3.5564085639405767 \t 3.828146585752626\n",
      "71     \t [0.53490211 0.56406942 0.85573193]. \t  \u001b[92m3.8302189077598787\u001b[0m \t 3.8302189077598787\n",
      "72     \t [0.60685686 0.4927932  0.86897999]. \t  3.6675990837326307 \t 3.8302189077598787\n",
      "73     \t [0.61187483 0.50018568 0.87173875]. \t  3.6860499494061676 \t 3.8302189077598787\n",
      "74     \t [0.45144548 0.55258884 0.87575155]. \t  3.802137038086445 \t 3.8302189077598787\n",
      "75     \t [0.27232657 0.4857755  0.8435257 ]. \t  3.694346074461981 \t 3.8302189077598787\n",
      "76     \t [0.1630235  0.55681312 0.92985151]. \t  3.2752450929370167 \t 3.8302189077598787\n",
      "77     \t [0.41214976 0.48253169 0.82417784]. \t  3.619332893908478 \t 3.8302189077598787\n",
      "78     \t [0.4523383  0.60690025 0.86053963]. \t  3.7487093451055014 \t 3.8302189077598787\n",
      "79     \t [0.57259171 0.52981537 0.84825637]. \t  3.8050452717261214 \t 3.8302189077598787\n",
      "80     \t [0.4072336  0.58649682 0.85948107]. \t  3.81623078462593 \t 3.8302189077598787\n",
      "81     \t [0.06373276 0.57931865 0.79055436]. \t  3.4947088291472594 \t 3.8302189077598787\n",
      "82     \t [0.51829727 0.52205488 0.87120612]. \t  3.7718256473903424 \t 3.8302189077598787\n",
      "83     \t [0.65345324 0.54499163 0.80821417]. \t  3.589999950512189 \t 3.8302189077598787\n",
      "84     \t [0.52519573 0.5521018  0.84112446]. \t  3.818189709401497 \t 3.8302189077598787\n",
      "85     \t [0.24852566 0.60340062 0.87849666]. \t  3.72130180491666 \t 3.8302189077598787\n",
      "86     \t [0.86022632 0.58190452 0.80448502]. \t  3.415670201527574 \t 3.8302189077598787\n",
      "87     \t [0.84742437 0.66607925 0.88646006]. \t  3.2479817195452076 \t 3.8302189077598787\n",
      "88     \t [0.5212387  0.59623909 0.84363437]. \t  3.755683283557917 \t 3.8302189077598787\n",
      "89     \t [0.6791164  0.55313926 0.8965976 ]. \t  3.627618240345153 \t 3.8302189077598787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.28184056 0.60835883 0.77613145]. \t  3.3048785437781314 \t 3.8302189077598787\n",
      "91     \t [0.86012448 0.58569984 0.91084545]. \t  3.4159895484461686 \t 3.8302189077598787\n",
      "92     \t [0.72632427 0.58316944 0.88920008]. \t  3.6462119675347906 \t 3.8302189077598787\n",
      "93     \t [0.72704217 0.56899231 0.8220823 ]. \t  3.648491082670728 \t 3.8302189077598787\n",
      "94     \t [0.04014729 0.54907774 0.8385101 ]. \t  3.80471362737942 \t 3.8302189077598787\n",
      "95     \t [0.50021313 0.54381991 0.88378563]. \t  3.748816061530942 \t 3.8302189077598787\n",
      "96     \t [0.38948828 0.58565125 0.8588018 ]. \t  3.820661690560071 \t 3.8302189077598787\n",
      "97     \t [0.09388166 0.61798116 0.82207012]. \t  3.6240691530269906 \t 3.8302189077598787\n",
      "98     \t [0.67164637 0.59232027 0.90549882]. \t  3.5112175888040444 \t 3.8302189077598787\n",
      "99     \t [0.85151759 0.52920255 0.86240174]. \t  3.7165331522468565 \t 3.8302189077598787\n",
      "100    \t [0.2873237  0.5756247  0.86647111]. \t  3.830020296478435 \t 3.8302189077598787\n"
     ]
    }
   ],
   "source": [
    "### 6(i). Bayesian optimization runs (x20): STP DF1 run number = 9\n",
    "\n",
    "np.random.seed(run_num_9)\n",
    "surrogate_stp_df1_9 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_9 = GPGO(surrogate_stp_df1_9, Acquisition_new(util_stp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_9.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.1102391432412486, -3.4246371929521673)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(i). Training Regret Minimisation: run number = 9\n",
    "\n",
    "gp_output_9 = np.append(np.max(gpgo_gp_9.GP.y[0:n_init]),gpgo_gp_9.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_9 = np.append(np.max(gpgo_stp_df1_9.GP.y[0:n_init]),gpgo_stp_df1_9.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_9 = np.log(y_global_orig - gp_output_9)\n",
    "regret_stp_df1_9 = np.log(y_global_orig - stp_df1_output_9)\n",
    "\n",
    "train_regret_gp_9 = min_max_array(regret_gp_9)\n",
    "train_regret_stp_df1_9 = min_max_array(regret_stp_df1_9)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 9\n",
    "min_train_regret_gp_9 = min(train_regret_gp_9)\n",
    "min_train_regret_stp_df1_9 = min(train_regret_stp_df1_9)\n",
    "\n",
    "min_train_regret_gp_9, min_train_regret_stp_df1_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.65358959 0.11500694 0.95028286]. \t  0.42730315147591735 \t 1.1029187088185965\n",
      "init   \t [0.4821914  0.87247454 0.21233268]. \t  0.06161964400032635 \t 1.1029187088185965\n",
      "init   \t [0.04070962 0.39719446 0.2331322 ]. \t  0.33269334660262956 \t 1.1029187088185965\n",
      "init   \t [0.84174072 0.20708234 0.74246953]. \t  1.1029187088185965 \t 1.1029187088185965\n",
      "init   \t [0.39215413 0.18225652 0.74353941]. \t  0.9779763535009853 \t 1.1029187088185965\n",
      "1      \t [0.65784615 0.04582828 0.06191159]. \t  0.20874301192282538 \t 1.1029187088185965\n",
      "2      \t [0.13064464 0.91108377 0.95539833]. \t  0.902351403909183 \t 1.1029187088185965\n",
      "3      \t [0.96893897 0.97974629 0.99865138]. \t  0.381043852508212 \t 1.1029187088185965\n",
      "4      \t [0.98139485 0.81725946 0.12404836]. \t  0.0024384188819132563 \t 1.1029187088185965\n",
      "5      \t [0.10355689 0.02892137 0.41095543]. \t  0.4089719355888731 \t 1.1029187088185965\n",
      "6      \t [0.02218954 0.56584252 0.99967923]. \t  \u001b[92m2.067141380028605\u001b[0m \t 2.067141380028605\n",
      "7      \t [0.05955692 0.03232917 0.89086014]. \t  0.27726435960872364 \t 2.067141380028605\n",
      "8      \t [0.98484883 0.56561537 0.99985617]. \t  2.015018280106272 \t 2.067141380028605\n",
      "9      \t [0.63184882 0.63023147 0.86946882]. \t  \u001b[92m3.5909750349985345\u001b[0m \t 3.5909750349985345\n",
      "10     \t [0.57930165 0.65764517 0.96407083]. \t  2.508814976030725 \t 3.5909750349985345\n",
      "11     \t [0.80785246 0.76226747 0.6322787 ]. \t  1.0200637423875345 \t 3.5909750349985345\n",
      "12     \t [0.         0.61493209 0.71370869]. \t  2.6640992266681955 \t 3.5909750349985345\n",
      "13     \t [0.68277861 0.58597413 0.        ]. \t  0.009721106432376475 \t 3.5909750349985345\n",
      "14     \t [0.66430626 0.44457057 0.93318238]. \t  2.8124401992306645 \t 3.5909750349985345\n",
      "15     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.5909750349985345\n",
      "16     \t [0.34880716 0.62390797 0.74426788]. \t  2.9064433751584025 \t 3.5909750349985345\n",
      "17     \t [0.56383078 0.96512321 0.01989752]. \t  0.00038527602653866237 \t 3.5909750349985345\n",
      "18     \t [0.93467313 0.59513961 0.96751112]. \t  2.57238835070557 \t 3.5909750349985345\n",
      "19     \t [0.55401803 0.59081142 0.69702173]. \t  2.2293241627036107 \t 3.5909750349985345\n",
      "20     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.5909750349985345\n",
      "21     \t [0.49838547 0.91412823 0.84602665]. \t  1.1959894323347209 \t 3.5909750349985345\n",
      "22     \t [0.93481963 0.15852344 0.01178922]. \t  0.05304559981970046 \t 3.5909750349985345\n",
      "23     \t [0.88131572 0.44114157 0.73043951]. \t  2.359362749579099 \t 3.5909750349985345\n",
      "24     \t [0.56774596 0.16752077 0.11092578]. \t  0.415755972019419 \t 3.5909750349985345\n",
      "25     \t [0.82727226 0.01716246 0.19996375]. \t  0.4206393454321978 \t 3.5909750349985345\n",
      "26     \t [0.71908065 0.38259867 0.3836828 ]. \t  0.27627310137628897 \t 3.5909750349985345\n",
      "27     \t [0.4990589  0.30974256 0.24859155]. \t  0.6536270178128792 \t 3.5909750349985345\n",
      "28     \t [0.85541574 0.11938326 0.13548969]. \t  0.2919036809750154 \t 3.5909750349985345\n",
      "29     \t [0.02059083 0.05214196 0.52110823]. \t  0.1461744279058926 \t 3.5909750349985345\n",
      "30     \t [0.54390625 0.96535455 0.95093515]. \t  0.6194232780140717 \t 3.5909750349985345\n",
      "31     \t [0.36931778 0.2013132  0.99354813]. \t  0.5996045984881392 \t 3.5909750349985345\n",
      "32     \t [0.37977161 0.12368017 0.6437221 ]. \t  0.3839468807083688 \t 3.5909750349985345\n",
      "33     \t [0.8047245  0.59171294 0.41136894]. \t  0.22314224425083254 \t 3.5909750349985345\n",
      "34     \t [0.44638342 0.89418366 0.11933375]. \t  0.008414858529026797 \t 3.5909750349985345\n",
      "35     \t [0.52924027 0.07235039 0.7635873 ]. \t  0.46807615193261143 \t 3.5909750349985345\n",
      "36     \t [0.7411776  0.1226393  0.57007957]. \t  0.20544785956289124 \t 3.5909750349985345\n",
      "37     \t [0.40426692 0.43087405 0.72122275]. \t  2.358668318450057 \t 3.5909750349985345\n",
      "38     \t [0.34323487 0.61523632 0.05239501]. \t  0.0215357272331486 \t 3.5909750349985345\n",
      "39     \t [0.35604083 0.00039971 0.11023479]. \t  0.41622529501959665 \t 3.5909750349985345\n",
      "40     \t [0.54347741 0.80014797 0.14840083]. \t  0.017026410789868152 \t 3.5909750349985345\n",
      "41     \t [0.92786235 0.39842196 0.1261783 ]. \t  0.09778381676669251 \t 3.5909750349985345\n",
      "42     \t [0.35526549 0.85732693 0.77594706]. \t  1.7393881605971755 \t 3.5909750349985345\n",
      "43     \t [0.13928129 0.037642   0.31364399]. \t  0.7523955330830968 \t 3.5909750349985345\n",
      "44     \t [0.43943336 0.35058606 0.65226717]. \t  1.2254093193788633 \t 3.5909750349985345\n",
      "45     \t [0.41287946 0.11110351 0.80531922]. \t  0.668777513157417 \t 3.5909750349985345\n",
      "46     \t [0.75394388 0.67847445 0.46045262]. \t  0.5040847138299055 \t 3.5909750349985345\n",
      "47     \t [0.41261872 0.95628583 0.82782702]. \t  0.9611467135683864 \t 3.5909750349985345\n",
      "48     \t [0.23896224 0.39284001 0.62793845]. \t  1.1987990305882834 \t 3.5909750349985345\n",
      "49     \t [0.90063347 0.85098353 0.19437089]. \t  0.010944354013264199 \t 3.5909750349985345\n",
      "50     \t [0.3981577  0.45222437 0.68939628]. \t  2.042203289076502 \t 3.5909750349985345\n",
      "51     \t [0.27320175 0.32743207 0.54484384]. \t  0.4868679427897529 \t 3.5909750349985345\n",
      "52     \t [0.70426147 0.94977949 0.91975555]. \t  0.7859185485075025 \t 3.5909750349985345\n",
      "53     \t [0.29938903 0.33951197 0.10493827]. \t  0.2727704257910028 \t 3.5909750349985345\n",
      "54     \t [0.04227152 0.51632077 0.06614304]. \t  0.04442441130910269 \t 3.5909750349985345\n",
      "55     \t [0.99248764 0.16035644 0.36489517]. \t  0.233568848059692 \t 3.5909750349985345\n",
      "56     \t [0.7384411  0.96115012 0.40008681]. \t  0.4147517720316982 \t 3.5909750349985345\n",
      "57     \t [0.21183532 0.81474443 0.39789438]. \t  1.352137891327086 \t 3.5909750349985345\n",
      "58     \t [0.22519599 0.91835787 0.61079726]. \t  2.704469251438837 \t 3.5909750349985345\n",
      "59     \t [0.86614404 0.60244622 0.09429213]. \t  0.018831020154467 \t 3.5909750349985345\n",
      "60     \t [0.        1.        0.5240776]. \t  2.4110191527289753 \t 3.5909750349985345\n",
      "61     \t [0.27980633 0.46148506 0.1752227 ]. \t  0.2378344066527602 \t 3.5909750349985345\n",
      "62     \t [0.91427037 0.73030932 0.37220707]. \t  0.13958483090937676 \t 3.5909750349985345\n",
      "63     \t [0.3715256  0.97661857 0.83547791]. \t  0.842797440952053 \t 3.5909750349985345\n",
      "64     \t [0.94554547 0.95144071 0.33096856]. \t  0.07758239833381178 \t 3.5909750349985345\n",
      "65     \t [0.62945952 0.81213003 0.07417604]. \t  0.0033818119848788857 \t 3.5909750349985345\n",
      "66     \t [0.73893811 0.68093479 0.04192289]. \t  0.006243422255177505 \t 3.5909750349985345\n",
      "67     \t [0.14245061 0.4693838  0.2147183 ]. \t  0.2462921799924161 \t 3.5909750349985345\n",
      "68     \t [0.34945726 0.51161119 0.28253862]. \t  0.28359048954848454 \t 3.5909750349985345\n",
      "69     \t [0.65959258 0.15693117 0.49238012]. \t  0.23201619425484993 \t 3.5909750349985345\n",
      "70     \t [0.07867937 0.90892995 0.60080682]. \t  2.896572730126719 \t 3.5909750349985345\n",
      "71     \t [0.39040893 0.32429144 0.01740015]. \t  0.0998205204225004 \t 3.5909750349985345\n",
      "72     \t [0.61052577 0.39268847 0.46810742]. \t  0.31167649069899306 \t 3.5909750349985345\n",
      "73     \t [0.36260274 0.54841149 0.04248908]. \t  0.03446239000851175 \t 3.5909750349985345\n",
      "74     \t [0.00589812 0.27171843 0.98890298]. \t  0.9666106673936532 \t 3.5909750349985345\n",
      "75     \t [0.89290771 0.30965735 0.87323794]. \t  2.1147184202197984 \t 3.5909750349985345\n",
      "76     \t [0.53783974 0.37586609 0.11422795]. \t  0.23297846283288623 \t 3.5909750349985345\n",
      "77     \t [0.1100138  0.86478416 0.52766993]. \t  2.9856105882883006 \t 3.5909750349985345\n",
      "78     \t [0.84135491 0.32663725 0.05460281]. \t  0.08491332223000136 \t 3.5909750349985345\n",
      "79     \t [0.46396811 0.57353837 0.95369955]. \t  2.9132276860785464 \t 3.5909750349985345\n",
      "80     \t [0.75053175 0.07049347 0.35769157]. \t  0.496737531071639 \t 3.5909750349985345\n",
      "81     \t [0.03047332 0.0352073  0.14699465]. \t  0.42970513491749057 \t 3.5909750349985345\n",
      "82     \t [0.91763429 0.51515885 0.54189505]. \t  0.43106468558061406 \t 3.5909750349985345\n",
      "83     \t [0.67524817 0.56782453 0.72559978]. \t  2.5187639523066165 \t 3.5909750349985345\n",
      "84     \t [0.1837141  0.8699628  0.15373861]. \t  0.02583793048070997 \t 3.5909750349985345\n",
      "85     \t [0.96033402 0.15484275 0.55027791]. \t  0.17937950478392242 \t 3.5909750349985345\n",
      "86     \t [0.41268804 0.21677931 0.07668479]. \t  0.3026346220661753 \t 3.5909750349985345\n",
      "87     \t [0.96856976 0.08974637 0.01914393]. \t  0.053200915564779426 \t 3.5909750349985345\n",
      "88     \t [0.95030821 0.19386301 0.97179269]. \t  0.6620893120506729 \t 3.5909750349985345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.62693302 0.84101297 0.42004557]. \t  0.7795783747167534 \t 3.5909750349985345\n",
      "90     \t [0.71755206 0.26757393 0.39225026]. \t  0.3691089467773529 \t 3.5909750349985345\n",
      "91     \t [0.91502671 0.69756974 0.91725515]. \t  2.787194121186799 \t 3.5909750349985345\n",
      "92     \t [0.9270585  0.78468855 0.49436225]. \t  0.3833080971914929 \t 3.5909750349985345\n",
      "93     \t [0.06804019 0.76189743 0.21971389]. \t  0.10216064197127789 \t 3.5909750349985345\n",
      "94     \t [0.0962197  0.81525186 0.35072737]. \t  0.838479048398054 \t 3.5909750349985345\n",
      "95     \t [0.49508671 0.39037781 0.57475127]. \t  0.7079714096790843 \t 3.5909750349985345\n",
      "96     \t [0.84554329 0.19749171 0.53590731]. \t  0.21072762937007228 \t 3.5909750349985345\n",
      "97     \t [0.71358188 0.73749073 0.55743709]. \t  1.0333471264077945 \t 3.5909750349985345\n",
      "98     \t [0.42325437 0.52968222 0.00214746]. \t  0.021975714792266725 \t 3.5909750349985345\n",
      "99     \t [0.55324297 0.28908356 0.80797698]. \t  2.0124792267115676 \t 3.5909750349985345\n",
      "100    \t [0.54656951 0.65652098 0.75129329]. \t  2.7054350876838593 \t 3.5909750349985345\n"
     ]
    }
   ],
   "source": [
    "### 6(j). Bayesian optimization runs (x20): GP run number = 10\n",
    "\n",
    "np.random.seed(run_num_10)\n",
    "surrogate_gp_10 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "gpgo_gp_10 = GPGO(surrogate_gp_10, Acquisition_new(util_gp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_10.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.65358959 0.11500694 0.95028286]. \t  0.42730315147591735 \t 1.1029187088185965\n",
      "init   \t [0.4821914  0.87247454 0.21233268]. \t  0.06161964400032635 \t 1.1029187088185965\n",
      "init   \t [0.04070962 0.39719446 0.2331322 ]. \t  0.33269334660262956 \t 1.1029187088185965\n",
      "init   \t [0.84174072 0.20708234 0.74246953]. \t  1.1029187088185965 \t 1.1029187088185965\n",
      "init   \t [0.39215413 0.18225652 0.74353941]. \t  0.9779763535009853 \t 1.1029187088185965\n",
      "1      \t [1. 0. 0.]. \t  0.03095471703300515 \t 1.1029187088185965\n",
      "2      \t [1. 1. 1.]. \t  0.31688362070415665 \t 1.1029187088185965\n",
      "3      \t [0. 1. 1.]. \t  0.330219860606422 \t 1.1029187088185965\n",
      "4      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 1.1029187088185965\n",
      "5      \t [0. 0. 1.]. \t  0.0902894676548261 \t 1.1029187088185965\n",
      "6      \t [ 0.00000000e+00  0.00000000e+00 -1.11022302e-16]. \t  0.06797411659013218 \t 1.1029187088185965\n",
      "7      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 1.1029187088185965\n",
      "8      \t [0.         1.         0.51363158]. \t  \u001b[92m2.3550976375067165\u001b[0m \t 2.3550976375067165\n",
      "9      \t [1.         0.         0.61905605]. \t  0.11255189209914762 \t 2.3550976375067165\n",
      "10     \t [1.         1.         0.51265471]. \t  0.23544349039004736 \t 2.3550976375067165\n",
      "11     \t [0.         0.51231091 1.        ]. \t  1.9969709209265998 \t 2.3550976375067165\n",
      "12     \t [1.         0.49102516 1.        ]. \t  1.897642224836895 \t 2.3550976375067165\n",
      "13     \t [0.51685346 0.74341239 1.        ]. \t  1.5382018204980386 \t 2.3550976375067165\n",
      "14     \t [1.         0.51898128 0.        ]. \t  0.007061262872822398 \t 2.3550976375067165\n",
      "15     \t [0.50676835 0.         0.        ]. \t  0.09657930273102729 \t 2.3550976375067165\n",
      "16     \t [0.         0.         0.48246027]. \t  0.16100021451814187 \t 2.3550976375067165\n",
      "17     \t [0.         0.70461929 0.69662881]. \t  \u001b[92m2.5113132264977014\u001b[0m \t 2.5113132264977014\n",
      "18     \t [0.37854886 1.         0.72740536]. \t  1.1147208950855985 \t 2.5113132264977014\n",
      "19     \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.5113132264977014\n",
      "20     \t [1.         0.3708103  0.35209145]. \t  0.13946601902112116 \t 2.5113132264977014\n",
      "21     \t [1.         0.69697378 0.75733865]. \t  2.1830680762835586 \t 2.5113132264977014\n",
      "22     \t [0.54323929 1.         0.        ]. \t  0.00018617759052461135 \t 2.5113132264977014\n",
      "23     \t [0.         0.30636008 0.76595492]. \t  1.9576622078519086 \t 2.5113132264977014\n",
      "24     \t [0.32916091 0.50782797 0.        ]. \t  0.025399039003284368 \t 2.5113132264977014\n",
      "25     \t [0.74602157 0.74887134 0.74011496]. \t  1.8673459897609184 \t 2.5113132264977014\n",
      "26     \t [0.         0.61481639 0.        ]. \t  0.006684606973481135 \t 2.5113132264977014\n",
      "27     \t [0.28559581 0.38829614 1.        ]. \t  1.5234192216436253 \t 2.5113132264977014\n",
      "28     \t [0.74809907 0.         0.28751979]. \t  0.5597595308210297 \t 2.5113132264977014\n",
      "29     \t [0.73906811 0.2641925  0.        ]. \t  0.06258852735230562 \t 2.5113132264977014\n",
      "30     \t [0.63831255 1.         1.        ]. \t  0.3292936296149726 \t 2.5113132264977014\n",
      "31     \t [0.85711472 0.79513513 0.01945899]. \t  0.000877316693102783 \t 2.5113132264977014\n",
      "32     \t [ 2.14250321e-01 -3.46944695e-18  8.88577986e-01]. \t  0.2040938205369217 \t 2.5113132264977014\n",
      "33     \t [1.11596368e-12 7.90421785e-01 3.45291624e-01]. \t  0.7325196904373102 \t 2.5113132264977014\n",
      "34     \t [0.19200206 0.00029924 0.15796781]. \t  0.5550759407053834 \t 2.5113132264977014\n",
      "35     \t [0.         1.         0.81576739]. \t  0.8016649479227476 \t 2.5113132264977014\n",
      "36     \t [0.15340953 0.2111669  0.        ]. \t  0.09334788576511814 \t 2.5113132264977014\n",
      "37     \t [0.18071522 0.53192553 0.62038056]. \t  1.7172437425641378 \t 2.5113132264977014\n",
      "38     \t [0.73800927 1.         0.37321669]. \t  0.29093827809470707 \t 2.5113132264977014\n",
      "39     \t [0.65948878 0.52064024 0.53654977]. \t  0.6464153253711715 \t 2.5113132264977014\n",
      "40     \t [0.83974324 0.72098205 1.        ]. \t  1.6203925143319486 \t 2.5113132264977014\n",
      "41     \t [0.16316278 1.         0.18065613]. \t  0.038364482480153686 \t 2.5113132264977014\n",
      "42     \t [0.92413653 1.         0.77080038]. \t  0.4621989210234579 \t 2.5113132264977014\n",
      "43     \t [0.23513867 0.8522281  0.99910031]. \t  0.9496279907124068 \t 2.5113132264977014\n",
      "44     \t [0.31140079 0.         1.        ]. \t  0.0916676284419243 \t 2.5113132264977014\n",
      "45     \t [0.9999997  0.22449613 0.99999991]. \t  0.6386420832466366 \t 2.5113132264977014\n",
      "46     \t [5.91106250e-01 2.41164489e-07 6.46491679e-01]. \t  0.15009490056471136 \t 2.5113132264977014\n",
      "47     \t [0.73499766 0.50584108 0.88042118]. \t  \u001b[92m3.635725362138416\u001b[0m \t 3.635725362138416\n",
      "48     \t [0.57838489 0.53560898 0.8832019 ]. \t  \u001b[92m3.7296812449041723\u001b[0m \t 3.7296812449041723\n",
      "49     \t [0.52876028 0.44757559 0.81068365]. \t  3.347294570475195 \t 3.7296812449041723\n",
      "50     \t [0.98474127 0.45944826 0.82474243]. \t  3.3510776558308804 \t 3.7296812449041723\n",
      "51     \t [0.5783509  0.49008829 0.81913902]. \t  3.5934784878492065 \t 3.7296812449041723\n",
      "52     \t [0.56951914 0.59930047 0.84925904]. \t  \u001b[92m3.7443575075518916\u001b[0m \t 3.7443575075518916\n",
      "53     \t [0.55999591 0.54622806 0.87094178]. \t  \u001b[92m3.8009464876602177\u001b[0m \t 3.8009464876602177\n",
      "54     \t [0.52144474 0.57629381 0.84423165]. \t  \u001b[92m3.8049784966861235\u001b[0m \t 3.8049784966861235\n",
      "55     \t [0.51884015 0.57083196 0.85833941]. \t  \u001b[92m3.825950160296678\u001b[0m \t 3.825950160296678\n",
      "56     \t [0.74583415 0.42774471 0.87925326]. \t  3.1963129277931643 \t 3.825950160296678\n",
      "57     \t [0.61069817 0.52212128 0.85684261]. \t  3.7843747403818386 \t 3.825950160296678\n",
      "58     \t [0.2284633  0.58820697 0.82333689]. \t  3.7399422942463936 \t 3.825950160296678\n",
      "59     \t [0.37585552 0.62273459 0.83621751]. \t  3.6639764003133237 \t 3.825950160296678\n",
      "60     \t [0.37041659 0.52731057 0.84465673]. \t  \u001b[92m3.827766112459771\u001b[0m \t 3.827766112459771\n",
      "61     \t [0.64710283 0.50404065 0.8305453 ]. \t  3.6794068185847455 \t 3.827766112459771\n",
      "62     \t [0.41558576 0.64010523 0.88541009]. \t  3.52166767484844 \t 3.827766112459771\n",
      "63     \t [0.45929237 0.58821551 0.83125875]. \t  3.7498614078639596 \t 3.827766112459771\n",
      "64     \t [0.78975248 0.55101093 0.83951351]. \t  3.730501160518135 \t 3.827766112459771\n",
      "65     \t [0.77182905 0.51562908 0.8418304 ]. \t  3.708130795534456 \t 3.827766112459771\n",
      "66     \t [0.26456337 0.41164979 0.83452644]. \t  3.199631139975319 \t 3.827766112459771\n",
      "67     \t [0.51121586 0.55263193 0.83892002]. \t  3.814949878381547 \t 3.827766112459771\n",
      "68     \t [0.7218618  0.54150807 0.8753091 ]. \t  3.7408297678985267 \t 3.827766112459771\n",
      "69     \t [0.48044068 0.53199783 0.85186832]. \t  \u001b[92m3.829046692233739\u001b[0m \t 3.829046692233739\n",
      "70     \t [0.35171343 0.48156892 0.87819285]. \t  3.6015197319732932 \t 3.829046692233739\n",
      "71     \t [0.36689723 0.46891745 0.80815362]. \t  3.4681345143155995 \t 3.829046692233739\n",
      "72     \t [0.55541481 0.52986889 0.87241764]. \t  3.777029789994937 \t 3.829046692233739\n",
      "73     \t [0.63358687 0.54431001 0.86028453]. \t  3.805957916572374 \t 3.829046692233739\n",
      "74     \t [0.6657422  0.53602642 0.87915674]. \t  3.7327606277135947 \t 3.829046692233739\n",
      "75     \t [0.51330789 0.54342151 0.83184174]. \t  3.7867593717298837 \t 3.829046692233739\n",
      "76     \t [0.91636757 0.48853314 0.83997223]. \t  3.565745858032302 \t 3.829046692233739\n",
      "77     \t [0.73619138 0.61631097 0.85374441]. \t  3.630111727402146 \t 3.829046692233739\n",
      "78     \t [0.37233617 0.60712217 0.8870054 ]. \t  3.662042686639273 \t 3.829046692233739\n",
      "79     \t [0.85263979 0.60109294 0.90910661]. \t  3.4012690951725326 \t 3.829046692233739\n",
      "80     \t [0.52470719 0.63607808 0.85480255]. \t  3.5999612110880417 \t 3.829046692233739\n",
      "81     \t [0.5128375  0.44325957 0.86197432]. \t  3.4221735433196785 \t 3.829046692233739\n",
      "82     \t [0.15421961 0.58138463 0.86315443]. \t  3.8170253126461455 \t 3.829046692233739\n",
      "83     \t [0.37579014 0.56459625 0.89313639]. \t  3.6997044489529882 \t 3.829046692233739\n",
      "84     \t [0.66850444 0.5572298  0.84118816]. \t  3.776586574530085 \t 3.829046692233739\n",
      "85     \t [0.03127939 0.60986704 0.8462831 ]. \t  3.7158909199603967 \t 3.829046692233739\n",
      "86     \t [0.03326941 0.44343024 0.84275344]. \t  3.4110461466072315 \t 3.829046692233739\n",
      "87     \t [0.26918102 0.53514104 0.85578544]. \t  \u001b[92m3.8460883962190495\u001b[0m \t 3.8460883962190495\n",
      "88     \t [0.69229404 0.54461999 0.80891656]. \t  3.5812292697255392 \t 3.8460883962190495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.52443718 0.54372326 0.79887095]. \t  3.5512242427520704 \t 3.8460883962190495\n",
      "90     \t [0.73039578 0.53077162 0.83070635]. \t  3.707336894373626 \t 3.8460883962190495\n",
      "91     \t [0.43274298 0.57436689 0.86959859]. \t  3.815312315540385 \t 3.8460883962190495\n",
      "92     \t [0.24782861 0.472361   0.83995492]. \t  3.6218761795245475 \t 3.8460883962190495\n",
      "93     \t [0.52212218 0.55121483 0.84680292]. \t  3.8310199425327123 \t 3.8460883962190495\n",
      "94     \t [0.09016755 0.56322698 0.8978165 ]. \t  3.6346934552031027 \t 3.8460883962190495\n",
      "95     \t [0.2326055  0.52617439 0.8524771 ]. \t  3.8300230674427915 \t 3.8460883962190495\n",
      "96     \t [0.51255852 0.53731988 0.8583987 ]. \t  3.8286105240815282 \t 3.8460883962190495\n",
      "97     \t [0.91728237 0.54051819 0.84522063]. \t  3.693393339505853 \t 3.8460883962190495\n",
      "98     \t [0.11961095 0.6077818  0.86856485]. \t  3.728720244616088 \t 3.8460883962190495\n",
      "99     \t [0.19321478 0.63427392 0.85924375]. \t  3.6423060541937153 \t 3.8460883962190495\n",
      "100    \t [0.38319117 0.55355744 0.85520417]. \t  \u001b[92m3.8574501572949016\u001b[0m \t 3.8574501572949016\n"
     ]
    }
   ],
   "source": [
    "### 6(j). Bayesian optimization runs (x20): STP DF1 run number = 10\n",
    "\n",
    "np.random.seed(run_num_10)\n",
    "surrogate_stp_df1_10 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_10 = GPGO(surrogate_stp_df1_10, Acquisition_new(util_stp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_10.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.3026705103183764, -5.234433552478444)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(j). Training Regret Minimisation: run number = 10\n",
    "\n",
    "gp_output_10 = np.append(np.max(gpgo_gp_10.GP.y[0:n_init]),gpgo_gp_10.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_10 = np.append(np.max(gpgo_stp_df1_10.GP.y[0:n_init]),gpgo_stp_df1_10.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_10 = np.log(y_global_orig - gp_output_10)\n",
    "regret_stp_df1_10 = np.log(y_global_orig - stp_df1_output_10)\n",
    "\n",
    "train_regret_gp_10 = min_max_array(regret_gp_10)\n",
    "train_regret_stp_df1_10 = min_max_array(regret_stp_df1_10)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 10\n",
    "min_train_regret_gp_10 = min(train_regret_gp_10)\n",
    "min_train_regret_stp_df1_10 = min(train_regret_stp_df1_10)\n",
    "\n",
    "\n",
    "min_train_regret_gp_10, min_train_regret_stp_df1_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.0955492  0.9250037  0.34357342]. \t  0.76782239395869 \t 2.199154292092583\n",
      "init   \t [0.31047694 0.00200984 0.23559472]. \t  0.8415362214733108 \t 2.199154292092583\n",
      "init   \t [0.23779172 0.73591587 0.49546808]. \t  2.199154292092583 \t 2.199154292092583\n",
      "init   \t [0.78442535 0.12650631 0.60664932]. \t  0.2758542674567045 \t 2.199154292092583\n",
      "init   \t [0.46612097 0.23713212 0.43515918]. \t  0.4121540443353543 \t 2.199154292092583\n",
      "1      \t [0.21420956 1.         1.        ]. \t  0.3340788109338956 \t 2.199154292092583\n",
      "2      \t [1.         0.95116041 0.83717089]. \t  0.7792669561606543 \t 2.199154292092583\n",
      "3      \t [0.         0.36432513 0.65931758]. \t  1.361991949226927 \t 2.199154292092583\n",
      "4      \t [0.02038466 0.33137075 0.04921354]. \t  0.10538913544412966 \t 2.199154292092583\n",
      "5      \t [0.78063522 0.963854   0.32927015]. \t  0.15611068808752132 \t 2.199154292092583\n",
      "6      \t [0.71355202 0.43199463 0.9872934 ]. \t  1.92226390904416 \t 2.199154292092583\n",
      "7      \t [0.90453105 0.05763944 0.97827915]. \t  0.19920681904339804 \t 2.199154292092583\n",
      "8      \t [0.96878983 0.06238086 0.17403002]. \t  0.2540104361602328 \t 2.199154292092583\n",
      "9      \t [0.93657777 0.42520104 0.86560482]. \t  \u001b[92m3.177862186402654\u001b[0m \t 3.177862186402654\n",
      "10     \t [0.87337737 0.66292    0.63618372]. \t  1.0824844774273568 \t 3.177862186402654\n",
      "11     \t [1.         0.49039275 1.        ]. \t  1.895843484713915 \t 3.177862186402654\n",
      "12     \t [0.36633405 0.89542601 0.64303959]. \t  2.191411188540912 \t 3.177862186402654\n",
      "13     \t [0.18963466 0.58601995 0.72977886]. \t  2.865739831026356 \t 3.177862186402654\n",
      "14     \t [0.58023907 0.94341282 0.85916376]. \t  0.9516153209357431 \t 3.177862186402654\n",
      "15     \t [0.64852001 0.85796268 0.82251866]. \t  1.5655796047937576 \t 3.177862186402654\n",
      "16     \t [0.42005329 0.9763038  0.14676668]. \t  0.014102924364097909 \t 3.177862186402654\n",
      "17     \t [0.00296894 0.72949691 0.9931047 ]. \t  1.6874506171869097 \t 3.177862186402654\n",
      "18     \t [0.11746799 0.65223565 0.77632065]. \t  3.1617376585815697 \t 3.177862186402654\n",
      "19     \t [0.         0.86881463 0.74046383]. \t  1.8640559007892272 \t 3.177862186402654\n",
      "20     \t [0.06269807 0.32405019 0.476599  ]. \t  0.33910061303051264 \t 3.177862186402654\n",
      "21     \t [0.59984265 0.         0.        ]. \t  0.08712848473858585 \t 3.177862186402654\n",
      "22     \t [0.96872203 0.98955148 0.10514351]. \t  0.0007406554327865173 \t 3.177862186402654\n",
      "23     \t [0.29930055 0.04697171 0.72911677]. \t  0.3445180113856281 \t 3.177862186402654\n",
      "24     \t [0.78960542 0.21515796 0.64719468]. \t  0.6473271087043581 \t 3.177862186402654\n",
      "25     \t [0.02595794 0.16247062 0.93902326]. \t  0.6700081373199703 \t 3.177862186402654\n",
      "26     \t [0.33795751 0.63878433 0.00851376]. \t  0.008978302485229421 \t 3.177862186402654\n",
      "27     \t [0.35377631 0.73344686 0.88461544]. \t  2.821357655730226 \t 3.177862186402654\n",
      "28     \t [0.35563591 0.25120222 0.68204835]. \t  1.039238871175968 \t 3.177862186402654\n",
      "29     \t [0.79154225 0.90691813 0.04181907]. \t  0.0005225975748786154 \t 3.177862186402654\n",
      "30     \t [0.22588043 0.00321789 0.35728117]. \t  0.6494392920681293 \t 3.177862186402654\n",
      "31     \t [0.83514753 0.01375708 0.3614735 ]. \t  0.3600704439975708 \t 3.177862186402654\n",
      "32     \t [0.16182596 0.70823681 0.52117122]. \t  2.318823232965646 \t 3.177862186402654\n",
      "33     \t [0.19727708 0.18385572 0.40362266]. \t  0.5243106189546548 \t 3.177862186402654\n",
      "34     \t [0.23102047 0.52125759 0.71113753]. \t  2.5717021019903035 \t 3.177862186402654\n",
      "35     \t [0.85055532 0.15654935 0.95280154]. \t  0.5816332136243205 \t 3.177862186402654\n",
      "36     \t [0.4370283  0.93455866 0.19545234]. \t  0.04462778780213082 \t 3.177862186402654\n",
      "37     \t [0.29063338 0.04584318 0.51477664]. \t  0.19179792990823755 \t 3.177862186402654\n",
      "38     \t [0.22752532 0.95114665 0.1562185 ]. \t  0.023720813711228916 \t 3.177862186402654\n",
      "39     \t [0.95431842 0.61137866 0.74010953]. \t  2.429418548965309 \t 3.177862186402654\n",
      "40     \t [0.62156064 0.05738168 0.45660452]. \t  0.28826279991262443 \t 3.177862186402654\n",
      "41     \t [0.72660986 0.81476232 0.49488332]. \t  0.8711697530902173 \t 3.177862186402654\n",
      "42     \t [0.20295154 0.12918772 0.59348883]. \t  0.27266167999305185 \t 3.177862186402654\n",
      "43     \t [0.73911678 0.03479409 0.80143563]. \t  0.34679270350559155 \t 3.177862186402654\n",
      "44     \t [0.72703116 0.94785048 0.45396074]. \t  0.6715014481621212 \t 3.177862186402654\n",
      "45     \t [0.85112008 0.37668729 0.83229424]. \t  2.824333326554184 \t 3.177862186402654\n",
      "46     \t [0.58196501 0.9273148  0.93077325]. \t  0.9008255789410076 \t 3.177862186402654\n",
      "47     \t [0.42475335 0.67607154 0.36513967]. \t  0.5503378902824597 \t 3.177862186402654\n",
      "48     \t [0.47008122 0.26258751 0.01541502]. \t  0.11695293158123195 \t 3.177862186402654\n",
      "49     \t [0.2355796  0.98048267 0.01773236]. \t  0.0005308835611316685 \t 3.177862186402654\n",
      "50     \t [0.92105989 0.88603358 0.74459159]. \t  0.9021081641433712 \t 3.177862186402654\n",
      "51     \t [0.76580641 0.50333    0.25043315]. \t  0.15218455783644175 \t 3.177862186402654\n",
      "52     \t [0.81761071 0.26793291 0.66008852]. \t  0.9128345567707787 \t 3.177862186402654\n",
      "53     \t [0.28366084 0.6875439  0.5167451 ]. \t  1.989961230873268 \t 3.177862186402654\n",
      "54     \t [0.75024444 0.25991389 0.14264368]. \t  0.3307785931820114 \t 3.177862186402654\n",
      "55     \t [0.17519894 0.21097185 0.7247702 ]. \t  1.068270011171695 \t 3.177862186402654\n",
      "56     \t [0.86485257 0.3238744  0.29565336]. \t  0.3085946950553315 \t 3.177862186402654\n",
      "57     \t [0.17084046 0.38104656 0.45863997]. \t  0.41382407286135636 \t 3.177862186402654\n",
      "58     \t [0.84721314 0.249123   0.00931861]. \t  0.057410997034564294 \t 3.177862186402654\n",
      "59     \t [0.84503297 0.63050783 0.39962041]. \t  0.19364644372093678 \t 3.177862186402654\n",
      "60     \t [0.01259054 0.87483154 0.7503009 ]. \t  1.7931516475480662 \t 3.177862186402654\n",
      "61     \t [0.98268011 0.18682721 0.04856508]. \t  0.07322137506862719 \t 3.177862186402654\n",
      "62     \t [0.912824   0.84821618 0.28128472]. \t  0.047648009608260045 \t 3.177862186402654\n",
      "63     \t [0.69841278 0.29833617 0.67907874]. \t  1.207837738858239 \t 3.177862186402654\n",
      "64     \t [0.38845746 0.11550506 0.64900969]. \t  0.3775339238900306 \t 3.177862186402654\n",
      "65     \t [0.88424174 0.92234786 0.9811403 ]. \t  0.6810630920082703 \t 3.177862186402654\n",
      "66     \t [0.65158756 0.04821609 0.85083925]. \t  0.3711478602742937 \t 3.177862186402654\n",
      "67     \t [0.95236893 0.33817348 0.18229429]. \t  0.17810183687006398 \t 3.177862186402654\n",
      "68     \t [0.75944277 0.54898823 0.37177291]. \t  0.1865803186985103 \t 3.177862186402654\n",
      "69     \t [0.02054028 0.01830503 0.95184837]. \t  0.16773755439332905 \t 3.177862186402654\n",
      "70     \t [0.4307093  0.84190738 0.33819025]. \t  0.5389485270005255 \t 3.177862186402654\n",
      "71     \t [0.71507546 0.61920962 0.94334284]. \t  2.9569282327242914 \t 3.177862186402654\n",
      "72     \t [0.63029378 0.48950542 0.90042181]. \t  \u001b[92m3.4477560381215833\u001b[0m \t 3.4477560381215833\n",
      "73     \t [0.65568702 0.64854347 0.66941713]. \t  1.7327253845207444 \t 3.4477560381215833\n",
      "74     \t [0.75813165 0.37289592 0.21528999]. \t  0.3063217923795169 \t 3.4477560381215833\n",
      "75     \t [0.45118829 0.26045726 0.55015306]. \t  0.3716051419891449 \t 3.4477560381215833\n",
      "76     \t [0.89347313 0.47368587 0.38494119]. \t  0.13380017317490786 \t 3.4477560381215833\n",
      "77     \t [0.73867599 0.43564363 0.50899222]. \t  0.3517740318203988 \t 3.4477560381215833\n",
      "78     \t [0.19491974 0.21513659 0.80672229]. \t  1.3604466514215363 \t 3.4477560381215833\n",
      "79     \t [0.88248213 0.84589691 0.83878189]. \t  1.6058275049665822 \t 3.4477560381215833\n",
      "80     \t [0.89035616 0.06046757 0.22362718]. \t  0.4046146058869937 \t 3.4477560381215833\n",
      "81     \t [0.08510476 0.31574277 0.3582156 ]. \t  0.4601694665793563 \t 3.4477560381215833\n",
      "82     \t [0.7746978  0.92341129 0.53879856]. \t  0.8083380378866887 \t 3.4477560381215833\n",
      "83     \t [0.72479971 0.54909333 0.87198395]. \t  \u001b[92m3.756880043170505\u001b[0m \t 3.756880043170505\n",
      "84     \t [0.92262509 0.72674168 0.97933863]. \t  1.8565669526521011 \t 3.756880043170505\n",
      "85     \t [0.03978229 0.854224   0.08425695]. \t  0.005006387255803139 \t 3.756880043170505\n",
      "86     \t [0.72071996 0.43452163 0.03138659]. \t  0.04743606249791333 \t 3.756880043170505\n",
      "87     \t [0.89967744 0.26707396 0.65250864]. \t  0.8441484231934844 \t 3.756880043170505\n",
      "88     \t [0.63529076 0.48045719 0.53682359]. \t  0.5960132984206354 \t 3.756880043170505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.68882206 0.7702599  0.88581002]. \t  2.3977579828951385 \t 3.756880043170505\n",
      "90     \t [0.5025118  0.22533682 0.79547646]. \t  1.4301561573808206 \t 3.756880043170505\n",
      "91     \t [0.37001215 0.65552141 0.19579096]. \t  0.07915255467123715 \t 3.756880043170505\n",
      "92     \t [0.34388649 0.92701192 0.80627747]. \t  1.21880651619953 \t 3.756880043170505\n",
      "93     \t [0.23355765 0.89789345 0.35876512]. \t  0.9021939531684978 \t 3.756880043170505\n",
      "94     \t [0.17449452 0.76681008 0.32977766]. \t  0.5923884725126657 \t 3.756880043170505\n",
      "95     \t [0.81584925 0.01172397 0.22746258]. \t  0.46875962026201284 \t 3.756880043170505\n",
      "96     \t [0.45860867 0.15171297 0.06692854]. \t  0.28920399724733603 \t 3.756880043170505\n",
      "97     \t [0.56942518 0.2593739  0.04837765]. \t  0.17186494007087336 \t 3.756880043170505\n",
      "98     \t [0.52316639 0.32052755 0.51823481]. \t  0.3573929363369281 \t 3.756880043170505\n",
      "99     \t [0.04340463 0.37481091 0.27242064]. \t  0.3971334511895749 \t 3.756880043170505\n",
      "100    \t [0.82693031 0.75261549 0.12798213]. \t  0.007584563228685214 \t 3.756880043170505\n"
     ]
    }
   ],
   "source": [
    "### 6(k). Bayesian optimization runs (x20): GP run number = 11\n",
    "\n",
    "np.random.seed(run_num_11)\n",
    "surrogate_gp_11 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "gpgo_gp_11 = GPGO(surrogate_gp_11, Acquisition_new(util_gp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_11.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.0955492  0.9250037  0.34357342]. \t  0.76782239395869 \t 2.199154292092583\n",
      "init   \t [0.31047694 0.00200984 0.23559472]. \t  0.8415362214733108 \t 2.199154292092583\n",
      "init   \t [0.23779172 0.73591587 0.49546808]. \t  2.199154292092583 \t 2.199154292092583\n",
      "init   \t [0.78442535 0.12650631 0.60664932]. \t  0.2758542674567045 \t 2.199154292092583\n",
      "init   \t [0.46612097 0.23713212 0.43515918]. \t  0.4121540443353543 \t 2.199154292092583\n",
      "1      \t [0.14110846 1.         1.        ]. \t  0.3332960290258554 \t 2.199154292092583\n",
      "2      \t [1.         1.         0.90863947]. \t  0.5168137192696699 \t 2.199154292092583\n",
      "3      \t [0. 0. 1.]. \t  0.0902894676548261 \t 2.199154292092583\n",
      "4      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.199154292092583\n",
      "5      \t [ 0.00000000e+00  0.00000000e+00 -5.55111512e-17]. \t  0.06797411659013224 \t 2.199154292092583\n",
      "6      \t [1. 0. 0.]. \t  0.03095471703300515 \t 2.199154292092583\n",
      "7      \t [0.57519145 1.         0.55217221]. \t  1.3559541011409735 \t 2.199154292092583\n",
      "8      \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.199154292092583\n",
      "9      \t [0.         0.55186628 0.67203411]. \t  2.1939447418777083 \t 2.199154292092583\n",
      "10     \t [0.49244507 0.5867492  1.        ]. \t  2.0778824396016122 \t 2.199154292092583\n",
      "11     \t [1.         0.54637246 1.        ]. \t  2.0002680394110297 \t 2.199154292092583\n",
      "12     \t [0.         0.         0.49838739]. \t  0.13844804559762117 \t 2.199154292092583\n",
      "13     \t [0.44874056 1.         0.        ]. \t  0.00022434079383753313 \t 2.199154292092583\n",
      "14     \t [1.         0.73518847 0.50614034]. \t  0.29495452324985494 \t 2.199154292092583\n",
      "15     \t [0.         0.58714086 0.        ]. \t  0.008673076181000135 \t 2.199154292092583\n",
      "16     \t [ 7.39580726e-01  5.62949190e-01 -2.77555756e-17]. \t  0.010660635841711754 \t 2.199154292092583\n",
      "17     \t [0.50586243 0.         1.        ]. \t  0.09163249361597738 \t 2.199154292092583\n",
      "18     \t [0.         0.64937077 1.        ]. \t  1.927961667508594 \t 2.199154292092583\n",
      "19     \t [0.59623242 0.         0.        ]. \t  0.08756201704239337 \t 2.199154292092583\n",
      "20     \t [0.74466642 0.85687145 1.        ]. \t  0.9030931168180119 \t 2.199154292092583\n",
      "21     \t [0.         1.         0.74673453]. \t  1.1349741686879369 \t 2.199154292092583\n",
      "22     \t [1.         0.         0.37987785]. \t  0.18210335691912652 \t 2.199154292092583\n",
      "23     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 2.199154292092583\n",
      "24     \t [0.24495376 0.71616697 0.80446544]. \t  \u001b[92m2.9408943218086736\u001b[0m \t 2.9408943218086736\n",
      "25     \t [1.         0.40247833 0.        ]. \t  0.015715146734166564 \t 2.9408943218086736\n",
      "26     \t [0.1726613  0.32985618 0.85703786]. \t  2.414239524098484 \t 2.9408943218086736\n",
      "27     \t [0.27604055 0.31618502 0.        ]. \t  0.07684722374043992 \t 2.9408943218086736\n",
      "28     \t [1.         0.32902003 0.77617377]. \t  2.1409994937424477 \t 2.9408943218086736\n",
      "29     \t [0.2908706  0.         0.71612147]. \t  0.2161549811202537 \t 2.9408943218086736\n",
      "30     \t [0.89387396 0.63142503 0.8014923 ]. \t  \u001b[92m3.1687830535565658\u001b[0m \t 3.1687830535565658\n",
      "31     \t [0.         0.20766495 0.25361255]. \t  0.6113232549669793 \t 3.1687830535565658\n",
      "32     \t [0.63007993 0.59220673 0.7865972 ]. \t  \u001b[92m3.3082824494980803\u001b[0m \t 3.3082824494980803\n",
      "33     \t [0.         0.36344442 1.        ]. \t  1.3687746178829665 \t 3.3082824494980803\n",
      "34     \t [0.83816202 0.46152031 0.92196251]. \t  3.020641040204052 \t 3.3082824494980803\n",
      "35     \t [0.81179234 1.         0.19434789]. \t  0.01200215643775369 \t 3.3082824494980803\n",
      "36     \t [0.87269178 0.39394442 0.35286001]. \t  0.19473473178666534 \t 3.3082824494980803\n",
      "37     \t [1.         0.27003312 1.        ]. \t  0.8479433711245755 \t 3.3082824494980803\n",
      "38     \t [1.00000000e+00 7.35055916e-01 6.20431659e-08]. \t  0.0008008540593511118 \t 3.3082824494980803\n",
      "39     \t [0.32931792 0.68397159 0.20404194]. \t  0.08081338811000273 \t 3.3082824494980803\n",
      "40     \t [0.25393006 0.4976172  0.657504  ]. \t  1.8843135847388073 \t 3.3082824494980803\n",
      "41     \t [0.27264718 0.         0.        ]. \t  0.09944394345042594 \t 3.3082824494980803\n",
      "42     \t [6.0599133e-08 1.0000000e+00 1.0000000e+00]. \t  0.3302198623624336 \t 3.3082824494980803\n",
      "43     \t [9.99999999e-01 2.22728858e-17 7.72173956e-01]. \t  0.24084314951641628 \t 3.3082824494980803\n",
      "44     \t [1.         1.         0.55599608]. \t  0.2620565206397391 \t 3.3082824494980803\n",
      "45     \t [0.         0.78911582 0.52079428]. \t  2.6843915051478624 \t 3.3082824494980803\n",
      "46     \t [0.21200656 0.08180249 1.        ]. \t  0.20997902845839855 \t 3.3082824494980803\n",
      "47     \t [0.78019307 0.99147551 0.72186684]. \t  0.570045451049892 \t 3.3082824494980803\n",
      "48     \t [0.99999915 0.74566969 0.84433068]. \t  2.5255089735278298 \t 3.3082824494980803\n",
      "49     \t [0.00574842 0.78190089 0.72881275]. \t  2.328231675668232 \t 3.3082824494980803\n",
      "50     \t [0.66590559 0.61893018 0.81327488]. \t  \u001b[92m3.451428120035538\u001b[0m \t 3.451428120035538\n",
      "51     \t [0.56488682 0.70560446 0.80878849]. \t  2.9054657877181067 \t 3.451428120035538\n",
      "52     \t [0.59765389 0.29002303 0.89737995]. \t  1.8456924035869051 \t 3.451428120035538\n",
      "53     \t [0.         1.         0.40504026]. \t  1.2599955279039787 \t 3.451428120035538\n",
      "54     \t [0.81229895 0.55700788 0.8002697 ]. \t  3.44205127365215 \t 3.451428120035538\n",
      "55     \t [0.91642298 0.15301584 0.08431878]. \t  0.14708309281412274 \t 3.451428120035538\n",
      "56     \t [0.01049718 0.11088657 0.68420569]. \t  0.4511076289721135 \t 3.451428120035538\n",
      "57     \t [0.80791938 0.5883603  0.80529864]. \t  3.4301763547751047 \t 3.451428120035538\n",
      "58     \t [0.70287827 0.67573201 0.81488139]. \t  3.1030640029016174 \t 3.451428120035538\n",
      "59     \t [0.64710693 0.57127431 0.79206773]. \t  3.4097006030704486 \t 3.451428120035538\n",
      "60     \t [0.73365675 0.55616228 0.82218342]. \t  \u001b[92m3.6615290238475566\u001b[0m \t 3.6615290238475566\n",
      "61     \t [0.46992217 0.59267738 0.78117572]. \t  3.3272177284131557 \t 3.6615290238475566\n",
      "62     \t [0.96517464 0.54909233 0.84517364]. \t  \u001b[92m3.6735321786348534\u001b[0m \t 3.6735321786348534\n",
      "63     \t [0.84460445 0.62167034 0.78704258]. \t  3.0844209877992674 \t 3.6735321786348534\n",
      "64     \t [0.69668832 0.48882651 0.82936968]. \t  3.608436130761292 \t 3.6735321786348534\n",
      "65     \t [0.79047456 0.56890046 0.88849423]. \t  3.6502225707605964 \t 3.6735321786348534\n",
      "66     \t [0.37707326 0.62521285 0.81279234]. \t  3.5315363190065963 \t 3.6735321786348534\n",
      "67     \t [0.70740644 0.47978357 0.8535107 ]. \t  3.613634560351813 \t 3.6735321786348534\n",
      "68     \t [0.55250667 0.55036951 0.85648008]. \t  \u001b[92m3.830457109248349\u001b[0m \t 3.830457109248349\n",
      "69     \t [0.46127638 0.69555757 0.86837327]. \t  3.1898833507558955 \t 3.830457109248349\n",
      "70     \t [0.64625495 0.48950646 0.87358085]. \t  3.627426942138648 \t 3.830457109248349\n",
      "71     \t [0.62951872 0.56117666 0.85783769]. \t  3.8084478262956702 \t 3.830457109248349\n",
      "72     \t [0.44290973 0.50420438 0.82606293]. \t  3.704979522301929 \t 3.830457109248349\n",
      "73     \t [0.62238313 0.59839897 0.76818315]. \t  3.073414712760359 \t 3.830457109248349\n",
      "74     \t [0.94968262 0.56451685 0.81801473]. \t  3.5287305558302515 \t 3.830457109248349\n",
      "75     \t [0.60827596 0.52614445 0.86032053]. \t  3.7897175332880693 \t 3.830457109248349\n",
      "76     \t [0.47554473 0.63362069 0.79885752]. \t  3.347835338547633 \t 3.830457109248349\n",
      "77     \t [0.41124234 0.60048146 0.92981581]. \t  3.243820692230668 \t 3.830457109248349\n",
      "78     \t [0.68397171 0.54575363 0.94805412]. \t  2.9720502862502167 \t 3.830457109248349\n",
      "79     \t [0.640549   0.53884897 0.866104  ]. \t  3.7900294299463697 \t 3.830457109248349\n",
      "80     \t [0.88787771 0.47560742 0.83014969]. \t  3.4912871785668167 \t 3.830457109248349\n",
      "81     \t [0.55057763 0.47398475 0.79594953]. \t  3.3642782101858577 \t 3.830457109248349\n",
      "82     \t [0.63360784 0.41769596 0.81572711]. \t  3.156994519492657 \t 3.830457109248349\n",
      "83     \t [0.72479971 0.54909333 0.87198395]. \t  3.756880043170505 \t 3.830457109248349\n",
      "84     \t [0.52161679 0.51021134 0.84441009]. \t  3.767699334079551 \t 3.830457109248349\n",
      "85     \t [0.60779733 0.50374111 0.86383126]. \t  3.72377203744269 \t 3.830457109248349\n",
      "86     \t [0.46917104 0.58069126 0.76733529]. \t  3.1938549161339522 \t 3.830457109248349\n",
      "87     \t [0.8558984  0.56172266 0.84285888]. \t  3.7084648422356636 \t 3.830457109248349\n",
      "88     \t [0.65292978 0.50236549 0.77773794]. \t  3.2310785379415456 \t 3.830457109248349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.86056941 0.53142679 0.87296757]. \t  3.6904490449650265 \t 3.830457109248349\n",
      "90     \t [0.87709089 0.52085851 0.81018458]. \t  3.5031629603177743 \t 3.830457109248349\n",
      "91     \t [0.50309266 0.53833176 0.82631712]. \t  3.7597763692066692 \t 3.830457109248349\n",
      "92     \t [0.52649859 0.47910045 0.89434011]. \t  3.4668561486985436 \t 3.830457109248349\n",
      "93     \t [0.684574   0.56143016 0.84688806]. \t  3.782749465277065 \t 3.830457109248349\n",
      "94     \t [0.30707249 0.51252029 0.81493735]. \t  3.6797309677221977 \t 3.830457109248349\n",
      "95     \t [0.90993363 0.53151436 0.86272037]. \t  3.6961581735657436 \t 3.830457109248349\n",
      "96     \t [0.23049018 0.5038741  0.91528457]. \t  3.367344531028351 \t 3.830457109248349\n",
      "97     \t [0.57501594 0.52563151 0.81265522]. \t  3.6401209224438107 \t 3.830457109248349\n",
      "98     \t [0.74802762 0.53751489 0.80694556]. \t  3.541320455171342 \t 3.830457109248349\n",
      "99     \t [0.42757901 0.55610716 0.85050888]. \t  \u001b[92m3.8515344015470903\u001b[0m \t 3.8515344015470903\n",
      "100    \t [0.54488469 0.59673092 0.83326294]. \t  3.711164710981717 \t 3.8515344015470903\n"
     ]
    }
   ],
   "source": [
    "### 6(k). Bayesian optimization runs (x20): STP DF1 run number = 11\n",
    "\n",
    "np.random.seed(run_num_11)\n",
    "surrogate_stp_df1_11 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_11 = GPGO(surrogate_stp_df1_11, Acquisition_new(util_stp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_11.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.245260434028356, -4.487778475519678)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(k). Training Regret Minimisation: run number = 11\n",
    "\n",
    "gp_output_11 = np.append(np.max(gpgo_gp_11.GP.y[0:n_init]),gpgo_gp_11.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_11 = np.append(np.max(gpgo_stp_df1_11.GP.y[0:n_init]),gpgo_stp_df1_11.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_11 = np.log(y_global_orig - gp_output_11)\n",
    "regret_stp_df1_11 = np.log(y_global_orig - stp_df1_output_11)\n",
    "\n",
    "train_regret_gp_11 = min_max_array(regret_gp_11)\n",
    "train_regret_stp_df1_11 = min_max_array(regret_stp_df1_11)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 11\n",
    "min_train_regret_gp_11 = min(train_regret_gp_11)\n",
    "min_train_regret_stp_df1_11 = min(train_regret_stp_df1_11)\n",
    "\n",
    "min_train_regret_gp_11, min_train_regret_stp_df1_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.06748528 0.33494299 0.36839425]. \t  0.41381266679589174 \t 0.41381266679589174\n",
      "init   \t [0.48359485 0.46381274 0.03788014]. \t  0.05965297274446339 \t 0.41381266679589174\n",
      "init   \t [0.06564924 0.0867977  0.95570991]. \t  0.318830723512337 \t 0.41381266679589174\n",
      "init   \t [0.17577016 0.12860266 0.5671431 ]. \t  0.23165598509668625 \t 0.41381266679589174\n",
      "init   \t [0.97377021 0.9670107  0.4822915 ]. \t  0.25781214991700163 \t 0.41381266679589174\n",
      "1      \t [0.02500309 0.87094552 0.88519674]. \t  \u001b[92m1.5142860113963001\u001b[0m \t 1.5142860113963001\n",
      "2      \t [0.96131921 0.43196609 0.96419772]. \t  \u001b[92m2.228610740669773\u001b[0m \t 2.228610740669773\n",
      "3      \t [0.79468845 0.78619135 0.97780345]. \t  1.5221526339967906 \t 2.228610740669773\n",
      "4      \t [0.78144079 0.03461647 0.96349373]. \t  0.18126023546033282 \t 2.228610740669773\n",
      "5      \t [0.88116543 0.02876599 0.28196664]. \t  0.4184583770374955 \t 2.228610740669773\n",
      "6      \t [0.05155272 0.90590276 0.12995584]. \t  0.013943959316780179 \t 2.228610740669773\n",
      "7      \t [0.02706919 0.0283094  0.08371955]. \t  0.23686955253049055 \t 2.228610740669773\n",
      "8      \t [0.30685113 0.58499665 0.9922842 ]. \t  2.2213241685129734 \t 2.228610740669773\n",
      "9      \t [0.90025585 0.51275852 0.57611224]. \t  0.6413162942343782 \t 2.228610740669773\n",
      "10     \t [0.89237508 0.77817864 0.05631744]. \t  0.0017129768105232106 \t 2.228610740669773\n",
      "11     \t [0.36225583 0.99169723 0.44174172]. \t  1.4698597553854076 \t 2.228610740669773\n",
      "12     \t [0.57918952 0.29810862 0.48056037]. \t  0.29635684436951876 \t 2.228610740669773\n",
      "13     \t [0.53258459 0.34573816 0.75884824]. \t  \u001b[92m2.2322269819502414\u001b[0m \t 2.2322269819502414\n",
      "14     \t [0.08793103 0.29065009 0.38619698]. \t  0.4351068821422328 \t 2.2322269819502414\n",
      "15     \t [0.47739408 0.49097877 0.23966288]. \t  0.25671406937111135 \t 2.2322269819502414\n",
      "16     \t [0.97260471 0.13625136 0.08929364]. \t  0.12903716646527563 \t 2.2322269819502414\n",
      "17     \t [0.58353518 0.13927937 0.46600708]. \t  0.30254451633062696 \t 2.2322269819502414\n",
      "18     \t [0.44366286 0.05548285 0.04255488]. \t  0.2080622007761323 \t 2.2322269819502414\n",
      "19     \t [0.38540357 0.95261124 0.43313412]. \t  1.4416545630953619 \t 2.2322269819502414\n",
      "20     \t [0.22274379 0.61623911 0.44196976]. \t  1.0865776390328954 \t 2.2322269819502414\n",
      "21     \t [0.43550905 0.93419891 0.74669829]. \t  1.2558725882709543 \t 2.2322269819502414\n",
      "22     \t [0.44490071 0.79008959 0.142274  ]. \t  0.0187625745304497 \t 2.2322269819502414\n",
      "23     \t [0.72388415 0.55409705 0.98475614]. \t  \u001b[92m2.3347589820394776\u001b[0m \t 2.3347589820394776\n",
      "24     \t [0.35196192 0.75744217 0.26159125]. \t  0.1836394807751446 \t 2.3347589820394776\n",
      "25     \t [0.44300827 0.62879424 0.90977853]. \t  \u001b[92m3.384686410643391\u001b[0m \t 3.384686410643391\n",
      "26     \t [0.96358387 0.0689226  0.04711773]. \t  0.07898755598172459 \t 3.384686410643391\n",
      "27     \t [0.99664247 0.74087166 0.86227165]. \t  2.6083518480771652 \t 3.384686410643391\n",
      "28     \t [0.04164057 0.83338385 0.4340767 ]. \t  1.893836013712837 \t 3.384686410643391\n",
      "29     \t [0.36562799 0.03977876 0.91942484]. \t  0.2616325917230127 \t 3.384686410643391\n",
      "30     \t [0.00964422 0.67620737 0.7347315 ]. \t  2.732736055723705 \t 3.384686410643391\n",
      "31     \t [0.71925186 0.18137615 0.58422106]. \t  0.3137294167116072 \t 3.384686410643391\n",
      "32     \t [0.74265581 0.74516237 0.57870149]. \t  1.0168460626795064 \t 3.384686410643391\n",
      "33     \t [0.87050292 0.09267717 0.62512797]. \t  0.25318708577704413 \t 3.384686410643391\n",
      "34     \t [0.12912083 0.20204047 0.83339007]. \t  1.2552360664009559 \t 3.384686410643391\n",
      "35     \t [0.3223547  0.75174385 0.14612714]. \t  0.02647704380227299 \t 3.384686410643391\n",
      "36     \t [0.43016928 0.29009029 0.4298421 ]. \t  0.40729290275031893 \t 3.384686410643391\n",
      "37     \t [0.20641503 0.56024233 0.36983274]. \t  0.4946678869068942 \t 3.384686410643391\n",
      "38     \t [0.37512223 0.56450844 0.97260264]. \t  2.588043770850369 \t 3.384686410643391\n",
      "39     \t [0.34826725 0.27740147 0.37811416]. \t  0.5707735932068756 \t 3.384686410643391\n",
      "40     \t [0.82062317 0.70224843 0.95076786]. \t  2.409742026769414 \t 3.384686410643391\n",
      "41     \t [0.37375    0.1253035  0.87347826]. \t  0.6818477087226322 \t 3.384686410643391\n",
      "42     \t [0.64275517 0.43588688 0.31955492]. \t  0.30406239350715436 \t 3.384686410643391\n",
      "43     \t [0.93408162 0.3432782  0.39916755]. \t  0.1638307208334303 \t 3.384686410643391\n",
      "44     \t [0.30219957 0.02152749 0.80036732]. \t  0.3092661742534253 \t 3.384686410643391\n",
      "45     \t [0.24197379 0.53104515 0.61757875]. \t  1.6652900056121802 \t 3.384686410643391\n",
      "46     \t [0.27531589 0.71233857 0.11457841]. \t  0.020360655597552246 \t 3.384686410643391\n",
      "47     \t [0.48671917 0.09856733 0.44696083]. \t  0.38299715963507536 \t 3.384686410643391\n",
      "48     \t [0.16101461 0.1572849  0.07177658]. \t  0.27452833685489825 \t 3.384686410643391\n",
      "49     \t [0.43928252 0.98770681 0.77728241]. \t  0.878454381682854 \t 3.384686410643391\n",
      "50     \t [0.79019954 0.6375542  0.60827938]. \t  1.0180810378828613 \t 3.384686410643391\n",
      "51     \t [0.99416785 0.04393068 0.03588901]. \t  0.058847808983380843 \t 3.384686410643391\n",
      "52     \t [0.13619407 0.32583617 0.18135213]. \t  0.442646317791203 \t 3.384686410643391\n",
      "53     \t [0.99687878 0.2799443  0.88620526]. \t  1.755492019477759 \t 3.384686410643391\n",
      "54     \t [0.92982159 0.78629716 0.89744502]. \t  2.1359913592804136 \t 3.384686410643391\n",
      "55     \t [0.39708813 0.43464137 0.3587997 ]. \t  0.39734424587274203 \t 3.384686410643391\n",
      "56     \t [0.31473077 0.70641177 0.2084663 ]. \t  0.08258428543342294 \t 3.384686410643391\n",
      "57     \t [0.51924    0.36387813 0.43740802]. \t  0.3443823794071162 \t 3.384686410643391\n",
      "58     \t [0.37320499 0.77070369 0.1251691 ]. \t  0.016248781745761793 \t 3.384686410643391\n",
      "59     \t [0.12897054 0.6899248  0.17534283]. \t  0.0530713065168545 \t 3.384686410643391\n",
      "60     \t [0.45695591 0.70080023 0.5025471 ]. \t  1.526717798942501 \t 3.384686410643391\n",
      "61     \t [0.05849534 0.46408352 0.41179857]. \t  0.4467417541888797 \t 3.384686410643391\n",
      "62     \t [0.30541264 0.98727113 0.09071907]. \t  0.003876883511273481 \t 3.384686410643391\n",
      "63     \t [0.72160604 0.24268177 0.50307149]. \t  0.23576165473482003 \t 3.384686410643391\n",
      "64     \t [0.72308307 0.18493451 0.53286396]. \t  0.22274197217449898 \t 3.384686410643391\n",
      "65     \t [0.85946609 0.33153386 0.89576344]. \t  2.211196163940107 \t 3.384686410643391\n",
      "66     \t [0.60042349 0.04043173 0.37197389]. \t  0.5803517524167418 \t 3.384686410643391\n",
      "67     \t [0.5076376  0.69881062 0.33272394]. \t  0.3431156257178581 \t 3.384686410643391\n",
      "68     \t [0.32011282 0.81976363 0.25806866]. \t  0.18925664364548206 \t 3.384686410643391\n",
      "69     \t [0.29818961 0.07253359 0.4932673 ]. \t  0.24686703877831 \t 3.384686410643391\n",
      "70     \t [0.5799221  0.93949465 0.21121711]. \t  0.04378456768988935 \t 3.384686410643391\n",
      "71     \t [0.94105475 0.61895742 0.84103899]. \t  \u001b[92m3.4990953271727396\u001b[0m \t 3.4990953271727396\n",
      "72     \t [0.61333564 0.58001302 0.47505182]. \t  0.599638287754767 \t 3.4990953271727396\n",
      "73     \t [0.01035362 0.31141221 0.01364545]. \t  0.06763938254174633 \t 3.4990953271727396\n",
      "74     \t [0.51583748 0.31826586 0.84896046]. \t  2.32066970102785 \t 3.4990953271727396\n",
      "75     \t [0.14382107 0.78677571 0.40552068]. \t  1.4349844004282388 \t 3.4990953271727396\n",
      "76     \t [0.27218041 0.81767696 0.06798362]. \t  0.00437913500866071 \t 3.4990953271727396\n",
      "77     \t [0.40986787 0.5928176  0.91177265]. \t  3.4849390205858963 \t 3.4990953271727396\n",
      "78     \t [0.24712999 0.10619489 0.52684936]. \t  0.21103223974617824 \t 3.4990953271727396\n",
      "79     \t [0.89955862 0.40870859 0.93107263]. \t  2.5399553848541836 \t 3.4990953271727396\n",
      "80     \t [0.98979221 0.39065413 0.21875953]. \t  0.13964437910967512 \t 3.4990953271727396\n",
      "81     \t [0.80513667 0.76031054 0.35100793]. \t  0.1869849940572382 \t 3.4990953271727396\n",
      "82     \t [0.07889659 0.84514028 0.19255741]. \t  0.06133080544803562 \t 3.4990953271727396\n",
      "83     \t [0.52961149 0.57394641 0.6778265 ]. \t  2.036894490767361 \t 3.4990953271727396\n",
      "84     \t [0.47275261 0.94261631 0.43659062]. \t  1.2696367167975604 \t 3.4990953271727396\n",
      "85     \t [0.10082931 0.54995718 0.90904948]. \t  \u001b[92m3.522614495335012\u001b[0m \t 3.522614495335012\n",
      "86     \t [0.6278761  0.75993885 0.93899347]. \t  2.149024598027252 \t 3.522614495335012\n",
      "87     \t [0.20492753 0.84043672 0.64271223]. \t  2.659763251623286 \t 3.522614495335012\n",
      "88     \t [0.9769218  0.49927438 0.79034934]. \t  3.234816987093855 \t 3.522614495335012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.22747823 0.65823173 0.64344295]. \t  2.3382352405547295 \t 3.522614495335012\n",
      "90     \t [0.28384702 0.30207948 0.2187787 ]. \t  0.6509549868159237 \t 3.522614495335012\n",
      "91     \t [0.55517769 0.89233173 0.11239132]. \t  0.005724117547359386 \t 3.522614495335012\n",
      "92     \t [0.84525445 0.93732113 0.67816325]. \t  0.6356276322129517 \t 3.522614495335012\n",
      "93     \t [0.23971537 0.16323464 0.62903904]. \t  0.4393019412255073 \t 3.522614495335012\n",
      "94     \t [0.64984303 0.25298094 0.09082673]. \t  0.2577368691051716 \t 3.522614495335012\n",
      "95     \t [0.10475629 0.65902428 0.65561448]. \t  2.42693762353661 \t 3.522614495335012\n",
      "96     \t [0.11819213 0.54477089 0.0954383 ]. \t  0.056596369763538185 \t 3.522614495335012\n",
      "97     \t [0.95739774 0.75075339 0.7651484 ]. \t  1.9131225859994838 \t 3.522614495335012\n",
      "98     \t [0.5234829  0.42514574 0.12745903]. \t  0.20131917504453287 \t 3.522614495335012\n",
      "99     \t [0.25442394 0.56131737 0.06955229]. \t  0.04222469560979187 \t 3.522614495335012\n",
      "100    \t [0.66094917 0.60605491 0.70309211]. \t  2.1643084262053947 \t 3.522614495335012\n"
     ]
    }
   ],
   "source": [
    "### 6(l). Bayesian optimization runs (x20): GP run number = 12\n",
    "\n",
    "np.random.seed(run_num_12)\n",
    "surrogate_gp_12 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "gpgo_gp_12 = GPGO(surrogate_gp_12, Acquisition_new(util_gp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_12.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.06748528 0.33494299 0.36839425]. \t  0.41381266679589174 \t 0.41381266679589174\n",
      "init   \t [0.48359485 0.46381274 0.03788014]. \t  0.05965297274446339 \t 0.41381266679589174\n",
      "init   \t [0.06564924 0.0867977  0.95570991]. \t  0.318830723512337 \t 0.41381266679589174\n",
      "init   \t [0.17577016 0.12860266 0.5671431 ]. \t  0.23165598509668625 \t 0.41381266679589174\n",
      "init   \t [0.97377021 0.9670107  0.4822915 ]. \t  0.25781214991700163 \t 0.41381266679589174\n",
      "1      \t [0. 1. 1.]. \t  0.330219860606422 \t 0.41381266679589174\n",
      "2      \t [ 1.00000000e+00 -1.11022302e-16  1.00000000e+00]. \t  0.08848201872702724 \t 0.41381266679589174\n",
      "3      \t [-1.38777878e-17  1.00000000e+00 -5.55111512e-17]. \t  0.0002735367680454458 \t 0.41381266679589174\n",
      "4      \t [ 1.00000000e+00 -5.55111512e-17  0.00000000e+00]. \t  0.030954717033005136 \t 0.41381266679589174\n",
      "5      \t [1. 1. 1.]. \t  0.31688362070415665 \t 0.41381266679589174\n",
      "6      \t [0. 0. 0.]. \t  0.06797411659013229 \t 0.41381266679589174\n",
      "7      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 0.41381266679589174\n",
      "8      \t [0.53444776 0.55658987 1.        ]. \t  \u001b[92m2.0805406625388487\u001b[0m \t 2.0805406625388487\n",
      "9      \t [1.         0.45156155 0.66531425]. \t  1.4614961937830868 \t 2.0805406625388487\n",
      "10     \t [0.37080157 1.         0.55941266]. \t  \u001b[92m2.107069206135631\u001b[0m \t 2.107069206135631\n",
      "11     \t [0.50047331 1.         1.        ]. \t  0.33231257901530437 \t 2.107069206135631\n",
      "12     \t [1.         0.50864482 1.        ]. \t  1.9423290981369836 \t 2.107069206135631\n",
      "13     \t [0.         1.         0.51696368]. \t  \u001b[92m2.374433038114873\u001b[0m \t 2.374433038114873\n",
      "14     \t [1.         0.50713839 0.        ]. \t  0.007754348712596613 \t 2.374433038114873\n",
      "15     \t [0.         0.61487242 0.83250522]. \t  \u001b[92m3.6524846934639705\u001b[0m \t 3.6524846934639705\n",
      "16     \t [0.         0.52105408 1.        ]. \t  2.0145028872090163 \t 3.6524846934639705\n",
      "17     \t [0.         0.71922717 0.71691165]. \t  2.530573647007562 \t 3.6524846934639705\n",
      "18     \t [0.         0.33356496 0.77916575]. \t  2.261581998740523 \t 3.6524846934639705\n",
      "19     \t [0.1969517  0.23053806 0.        ]. \t  0.09432233538074154 \t 3.6524846934639705\n",
      "20     \t [0.48531258 1.         0.        ]. \t  0.0002099591022968721 \t 3.6524846934639705\n",
      "21     \t [0.20736229 0.57353841 0.8619583 ]. \t  \u001b[92m3.8377985766264398\u001b[0m \t 3.8377985766264398\n",
      "22     \t [0.51863535 0.         1.        ]. \t  0.0916059342436569 \t 3.8377985766264398\n",
      "23     \t [1.         0.         0.36859957]. \t  0.19525062000112967 \t 3.8377985766264398\n",
      "24     \t [0.77447021 0.23627439 0.18836061]. \t  0.4395177906439357 \t 3.8377985766264398\n",
      "25     \t [0.75048489 0.74872582 0.81310304]. \t  2.4561665646053585 \t 3.8377985766264398\n",
      "26     \t [0.         0.36173275 0.        ]. \t  0.04284366775922528 \t 3.8377985766264398\n",
      "27     \t [0.05824135 1.         0.75167668]. \t  1.1252157970776295 \t 3.8377985766264398\n",
      "28     \t [0.69072295 1.         0.25409671]. \t  0.06186112792996655 \t 3.8377985766264398\n",
      "29     \t [0.17530457 1.         0.22077751]. \t  0.0892294309016079 \t 3.8377985766264398\n",
      "30     \t [0.42936315 0.45875117 0.75804687]. \t  2.928272081996863 \t 3.8377985766264398\n",
      "31     \t [0.6313577 0.        0.       ]. \t  0.08315740085762942 \t 3.8377985766264398\n",
      "32     \t [0.77332648 0.2449273  0.        ]. \t  0.060944592896277044 \t 3.8377985766264398\n",
      "33     \t [0.4341748  0.68164681 0.78167455]. \t  2.9534647696171152 \t 3.8377985766264398\n",
      "34     \t [0.85665649 1.         0.80481884]. \t  0.5310863051652456 \t 3.8377985766264398\n",
      "35     \t [0.2188313  0.50119287 0.85506862]. \t  3.7555738547569506 \t 3.8377985766264398\n",
      "36     \t [1.         0.74904374 1.        ]. \t  1.4511754428459491 \t 3.8377985766264398\n",
      "37     \t [0.80491626 0.62662944 0.45722422]. \t  0.3511799740765865 \t 3.8377985766264398\n",
      "38     \t [0.         0.         0.74313868]. \t  0.2312285641212957 \t 3.8377985766264398\n",
      "39     \t [0.84069393 0.05203192 0.69994975]. \t  0.3113292860443439 \t 3.8377985766264398\n",
      "40     \t [0.12596005 0.69840693 0.99550908]. \t  1.8232408029140883 \t 3.8377985766264398\n",
      "41     \t [0.14705998 0.48017812 0.80228095]. \t  3.4716822088358397 \t 3.8377985766264398\n",
      "42     \t [1.28458282e-08 1.36545836e-08 2.46492962e-01]. \t  0.5723812728584171 \t 3.8377985766264398\n",
      "43     \t [0.73796265 0.78516002 0.        ]. \t  0.0009800299795842363 \t 3.8377985766264398\n",
      "44     \t [6.89966040e-01 6.47741263e-08 3.46303351e-01]. \t  0.5315731050988087 \t 3.8377985766264398\n",
      "45     \t [0.         0.73168733 0.10045674]. \t  0.011449465561189564 \t 3.8377985766264398\n",
      "46     \t [0.2727488  0.49589439 0.91337222]. \t  3.3620875460278206 \t 3.8377985766264398\n",
      "47     \t [0.24898068 0.         0.20299349]. \t  0.7378324925245331 \t 3.8377985766264398\n",
      "48     \t [0.72140945 0.18981381 0.92645167]. \t  0.8828189947099504 \t 3.8377985766264398\n",
      "49     \t [0.04175585 0.51444435 0.79434151]. \t  3.4953776006292783 \t 3.8377985766264398\n",
      "50     \t [0.24975058 0.60475854 0.86087672]. \t  3.770512687073123 \t 3.8377985766264398\n",
      "51     \t [0.22945686 0.5620099  0.7902451 ]. \t  3.5283739505824605 \t 3.8377985766264398\n",
      "52     \t [0.26843027 0.66278933 0.81435334]. \t  3.36169040057472 \t 3.8377985766264398\n",
      "53     \t [0.32696591 0.61650095 0.8096833 ]. \t  3.5585676524577554 \t 3.8377985766264398\n",
      "54     \t [0.17711176 0.42845365 0.87925496]. \t  3.2367095325655746 \t 3.8377985766264398\n",
      "55     \t [0.2044676  0.50437638 0.86946887]. \t  3.732483635719078 \t 3.8377985766264398\n",
      "56     \t [0.25054991 0.         0.80153682]. \t  0.2511141253484843 \t 3.8377985766264398\n",
      "57     \t [0.16942444 0.62370822 0.79818896]. \t  3.459267988300413 \t 3.8377985766264398\n",
      "58     \t [0.99993744 0.77592878 0.86590028]. \t  2.2724960793056788 \t 3.8377985766264398\n",
      "59     \t [0.82988192 0.57081426 0.88958374]. \t  3.6278343054640763 \t 3.8377985766264398\n",
      "60     \t [0.35580412 0.56653185 0.84995573]. \t  \u001b[92m3.8530716229321293\u001b[0m \t 3.8530716229321293\n",
      "61     \t [0.28952194 0.56424419 0.85281222]. \t  \u001b[92m3.8584029934569237\u001b[0m \t 3.8584029934569237\n",
      "62     \t [0.3634875  0.55830222 0.8060865 ]. \t  3.6546640185391097 \t 3.8584029934569237\n",
      "63     \t [0.20421771 0.53014311 0.82098502]. \t  3.748954541103207 \t 3.8584029934569237\n",
      "64     \t [0.48138184 0.55604378 0.83794475]. \t  3.81753206342964 \t 3.8584029934569237\n",
      "65     \t [0.82849486 0.57046032 0.85267451]. \t  3.729431934576418 \t 3.8584029934569237\n",
      "66     \t [0.65418423 0.56224393 0.86155351]. \t  3.79819802927003 \t 3.8584029934569237\n",
      "67     \t [0.63363462 0.5439144  0.77720249]. \t  3.2708076612884485 \t 3.8584029934569237\n",
      "68     \t [0.2960432  0.61494059 0.81739067]. \t  3.620145943574278 \t 3.8584029934569237\n",
      "69     \t [0.08908113 0.56758707 0.851996  ]. \t  3.8319222442805554 \t 3.8584029934569237\n",
      "70     \t [0.21011389 0.61946034 0.87391563]. \t  3.678733100560872 \t 3.8584029934569237\n",
      "71     \t [0.25289938 0.54831515 0.86614987]. \t  3.840475491012783 \t 3.8584029934569237\n",
      "72     \t [0.25581212 0.47933783 0.83533839]. \t  3.6484265448166413 \t 3.8584029934569237\n",
      "73     \t [0.17540356 0.51997835 0.81129892]. \t  3.6684907782300904 \t 3.8584029934569237\n",
      "74     \t [0.03435533 0.52907178 0.85925044]. \t  3.792219799113874 \t 3.8584029934569237\n",
      "75     \t [0.30100229 0.65991828 0.8018702 ]. \t  3.2959048219722895 \t 3.8584029934569237\n",
      "76     \t [0.19659615 0.51793649 0.83943687]. \t  3.794994611370131 \t 3.8584029934569237\n",
      "77     \t [0.19981015 0.49822964 0.78909609]. \t  3.432481828299779 \t 3.8584029934569237\n",
      "78     \t [0.36446463 0.60865232 0.85531566]. \t  3.75685308329379 \t 3.8584029934569237\n",
      "79     \t [0.66073746 0.54082737 0.8479411 ]. \t  3.7932849790513594 \t 3.8584029934569237\n",
      "80     \t [0.64482832 0.48440617 0.79105784]. \t  3.330261651215864 \t 3.8584029934569237\n",
      "81     \t [0.42845007 0.55473642 0.82426685]. \t  3.7684106754830013 \t 3.8584029934569237\n",
      "82     \t [0.58903841 0.54276895 0.89173107]. \t  3.6791406097899135 \t 3.8584029934569237\n",
      "83     \t [0.64815573 0.54639998 0.83053725]. \t  3.7435376550798365 \t 3.8584029934569237\n",
      "84     \t [0.29406064 0.55683463 0.87526729]. \t  3.8121102501348143 \t 3.8584029934569237\n",
      "85     \t [0.26532349 0.60303097 0.8461281 ]. \t  3.775596428059256 \t 3.8584029934569237\n",
      "86     \t [0.53518773 0.54656607 0.8443752 ]. \t  3.8228910065128914 \t 3.8584029934569237\n",
      "87     \t [0.36972199 0.55516719 0.87499185]. \t  3.8124350105008373 \t 3.8584029934569237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.99380277 0.54864457 0.83495698]. \t  3.624757193381667 \t 3.8584029934569237\n",
      "89     \t [0.39531197 0.57960699 0.84869688]. \t  3.8302577111417673 \t 3.8584029934569237\n",
      "90     \t [0.85254317 0.47955627 0.86862398]. \t  3.54019110453531 \t 3.8584029934569237\n",
      "91     \t [0.20863368 0.5683652  0.93356592]. \t  3.227732953717781 \t 3.8584029934569237\n",
      "92     \t [0.32392402 0.58006914 0.84690144]. \t  3.833695821016961 \t 3.8584029934569237\n",
      "93     \t [0.51513467 0.51855535 0.82040137]. \t  3.6967896137582112 \t 3.8584029934569237\n",
      "94     \t [0.82080592 0.54435401 0.85956797]. \t  3.7465302069608057 \t 3.8584029934569237\n",
      "95     \t [0.4461135  0.60031357 0.85017319]. \t  3.77139620175972 \t 3.8584029934569237\n",
      "96     \t [0.66429893 0.52581646 0.80790886]. \t  3.573217322133325 \t 3.8584029934569237\n",
      "97     \t [0.99101739 0.56019251 0.86666982]. \t  3.6653617789259263 \t 3.8584029934569237\n",
      "98     \t [0.6302019  0.53212134 0.82047155]. \t  3.684873561168798 \t 3.8584029934569237\n",
      "99     \t [0.42582647 0.62006471 0.84863032]. \t  3.6967114089556325 \t 3.8584029934569237\n",
      "100    \t [0.74922722 0.57813971 0.81259076]. \t  3.549478198714903 \t 3.8584029934569237\n"
     ]
    }
   ],
   "source": [
    "### 6(l). Bayesian optimization runs (x20): STP DF1 run number = 12\n",
    "\n",
    "np.random.seed(run_num_12)\n",
    "surrogate_stp_df1_12 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_12 = GPGO(surrogate_stp_df1_12, Acquisition_new(util_stp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_12.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0783230013836795, -5.431390225897604)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(l). Training Regret Minimisation: run number = 12\n",
    "\n",
    "gp_output_12 = np.append(np.max(gpgo_gp_12.GP.y[0:n_init]),gpgo_gp_12.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_12 = np.append(np.max(gpgo_stp_df1_12.GP.y[0:n_init]),gpgo_stp_df1_12.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_12 = np.log(y_global_orig - gp_output_12)\n",
    "regret_stp_df1_12 = np.log(y_global_orig - stp_df1_output_12)\n",
    "\n",
    "train_regret_gp_12 = min_max_array(regret_gp_12)\n",
    "train_regret_stp_df1_12 = min_max_array(regret_stp_df1_12)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 12\n",
    "min_train_regret_gp_12 = min(train_regret_gp_12)\n",
    "min_train_regret_stp_df1_12 = min(train_regret_stp_df1_12)\n",
    "\n",
    "min_train_regret_gp_12, min_train_regret_stp_df1_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.29958543 0.01256984 0.38676125]. \t  0.5788268166842954 \t 3.5405564208524494\n",
      "init   \t [0.8033012  0.5797875  0.81484367]. \t  3.5405564208524494 \t 3.5405564208524494\n",
      "init   \t [0.935837   0.589375   0.92609909]. \t  3.2065662389551366 \t 3.5405564208524494\n",
      "init   \t [0.64399817 0.38313651 0.48317905]. \t  0.3054939862962379 \t 3.5405564208524494\n",
      "init   \t [0.92205252 0.87147882 0.32466543]. \t  0.08604693942678583 \t 3.5405564208524494\n",
      "1      \t [0.15425318 1.         1.        ]. \t  0.3334775322166951 \t 3.5405564208524494\n",
      "2      \t [0.16722258 0.         1.        ]. \t  0.09124687381488106 \t 3.5405564208524494\n",
      "3      \t [0.80211508 1.         0.94662522]. \t  0.4637695186804275 \t 3.5405564208524494\n",
      "4      \t [0.60843341 0.55341255 0.9067799 ]. \t  \u001b[92m3.5485434419917237\u001b[0m \t 3.5485434419917237\n",
      "5      \t [0.0097863  0.01491847 0.42719019]. \t  0.29087450431370876 \t 3.5485434419917237\n",
      "6      \t [0.76195589 0.36134889 0.93457715]. \t  2.1524274762647084 \t 3.5485434419917237\n",
      "7      \t [0.9931172  0.76984761 0.71494461]. \t  1.2802836647384264 \t 3.5485434419917237\n",
      "8      \t [0.70244613 0.67195473 0.95315585]. \t  2.58350711373178 \t 3.5485434419917237\n",
      "9      \t [0.26998018 1.         0.19707229]. \t  0.051304005233089665 \t 3.5485434419917237\n",
      "10     \t [0.49379987 1.         0.55566705]. \t  1.6651935856609559 \t 3.5485434419917237\n",
      "11     \t [0.82526104 1.         0.        ]. \t  7.952654068270527e-05 \t 3.5485434419917237\n",
      "12     \t [0.22676134 0.32458173 0.75188651]. \t  2.0303739849553515 \t 3.5485434419917237\n",
      "13     \t [0.58465401 0.49415715 0.80113575]. \t  3.473756667700524 \t 3.5485434419917237\n",
      "14     \t [0.11392155 0.33936092 1.        ]. \t  1.2459282201175725 \t 3.5485434419917237\n",
      "15     \t [0.20304791 0.15428367 0.04496843]. \t  0.20611355635178372 \t 3.5485434419917237\n",
      "16     \t [0.97312915 0.18209783 0.01425055]. \t  0.046950209536629484 \t 3.5485434419917237\n",
      "17     \t [0.92098097 0.06953523 0.47017535]. \t  0.1357307891562983 \t 3.5485434419917237\n",
      "18     \t [1.         0.40734099 0.71631601]. \t  2.0160190815103958 \t 3.5485434419917237\n",
      "19     \t [0.07718933 0.89141199 0.74006497]. \t  1.7838102545081989 \t 3.5485434419917237\n",
      "20     \t [0.79257227 0.04030681 0.03261217]. \t  0.10543480982239585 \t 3.5485434419917237\n",
      "21     \t [0.78867925 0.49312464 0.33804986]. \t  0.16995648055562376 \t 3.5485434419917237\n",
      "22     \t [0.9125467  0.72588195 0.31337558]. \t  0.07091377394876025 \t 3.5485434419917237\n",
      "23     \t [0.60509471 0.50020526 0.31168597]. \t  0.24583180178167538 \t 3.5485434419917237\n",
      "24     \t [0.03137925 0.72123982 0.72003679]. \t  2.554311908554907 \t 3.5485434419917237\n",
      "25     \t [0.32887926 0.68809247 0.9130779 ]. \t  3.005796605770871 \t 3.5485434419917237\n",
      "26     \t [0.74139312 0.52673432 0.29365645]. \t  0.15661159346502906 \t 3.5485434419917237\n",
      "27     \t [0.1000151  0.67724878 0.04572148]. \t  0.008858801304649942 \t 3.5485434419917237\n",
      "28     \t [0.7257924  0.88633664 0.17193461]. \t  0.013202033421941626 \t 3.5485434419917237\n",
      "29     \t [0.08595087 0.24647847 0.80760586]. \t  1.6159359018372403 \t 3.5485434419917237\n",
      "30     \t [0.62964284 0.38408109 0.45511797]. \t  0.2916010420255309 \t 3.5485434419917237\n",
      "31     \t [0.51647624 0.56831905 0.81179957]. \t  \u001b[92m3.651207679080891\u001b[0m \t 3.651207679080891\n",
      "32     \t [0.75398714 0.00571101 0.63166037]. \t  0.1392464665378098 \t 3.651207679080891\n",
      "33     \t [0.1479493  0.71894486 0.87665484]. \t  2.984108396612967 \t 3.651207679080891\n",
      "34     \t [0.24174582 0.68739523 0.53464085]. \t  2.1681614837413514 \t 3.651207679080891\n",
      "35     \t [0.88909337 0.14710933 0.14637579]. \t  0.2838003183774472 \t 3.651207679080891\n",
      "36     \t [0.4777367  0.34513559 0.4154171 ]. \t  0.3898953559192299 \t 3.651207679080891\n",
      "37     \t [0.70988493 0.87579081 0.28112526]. \t  0.10988237975884263 \t 3.651207679080891\n",
      "38     \t [0.47659256 0.56826163 0.09914347]. \t  0.055524219099222574 \t 3.651207679080891\n",
      "39     \t [0.55645589 0.03294665 0.21743575]. \t  0.7782569939296579 \t 3.651207679080891\n",
      "40     \t [0.01957374 0.18131416 0.7902635 ]. \t  1.0722028365258023 \t 3.651207679080891\n",
      "41     \t [0.2061757  0.44562355 0.33770681]. \t  0.38793014638952655 \t 3.651207679080891\n",
      "42     \t [0.8002181  0.29951834 0.09348645]. \t  0.16574647198851283 \t 3.651207679080891\n",
      "43     \t [0.30707685 0.32756252 0.64620058]. \t  1.100698651703412 \t 3.651207679080891\n",
      "44     \t [0.04788147 0.66467334 0.71290498]. \t  2.647636531991435 \t 3.651207679080891\n",
      "45     \t [0.39391952 0.836942   0.13892216]. \t  0.016399972692537245 \t 3.651207679080891\n",
      "46     \t [0.58472302 0.69221968 0.88395781]. \t  3.1422142435291858 \t 3.651207679080891\n",
      "47     \t [0.61137226 0.97829661 0.08070401]. \t  0.0016672422244322605 \t 3.651207679080891\n",
      "48     \t [0.77190689 0.02969059 0.26734632]. \t  0.5693548115661686 \t 3.651207679080891\n",
      "49     \t [0.59812634 0.46646389 0.29744578]. \t  0.2845541679356608 \t 3.651207679080891\n",
      "50     \t [0.1782969  0.57423833 0.57494692]. \t  1.6684109789571253 \t 3.651207679080891\n",
      "51     \t [0.5305182  0.56259722 0.30301662]. \t  0.22358433962724938 \t 3.651207679080891\n",
      "52     \t [0.10316077 0.72036273 0.88671697]. \t  2.9213721500041676 \t 3.651207679080891\n",
      "53     \t [0.84943473 0.7914976  0.59276459]. \t  0.7701972492616265 \t 3.651207679080891\n",
      "54     \t [0.60888946 0.05674928 0.68815662]. \t  0.30977252233962854 \t 3.651207679080891\n",
      "55     \t [0.97561985 0.41087112 0.57782929]. \t  0.5618487971116986 \t 3.651207679080891\n",
      "56     \t [0.66324906 0.59086199 0.72739443]. \t  2.5153777215839805 \t 3.651207679080891\n",
      "57     \t [0.77327809 0.85403397 0.53449178]. \t  0.850625239135785 \t 3.651207679080891\n",
      "58     \t [0.0966172  0.33026802 0.26993853]. \t  0.5220878026623681 \t 3.651207679080891\n",
      "59     \t [0.28503334 0.60773405 0.85111978]. \t  \u001b[92m3.76413641790864\u001b[0m \t 3.76413641790864\n",
      "60     \t [0.04963668 0.78686025 0.54938434]. \t  2.8841235826073928 \t 3.76413641790864\n",
      "61     \t [0.94640727 0.43565679 0.01105827]. \t  0.018585936810108613 \t 3.76413641790864\n",
      "62     \t [0.40900047 0.77350358 0.13052753]. \t  0.017016651469255545 \t 3.76413641790864\n",
      "63     \t [0.60778927 0.02994123 0.97590337]. \t  0.15740575635714693 \t 3.76413641790864\n",
      "64     \t [0.57649047 0.18228933 0.48520615]. \t  0.2731377838872698 \t 3.76413641790864\n",
      "65     \t [0.19797452 0.71915644 0.92939466]. \t  2.5999862418072315 \t 3.76413641790864\n",
      "66     \t [0.52997485 0.14389858 0.12020186]. \t  0.47991659882492693 \t 3.76413641790864\n",
      "67     \t [0.97397741 0.63016717 0.09616931]. \t  0.0102699582850341 \t 3.76413641790864\n",
      "68     \t [0.23828731 0.04646259 0.73102566]. \t  0.34454749257481326 \t 3.76413641790864\n",
      "69     \t [0.52022594 0.81779692 0.65930093]. \t  1.7870056312151121 \t 3.76413641790864\n",
      "70     \t [0.99475732 0.21854638 0.04430708]. \t  0.0626646115180008 \t 3.76413641790864\n",
      "71     \t [0.73189749 0.97562842 0.7832757 ]. \t  0.6861157897571284 \t 3.76413641790864\n",
      "72     \t [0.72792918 0.27958625 0.97620582]. \t  1.1247430581517424 \t 3.76413641790864\n",
      "73     \t [0.0039455  0.43418808 0.78021341]. \t  3.015168739844583 \t 3.76413641790864\n",
      "74     \t [0.44171499 0.99753661 0.99081312]. \t  0.36730107894299074 \t 3.76413641790864\n",
      "75     \t [0.83696784 0.0468548  0.3357157 ]. \t  0.42959453505200523 \t 3.76413641790864\n",
      "76     \t [0.12046715 0.15849121 0.32629024]. \t  0.740727122254246 \t 3.76413641790864\n",
      "77     \t [0.81688321 0.3574389  0.39984916]. \t  0.2210066834793336 \t 3.76413641790864\n",
      "78     \t [0.04167306 0.59179094 0.80165248]. \t  3.5592055915694756 \t 3.76413641790864\n",
      "79     \t [0.56741271 0.49916774 0.77762704]. \t  3.2537598901327467 \t 3.76413641790864\n",
      "80     \t [0.1119598  0.94353358 0.64471104]. \t  2.4150675122179974 \t 3.76413641790864\n",
      "81     \t [0.19577173 0.2131923  0.83370208]. \t  1.349864392831174 \t 3.76413641790864\n",
      "82     \t [0.69564951 0.32723087 0.81083661]. \t  2.357460351513275 \t 3.76413641790864\n",
      "83     \t [0.81598393 0.14908526 0.45843757]. \t  0.21301939943172551 \t 3.76413641790864\n",
      "84     \t [0.27449342 0.84897413 0.92296165]. \t  1.5427629788969646 \t 3.76413641790864\n",
      "85     \t [0.9392639  0.93001928 0.07523053]. \t  0.0005393425323423597 \t 3.76413641790864\n",
      "86     \t [0.97151791 0.75006583 0.89507002]. \t  2.4716899831397092 \t 3.76413641790864\n",
      "87     \t [0.6860823  0.64579818 0.26237228]. \t  0.09606354182847256 \t 3.76413641790864\n",
      "88     \t [0.88087303 0.11881729 0.50869267]. \t  0.14192369916905595 \t 3.76413641790864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.40413383 0.28389434 0.2024118 ]. \t  0.6663573856823652 \t 3.76413641790864\n",
      "90     \t [0.87941754 0.59998395 0.49546625]. \t  0.3407556195266339 \t 3.76413641790864\n",
      "91     \t [0.37760511 0.13104474 0.06496837]. \t  0.2921998700153626 \t 3.76413641790864\n",
      "92     \t [0.77020157 0.64076329 0.94526267]. \t  2.835344456689959 \t 3.76413641790864\n",
      "93     \t [0.05444924 0.97200523 0.56542255]. \t  2.7282534393980105 \t 3.76413641790864\n",
      "94     \t [0.63332527 0.07764083 0.7975454 ]. \t  0.5069887766065075 \t 3.76413641790864\n",
      "95     \t [0.94133602 0.68155749 0.75223515]. \t  2.252838512055025 \t 3.76413641790864\n",
      "96     \t [0.978496   0.51938974 0.95544017]. \t  2.731028952770469 \t 3.76413641790864\n",
      "97     \t [0.87201551 0.35992302 0.33053647]. \t  0.24097594006330864 \t 3.76413641790864\n",
      "98     \t [0.57139237 0.20101837 0.77144908]. \t  1.1847050515004685 \t 3.76413641790864\n",
      "99     \t [0.23297882 0.83919046 0.01986968]. \t  0.0013500453187682373 \t 3.76413641790864\n",
      "100    \t [0.39803368 0.07890229 0.35984521]. \t  0.7634436619491654 \t 3.76413641790864\n"
     ]
    }
   ],
   "source": [
    "### 6(m). Bayesian optimization runs (x20): GP run number = 13\n",
    "\n",
    "np.random.seed(run_num_13)\n",
    "surrogate_gp_13 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "gpgo_gp_13 = GPGO(surrogate_gp_13, Acquisition_new(util_gp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_13.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.29958543 0.01256984 0.38676125]. \t  0.5788268166842954 \t 3.5405564208524494\n",
      "init   \t [0.8033012  0.5797875  0.81484367]. \t  3.5405564208524494 \t 3.5405564208524494\n",
      "init   \t [0.935837   0.589375   0.92609909]. \t  3.2065662389551366 \t 3.5405564208524494\n",
      "init   \t [0.64399817 0.38313651 0.48317905]. \t  0.3054939862962379 \t 3.5405564208524494\n",
      "init   \t [0.92205252 0.87147882 0.32466543]. \t  0.08604693942678583 \t 3.5405564208524494\n",
      "1      \t [0.09970048 1.         1.        ]. \t  0.33260654011442053 \t 3.5405564208524494\n",
      "2      \t [0.0730576 0.        1.       ]. \t  0.09076898794602953 \t 3.5405564208524494\n",
      "3      \t [0.77902623 1.         1.        ]. \t  0.32516827838107426 \t 3.5405564208524494\n",
      "4      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.5405564208524494\n",
      "5      \t [0.90882898 0.         1.        ]. \t  0.08938257281298698 \t 3.5405564208524494\n",
      "6      \t [ 0.00000000e+00 -1.11022302e-16  0.00000000e+00]. \t  0.06797411659013226 \t 3.5405564208524494\n",
      "7      \t [0.         0.56504616 0.59733792]. \t  1.702708810974993 \t 3.5405564208524494\n",
      "8      \t [0.54316266 0.49165338 1.        ]. \t  1.968273564567473 \t 3.5405564208524494\n",
      "9      \t [0.30903546 1.         0.55730064]. \t  2.2926799748168953 \t 3.5405564208524494\n",
      "10     \t [0.         0.         0.52309936]. \t  0.11367929590477671 \t 3.5405564208524494\n",
      "11     \t [1.         0.87393057 0.71202185]. \t  0.7523071674125925 \t 3.5405564208524494\n",
      "12     \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.5405564208524494\n",
      "13     \t [0.         1.         0.57659197]. \t  2.467196009044776 \t 3.5405564208524494\n",
      "14     \t [0.67258976 1.         0.        ]. \t  0.00013308768317732777 \t 3.5405564208524494\n",
      "15     \t [1.         0.         0.54537206]. \t  0.069242257956527 \t 3.5405564208524494\n",
      "16     \t [0.53995784 0.         0.        ]. \t  0.09365381013932282 \t 3.5405564208524494\n",
      "17     \t [0.60469577 0.74440138 0.79887457]. \t  2.4947181535315925 \t 3.5405564208524494\n",
      "18     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 3.5405564208524494\n",
      "19     \t [0.         0.47186941 1.        ]. \t  1.8809250698811137 \t 3.5405564208524494\n",
      "20     \t [1.         0.37594897 0.82627266]. \t  2.7609290281247514 \t 3.5405564208524494\n",
      "21     \t [0.6043493  0.         0.75733452]. \t  0.2427235405542447 \t 3.5405564208524494\n",
      "22     \t [0.71813408 1.         0.53552356]. \t  0.8485943104361371 \t 3.5405564208524494\n",
      "23     \t [1.         0.54376166 0.        ]. \t  0.00575320569548371 \t 3.5405564208524494\n",
      "24     \t [0.13513135 0.55984429 0.        ]. \t  0.014112199173250656 \t 3.5405564208524494\n",
      "25     \t [0.         0.83335381 0.29315715]. \t  0.37004351363812577 \t 3.5405564208524494\n",
      "26     \t [0.         0.260281   0.18606681]. \t  0.4453432005374598 \t 3.5405564208524494\n",
      "27     \t [0.14467261 0.25762242 0.77297254]. \t  1.617310659759504 \t 3.5405564208524494\n",
      "28     \t [0.2318781  1.         0.13731937]. \t  0.013354490791443695 \t 3.5405564208524494\n",
      "29     \t [0.78789768 0.35417439 0.9588737 ]. \t  1.813476985786085 \t 3.5405564208524494\n",
      "30     \t [1.         0.50256885 0.63977274]. \t  1.1767666496971028 \t 3.5405564208524494\n",
      "31     \t [0.2012767  0.61843078 0.84748767]. \t  \u001b[92m3.7170440651064998\u001b[0m \t 3.7170440651064998\n",
      "32     \t [1. 1. 1.]. \t  0.31688362179679885 \t 3.7170440651064998\n",
      "33     \t [1.         0.         0.82614465]. \t  0.23849003691466303 \t 3.7170440651064998\n",
      "34     \t [7.52425071e-01 3.34477830e-05 2.55690667e-01]. \t  0.5587736600935828 \t 3.7170440651064998\n",
      "35     \t [0.727776   0.6121424  0.01930559]. \t  0.009339222940826206 \t 3.7170440651064998\n",
      "36     \t [0.         0.31060374 0.        ]. \t  0.05359277876816129 \t 3.7170440651064998\n",
      "37     \t [0.20469812 0.67302726 0.929601  ]. \t  2.92329629231747 \t 3.7170440651064998\n",
      "38     \t [0.99999999 0.47822849 0.99999999]. \t  1.8587443779745556 \t 3.7170440651064998\n",
      "39     \t [0.30180247 0.         1.        ]. \t  0.09165140429721233 \t 3.7170440651064998\n",
      "40     \t [0.25611662 0.5204923  0.79910007]. \t  3.579953416771469 \t 3.7170440651064998\n",
      "41     \t [8.50246478e-01 2.22629322e-01 8.77139484e-09]. \t  0.05233333908035049 \t 3.7170440651064998\n",
      "42     \t [0.33070711 1.         0.86977232]. \t  0.6617629290157012 \t 3.7170440651064998\n",
      "43     \t [0.18993758 0.6135152  0.73634402]. \t  2.9127416823173835 \t 3.7170440651064998\n",
      "44     \t [7.75010291e-08 9.49260347e-01 8.32353041e-01]. \t  1.0447904363575136 \t 3.7170440651064998\n",
      "45     \t [0.23852847 0.20258123 0.07369478]. \t  0.2869022547792859 \t 3.7170440651064998\n",
      "46     \t [0.39688764 0.5784668  0.81941254]. \t  \u001b[92m3.720724707681207\u001b[0m \t 3.720724707681207\n",
      "47     \t [0.32031305 0.54166839 0.79907429]. \t  3.601487249365757 \t 3.720724707681207\n",
      "48     \t [0.31916361 0.49431968 0.85918651]. \t  \u001b[92m3.728552768354191\u001b[0m \t 3.728552768354191\n",
      "49     \t [0.99849527 0.2844213  0.2092543 ]. \t  0.20821158406420057 \t 3.728552768354191\n",
      "50     \t [0.43133978 0.53042504 0.8468122 ]. \t  \u001b[92m3.829879005315518\u001b[0m \t 3.829879005315518\n",
      "51     \t [0.33691432 0.64240984 0.79405354]. \t  3.320776972940296 \t 3.829879005315518\n",
      "52     \t [0.24778732 0.45158952 0.85479978]. \t  3.4992797642555558 \t 3.829879005315518\n",
      "53     \t [0.27390727 0.52807814 0.87336504]. \t  3.7900791379610945 \t 3.829879005315518\n",
      "54     \t [0.9688823  0.58615792 0.88216368]. \t  3.589514973690995 \t 3.829879005315518\n",
      "55     \t [0.35632355 0.52916335 0.8082031 ]. \t  3.660260909133447 \t 3.829879005315518\n",
      "56     \t [0.23379856 0.53250493 0.90642901]. \t  3.547925017623491 \t 3.829879005315518\n",
      "57     \t [0.27866586 0.58587672 0.87197843]. \t  3.7954511102650152 \t 3.829879005315518\n",
      "58     \t [0.39903514 0.58110665 0.81608387]. \t  3.69354349547725 \t 3.829879005315518\n",
      "59     \t [0.28503334 0.60773405 0.85111978]. \t  3.76413641790864 \t 3.829879005315518\n",
      "60     \t [0.4359859  0.54468388 0.86646554]. \t  \u001b[92m3.8324380767918513\u001b[0m \t 3.8324380767918513\n",
      "61     \t [0.99171051 0.57406146 0.85870611]. \t  3.6564607555425392 \t 3.8324380767918513\n",
      "62     \t [0.19152279 0.46981169 0.8256123 ]. \t  3.5633466207996225 \t 3.8324380767918513\n",
      "63     \t [0.28780531 0.47353854 0.7961496 ]. \t  3.4028764341772124 \t 3.8324380767918513\n",
      "64     \t [0.90436451 0.57606617 0.83113565]. \t  3.615636791334068 \t 3.8324380767918513\n",
      "65     \t [0.37103213 0.55079755 0.85408276]. \t  \u001b[92m3.858251707862604\u001b[0m \t 3.858251707862604\n",
      "66     \t [0.33261615 0.53184454 0.81333345]. \t  3.703547123805014 \t 3.858251707862604\n",
      "67     \t [0.99474582 0.57127635 0.87324599]. \t  3.6376124694987095 \t 3.858251707862604\n",
      "68     \t [0.2313155  0.50070078 0.92029961]. \t  3.2935584016976276 \t 3.858251707862604\n",
      "69     \t [0.39901671 0.57290372 0.86029083]. \t  3.840261376920451 \t 3.858251707862604\n",
      "70     \t [0.47835786 0.50374733 0.80522543]. \t  3.5630904343710084 \t 3.858251707862604\n",
      "71     \t [0.84685784 0.5644942  0.87620546]. \t  3.69673570215386 \t 3.858251707862604\n",
      "72     \t [0.83730452 0.54650061 0.90149968]. \t  3.53490733022989 \t 3.858251707862604\n",
      "73     \t [0.27698653 0.53826895 0.87603062]. \t  3.7953624702939157 \t 3.858251707862604\n",
      "74     \t [0.34526425 0.54704507 0.82702446]. \t  3.794462322596371 \t 3.858251707862604\n",
      "75     \t [0.5162228  0.54110666 0.80201827]. \t  3.582470423493116 \t 3.858251707862604\n",
      "76     \t [0.88976753 0.67081112 0.88012404]. \t  3.223511109506307 \t 3.858251707862604\n",
      "77     \t [0.50143394 0.45998583 0.89451134]. \t  3.3589586778923044 \t 3.858251707862604\n",
      "78     \t [0.43027617 0.58621521 0.79716004]. \t  3.5208780942022826 \t 3.858251707862604\n",
      "79     \t [0.5084776  0.58847699 0.81860619]. \t  3.663673200030202 \t 3.858251707862604\n",
      "80     \t [0.31870974 0.53133805 0.83191392]. \t  3.8034018343568845 \t 3.858251707862604\n",
      "81     \t [0.87282144 0.50954081 0.84332947]. \t  3.659206518398622 \t 3.858251707862604\n",
      "82     \t [0.41658158 0.54075592 0.81909677]. \t  3.7379558607610655 \t 3.858251707862604\n",
      "83     \t [0.29838188 0.55092087 0.7903958 ]. \t  3.5265808836870134 \t 3.858251707862604\n",
      "84     \t [0.92514572 0.6622054  0.89378755]. \t  3.2072923955910073 \t 3.858251707862604\n",
      "85     \t [0.38084568 0.46636432 0.85675842]. \t  3.5915786170214914 \t 3.858251707862604\n",
      "86     \t [0.45701418 0.59336184 0.85997122]. \t  3.7915491335181777 \t 3.858251707862604\n",
      "87     \t [0.32784624 0.58983651 0.8529748 ]. \t  3.817534121434633 \t 3.858251707862604\n",
      "88     \t [0.43417008 0.51581977 0.92575284]. \t  3.277504831476311 \t 3.858251707862604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.22644028 0.52384959 0.84004382]. \t  3.8125628648995464 \t 3.858251707862604\n",
      "90     \t [0.89180865 0.62409234 0.86280227]. \t  3.536661020345214 \t 3.858251707862604\n",
      "91     \t [0.5727378  0.55619466 0.85343262]. \t  3.825349131316015 \t 3.858251707862604\n",
      "92     \t [0.22065195 0.60006893 0.83077467]. \t  3.7423497854364984 \t 3.858251707862604\n",
      "93     \t [0.5472703  0.61701169 0.8414422 ]. \t  3.6642919113744825 \t 3.858251707862604\n",
      "94     \t [0.10847708 0.52531675 0.87147522]. \t  3.7700675294316044 \t 3.858251707862604\n",
      "95     \t [0.44714604 0.4147138  0.8330596 ]. \t  3.2198838272772656 \t 3.858251707862604\n",
      "96     \t [0.95680069 0.63106002 0.90283832]. \t  3.3067348455141223 \t 3.858251707862604\n",
      "97     \t [0.23591741 0.53063598 0.88784248]. \t  3.7095820028790114 \t 3.858251707862604\n",
      "98     \t [0.23237797 0.56876599 0.86791673]. \t  3.8310780629900645 \t 3.858251707862604\n",
      "99     \t [0.87693437 0.57093914 0.93449   ]. \t  3.135444762950025 \t 3.858251707862604\n",
      "100    \t [0.30169521 0.58251672 0.85364505]. \t  3.8346349654700105 \t 3.858251707862604\n"
     ]
    }
   ],
   "source": [
    "### 6(m). Bayesian optimization runs (x20): STP DF1 run number = 13\n",
    "\n",
    "np.random.seed(run_num_13)\n",
    "surrogate_stp_df1_13 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_13 = GPGO(surrogate_stp_df1_13, Acquisition_new(util_stp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_13.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.3162421059906046, -5.397410422186369)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(m). Training Regret Minimisation: run number = 13\n",
    "\n",
    "gp_output_13 = np.append(np.max(gpgo_gp_13.GP.y[0:n_init]),gpgo_gp_13.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_13 = np.append(np.max(gpgo_stp_df1_13.GP.y[0:n_init]),gpgo_stp_df1_13.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_13 = np.log(y_global_orig - gp_output_13)\n",
    "regret_stp_df1_13 = np.log(y_global_orig - stp_df1_output_13)\n",
    "\n",
    "train_regret_gp_13 = min_max_array(regret_gp_13)\n",
    "train_regret_stp_df1_13 = min_max_array(regret_stp_df1_13)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 13\n",
    "min_train_regret_gp_13 = min(train_regret_gp_13)\n",
    "min_train_regret_stp_df1_13 = min(train_regret_stp_df1_13)\n",
    "\n",
    "min_train_regret_gp_13, min_train_regret_stp_df1_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.2675596  0.81347184 0.49218523]. \t  2.428328343373925 \t 2.428328343373925\n",
      "init   \t [0.56304629 0.02422557 0.21370975]. \t  0.7518304825980339 \t 2.428328343373925\n",
      "init   \t [0.19880715 0.05679138 0.9538148 ]. \t  0.24566381538681226 \t 2.428328343373925\n",
      "init   \t [0.44009759 0.56618711 0.0651964 ]. \t  0.039090337622666706 \t 2.428328343373925\n",
      "init   \t [0.04583571 0.63038572 0.52252221]. \t  1.7734998804260111 \t 2.428328343373925\n",
      "1      \t [0.39701695 1.         1.        ]. \t  0.33375069320882167 \t 2.428328343373925\n",
      "2      \t [0.93790866 0.08987892 0.67929751]. \t  0.3686845900760184 \t 2.428328343373925\n",
      "3      \t [0.08397767 0.94219938 0.02264552]. \t  0.0007288825702514919 \t 2.428328343373925\n",
      "4      \t [0.9843473  0.96002108 0.28753399]. \t  0.03314505746640488 \t 2.428328343373925\n",
      "5      \t [0.00141916 0.08927156 0.43827593]. \t  0.29207940676613753 \t 2.428328343373925\n",
      "6      \t [0.05249235 0.90039608 0.86178945]. \t  1.3387190887502154 \t 2.428328343373925\n",
      "7      \t [0.67154288 0.5638965  0.72279076]. \t  \u001b[92m2.486070515584066\u001b[0m \t 2.486070515584066\n",
      "8      \t [0.98166222 0.88634299 0.99164299]. \t  0.7904624893285194 \t 2.486070515584066\n",
      "9      \t [0.01287908 0.98329883 0.47270956]. \t  2.118758918316354 \t 2.486070515584066\n",
      "10     \t [0.94228786 0.02848665 0.06822564]. \t  0.10502230919416444 \t 2.486070515584066\n",
      "11     \t [5.70546195e-02 1.20030265e-02 3.00079969e-06]. \t  0.07844052386256575 \t 2.486070515584066\n",
      "12     \t [0.9719627  0.60524857 0.58564282]. \t  0.6494606260602995 \t 2.486070515584066\n",
      "13     \t [0.6177948  0.45310163 0.99935869]. \t  1.8369418792705259 \t 2.486070515584066\n",
      "14     \t [0.6536398  0.61983127 0.32477183]. \t  0.19119441922572802 \t 2.486070515584066\n",
      "15     \t [0.06189965 0.5485517  0.26413026]. \t  0.19982250326047096 \t 2.486070515584066\n",
      "16     \t [0.37962127 0.25805455 0.58833263]. \t  0.5051811235581931 \t 2.486070515584066\n",
      "17     \t [0.61788058 0.98256398 0.65791479]. \t  1.0388957346592305 \t 2.486070515584066\n",
      "18     \t [0.93521836 0.53455641 0.14505492]. \t  0.04348527894006298 \t 2.486070515584066\n",
      "19     \t [0.79382902 0.22044949 0.87885041]. \t  1.3007230780541872 \t 2.486070515584066\n",
      "20     \t [0.78668805 0.50425892 0.19398524]. \t  0.11646317305039454 \t 2.486070515584066\n",
      "21     \t [0.03624089 0.3160183  0.61463684]. \t  0.8115090444999788 \t 2.486070515584066\n",
      "22     \t [0.45904164 0.0572742  0.0976073 ]. \t  0.3969663971354894 \t 2.486070515584066\n",
      "23     \t [0.02986861 0.15242283 0.70420298]. \t  0.6716203903112046 \t 2.486070515584066\n",
      "24     \t [0.13397602 0.92728876 0.05524505]. \t  0.0019445211277765893 \t 2.486070515584066\n",
      "25     \t [0.2555581  0.58601583 0.24209672]. \t  0.17049298816197947 \t 2.486070515584066\n",
      "26     \t [0.58366198 0.52609183 0.0509107 ]. \t  0.040314583079031595 \t 2.486070515584066\n",
      "27     \t [0.79297839 0.54579477 0.47803573]. \t  0.3304103630366546 \t 2.486070515584066\n",
      "28     \t [0.19326656 0.64543659 0.96643949]. \t  \u001b[92m2.5278393139252318\u001b[0m \t 2.5278393139252318\n",
      "29     \t [0.35115228 0.74377156 0.81420043]. \t  \u001b[92m2.722001494373778\u001b[0m \t 2.722001494373778\n",
      "30     \t [0.86951526 0.35999291 0.53701299]. \t  0.33435462486930484 \t 2.722001494373778\n",
      "31     \t [0.49711384 0.97131729 0.73536371]. \t  1.0302275176487525 \t 2.722001494373778\n",
      "32     \t [0.93934179 0.76304212 0.58639099]. \t  0.5928372623961471 \t 2.722001494373778\n",
      "33     \t [0.85688627 0.40560786 0.74723374]. \t  2.4272104928614424 \t 2.722001494373778\n",
      "34     \t [0.53866514 0.24264643 0.67689494]. \t  0.954337108320203 \t 2.722001494373778\n",
      "35     \t [0.76669117 0.99806956 0.09549422]. \t  0.0013640515032059127 \t 2.722001494373778\n",
      "36     \t [0.56232866 0.27963159 0.01119956]. \t  0.09591559082489848 \t 2.722001494373778\n",
      "37     \t [0.14775685 0.10700293 0.02520449]. \t  0.14867288200637946 \t 2.722001494373778\n",
      "38     \t [0.60988022 0.19615521 0.43169496]. \t  0.3810476103737399 \t 2.722001494373778\n",
      "39     \t [0.53889881 0.2501758  0.15203468]. \t  0.5157666716041848 \t 2.722001494373778\n",
      "40     \t [0.99067327 0.08089385 0.6788831 ]. \t  0.341285901207815 \t 2.722001494373778\n",
      "41     \t [0.60947943 0.81658198 0.42963335]. \t  0.8700659019520931 \t 2.722001494373778\n",
      "42     \t [0.4442809  0.18194115 0.87752337]. \t  1.0281436522982392 \t 2.722001494373778\n",
      "43     \t [5.58571316e-04 2.76072659e-01 9.15216554e-01]. \t  1.5855429145652107 \t 2.722001494373778\n",
      "44     \t [0.84504379 0.8539982  0.96662324]. \t  1.1626670097926524 \t 2.722001494373778\n",
      "45     \t [0.87537573 0.80231356 0.39491727]. \t  0.23499393762459136 \t 2.722001494373778\n",
      "46     \t [0.65796608 0.24531845 0.01666181]. \t  0.10027096361361369 \t 2.722001494373778\n",
      "47     \t [0.1256818  0.83077757 0.9387248 ]. \t  1.5817309889619955 \t 2.722001494373778\n",
      "48     \t [0.74170453 0.57638718 0.01832345]. \t  0.012504848731735784 \t 2.722001494373778\n",
      "49     \t [0.4457932  0.06784038 0.99479359]. \t  0.19398576552933466 \t 2.722001494373778\n",
      "50     \t [0.27771379 0.33837756 0.9154406 ]. \t  2.1687678913229056 \t 2.722001494373778\n",
      "51     \t [0.68652106 0.69239007 0.29355389]. \t  0.1297443253734214 \t 2.722001494373778\n",
      "52     \t [0.78481    0.09601814 0.6805076 ]. \t  0.39551023920520195 \t 2.722001494373778\n",
      "53     \t [0.62049881 0.29126376 0.90760858]. \t  1.7893066800758612 \t 2.722001494373778\n",
      "54     \t [0.41359041 0.31851781 0.60920207]. \t  0.770950883238929 \t 2.722001494373778\n",
      "55     \t [0.08499999 0.87812935 0.51222434]. \t  \u001b[92m2.8719090663093145\u001b[0m \t 2.8719090663093145\n",
      "56     \t [0.52906221 0.13665939 0.7896535 ]. \t  0.8009288836461138 \t 2.8719090663093145\n",
      "57     \t [0.35291926 0.46717031 0.09546157]. \t  0.1217607472737993 \t 2.8719090663093145\n",
      "58     \t [0.48891981 0.29756857 0.1670603 ]. \t  0.5121363950193683 \t 2.8719090663093145\n",
      "59     \t [0.58229235 0.21174363 0.00232764]. \t  0.09703709983490341 \t 2.8719090663093145\n",
      "60     \t [0.6234229  0.42556405 0.53846266]. \t  0.5175187014561254 \t 2.8719090663093145\n",
      "61     \t [0.1042955  0.61883976 0.49175003]. \t  1.51136785487778 \t 2.8719090663093145\n",
      "62     \t [0.1503993  0.72382132 0.56173613]. \t  2.6133180286359443 \t 2.8719090663093145\n",
      "63     \t [0.46402562 0.27796231 0.57393197]. \t  0.47553532035710133 \t 2.8719090663093145\n",
      "64     \t [0.30274407 0.12239685 0.01375379]. \t  0.14341952099630148 \t 2.8719090663093145\n",
      "65     \t [0.11647501 0.73442552 0.25677962]. \t  0.19089839230896827 \t 2.8719090663093145\n",
      "66     \t [0.08742022 0.3302351  0.7882061 ]. \t  2.2998947292093055 \t 2.8719090663093145\n",
      "67     \t [0.84730971 0.98303365 0.3338511 ]. \t  0.12043306697152015 \t 2.8719090663093145\n",
      "68     \t [0.16531321 0.47711134 0.97474701]. \t  2.349559525792629 \t 2.8719090663093145\n",
      "69     \t [0.84054615 0.16538481 0.19935514]. \t  0.436480233307099 \t 2.8719090663093145\n",
      "70     \t [0.74519228 0.90139142 0.04672105]. \t  0.0007073965179978573 \t 2.8719090663093145\n",
      "71     \t [0.81159559 0.46329418 0.32160605]. \t  0.18031986407001088 \t 2.8719090663093145\n",
      "72     \t [0.63039655 0.16023951 0.82790739]. \t  0.9545856235970492 \t 2.8719090663093145\n",
      "73     \t [0.48847744 0.94511097 0.42398206]. \t  1.1113288078985712 \t 2.8719090663093145\n",
      "74     \t [0.81048679 0.35722558 0.76505498]. \t  2.3174030886944195 \t 2.8719090663093145\n",
      "75     \t [0.70648192 0.28343681 0.39053567]. \t  0.3668683258441915 \t 2.8719090663093145\n",
      "76     \t [0.9955065  0.40350797 0.508882  ]. \t  0.22862223758871128 \t 2.8719090663093145\n",
      "77     \t [0.36404512 0.11671039 0.21158684]. \t  0.9112750715632173 \t 2.8719090663093145\n",
      "78     \t [0.52103108 0.91922921 0.14486537]. \t  0.012395169938414392 \t 2.8719090663093145\n",
      "79     \t [0.20078505 0.79641546 0.66077686]. \t  2.570128198344724 \t 2.8719090663093145\n",
      "80     \t [0.91667342 0.31148452 0.95045742]. \t  1.5629338369167844 \t 2.8719090663093145\n",
      "81     \t [0.84838229 0.20551767 0.16274029]. \t  0.33426164618721066 \t 2.8719090663093145\n",
      "82     \t [0.77347584 0.87045509 0.44719768]. \t  0.5746339792531419 \t 2.8719090663093145\n",
      "83     \t [0.16139631 0.46238916 0.78552255]. \t  \u001b[92m3.251086026222847\u001b[0m \t 3.251086026222847\n",
      "84     \t [0.10866201 0.99233189 0.23197554]. \t  0.11479906331647455 \t 3.251086026222847\n",
      "85     \t [0.37476994 0.31829582 0.60200766]. \t  0.7296656666731571 \t 3.251086026222847\n",
      "86     \t [0.18500925 0.22300896 0.9396028 ]. \t  1.0334441944355386 \t 3.251086026222847\n",
      "87     \t [0.54308473 0.00958795 0.70005553]. \t  0.22044441229983486 \t 3.251086026222847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.9829968  0.40932226 0.31918833]. \t  0.13524209048613012 \t 3.251086026222847\n",
      "89     \t [0.37442103 0.14720456 0.47257362]. \t  0.3282547562424142 \t 3.251086026222847\n",
      "90     \t [0.26555416 0.90367243 0.20509091]. \t  0.07236249546661194 \t 3.251086026222847\n",
      "91     \t [0.98474665 0.26011823 0.27835028]. \t  0.2613051291398584 \t 3.251086026222847\n",
      "92     \t [0.18076704 0.39008464 0.72453103]. \t  2.2078290196667414 \t 3.251086026222847\n",
      "93     \t [0.80312929 0.83662864 0.03393309]. \t  0.0008291148136824575 \t 3.251086026222847\n",
      "94     \t [0.09302491 0.79875913 0.5630793 ]. \t  2.9832761741344616 \t 3.251086026222847\n",
      "95     \t [0.17686814 0.49253737 0.3987434 ]. \t  0.48243138060499385 \t 3.251086026222847\n",
      "96     \t [0.9275797  0.07469242 0.68413499]. \t  0.3388110912471061 \t 3.251086026222847\n",
      "97     \t [0.44349814 0.58277788 0.84609866]. \t  \u001b[92m3.812892773492558\u001b[0m \t 3.812892773492558\n",
      "98     \t [0.30155503 0.20821853 0.35423729]. \t  0.7364550500034381 \t 3.812892773492558\n",
      "99     \t [0.16661083 0.071875   0.54314697]. \t  0.17001335525364347 \t 3.812892773492558\n",
      "100    \t [0.60423685 0.40628405 0.97634747]. \t  1.9563753652284963 \t 3.812892773492558\n"
     ]
    }
   ],
   "source": [
    "### 6(n). Bayesian optimization runs (x20): GP run number = 14\n",
    "\n",
    "np.random.seed(run_num_14)\n",
    "surrogate_gp_14 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "gpgo_gp_14 = GPGO(surrogate_gp_14, Acquisition_new(util_gp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_14.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.2675596  0.81347184 0.49218523]. \t  2.428328343373925 \t 2.428328343373925\n",
      "init   \t [0.56304629 0.02422557 0.21370975]. \t  0.7518304825980339 \t 2.428328343373925\n",
      "init   \t [0.19880715 0.05679138 0.9538148 ]. \t  0.24566381538681226 \t 2.428328343373925\n",
      "init   \t [0.44009759 0.56618711 0.0651964 ]. \t  0.039090337622666706 \t 2.428328343373925\n",
      "init   \t [0.04583571 0.63038572 0.52252221]. \t  1.7734998804260111 \t 2.428328343373925\n",
      "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.428328343373925\n",
      "2      \t [0. 1. 1.]. \t  0.330219860606422 \t 2.428328343373925\n",
      "3      \t [1.         0.         0.96232079]. \t  0.12480718829816771 \t 2.428328343373925\n",
      "4      \t [0.         1.         0.06417483]. \t  0.0018858181137534889 \t 2.428328343373925\n",
      "5      \t [1.         1.         0.25428756]. \t  0.015887764445075205 \t 2.428328343373925\n",
      "6      \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.428328343373925\n",
      "7      \t [1. 0. 0.]. \t  0.03095471703300515 \t 2.428328343373925\n",
      "8      \t [1.         0.47679518 0.59874449]. \t  0.7564954108550984 \t 2.428328343373925\n",
      "9      \t [0.51790019 0.5919769  1.        ]. \t  2.0718440367834994 \t 2.428328343373925\n",
      "10     \t [0.53847819 1.         0.82574134]. \t  0.6686175262333268 \t 2.428328343373925\n",
      "11     \t [0.         0.49016624 1.        ]. \t  1.9402310325018288 \t 2.428328343373925\n",
      "12     \t [0.57197958 1.         0.        ]. \t  0.000174186615099307 \t 2.428328343373925\n",
      "13     \t [1.         0.48750828 1.        ]. \t  1.8874737138341708 \t 2.428328343373925\n",
      "14     \t [0.        0.        0.4910536]. \t  0.1481976412314881 \t 2.428328343373925\n",
      "15     \t [1.         0.62376425 0.        ]. \t  0.002736447171928035 \t 2.428328343373925\n",
      "16     \t [-1.38777878e-17  1.00000000e+00  5.24149966e-01]. \t  2.4113577089306166 \t 2.428328343373925\n",
      "17     \t [ 4.3450491e-01  0.0000000e+00 -6.9388939e-18]. \t  0.10093488508897662 \t 2.428328343373925\n",
      "18     \t [0.40294706 0.34723361 0.70781201]. \t  1.7735551233288778 \t 2.428328343373925\n",
      "19     \t [0.70240281 0.74336399 0.50941084]. \t  0.9149543314691648 \t 2.428328343373925\n",
      "20     \t [1.         0.         0.52562896]. \t  0.06773704194655396 \t 2.428328343373925\n",
      "21     \t [0.29669433 1.         0.32720941]. \t  0.4870855647284487 \t 2.428328343373925\n",
      "22     \t [0.16030758 0.76331824 0.79615317]. \t  \u001b[92m2.5526091382236227\u001b[0m \t 2.5526091382236227\n",
      "23     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.5526091382236227\n",
      "24     \t [1.         1.         0.70580147]. \t  0.3341225323529623 \t 2.5526091382236227\n",
      "25     \t [0. 0. 1.]. \t  0.0902894676548261 \t 2.5526091382236227\n",
      "26     \t [0.74501377 0.22745363 1.        ]. \t  0.6674837395584828 \t 2.5526091382236227\n",
      "27     \t [0.         0.65495383 0.        ]. \t  0.004491042249306987 \t 2.5526091382236227\n",
      "28     \t [0.53731506 0.         0.67851015]. \t  0.18146570181122065 \t 2.5526091382236227\n",
      "29     \t [0.28373499 0.         0.32727558]. \t  0.7666610885854449 \t 2.5526091382236227\n",
      "30     \t [1.         0.21099268 0.18810371]. \t  0.22968460071351954 \t 2.5526091382236227\n",
      "31     \t [0.80570262 0.25690882 0.        ]. \t  0.05438946820126229 \t 2.5526091382236227\n",
      "32     \t [1.         0.20821119 0.82362421]. \t  1.2721328744873275 \t 2.5526091382236227\n",
      "33     \t [0.27833156 1.         1.        ]. \t  0.33432016652568614 \t 2.5526091382236227\n",
      "34     \t [0.         0.20081643 0.05871764]. \t  0.16803308180121002 \t 2.5526091382236227\n",
      "35     \t [1.         0.77902888 0.93495192]. \t  1.951084772101676 \t 2.5526091382236227\n",
      "36     \t [0.01171911 0.24260937 0.80517108]. \t  1.5698304168841728 \t 2.5526091382236227\n",
      "37     \t [0.27536445 0.98439867 0.        ]. \t  0.00030075644334877447 \t 2.5526091382236227\n",
      "38     \t [0.00870206 0.73928667 0.84972107]. \t  \u001b[92m2.821226444291576\u001b[0m \t 2.821226444291576\n",
      "39     \t [0.8367932  0.88835361 0.99999988]. \t  0.7410410818442232 \t 2.821226444291576\n",
      "40     \t [1.15907514e-08 7.45138335e-01 3.26643004e-01]. \t  0.5286925180866969 \t 2.821226444291576\n",
      "41     \t [5.04856055e-08 1.00000000e+00 7.41439738e-01]. \t  1.1719786984659941 \t 2.821226444291576\n",
      "42     \t [2.80849349e-01 2.06065523e-01 2.09469858e-07]. \t  0.1058183779238044 \t 2.821226444291576\n",
      "43     \t [0.49584545 0.         1.        ]. \t  0.09165123595322316 \t 2.821226444291576\n",
      "44     \t [0.401542   0.75327527 0.7405823 ]. \t  2.302604812473974 \t 2.821226444291576\n",
      "45     \t [2.62309614e-08 7.50117529e-01 1.00000000e+00]. \t  1.4854118354546213 \t 2.821226444291576\n",
      "46     \t [0.77901261 0.78018607 0.01186368]. \t  0.0011524023449615128 \t 2.821226444291576\n",
      "47     \t [0.7425171  0.59858601 0.82875014]. \t  \u001b[92m3.6092423133595553\u001b[0m \t 3.6092423133595553\n",
      "48     \t [0.75185789 0.54981327 0.91445898]. \t  3.4298806027153446 \t 3.6092423133595553\n",
      "49     \t [0.85208302 0.5397985  0.86791366]. \t  \u001b[92m3.7188284275491386\u001b[0m \t 3.7188284275491386\n",
      "50     \t [0.91682721 0.56944833 0.87998434]. \t  3.648766907524473 \t 3.7188284275491386\n",
      "51     \t [0.76206431 0.54310416 0.86481002]. \t  \u001b[92m3.760228251906166\u001b[0m \t 3.760228251906166\n",
      "52     \t [0.71825401 0.50236372 0.84640334]. \t  3.697956806135659 \t 3.760228251906166\n",
      "53     \t [0.79581973 0.52037045 0.90075917]. \t  3.512279227296796 \t 3.760228251906166\n",
      "54     \t [0.80658856 0.62260113 0.87204099]. \t  3.5652788824034465 \t 3.760228251906166\n",
      "55     \t [0.73691661 0.52504988 0.82126201]. \t  3.6453964815268702 \t 3.760228251906166\n",
      "56     \t [0.84078387 0.60144896 0.88914332]. \t  3.5634857099621704 \t 3.760228251906166\n",
      "57     \t [0.83091083 0.57609002 0.89956237]. \t  3.5466877229648905 \t 3.760228251906166\n",
      "58     \t [3.59713022e-05 0.00000000e+00 2.06060210e-01]. \t  0.5181459005459511 \t 3.760228251906166\n",
      "59     \t [0.78313481 0.62690027 0.78570593]. \t  3.0776873276733077 \t 3.760228251906166\n",
      "60     \t [0.86723216 0.49263278 0.79866458]. \t  3.3481182310469007 \t 3.760228251906166\n",
      "61     \t [0.83415323 0.5790825  0.86409468]. \t  3.7126150455330427 \t 3.760228251906166\n",
      "62     \t [0.81998863 0.43067742 0.76762711]. \t  2.796415739159193 \t 3.760228251906166\n",
      "63     \t [0.63038908 0.5944093  0.83680103]. \t  3.7055577618055504 \t 3.760228251906166\n",
      "64     \t [0.53056133 0.60182287 0.76698738]. \t  3.1062989420861387 \t 3.760228251906166\n",
      "65     \t [0.63361018 0.54576757 0.90436633]. \t  3.562972061264225 \t 3.760228251906166\n",
      "66     \t [0.74325082 0.57598484 0.81746879]. \t  3.595815496442756 \t 3.760228251906166\n",
      "67     \t [0.28448905 0.5748572  0.82456589]. \t  \u001b[92m3.7704450862539716\u001b[0m \t 3.7704450862539716\n",
      "68     \t [0.02191762 0.66578366 0.71000136]. \t  2.616245356035985 \t 3.7704450862539716\n",
      "69     \t [0.84082956 0.00722323 0.76131241]. \t  0.2580120013156531 \t 3.7704450862539716\n",
      "70     \t [0.57248872 0.64398303 0.80758809]. \t  3.320082130387651 \t 3.7704450862539716\n",
      "71     \t [0.61886487 0.54322164 0.84081597]. \t  \u001b[92m3.791391046194009\u001b[0m \t 3.791391046194009\n",
      "72     \t [0.92137585 0.53884103 0.89325863]. \t  3.56475931723819 \t 3.791391046194009\n",
      "73     \t [0.18690052 0.56048546 0.79116385]. \t  3.537292499561646 \t 3.791391046194009\n",
      "74     \t [0.82716083 0.53904936 0.8994517 ]. \t  3.549111687234875 \t 3.791391046194009\n",
      "75     \t [0.59471786 0.5754837  0.84764494]. \t  \u001b[92m3.794037429761734\u001b[0m \t 3.794037429761734\n",
      "76     \t [0.61143215 0.53848258 0.88830694]. \t  3.6943903210714275 \t 3.794037429761734\n",
      "77     \t [0.2628241  0.58797375 0.80691155]. \t  3.6343890577040625 \t 3.794037429761734\n",
      "78     \t [0.43755615 0.52876305 0.83998916]. \t  \u001b[92m3.8137878442115394\u001b[0m \t 3.8137878442115394\n",
      "79     \t [0.25852174 0.65971272 0.79553018]. \t  3.2625669283918626 \t 3.8137878442115394\n",
      "80     \t [0.77726928 0.53850342 0.88823768]. \t  3.6512352500845395 \t 3.8137878442115394\n",
      "81     \t [0.39296457 0.62285751 0.86057507]. \t  3.6918082372164847 \t 3.8137878442115394\n",
      "82     \t [0.79115306 0.55860224 0.89475028]. \t  3.6109395093382823 \t 3.8137878442115394\n",
      "83     \t [0.26713644 0.55405265 0.85013779]. \t  \u001b[92m3.860389750337692\u001b[0m \t 3.860389750337692\n",
      "84     \t [0.36688031 0.56599877 0.84931128]. \t  3.852114261249267 \t 3.860389750337692\n",
      "85     \t [0.74350771 0.54254395 0.82495509]. \t  3.6790386912614106 \t 3.860389750337692\n",
      "86     \t [0.43892641 0.51932081 0.85831093]. \t  3.8069176870036165 \t 3.860389750337692\n",
      "87     \t [0.95409673 0.58287662 0.81374186]. \t  3.4536296427897857 \t 3.860389750337692\n",
      "88     \t [0.89361757 0.5418451  0.87303569]. \t  3.6893727025012533 \t 3.860389750337692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.60760785 0.4654605  0.88208088]. \t  3.4666473381068696 \t 3.860389750337692\n",
      "90     \t [0.58011256 0.54431529 0.87390556]. \t  3.7847800893764356 \t 3.860389750337692\n",
      "91     \t [0.39722201 0.38839939 0.89057885]. \t  2.8360217552444116 \t 3.860389750337692\n",
      "92     \t [0.25169817 0.51473109 0.88414634]. \t  3.6967660609933874 \t 3.860389750337692\n",
      "93     \t [0.70846056 0.56409575 0.85367969]. \t  3.781570842206442 \t 3.860389750337692\n",
      "94     \t [0.75049077 0.59909164 0.86914242]. \t  3.684161854756759 \t 3.860389750337692\n",
      "95     \t [0.17610202 0.61119845 0.78232732]. \t  3.368444879224569 \t 3.860389750337692\n",
      "96     \t [0.79118395 0.55266546 0.88412569]. \t  3.682077170632363 \t 3.860389750337692\n",
      "97     \t [0.44349812 0.58277786 0.84609863]. \t  3.8128927669305543 \t 3.860389750337692\n",
      "98     \t [0.98453707 0.59520663 0.88698695]. \t  3.5359705415627696 \t 3.860389750337692\n",
      "99     \t [0.27403501 0.50165326 0.77545875]. \t  3.303860227199906 \t 3.860389750337692\n",
      "100    \t [0.3466964  0.49275043 0.81526213]. \t  3.6183034763105133 \t 3.860389750337692\n"
     ]
    }
   ],
   "source": [
    "### 6(n). Bayesian optimization runs (x20): STP DF1 run number = 14\n",
    "\n",
    "np.random.seed(run_num_14)\n",
    "surrogate_stp_df1_14 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_14 = GPGO(surrogate_stp_df1_14, Acquisition_new(util_stp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_14.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.9979902908083966, -6.036357457277745)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(n). Training Regret Minimisation: run number = 14\n",
    "\n",
    "gp_output_14 = np.append(np.max(gpgo_gp_14.GP.y[0:n_init]),gpgo_gp_14.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_14 = np.append(np.max(gpgo_stp_df1_14.GP.y[0:n_init]),gpgo_stp_df1_14.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_14 = np.log(y_global_orig - gp_output_14)\n",
    "regret_stp_df1_14 = np.log(y_global_orig - stp_df1_output_14)\n",
    "\n",
    "train_regret_gp_14 = min_max_array(regret_gp_14)\n",
    "train_regret_stp_df1_14 = min_max_array(regret_stp_df1_14)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 14\n",
    "min_train_regret_gp_14 = min(train_regret_gp_14)\n",
    "min_train_regret_stp_df1_14 = min(train_regret_stp_df1_14)\n",
    "\n",
    "min_train_regret_gp_14, min_train_regret_stp_df1_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.06316912 0.76071872 0.59230514]. \t  2.815992429281572 \t 2.815992429281572\n",
      "init   \t [0.8572013  0.99712197 0.76862666]. \t  0.5047463350328246 \t 2.815992429281572\n",
      "init   \t [0.22375561 0.05793206 0.93742381]. \t  0.27856475705289063 \t 2.815992429281572\n",
      "init   \t [0.95453652 0.99278142 0.76539157]. \t  0.46591181545515986 \t 2.815992429281572\n",
      "init   \t [0.30237363 0.42979586 0.31179806]. \t  0.41512025652926615 \t 2.815992429281572\n",
      "1      \t [0. 1. 1.]. \t  0.330219860606422 \t 2.815992429281572\n",
      "2      \t [0.         1.         0.17421032]. \t  0.03224223892902828 \t 2.815992429281572\n",
      "3      \t [0.02669843 0.47112713 0.48823069]. \t  0.6816380024830667 \t 2.815992429281572\n",
      "4      \t [1.         0.41025789 0.60632505]. \t  0.7751657466388402 \t 2.815992429281572\n",
      "5      \t [0.90339828 0.97077761 0.10542623]. \t  0.0010957559899661433 \t 2.815992429281572\n",
      "6      \t [0.29526442 0.61780464 0.76910819]. \t  \u001b[92m3.2038470358622266\u001b[0m \t 3.2038470358622266\n",
      "7      \t [0.95043929 0.16247814 0.88163648]. \t  0.8581953732852365 \t 3.2038470358622266\n",
      "8      \t [0.69812289 0.4531111  0.93002975]. \t  2.901300173352809 \t 3.2038470358622266\n",
      "9      \t [0.29380178 0.61316502 0.98942281]. \t  2.2332694531023587 \t 3.2038470358622266\n",
      "10     \t [0.37838935 0.77994242 0.58766715]. \t  2.39533984072221 \t 3.2038470358622266\n",
      "11     \t [0.5447637  0.19363751 0.74113823]. \t  1.0351989877586254 \t 3.2038470358622266\n",
      "12     \t [0.80892377 0.36851258 0.06682555]. \t  0.08904027805896103 \t 3.2038470358622266\n",
      "13     \t [0.84300747 0.61662339 0.47891499]. \t  0.3506452111120594 \t 3.2038470358622266\n",
      "14     \t [0.95468859 0.50612166 0.94762744]. \t  2.826126162869765 \t 3.2038470358622266\n",
      "15     \t [0.57855227 0.41279245 0.56837527]. \t  0.675883659802959 \t 3.2038470358622266\n",
      "16     \t [0.41244311 0.14574025 0.02781816]. \t  0.1764921465299794 \t 3.2038470358622266\n",
      "17     \t [0.21716153 0.95873902 0.22191961]. \t  0.09785370385138205 \t 3.2038470358622266\n",
      "18     \t [0.11370372 0.69805197 0.19949817]. \t  0.07463000369489746 \t 3.2038470358622266\n",
      "19     \t [0.2196108  0.94463538 0.66650583]. \t  2.1197828289287006 \t 3.2038470358622266\n",
      "20     \t [0.82446884 0.73015087 0.99204038]. \t  1.6844545386492624 \t 3.2038470358622266\n",
      "21     \t [0.51630631 0.74864284 0.66915589]. \t  1.902955768509601 \t 3.2038470358622266\n",
      "22     \t [0.51545516 0.70153362 0.88629729]. \t  3.0708104215134746 \t 3.2038470358622266\n",
      "23     \t [0.06667919 0.85833706 0.91251487]. \t  1.5128002687861792 \t 3.2038470358622266\n",
      "24     \t [0.06856638 0.31883947 0.9311414 ]. \t  1.8345672982939993 \t 3.2038470358622266\n",
      "25     \t [0.84375477 0.3818965  0.06661537]. \t  0.07533314332898496 \t 3.2038470358622266\n",
      "26     \t [0.5388972  0.04050493 0.50114925]. \t  0.1999583811484369 \t 3.2038470358622266\n",
      "27     \t [0.73124469 0.86473898 0.20105776]. \t  0.02424538897543339 \t 3.2038470358622266\n",
      "28     \t [0.88713179 0.14395217 0.5778976 ]. \t  0.22763732354974492 \t 3.2038470358622266\n",
      "29     \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.2038470358622266\n",
      "30     \t [0.7649158  0.33621666 0.8101996 ]. \t  2.4253396020481377 \t 3.2038470358622266\n",
      "31     \t [0.04983372 0.09650037 0.45703684]. \t  0.27436049563012854 \t 3.2038470358622266\n",
      "32     \t [0.50354723 0.7062012  0.70981095]. \t  2.192576773047204 \t 3.2038470358622266\n",
      "33     \t [0.42818886 0.59932441 0.77816143]. \t  \u001b[92m3.2966376313752663\u001b[0m \t 3.2966376313752663\n",
      "34     \t [0.93309439 0.69406248 0.82608633]. \t  2.9256013639999696 \t 3.2966376313752663\n",
      "35     \t [0.21254828 0.68551302 0.12293952]. \t  0.027242714267588525 \t 3.2966376313752663\n",
      "36     \t [0.74370817 0.67644337 0.87702804]. \t  3.2434280019403805 \t 3.2966376313752663\n",
      "37     \t [0.16644168 0.65393778 0.38296435]. \t  0.7990458500046351 \t 3.2966376313752663\n",
      "38     \t [0.41707989 0.77654625 0.21425985]. \t  0.07532208216965296 \t 3.2966376313752663\n",
      "39     \t [0.75288249 0.46006609 0.35467232]. \t  0.21035671832041264 \t 3.2966376313752663\n",
      "40     \t [0.87896481 0.30050093 0.57231214]. \t  0.42455202023487115 \t 3.2966376313752663\n",
      "41     \t [0.67333597 0.6033247  0.99173011]. \t  2.1907339471298144 \t 3.2966376313752663\n",
      "42     \t [0.79354321 0.9418323  0.2314953 ]. \t  0.03120127854084222 \t 3.2966376313752663\n",
      "43     \t [0.33749499 0.68726912 0.79295916]. \t  3.0534762090298955 \t 3.2966376313752663\n",
      "44     \t [0.10039698 0.95124625 0.14738827]. \t  0.019952856316917282 \t 3.2966376313752663\n",
      "45     \t [0.0233801  0.94807299 0.26655372]. \t  0.230575616596963 \t 3.2966376313752663\n",
      "46     \t [0.14161798 0.71515604 0.99324375]. \t  1.7759665350918103 \t 3.2966376313752663\n",
      "47     \t [0.64028076 0.71282693 0.529977  ]. \t  1.1219804648143206 \t 3.2966376313752663\n",
      "48     \t [0.92282724 0.61096664 0.10881235]. \t  0.016872832060101057 \t 3.2966376313752663\n",
      "49     \t [0.6038186  0.11510028 0.60229056]. \t  0.2601323453802982 \t 3.2966376313752663\n",
      "50     \t [0.62411196 0.6226603  0.28780791]. \t  0.1488111882723314 \t 3.2966376313752663\n",
      "51     \t [0.86962667 0.03499691 0.96012405]. \t  0.18549291941528026 \t 3.2966376313752663\n",
      "52     \t [0.02236032 0.0672237  0.2014036 ]. \t  0.5974253006329651 \t 3.2966376313752663\n",
      "53     \t [0.36687357 0.19442051 0.53988076]. \t  0.28512465086938316 \t 3.2966376313752663\n",
      "54     \t [0.65326538 0.98710242 0.27744366]. \t  0.10843291381178163 \t 3.2966376313752663\n",
      "55     \t [0.07294658 0.69587552 0.49641757]. \t  2.0586132804459814 \t 3.2966376313752663\n",
      "56     \t [0.04721857 0.62269705 0.54609772]. \t  1.8485140567948626 \t 3.2966376313752663\n",
      "57     \t [0.90157601 0.86666908 0.35482778]. \t  0.13964481718420316 \t 3.2966376313752663\n",
      "58     \t [0.85414093 0.51355974 0.56223869]. \t  0.584103007026971 \t 3.2966376313752663\n",
      "59     \t [0.85824984 0.74524528 0.74924054]. \t  1.8646001166767896 \t 3.2966376313752663\n",
      "60     \t [0.64227937 0.50426213 0.70335951]. \t  2.22220614946086 \t 3.2966376313752663\n",
      "61     \t [0.91666806 0.60831868 0.94902463]. \t  2.8497384667315075 \t 3.2966376313752663\n",
      "62     \t [0.81943243 0.90007382 0.8407786 ]. \t  1.1810197455017497 \t 3.2966376313752663\n",
      "63     \t [0.93811628 0.85101672 0.73331915]. \t  1.0187964599097161 \t 3.2966376313752663\n",
      "64     \t [0.93619507 0.63924542 0.59777593]. \t  0.7452385661614155 \t 3.2966376313752663\n",
      "65     \t [0.44485289 0.4762383  0.65698415]. \t  1.7047519059669405 \t 3.2966376313752663\n",
      "66     \t [0.83595694 0.12972837 0.52357605]. \t  0.1588934422945956 \t 3.2966376313752663\n",
      "67     \t [0.75807223 0.30633117 0.93407259]. \t  1.6972320629720699 \t 3.2966376313752663\n",
      "68     \t [0.73381428 0.97690053 0.0531491 ]. \t  0.0005452223280698152 \t 3.2966376313752663\n",
      "69     \t [0.98605122 0.20517633 0.77146236]. \t  1.1760282810866616 \t 3.2966376313752663\n",
      "70     \t [0.56392166 0.59906558 0.84312232]. \t  \u001b[92m3.73350966988941\u001b[0m \t 3.73350966988941\n",
      "71     \t [0.05619795 0.90132437 0.96560794]. \t  0.8995147331165977 \t 3.73350966988941\n",
      "72     \t [0.23876075 0.01783234 0.77168053]. \t  0.2933487655261241 \t 3.73350966988941\n",
      "73     \t [0.9023515  0.52148907 0.1976739 ]. \t  0.07460848473203677 \t 3.73350966988941\n",
      "74     \t [0.86276321 0.30769385 0.30136087]. \t  0.327200797304597 \t 3.73350966988941\n",
      "75     \t [0.26276875 0.97210142 0.78642795]. \t  1.0462616982315733 \t 3.73350966988941\n",
      "76     \t [0.65655842 0.86051706 0.09852464]. \t  0.0036886269997759413 \t 3.73350966988941\n",
      "77     \t [0.94544274 0.47914846 0.13849199]. \t  0.0608519201150001 \t 3.73350966988941\n",
      "78     \t [0.16712037 0.13952191 0.70477072]. \t  0.6248432608750999 \t 3.73350966988941\n",
      "79     \t [0.36624412 0.28822171 0.50014611]. \t  0.3417730975066064 \t 3.73350966988941\n",
      "80     \t [0.64773767 0.44227878 0.73208168]. \t  2.468773431467027 \t 3.73350966988941\n",
      "81     \t [0.95114903 0.73250751 0.97559013]. \t  1.865117032496329 \t 3.73350966988941\n",
      "82     \t [0.67437208 0.74742522 0.37535168]. \t  0.3880860965191917 \t 3.73350966988941\n",
      "83     \t [0.7481914  0.25278779 0.95609257]. \t  1.1130935976993979 \t 3.73350966988941\n",
      "84     \t [0.67458098 0.17856566 0.24538005]. \t  0.7176422032261535 \t 3.73350966988941\n",
      "85     \t [0.28677243 0.51358561 0.67640074]. \t  2.1250079354431715 \t 3.73350966988941\n",
      "86     \t [0.27406537 0.74775875 0.41867983]. \t  1.3770341742763172 \t 3.73350966988941\n",
      "87     \t [0.51601731 0.08731972 0.3873258 ]. \t  0.6084112240149054 \t 3.73350966988941\n",
      "88     \t [0.33686471 0.61925688 0.19953727]. \t  0.10035134231975107 \t 3.73350966988941\n",
      "89     \t [0.86972299 0.88337309 0.77441157]. \t  1.080176385557773 \t 3.73350966988941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.38647637 0.95486288 0.18641965]. \t  0.038825273594486495 \t 3.73350966988941\n",
      "91     \t [0.03287067 0.48218412 0.05462369]. \t  0.04870324137371465 \t 3.73350966988941\n",
      "92     \t [0.29363417 0.09708416 0.79674127]. \t  0.5959012412009217 \t 3.73350966988941\n",
      "93     \t [0.94179129 0.50184413 0.34676439]. \t  0.10035288614329778 \t 3.73350966988941\n",
      "94     \t [0.80217129 0.08328516 0.5263348 ]. \t  0.14090515988897215 \t 3.73350966988941\n",
      "95     \t [0.32713242 0.98126207 0.7780154 ]. \t  0.9952313472044709 \t 3.73350966988941\n",
      "96     \t [0.92947639 0.69850932 0.36485003]. \t  0.11335234788684327 \t 3.73350966988941\n",
      "97     \t [0.42212143 0.11326198 0.90871572]. \t  0.5397724115912389 \t 3.73350966988941\n",
      "98     \t [0.57706903 0.27418219 0.81165212]. \t  1.8808138027415966 \t 3.73350966988941\n",
      "99     \t [0.50632792 0.69084152 0.88904218]. \t  3.145836729670486 \t 3.73350966988941\n",
      "100    \t [0.85272869 0.62203701 0.11639545]. \t  0.02048009780823121 \t 3.73350966988941\n"
     ]
    }
   ],
   "source": [
    "### 6(o). Bayesian optimization runs (x20): GP run number = 15\n",
    "\n",
    "np.random.seed(run_num_15)\n",
    "surrogate_gp_15 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "gpgo_gp_15 = GPGO(surrogate_gp_15, Acquisition_new(util_gp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_15.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.06316912 0.76071872 0.59230514]. \t  2.815992429281572 \t 2.815992429281572\n",
      "init   \t [0.8572013  0.99712197 0.76862666]. \t  0.5047463350328246 \t 2.815992429281572\n",
      "init   \t [0.22375561 0.05793206 0.93742381]. \t  0.27856475705289063 \t 2.815992429281572\n",
      "init   \t [0.95453652 0.99278142 0.76539157]. \t  0.46591181545515986 \t 2.815992429281572\n",
      "init   \t [0.30237363 0.42979586 0.31179806]. \t  0.41512025652926615 \t 2.815992429281572\n",
      "1      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 2.815992429281572\n",
      "2      \t [0. 1. 1.]. \t  0.330219860606422 \t 2.815992429281572\n",
      "3      \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.815992429281572\n",
      "4      \t [1. 1. 0.]. \t  3.772718517944387e-05 \t 2.815992429281572\n",
      "5      \t [1. 0. 0.]. \t  0.03095471703300515 \t 2.815992429281572\n",
      "6      \t [0.         0.         0.31868162]. \t  0.5361535890336683 \t 2.815992429281572\n",
      "7      \t [0.         1.         0.50814235]. \t  2.3203018435864897 \t 2.815992429281572\n",
      "8      \t [1.         0.42829136 0.51727942]. \t  0.26134642778763656 \t 2.815992429281572\n",
      "9      \t [1.         0.58540693 1.        ]. \t  2.002329575682301 \t 2.815992429281572\n",
      "10     \t [0.         0.37709303 1.        ]. \t  1.4426865843251713 \t 2.815992429281572\n",
      "11     \t [0.         0.34259005 0.        ]. \t  0.04687425951809724 \t 2.815992429281572\n",
      "12     \t [1.         0.51733512 0.        ]. \t  0.007154949763866186 \t 2.815992429281572\n",
      "13     \t [0.48131636 0.5147023  1.        ]. \t  2.0296174371670324 \t 2.815992429281572\n",
      "14     \t [0.         0.4395687  0.54533394]. \t  0.7951045735001662 \t 2.815992429281572\n",
      "15     \t [0.31011583 0.         0.        ]. \t  0.10119214184534725 \t 2.815992429281572\n",
      "16     \t [0.33532544 1.         0.38222162]. \t  0.8983208021434519 \t 2.815992429281572\n",
      "17     \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.815992429281572\n",
      "18     \t [0. 0. 1.]. \t  0.0902894676548261 \t 2.815992429281572\n",
      "19     \t [0.71782626 0.         0.52090598]. \t  0.11863885220624643 \t 2.815992429281572\n",
      "20     \t [0.58867586 0.80214139 0.        ]. \t  0.0010680907424097093 \t 2.815992429281572\n",
      "21     \t [0.         0.78605619 0.28568387]. \t  0.31371110974198924 \t 2.815992429281572\n",
      "22     \t [0.25900258 0.74799586 0.78530429]. \t  2.6166138805600463 \t 2.815992429281572\n",
      "23     \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.815992429281572\n",
      "24     \t [0.         0.80230736 0.79697623]. \t  2.2090156862498604 \t 2.815992429281572\n",
      "25     \t [1.         0.         0.46767146]. \t  0.09064232698432073 \t 2.815992429281572\n",
      "26     \t [0.8735053  0.2615685  0.89456646]. \t  1.5718332836276503 \t 2.815992429281572\n",
      "27     \t [0.16979333 0.99999999 0.710134  ]. \t  1.4556335931218831 \t 2.815992429281572\n",
      "28     \t [3.56113312e-09 0.00000000e+00 6.94837643e-01]. \t  0.19277223690175266 \t 2.815992429281572\n",
      "29     \t [0.76065517 0.67850117 0.81621801]. \t  \u001b[92m3.0629925821407524\u001b[0m \t 3.0629925821407524\n",
      "30     \t [0.30789006 1.         0.        ]. \t  0.00026995401871248375 \t 3.0629925821407524\n",
      "31     \t [0.74077423 0.22848808 0.        ]. \t  0.06838337316922515 \t 3.0629925821407524\n",
      "32     \t [0.68437668 0.         1.        ]. \t  0.09099248804805009 \t 3.0629925821407524\n",
      "33     \t [0.49962456 0.58431558 0.6787819 ]. \t  2.0883979849343657 \t 3.0629925821407524\n",
      "34     \t [1.         0.94625372 0.31490176]. \t  0.04727301765011998 \t 3.0629925821407524\n",
      "35     \t [0.74388522 0.74901329 1.        ]. \t  1.4894398332478669 \t 3.0629925821407524\n",
      "36     \t [7.89426370e-01 3.41113747e-10 1.68679086e-01]. \t  0.38320500197028445 \t 3.0629925821407524\n",
      "37     \t [0.80250824 0.76556253 0.53619686]. \t  0.7446219331170731 \t 3.0629925821407524\n",
      "38     \t [2.34080325e-01 6.24294858e-01 5.04517890e-09]. \t  0.008618722332020485 \t 3.0629925821407524\n",
      "39     \t [0.99999999 0.68083886 0.77856962]. \t  2.549154064559517 \t 3.0629925821407524\n",
      "40     \t [0.18893884 0.50506904 0.8512768 ]. \t  \u001b[92m3.768020611074639\u001b[0m \t 3.768020611074639\n",
      "41     \t [0.16205141 0.56127915 0.81250427]. \t  3.7076925576830737 \t 3.768020611074639\n",
      "42     \t [0.15223994 0.57474367 0.86617659]. \t  \u001b[92m3.8198289980820936\u001b[0m \t 3.8198289980820936\n",
      "43     \t [0.77798506 1.         0.02265967]. \t  0.00017819135255724982 \t 3.8198289980820936\n",
      "44     \t [0.07999052 0.64910024 0.96308797]. \t  2.5532051665675186 \t 3.8198289980820936\n",
      "45     \t [0.97710236 0.         0.77161137]. \t  0.24130348454569117 \t 3.8198289980820936\n",
      "46     \t [0.15701454 0.5966945  0.79807407]. \t  3.5438814171872477 \t 3.8198289980820936\n",
      "47     \t [0.20606369 0.53431204 0.8100683 ]. \t  3.6859705940716054 \t 3.8198289980820936\n",
      "48     \t [0.42964071 0.99999999 1.        ]. \t  0.3333841224782849 \t 3.8198289980820936\n",
      "49     \t [0.92038554 0.53747339 0.88794517]. \t  3.60050568425834 \t 3.8198289980820936\n",
      "50     \t [0.23226862 0.00502955 0.47760467]. \t  0.23732929527136326 \t 3.8198289980820936\n",
      "51     \t [0.71604302 0.57187742 0.86780759]. \t  3.7598150077733368 \t 3.8198289980820936\n",
      "52     \t [0.83204629 0.50500535 0.83482389]. \t  3.6378273056511174 \t 3.8198289980820936\n",
      "53     \t [0.85620702 0.55586329 0.88464066]. \t  3.656424221133082 \t 3.8198289980820936\n",
      "54     \t [0.22084032 0.2126845  0.00318435]. \t  0.10539981013500938 \t 3.8198289980820936\n",
      "55     \t [0.66015422 0.51658058 0.873215  ]. \t  3.721736728156196 \t 3.8198289980820936\n",
      "56     \t [0.13575133 0.58015888 0.81737922]. \t  3.7142702654634783 \t 3.8198289980820936\n",
      "57     \t [0.69936931 0.55842728 0.8485485 ]. \t  3.7829447224558685 \t 3.8198289980820936\n",
      "58     \t [0.12936716 0.63064212 0.75012294]. \t  3.0130144855646788 \t 3.8198289980820936\n",
      "59     \t [0.39485031 0.53712384 0.87722356]. \t  3.787384968824324 \t 3.8198289980820936\n",
      "60     \t [0.79303374 0.53800355 0.84312094]. \t  3.7369740500416553 \t 3.8198289980820936\n",
      "61     \t [0.74663349 0.55018923 0.78684815]. \t  3.331253586012915 \t 3.8198289980820936\n",
      "62     \t [0.15891933 0.47868124 0.86537293]. \t  3.6262460102982814 \t 3.8198289980820936\n",
      "63     \t [0.21203735 0.52544823 0.7984502 ]. \t  3.5825565792795757 \t 3.8198289980820936\n",
      "64     \t [0.18296758 0.58845311 0.81452389]. \t  3.686115919928222 \t 3.8198289980820936\n",
      "65     \t [0.65448019 0.54288478 0.82386221]. \t  3.704407429440085 \t 3.8198289980820936\n",
      "66     \t [0.13303201 0.59901735 0.8672027 ]. \t  3.7629994513763467 \t 3.8198289980820936\n",
      "67     \t [0.7771475  0.54587305 0.87889825]. \t  3.7111060577530814 \t 3.8198289980820936\n",
      "68     \t [0.28967536 0.58768216 0.80963756]. \t  3.6527837123399496 \t 3.8198289980820936\n",
      "69     \t [0.10091137 0.5306309  0.88671298]. \t  3.6966921380736864 \t 3.8198289980820936\n",
      "70     \t [0.17770074 0.51949727 0.7998764 ]. \t  3.5812322605415803 \t 3.8198289980820936\n",
      "71     \t [0.54752165 0.52736737 0.87138243]. \t  3.7776432001774642 \t 3.8198289980820936\n",
      "72     \t [0.16025334 0.48128459 0.88694844]. \t  3.530190861154934 \t 3.8198289980820936\n",
      "73     \t [0.20256594 0.6170095  0.82726376]. \t  3.666834903583533 \t 3.8198289980820936\n",
      "74     \t [0.33919048 0.57079911 0.88728381]. \t  3.739205211241676 \t 3.8198289980820936\n",
      "75     \t [0.88413943 0.57154155 0.85385937]. \t  3.7062450160442983 \t 3.8198289980820936\n",
      "76     \t [0.12788334 0.56687299 0.81730311]. \t  3.729418765356158 \t 3.8198289980820936\n",
      "77     \t [0.99836015 0.53200561 0.84572781]. \t  3.650978649702005 \t 3.8198289980820936\n",
      "78     \t [0.46719305 0.56981365 0.87737841]. \t  3.786846716912595 \t 3.8198289980820936\n",
      "79     \t [0.68037186 0.55748272 0.87407648]. \t  3.763132481140425 \t 3.8198289980820936\n",
      "80     \t [0.75671172 0.48909337 0.79643158]. \t  3.3589971934304272 \t 3.8198289980820936\n",
      "81     \t [0.56482185 0.5081557  0.86211499]. \t  3.7507096426379816 \t 3.8198289980820936\n",
      "82     \t [0.39867388 0.59041775 0.86547852]. \t  3.7990396373681574 \t 3.8198289980820936\n",
      "83     \t [0.22908738 0.51489482 0.91288192]. \t  3.4325144003902572 \t 3.8198289980820936\n",
      "84     \t [0.72319204 0.51933297 0.88232645]. \t  3.6682097194166152 \t 3.8198289980820936\n",
      "85     \t [0.13675968 0.53682185 0.85331081]. \t  \u001b[92m3.8347921626218113\u001b[0m \t 3.8347921626218113\n",
      "86     \t [0.22605233 0.52859194 0.79087928]. \t  3.5193168318822 \t 3.8347921626218113\n",
      "87     \t [0.23154661 0.60569064 0.83620661]. \t  3.743700000477597 \t 3.8347921626218113\n",
      "88     \t [0.70564437 0.49429143 0.89412955]. \t  3.5069160314380774 \t 3.8347921626218113\n",
      "89     \t [0.52939384 0.49277182 0.86710498]. \t  3.687385667409025 \t 3.8347921626218113\n",
      "90     \t [0.37999286 0.52411717 0.82024666]. \t  3.730708553848684 \t 3.8347921626218113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91     \t [0.60229199 0.52018284 0.85684017]. \t  3.7821129930631385 \t 3.8347921626218113\n",
      "92     \t [0.24951463 0.58223351 0.84326031]. \t  3.8248882089886997 \t 3.8347921626218113\n",
      "93     \t [0.18828832 0.49052835 0.82742073]. \t  3.666893149720266 \t 3.8347921626218113\n",
      "94     \t [0.44983003 0.49831575 0.88202988]. \t  3.6516163660896535 \t 3.8347921626218113\n",
      "95     \t [0.86363676 0.50736784 0.84873837]. \t  3.665639706641312 \t 3.8347921626218113\n",
      "96     \t [0.7542967  0.55346234 0.85539877]. \t  3.772709795722934 \t 3.8347921626218113\n",
      "97     \t [0.20481654 0.60033631 0.86247222]. \t  3.7793107204056793 \t 3.8347921626218113\n",
      "98     \t [0.08661468 0.62386668 0.83974456]. \t  3.6625885258202495 \t 3.8347921626218113\n",
      "99     \t [0.15048833 0.58883546 0.87183725]. \t  3.77755694245171 \t 3.8347921626218113\n",
      "100    \t [0.82464313 0.59009936 0.84611265]. \t  3.676580428189725 \t 3.8347921626218113\n"
     ]
    }
   ],
   "source": [
    "### 6(o). Bayesian optimization runs (x20): STP DF1 run number = 15\n",
    "\n",
    "np.random.seed(run_num_15)\n",
    "surrogate_stp_df1_15 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_15 = GPGO(surrogate_stp_df1_15, Acquisition_new(util_stp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_15.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.045849485037573, -3.5759852425273815)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(o). Training Regret Minimisation: run number = 15\n",
    "\n",
    "gp_output_15 = np.append(np.max(gpgo_gp_15.GP.y[0:n_init]),gpgo_gp_15.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_15 = np.append(np.max(gpgo_stp_df1_15.GP.y[0:n_init]),gpgo_stp_df1_15.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_15 = np.log(y_global_orig - gp_output_15)\n",
    "regret_stp_df1_15 = np.log(y_global_orig - stp_df1_output_15)\n",
    "\n",
    "train_regret_gp_15 = min_max_array(regret_gp_15)\n",
    "train_regret_stp_df1_15 = min_max_array(regret_stp_df1_15)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 15\n",
    "min_train_regret_gp_15 = min(train_regret_gp_15)\n",
    "min_train_regret_stp_df1_15 = min(train_regret_stp_df1_15)\n",
    "\n",
    "min_train_regret_gp_15, min_train_regret_stp_df1_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.86394833 0.316366   0.67215078]. \t  1.1947633822631252 \t 3.8084053754826726\n",
      "init   \t [0.50791167 0.78166043 0.28368168]. \t  0.20052765717741963 \t 3.8084053754826726\n",
      "init   \t [0.23370878 0.56379969 0.87502436]. \t  3.8084053754826726 \t 3.8084053754826726\n",
      "init   \t [0.71894292 0.18213174 0.24380041]. \t  0.6532826728302027 \t 3.8084053754826726\n",
      "init   \t [0.08605673 0.53424539 0.83969965]. \t  3.8072565344663847 \t 3.8084053754826726\n",
      "1      \t [0.08056174 0.45877104 1.        ]. \t  1.8425210844311528 \t 3.8084053754826726\n",
      "2      \t [0.         1.         0.75308228]. \t  1.0928786208317511 \t 3.8084053754826726\n",
      "3      \t [0.         0.22301287 0.34500762]. \t  0.5095972263905909 \t 3.8084053754826726\n",
      "4      \t [0.24662824 0.55095709 0.66400737]. \t  2.124429493753792 \t 3.8084053754826726\n",
      "5      \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.8084053754826726\n",
      "6      \t [1.         0.61092587 0.        ]. \t  0.0031086595293128806 \t 3.8084053754826726\n",
      "7      \t [0.         0.78457796 1.        ]. \t  1.2979613051501944 \t 3.8084053754826726\n",
      "8      \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.8084053754826726\n",
      "9      \t [0.79982762 1.         0.        ]. \t  8.742196160431033e-05 \t 3.8084053754826726\n",
      "10     \t [0.69045651 0.68193554 1.        ]. \t  1.8176731180344796 \t 3.8084053754826726\n",
      "11     \t [0.58953398 1.         0.77429423]. \t  0.6953749263508124 \t 3.8084053754826726\n",
      "12     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.8084053754826726\n",
      "13     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.8084053754826726\n",
      "14     \t [0.25533903 0.76562693 0.84282053]. \t  2.6001998657892154 \t 3.8084053754826726\n",
      "15     \t [0.4369566  0.31390697 0.94432149]. \t  1.6861912722697086 \t 3.8084053754826726\n",
      "16     \t [0.97582172 0.68367138 0.60172171]. \t  0.6855234655190374 \t 3.8084053754826726\n",
      "17     \t [0.53799443 0.42946953 0.00548847]. \t  0.04425139695826853 \t 3.8084053754826726\n",
      "18     \t [0.56661062 0.34269926 0.26922376]. \t  0.5430076964051797 \t 3.8084053754826726\n",
      "19     \t [0.90848465 0.94727411 0.66475276]. \t  0.5069273890831931 \t 3.8084053754826726\n",
      "20     \t [0.83448508 0.12077406 0.25541979]. \t  0.519844097608949 \t 3.8084053754826726\n",
      "21     \t [0.04023156 0.45894557 0.70851272]. \t  2.3323726899042696 \t 3.8084053754826726\n",
      "22     \t [1.         0.45063107 1.        ]. \t  1.7582944703089993 \t 3.8084053754826726\n",
      "23     \t [0.86840893 0.50347298 0.18496287]. \t  0.08894346754563025 \t 3.8084053754826726\n",
      "24     \t [0.15507555 0.57267199 0.87068821]. \t  \u001b[92m3.8090213016418613\u001b[0m \t 3.8090213016418613\n",
      "25     \t [0.21207837 0.9958074  0.01335284]. \t  0.00043942655710574075 \t 3.8090213016418613\n",
      "26     \t [0.05885562 0.6198107  0.89982702]. \t  3.489981524852336 \t 3.8090213016418613\n",
      "27     \t [1.39120962e-01 2.82077871e-04 4.22672869e-01]. \t  0.36632513317281906 \t 3.8090213016418613\n",
      "28     \t [0.71969094 0.12141137 0.88384231]. \t  0.633205118123529 \t 3.8090213016418613\n",
      "29     \t [0.3437446  0.42971995 0.46588041]. \t  0.4770678941020929 \t 3.8090213016418613\n",
      "30     \t [0.43569532 0.69456793 0.64220519]. \t  2.048398826348916 \t 3.8090213016418613\n",
      "31     \t [0.38982814 0.55892186 0.89282072]. \t  3.702871815898514 \t 3.8090213016418613\n",
      "32     \t [0.99656962 0.06250794 0.78544588]. \t  0.43045263052217053 \t 3.8090213016418613\n",
      "33     \t [0.88781016 0.04638648 0.8933457 ]. \t  0.310998536788925 \t 3.8090213016418613\n",
      "34     \t [0.46625412 0.34168985 0.1594325 ]. \t  0.41493325966955924 \t 3.8090213016418613\n",
      "35     \t [0.75458476 0.91661048 0.98789737]. \t  0.6831435742486909 \t 3.8090213016418613\n",
      "36     \t [0.78144854 0.69357355 0.53307138]. \t  0.7174412868429118 \t 3.8090213016418613\n",
      "37     \t [0.49477579 0.98071266 0.09779905]. \t  0.003492061104007494 \t 3.8090213016418613\n",
      "38     \t [0.38225454 0.54813721 0.85226666]. \t  \u001b[92m3.8564430066401205\u001b[0m \t 3.8564430066401205\n",
      "39     \t [0.5272702  0.65121755 0.97036025]. \t  2.440683465823292 \t 3.8564430066401205\n",
      "40     \t [0.05773691 0.81600649 0.83255251]. \t  2.1203882471503688 \t 3.8564430066401205\n",
      "41     \t [0.56054202 0.5367984  0.09062523]. \t  0.06109184484329125 \t 3.8564430066401205\n",
      "42     \t [0.44468499 0.00937981 0.74491765]. \t  0.25889728035744847 \t 3.8564430066401205\n",
      "43     \t [0.64608835 0.5730285  0.80072763]. \t  3.496451410603565 \t 3.8564430066401205\n",
      "44     \t [0.87162801 0.47921632 0.9186832 ]. \t  3.1441940720976573 \t 3.8564430066401205\n",
      "45     \t [0.96714671 0.31877611 0.8109514 ]. \t  2.224195426707524 \t 3.8564430066401205\n",
      "46     \t [0.06196911 0.9379467  0.18851526]. \t  0.05190659427104055 \t 3.8564430066401205\n",
      "47     \t [0.25289096 0.16187821 0.28078821]. \t  0.9383184957295628 \t 3.8564430066401205\n",
      "48     \t [0.43728799 0.33399021 0.54974035]. \t  0.48720358065663055 \t 3.8564430066401205\n",
      "49     \t [0.31540751 0.4925806  0.18533984]. \t  0.20812756425445086 \t 3.8564430066401205\n",
      "50     \t [0.55388742 0.80271077 0.8283371 ]. \t  2.1264732229035648 \t 3.8564430066401205\n",
      "51     \t [0.01141799 0.7958181  0.04024629]. \t  0.0024257263715949007 \t 3.8564430066401205\n",
      "52     \t [0.38568653 0.43133829 0.84687915]. \t  3.36473683347935 \t 3.8564430066401205\n",
      "53     \t [0.96522448 0.72693724 0.54356502]. \t  0.43203529307071536 \t 3.8564430066401205\n",
      "54     \t [0.78805564 0.61380732 0.41349227]. \t  0.256670904458279 \t 3.8564430066401205\n",
      "55     \t [0.97318955 0.43407487 0.41427682]. \t  0.11515215923246044 \t 3.8564430066401205\n",
      "56     \t [0.90603606 0.25244893 0.40295125]. \t  0.21999200267847413 \t 3.8564430066401205\n",
      "57     \t [0.51003172 0.40408284 0.06036259]. \t  0.11447049881241596 \t 3.8564430066401205\n",
      "58     \t [0.04398814 0.28878408 0.93202463]. \t  1.5716951794842502 \t 3.8564430066401205\n",
      "59     \t [0.97372688 0.12096872 0.72139876]. \t  0.5768109144263086 \t 3.8564430066401205\n",
      "60     \t [0.93617363 0.31811842 0.34290007]. \t  0.22210349730127474 \t 3.8564430066401205\n",
      "61     \t [0.0918845  0.35222564 0.4215209 ]. \t  0.3685063985974734 \t 3.8564430066401205\n",
      "62     \t [0.58821847 0.87099    0.19609719]. \t  0.0343397149464509 \t 3.8564430066401205\n",
      "63     \t [0.49773967 0.30088814 0.81176042]. \t  2.135118119974003 \t 3.8564430066401205\n",
      "64     \t [0.13141619 0.40399005 0.92938214]. \t  2.5762376586942803 \t 3.8564430066401205\n",
      "65     \t [0.05166908 0.89712758 0.83222533]. \t  1.4256201907466846 \t 3.8564430066401205\n",
      "66     \t [0.6074091  0.5651644  0.76113511]. \t  3.062024712880875 \t 3.8564430066401205\n",
      "67     \t [0.3279515  0.0692452  0.67134912]. \t  0.31247617797407107 \t 3.8564430066401205\n",
      "68     \t [0.0961328  0.21997547 0.097758  ]. \t  0.30382130390439555 \t 3.8564430066401205\n",
      "69     \t [0.00527737 0.84067748 0.99465264]. \t  1.0374991182043767 \t 3.8564430066401205\n",
      "70     \t [0.73828086 0.51416956 0.16225059]. \t  0.10097721825159194 \t 3.8564430066401205\n",
      "71     \t [0.23621156 0.77273158 0.72314536]. \t  2.3689595603050706 \t 3.8564430066401205\n",
      "72     \t [0.69179496 0.9626543  0.04608057]. \t  0.00055817181923092 \t 3.8564430066401205\n",
      "73     \t [0.01135413 0.13766301 0.42708938]. \t  0.3369781391191165 \t 3.8564430066401205\n",
      "74     \t [0.01821837 0.06125134 0.50305178]. \t  0.16691697532744534 \t 3.8564430066401205\n",
      "75     \t [0.75887025 0.98992728 0.04830322]. \t  0.00041008097929826304 \t 3.8564430066401205\n",
      "76     \t [0.45034395 0.31184179 0.49118223]. \t  0.34019795084950083 \t 3.8564430066401205\n",
      "77     \t [0.34516894 0.87245412 0.01261572]. \t  0.0008504359352652114 \t 3.8564430066401205\n",
      "78     \t [0.5402775  0.11084036 0.71951798]. \t  0.5495022323459491 \t 3.8564430066401205\n",
      "79     \t [0.85245056 0.8370809  0.2820212 ]. \t  0.06353125045560243 \t 3.8564430066401205\n",
      "80     \t [0.65723325 0.81839565 0.23693994]. \t  0.0627265516022317 \t 3.8564430066401205\n",
      "81     \t [0.19064436 0.41679247 0.29594489]. \t  0.4111794760569056 \t 3.8564430066401205\n",
      "82     \t [0.10264431 0.13774606 0.43768167]. \t  0.36310734409873674 \t 3.8564430066401205\n",
      "83     \t [0.44394081 0.80240406 0.74670846]. \t  1.9883099573903038 \t 3.8564430066401205\n",
      "84     \t [0.1070112  0.18095404 0.22045493]. \t  0.7325303941307997 \t 3.8564430066401205\n",
      "85     \t [0.98222071 0.56216458 0.37437893]. \t  0.08341834321777766 \t 3.8564430066401205\n",
      "86     \t [0.13569583 0.80181312 0.26061919]. \t  0.2203440587860694 \t 3.8564430066401205\n",
      "87     \t [0.63531704 0.15550047 0.43784079]. \t  0.3572604518527468 \t 3.8564430066401205\n",
      "88     \t [0.7149028  0.61242382 0.36241602]. \t  0.21763972793491507 \t 3.8564430066401205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.72451742 0.87865968 0.01700706]. \t  0.00048062661902852907 \t 3.8564430066401205\n",
      "90     \t [0.06980833 0.06735126 0.19189161]. \t  0.6291019355032829 \t 3.8564430066401205\n",
      "91     \t [0.02629512 0.25758702 0.77117475]. \t  1.5962871548899078 \t 3.8564430066401205\n",
      "92     \t [0.057924   0.12644384 0.43121119]. \t  0.35490505058061267 \t 3.8564430066401205\n",
      "93     \t [0.23209014 0.82288377 0.68282029]. \t  2.3689475747338604 \t 3.8564430066401205\n",
      "94     \t [0.35882936 0.0239903  0.06063885]. \t  0.25459726411542455 \t 3.8564430066401205\n",
      "95     \t [0.73959719 0.3018457  0.40047305]. \t  0.3086314264790407 \t 3.8564430066401205\n",
      "96     \t [0.60526256 0.23723161 0.17647999]. \t  0.5717865799947605 \t 3.8564430066401205\n",
      "97     \t [0.58961367 0.45489445 0.43543263]. \t  0.33085397053812077 \t 3.8564430066401205\n",
      "98     \t [0.51236923 0.53830879 0.23496533]. \t  0.18246633710710705 \t 3.8564430066401205\n",
      "99     \t [0.97546352 0.36504299 0.38991969]. \t  0.13791651637606442 \t 3.8564430066401205\n",
      "100    \t [0.73376058 0.30712204 0.79077983]. \t  2.0952712170015175 \t 3.8564430066401205\n"
     ]
    }
   ],
   "source": [
    "### 6(p). Bayesian optimization runs (x20): GP run number = 16\n",
    "\n",
    "np.random.seed(run_num_16)\n",
    "surrogate_gp_16 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "gpgo_gp_16 = GPGO(surrogate_gp_16, Acquisition_new(util_gp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_16.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.86394833 0.316366   0.67215078]. \t  1.1947633822631252 \t 3.8084053754826726\n",
      "init   \t [0.50791167 0.78166043 0.28368168]. \t  0.20052765717741963 \t 3.8084053754826726\n",
      "init   \t [0.23370878 0.56379969 0.87502436]. \t  3.8084053754826726 \t 3.8084053754826726\n",
      "init   \t [0.71894292 0.18213174 0.24380041]. \t  0.6532826728302027 \t 3.8084053754826726\n",
      "init   \t [0.08605673 0.53424539 0.83969965]. \t  3.8072565344663847 \t 3.8084053754826726\n",
      "1      \t [0.07508666 0.11940135 1.        ]. \t  0.2929014910412538 \t 3.8084053754826726\n",
      "2      \t [-1.11022302e-16  1.00000000e+00  1.00000000e+00]. \t  0.330219860606422 \t 3.8084053754826726\n",
      "3      \t [0.         0.17281808 0.        ]. \t  0.07555691411200055 \t 3.8084053754826726\n",
      "4      \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.8084053754826726\n",
      "5      \t [0.17207966 0.49088424 0.64144899]. \t  1.7223498165822642 \t 3.8084053754826726\n",
      "6      \t [1.         0.52505056 0.        ]. \t  0.006723214508968703 \t 3.8084053754826726\n",
      "7      \t [0.         0.55747211 1.        ]. \t  2.055857281258685 \t 3.8084053754826726\n",
      "8      \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.8084053754826726\n",
      "9      \t [ 1.00000000e+00  1.00000000e+00 -5.55111512e-17]. \t  3.7727185179443895e-05 \t 3.8084053754826726\n",
      "10     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.8084053754826726\n",
      "11     \t [1. 0. 1.]. \t  0.08848201872702738 \t 3.8084053754826726\n",
      "12     \t [ 5.02963878e-01  0.00000000e+00 -1.11022302e-16]. \t  0.09687952005473086 \t 3.8084053754826726\n",
      "13     \t [0.48874539 1.         0.78761887]. \t  0.7542131805964291 \t 3.8084053754826726\n",
      "14     \t [0.57435247 1.         0.        ]. \t  0.00017319630103418881 \t 3.8084053754826726\n",
      "15     \t [0.         0.89850718 0.68903455]. \t  2.0960946757075294 \t 3.8084053754826726\n",
      "16     \t [1.         0.         0.52555528]. \t  0.06774038946003862 \t 3.8084053754826726\n",
      "17     \t [1.         1.         0.52015172]. \t  0.24115476574720343 \t 3.8084053754826726\n",
      "18     \t [0.         0.61084602 0.        ]. \t  0.006944228012526091 \t 3.8084053754826726\n",
      "19     \t [0.         0.         0.66734393]. \t  0.1658712796286581 \t 3.8084053754826726\n",
      "20     \t [0.60469101 0.         0.93472441]. \t  0.15798484439265142 \t 3.8084053754826726\n",
      "21     \t [0.64002686 0.65839309 1.        ]. \t  1.9119040433490377 \t 3.8084053754826726\n",
      "22     \t [0.16198784 0.69011311 0.8536617 ]. \t  3.273335922019937 \t 3.8084053754826726\n",
      "23     \t [0.67963366 0.51995224 0.        ]. \t  0.017335227701726363 \t 3.8084053754826726\n",
      "24     \t [1.         0.45072789 1.        ]. \t  1.7586829482874535 \t 3.8084053754826726\n",
      "25     \t [0.26994313 0.44040687 0.91946611]. \t  2.9785879060221734 \t 3.8084053754826726\n",
      "26     \t [0.66902963 0.65217573 0.71898932]. \t  2.2247867925077864 \t 3.8084053754826726\n",
      "27     \t [1.         0.60920315 0.34210478]. \t  0.061222790512576356 \t 3.8084053754826726\n",
      "28     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.8084053754826726\n",
      "29     \t [0.7565652  0.99480266 0.23856811]. \t  0.036983020854183306 \t 3.8084053754826726\n",
      "30     \t [0.7739515 0.        0.5872044]. \t  0.10157785948852019 \t 3.8084053754826726\n",
      "31     \t [0.         0.46090752 0.81760928]. \t  3.437559443187076 \t 3.8084053754826726\n",
      "32     \t [0.28840213 0.34115161 0.        ]. \t  0.06959019213886579 \t 3.8084053754826726\n",
      "33     \t [0.24194782 0.99202442 0.32014808]. \t  0.4747948615964119 \t 3.8084053754826726\n",
      "34     \t [0.9951049  0.71464316 0.90733627]. \t  2.7048829481813064 \t 3.8084053754826726\n",
      "35     \t [2.52203387e-01 2.14790381e-08 8.04688756e-01]. \t  0.2508329542846783 \t 3.8084053754826726\n",
      "36     \t [0.         0.         0.92580849]. \t  0.1651985230656637 \t 3.8084053754826726\n",
      "37     \t [2.89087135e-01 1.77219066e-08 2.08949378e-01]. \t  0.7725276849318026 \t 3.8084053754826726\n",
      "38     \t [5.73882832e-08 9.99999878e-01 7.95796602e-01]. \t  0.8723700154092895 \t 3.8084053754826726\n",
      "39     \t [4.54247289e-04 5.75026066e-01 8.25804832e-01]. \t  3.7305862577946507 \t 3.8084053754826726\n",
      "40     \t [2.38568214e-04 7.09074715e-01 2.76777238e-01]. \t  0.2381129185172522 \t 3.8084053754826726\n",
      "41     \t [0.01701958 0.56729534 0.84207351]. \t  3.800848749032318 \t 3.8084053754826726\n",
      "42     \t [7.01910614e-21 5.58455081e-01 8.23767150e-01]. \t  3.7346720502900963 \t 3.8084053754826726\n",
      "43     \t [0.28720109 1.         0.99999996]. \t  0.33432207650006135 \t 3.8084053754826726\n",
      "44     \t [1.         0.         0.20130748]. \t  0.23168738277007414 \t 3.8084053754826726\n",
      "45     \t [1.00000000e+00 2.37025969e-01 5.54846933e-08]. \t  0.030733984738682527 \t 3.8084053754826726\n",
      "46     \t [0.00137965 0.69105754 0.82961322]. \t  3.1972227266287736 \t 3.8084053754826726\n",
      "47     \t [0.75662118 1.         1.        ]. \t  0.3258877835182035 \t 3.8084053754826726\n",
      "48     \t [0.99999984 0.5923133  0.78395981]. \t  3.0852621891568575 \t 3.8084053754826726\n",
      "49     \t [0.3911469  0.58778692 0.82687772]. \t  3.744434144397058 \t 3.8084053754826726\n",
      "50     \t [0.12966175 0.61321656 0.85641987]. \t  3.731291338850071 \t 3.8084053754826726\n",
      "51     \t [0.         0.01780837 0.21042955]. \t  0.5468596640365929 \t 3.8084053754826726\n",
      "52     \t [0.22267743 0.54969876 0.83456524]. \t  \u001b[92m3.8275453427977477\u001b[0m \t 3.8275453427977477\n",
      "53     \t [0.00548988 0.45819649 0.87946544]. \t  3.411207218777651 \t 3.8275453427977477\n",
      "54     \t [0.43283676 0.71658128 0.88333008]. \t  2.969202208907866 \t 3.8275453427977477\n",
      "55     \t [0.53014497 0.34341457 0.88384683]. \t  2.446400293040283 \t 3.8275453427977477\n",
      "56     \t [0.         0.9999985  0.41790301]. \t  1.4075039241396774 \t 3.8275453427977477\n",
      "57     \t [0.22919795 0.64189721 0.84653794]. \t  3.601704518251694 \t 3.8275453427977477\n",
      "58     \t [0.06273922 0.40303171 0.85885863]. \t  3.087972800690218 \t 3.8275453427977477\n",
      "59     \t [0.86893638 0.83906845 0.        ]. \t  0.00035269806527527496 \t 3.8275453427977477\n",
      "60     \t [0.29070987 0.4519531  0.95108529]. \t  2.6204075753597458 \t 3.8275453427977477\n",
      "61     \t [0.11280495 0.53382277 0.80833149]. \t  3.6605056120633757 \t 3.8275453427977477\n",
      "62     \t [0.36749204 0.53530764 0.87486016]. \t  3.797084062988681 \t 3.8275453427977477\n",
      "63     \t [0.12914339 0.61558147 0.82700343]. \t  3.6635338292434882 \t 3.8275453427977477\n",
      "64     \t [0.08396723 0.47948766 0.8988177 ]. \t  3.410459032783401 \t 3.8275453427977477\n",
      "65     \t [0.31829415 0.51272439 0.83250722]. \t  3.767160535122776 \t 3.8275453427977477\n",
      "66     \t [0.06259595 0.48840221 0.85312274]. \t  3.6763291813819396 \t 3.8275453427977477\n",
      "67     \t [0.99823486 0.1354209  0.84880465]. \t  0.754787361350771 \t 3.8275453427977477\n",
      "68     \t [0.0796643  0.536108   0.88064967]. \t  3.7395446042988794 \t 3.8275453427977477\n",
      "69     \t [0.42266483 0.50011582 0.77898926]. \t  3.3141940769484144 \t 3.8275453427977477\n",
      "70     \t [0.2379653  0.56620641 0.84659914]. \t  \u001b[92m3.8514974443651546\u001b[0m \t 3.8514974443651546\n",
      "71     \t [0.44069882 0.47538723 0.79461575]. \t  3.3819593056487145 \t 3.8514974443651546\n",
      "72     \t [0.25025865 0.49817968 0.80930379]. \t  3.6007228933938826 \t 3.8514974443651546\n",
      "73     \t [0.96569497 0.57587985 0.82437995]. \t  3.54629331463938 \t 3.8514974443651546\n",
      "74     \t [0.05736842 0.4879553  0.82102697]. \t  3.602568948532407 \t 3.8514974443651546\n",
      "75     \t [0.40831855 0.54854732 0.85296995]. \t  \u001b[92m3.8542205556490714\u001b[0m \t 3.8542205556490714\n",
      "76     \t [0.51989285 0.59820406 0.81655194]. \t  3.6173032389054507 \t 3.8542205556490714\n",
      "77     \t [0.06850738 0.5834095  0.84149613]. \t  3.7927124357542685 \t 3.8542205556490714\n",
      "78     \t [0.31570183 0.58666343 0.88680729]. \t  3.7196564196630884 \t 3.8542205556490714\n",
      "79     \t [0.49254834 0.57721608 0.81827002]. \t  3.6914781316349057 \t 3.8542205556490714\n",
      "80     \t [0.08592806 0.64802937 0.85836576]. \t  3.549999912819504 \t 3.8542205556490714\n",
      "81     \t [0.50582512 0.56983854 0.90053025]. \t  3.622777062742224 \t 3.8542205556490714\n",
      "82     \t [0.45538391 0.54676818 0.85461998]. \t  3.8474941743476574 \t 3.8542205556490714\n",
      "83     \t [0.66839054 0.55267888 0.84459152]. \t  3.787407413649402 \t 3.8542205556490714\n",
      "84     \t [0.06028526 0.65060778 0.78171677]. \t  3.1984273993169476 \t 3.8542205556490714\n",
      "85     \t [0.43835731 0.65505978 0.8687499 ]. \t  3.4934118672984944 \t 3.8542205556490714\n",
      "86     \t [0.73550378 0.57306883 0.88608319]. \t  3.678165752837509 \t 3.8542205556490714\n",
      "87     \t [0.32014186 0.4927334  0.85450152]. \t  3.7278381942572056 \t 3.8542205556490714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.13044132 0.58809795 0.87017856]. \t  3.781360571917484 \t 3.8542205556490714\n",
      "89     \t [0.07412994 0.60184889 0.84501703]. \t  3.7536695536451683 \t 3.8542205556490714\n",
      "90     \t [0.59647757 0.46444458 0.85153307]. \t  3.5597149620177264 \t 3.8542205556490714\n",
      "91     \t [0.70931546 0.4987543  0.90007687]. \t  3.47281160939713 \t 3.8542205556490714\n",
      "92     \t [0.40161686 0.51766142 0.84997913]. \t  3.8096405055789813 \t 3.8542205556490714\n",
      "93     \t [0.21066949 0.56809281 0.79998352]. \t  3.611303020689033 \t 3.8542205556490714\n",
      "94     \t [0.35389819 0.53837774 0.87350453]. \t  3.807640544808489 \t 3.8542205556490714\n",
      "95     \t [0.28068289 0.58823486 0.79639663]. \t  3.54614744695706 \t 3.8542205556490714\n",
      "96     \t [0.24743467 0.51561651 0.85220015]. \t  3.8061390372578923 \t 3.8542205556490714\n",
      "97     \t [0.25198225 0.5943798  0.79597747]. \t  3.53193393321682 \t 3.8542205556490714\n",
      "98     \t [0.60075337 0.60321411 0.81045252]. \t  3.521705392593244 \t 3.8542205556490714\n",
      "99     \t [0.01656921 0.65373082 0.83730304]. \t  3.4785760357885573 \t 3.8542205556490714\n",
      "100    \t [0.21354019 0.59635179 0.86288733]. \t  3.7909788095095998 \t 3.8542205556490714\n"
     ]
    }
   ],
   "source": [
    "### 6(p). Bayesian optimization runs (x20): STP DF1 run number = 16\n",
    "\n",
    "np.random.seed(run_num_16)\n",
    "surrogate_stp_df1_16 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_16 = GPGO(surrogate_stp_df1_16, Acquisition_new(util_stp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_16.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.061350856474767, -4.760720003210089)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(p). Training Regret Minimisation: run number = 16\n",
    "\n",
    "gp_output_16 = np.append(np.max(gpgo_gp_16.GP.y[0:n_init]),gpgo_gp_16.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_16 = np.append(np.max(gpgo_stp_df1_16.GP.y[0:n_init]),gpgo_stp_df1_16.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_16 = np.log(y_global_orig - gp_output_16)\n",
    "regret_stp_df1_16 = np.log(y_global_orig - stp_df1_output_16)\n",
    "\n",
    "train_regret_gp_16 = min_max_array(regret_gp_16)\n",
    "train_regret_stp_df1_16 = min_max_array(regret_stp_df1_16)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 16\n",
    "min_train_regret_gp_16 = min(train_regret_gp_16)\n",
    "min_train_regret_stp_df1_16 = min(train_regret_stp_df1_16)\n",
    "\n",
    "min_train_regret_gp_16, min_train_regret_stp_df1_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.03062042 0.07928781 0.63744103]. \t  0.26151180763801685 \t 1.1847048361623822\n",
      "init   \t [0.55183161 0.74085359 0.47194689]. \t  1.1847048361623822 \t 1.1847048361623822\n",
      "init   \t [0.72321875 0.51048559 0.15857243]. \t  0.10467677070366013 \t 1.1847048361623822\n",
      "init   \t [0.30441635 0.28779361 0.52050509]. \t  0.36417978775253973 \t 1.1847048361623822\n",
      "init   \t [0.44168686 0.01576299 0.59173219]. \t  0.13265326732084362 \t 1.1847048361623822\n",
      "1      \t [0.97037533 0.84307362 0.8941645 ]. \t  \u001b[92m1.6110838908523433\u001b[0m \t 1.6110838908523433\n",
      "2      \t [0.06173379 0.8865066  0.95985062]. \t  1.0250276171872894 \t 1.6110838908523433\n",
      "3      \t [0.89879181 0.11408242 0.92331109]. \t  0.49216626215170195 \t 1.6110838908523433\n",
      "4      \t [0.02384013 0.95293591 0.18250482]. \t  0.04368989027491074 \t 1.6110838908523433\n",
      "5      \t [0.98997131 0.93792327 0.43331401]. \t  0.18424918202942797 \t 1.6110838908523433\n",
      "6      \t [0.62087692 0.51728777 0.98544061]. \t  \u001b[92m2.2854406153634317\u001b[0m \t 2.2854406153634317\n",
      "7      \t [0.54439339 0.91789487 0.92435396]. \t  0.99038936280432 \t 2.2854406153634317\n",
      "8      \t [0.0801689  0.12727256 0.99154522]. \t  0.33947682707400967 \t 2.2854406153634317\n",
      "9      \t [0.71799163 0.97202171 0.00764539]. \t  0.00017421636398177954 \t 2.2854406153634317\n",
      "10     \t [0.02320033 0.05289773 0.05464968]. \t  0.17269828225149914 \t 2.2854406153634317\n",
      "11     \t [0.99892154 0.13736687 0.04465078]. \t  0.06841917523957156 \t 2.2854406153634317\n",
      "12     \t [0.94167622 0.47284314 0.81360456]. \t  \u001b[92m3.3692804458027497\u001b[0m \t 3.3692804458027497\n",
      "13     \t [0.98596326 0.13274387 0.50468092]. \t  0.12148740305210944 \t 3.3692804458027497\n",
      "14     \t [0.81147213 0.80758404 0.00864017]. \t  0.0007190290976965352 \t 3.3692804458027497\n",
      "15     \t [0.97250143 0.52754201 0.99416644]. \t  2.0863092538903247 \t 3.3692804458027497\n",
      "16     \t [0.50862739 0.14370391 0.61923968]. \t  0.35862502939174384 \t 3.3692804458027497\n",
      "17     \t [0.43719796 0.11310175 0.93955807]. \t  0.4542425547196239 \t 3.3692804458027497\n",
      "18     \t [0.31102954 0.92901964 0.78129221]. \t  1.2889011203393506 \t 3.3692804458027497\n",
      "19     \t [0.87942001 0.28547633 0.70601556]. \t  1.342924438072672 \t 3.3692804458027497\n",
      "20     \t [0.93022413 0.6143208  0.67589724]. \t  1.5369269870503581 \t 3.3692804458027497\n",
      "21     \t [0.33346242 0.60633113 0.        ]. \t  0.010779309784091463 \t 3.3692804458027497\n",
      "22     \t [0.41216659 0.520924   0.9045095 ]. \t  \u001b[92m3.546204917437737\u001b[0m \t 3.546204917437737\n",
      "23     \t [0.14877169 0.99279523 0.28950983]. \t  0.31426301789602873 \t 3.546204917437737\n",
      "24     \t [0.63437355 0.09797607 0.06862989]. \t  0.24681132916199494 \t 3.546204917437737\n",
      "25     \t [0.81380051 0.02843622 0.02276578]. \t  0.08491088358801745 \t 3.546204917437737\n",
      "26     \t [1.         0.34894859 0.86243593]. \t  2.5029139966958267 \t 3.546204917437737\n",
      "27     \t [0.3544685  0.14557148 0.21255228]. \t  0.9064059370369847 \t 3.546204917437737\n",
      "28     \t [0.69320497 0.97549569 0.81428123]. \t  0.7233995744476222 \t 3.546204917437737\n",
      "29     \t [0.63290488 0.58755164 0.34056157]. \t  0.22569166370200963 \t 3.546204917437737\n",
      "30     \t [0.6921123  0.36118782 0.82173349]. \t  2.700222258506918 \t 3.546204917437737\n",
      "31     \t [0.01880844 0.44923959 0.1494709 ]. \t  0.15489237697644975 \t 3.546204917437737\n",
      "32     \t [0.40878084 0.88556706 0.9382514 ]. \t  1.1634261382517104 \t 3.546204917437737\n",
      "33     \t [0.33761388 0.70735632 0.48401094]. \t  1.7464423334809325 \t 3.546204917437737\n",
      "34     \t [0.         0.61527357 0.888097  ]. \t  \u001b[92m3.5794879340456283\u001b[0m \t 3.5794879340456283\n",
      "35     \t [0.41628855 0.89224253 0.80056938]. \t  1.4249324011758402 \t 3.5794879340456283\n",
      "36     \t [0.62950246 0.70061029 0.9226051 ]. \t  2.786931094703611 \t 3.5794879340456283\n",
      "37     \t [0.65859188 0.41077591 0.27606611]. \t  0.34165086031415864 \t 3.5794879340456283\n",
      "38     \t [0.93772382 0.7569913  0.40910134]. \t  0.18903449519267562 \t 3.5794879340456283\n",
      "39     \t [0.84626984 0.46563106 0.0949422 ]. \t  0.061599943092557584 \t 3.5794879340456283\n",
      "40     \t [0.21522866 0.76512892 0.39466406]. \t  1.2112781033075477 \t 3.5794879340456283\n",
      "41     \t [0.26646903 0.18172009 0.31415665]. \t  0.8750287275655869 \t 3.5794879340456283\n",
      "42     \t [0.70199821 0.56064867 0.19702261]. \t  0.09485580985202205 \t 3.5794879340456283\n",
      "43     \t [0.43536181 0.94141301 0.24865153]. \t  0.12638515469846487 \t 3.5794879340456283\n",
      "44     \t [0.17474163 0.99699503 0.08623356]. \t  0.003657384163100385 \t 3.5794879340456283\n",
      "45     \t [0.19169703 0.93925696 0.59576296]. \t  2.7657020279685236 \t 3.5794879340456283\n",
      "46     \t [0.         0.82091787 0.70247083]. \t  2.267162103193154 \t 3.5794879340456283\n",
      "47     \t [0.41118365 0.69530436 0.12115392]. \t  0.024405155235106134 \t 3.5794879340456283\n",
      "48     \t [0.10185067 0.5527727  0.94867066]. \t  2.975759445593671 \t 3.5794879340456283\n",
      "49     \t [0.70259225 0.29140011 0.44863585]. \t  0.26672841142615117 \t 3.5794879340456283\n",
      "50     \t [0.03226274 0.02807275 0.80622714]. \t  0.3236303373065826 \t 3.5794879340456283\n",
      "51     \t [0.26971143 0.39801014 0.64797342]. \t  1.4033415839484646 \t 3.5794879340456283\n",
      "52     \t [0.44285271 0.40162261 0.8433722 ]. \t  3.122493947150961 \t 3.5794879340456283\n",
      "53     \t [0.90917941 0.69861355 0.69586967]. \t  1.5097663435700164 \t 3.5794879340456283\n",
      "54     \t [0.97796046 0.9924435  0.04367178]. \t  0.00014139590873981907 \t 3.5794879340456283\n",
      "55     \t [0.44697237 0.85041606 0.00287931]. \t  0.0007843948616288213 \t 3.5794879340456283\n",
      "56     \t [0.72107966 0.17494753 0.74261763]. \t  0.9203751210475772 \t 3.5794879340456283\n",
      "57     \t [0.73733164 0.26686976 0.64153403]. \t  0.7874486009196768 \t 3.5794879340456283\n",
      "58     \t [0.55801824 0.69335573 0.20978559]. \t  0.0628052104085411 \t 3.5794879340456283\n",
      "59     \t [0.22576863 0.10189156 0.73390999]. \t  0.5433176084827087 \t 3.5794879340456283\n",
      "60     \t [0.43685113 0.03812342 0.30071845]. \t  0.8967400649847538 \t 3.5794879340456283\n",
      "61     \t [0.61186107 0.6482006  0.12829989]. \t  0.03154278522547546 \t 3.5794879340456283\n",
      "62     \t [0.63125855 0.49413009 0.69220304]. \t  2.060432965714162 \t 3.5794879340456283\n",
      "63     \t [0.17818482 0.60709371 0.14718012]. \t  0.0626547292127453 \t 3.5794879340456283\n",
      "64     \t [0.17658062 0.09026825 0.12089963]. \t  0.46717630088704964 \t 3.5794879340456283\n",
      "65     \t [0.56849364 0.86283928 0.5248536 ]. \t  1.6005021891058346 \t 3.5794879340456283\n",
      "66     \t [0.2819197  0.90041896 0.94459452]. \t  1.030226927482224 \t 3.5794879340456283\n",
      "67     \t [0.76499868 0.14117966 0.85313567]. \t  0.7996730598839443 \t 3.5794879340456283\n",
      "68     \t [0.77230711 0.18114963 0.3599644 ]. \t  0.46064680261304086 \t 3.5794879340456283\n",
      "69     \t [0.03938974 0.09679018 0.81269872]. \t  0.5880579195124129 \t 3.5794879340456283\n",
      "70     \t [0.22178838 0.85339669 0.76290127]. \t  1.8809580438129272 \t 3.5794879340456283\n",
      "71     \t [0.65859679 0.70689507 0.6686392 ]. \t  1.6278789423005389 \t 3.5794879340456283\n",
      "72     \t [0.50816398 0.17746089 0.17139936]. \t  0.6904726540590452 \t 3.5794879340456283\n",
      "73     \t [0.61941138 0.34553626 0.58888953]. \t  0.6533360450877037 \t 3.5794879340456283\n",
      "74     \t [0.70022418 0.49216843 0.27444025]. \t  0.19961447446891634 \t 3.5794879340456283\n",
      "75     \t [0.72611279 0.58922517 0.06147868]. \t  0.02086516224067775 \t 3.5794879340456283\n",
      "76     \t [0.96810402 0.83441132 0.73592575]. \t  1.1027423790372501 \t 3.5794879340456283\n",
      "77     \t [0.5096701  0.71960658 0.35036717]. \t  0.4410665522219048 \t 3.5794879340456283\n",
      "78     \t [0.97586604 0.44756822 0.06174239]. \t  0.031289605075420335 \t 3.5794879340456283\n",
      "79     \t [0.653701   0.32637465 0.00269634]. \t  0.06191096732921023 \t 3.5794879340456283\n",
      "80     \t [0.97171691 0.10534113 0.14025999]. \t  0.20686902113569489 \t 3.5794879340456283\n",
      "81     \t [0.37456377 0.3008954  0.60290912]. \t  0.687010013332066 \t 3.5794879340456283\n",
      "82     \t [0.61628564 0.87841673 0.46096975]. \t  1.0782937790575122 \t 3.5794879340456283\n",
      "83     \t [0.65471385 0.46244348 0.64920741]. \t  1.4428257753963851 \t 3.5794879340456283\n",
      "84     \t [0.33539448 0.0790926  0.04901014]. \t  0.23521809608926925 \t 3.5794879340456283\n",
      "85     \t [0.54532034 0.60844473 0.02009998]. \t  0.013172177955310243 \t 3.5794879340456283\n",
      "86     \t [0.70923173 0.62839448 0.99199627]. \t  2.1291018309678384 \t 3.5794879340456283\n",
      "87     \t [0.02067971 0.10598641 0.27827668]. \t  0.6927034644196612 \t 3.5794879340456283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.51731162 0.65292531 0.58681119]. \t  1.5372716596077303 \t 3.5794879340456283\n",
      "89     \t [0.8370855  0.70703931 0.2907795 ]. \t  0.07352539596712124 \t 3.5794879340456283\n",
      "90     \t [0.80261449 0.35540949 0.2334295 ]. \t  0.31356208025933857 \t 3.5794879340456283\n",
      "91     \t [0.41371495 0.8200785  0.49158303]. \t  1.9975220018422604 \t 3.5794879340456283\n",
      "92     \t [0.3960182 0.1793876 0.2520346]. \t  0.9543546064178693 \t 3.5794879340456283\n",
      "93     \t [0.30030434 0.08357935 0.95179458]. \t  0.32177333267685415 \t 3.5794879340456283\n",
      "94     \t [0.86935936 0.81714613 0.51583662]. \t  0.5501633715712599 \t 3.5794879340456283\n",
      "95     \t [0.52321311 0.02434716 0.24445062]. \t  0.8412886044301574 \t 3.5794879340456283\n",
      "96     \t [0.07312282 0.36328509 0.21402098]. \t  0.3920078257588973 \t 3.5794879340456283\n",
      "97     \t [0.55161496 0.36924677 0.93847069]. \t  2.1983621831804747 \t 3.5794879340456283\n",
      "98     \t [0.14233894 0.1800143  0.7140764 ]. \t  0.8470511622859529 \t 3.5794879340456283\n",
      "99     \t [0.62240037 0.15377671 0.01426645]. \t  0.11918459349679077 \t 3.5794879340456283\n",
      "100    \t [0.28924208 0.63271356 0.92829368]. \t  3.15714534073116 \t 3.5794879340456283\n"
     ]
    }
   ],
   "source": [
    "### 6(q). Bayesian optimization runs (x20): GP run number = 17\n",
    "\n",
    "np.random.seed(run_num_17)\n",
    "surrogate_gp_17 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "gpgo_gp_17 = GPGO(surrogate_gp_17, Acquisition_new(util_gp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_17.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.03062042 0.07928781 0.63744103]. \t  0.26151180763801685 \t 1.1847048361623822\n",
      "init   \t [0.55183161 0.74085359 0.47194689]. \t  1.1847048361623822 \t 1.1847048361623822\n",
      "init   \t [0.72321875 0.51048559 0.15857243]. \t  0.10467677070366013 \t 1.1847048361623822\n",
      "init   \t [0.30441635 0.28779361 0.52050509]. \t  0.36417978775253973 \t 1.1847048361623822\n",
      "init   \t [0.44168686 0.01576299 0.59173219]. \t  0.13265326732084362 \t 1.1847048361623822\n",
      "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 1.1847048361623822\n",
      "2      \t [0. 1. 1.]. \t  0.330219860606422 \t 1.1847048361623822\n",
      "3      \t [-2.77555756e-17  1.00000000e+00  0.00000000e+00]. \t  0.0002735367680454459 \t 1.1847048361623822\n",
      "4      \t [1. 0. 1.]. \t  0.0884820187270272 \t 1.1847048361623822\n",
      "5      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 1.1847048361623822\n",
      "6      \t [0. 0. 0.]. \t  0.06797411659013229 \t 1.1847048361623822\n",
      "7      \t [1. 0. 0.]. \t  0.03095471703300515 \t 1.1847048361623822\n",
      "8      \t [0.40756192 0.43314035 1.        ]. \t  \u001b[92m1.7495208381617706\u001b[0m \t 1.7495208381617706\n",
      "9      \t [1.         0.52118631 0.61765634]. \t  0.9353855436626424 \t 1.7495208381617706\n",
      "10     \t [0. 0. 1.]. \t  0.0902894676548261 \t 1.7495208381617706\n",
      "11     \t [0.48961824 1.         1.        ]. \t  0.3324999201833007 \t 1.7495208381617706\n",
      "12     \t [0.        1.        0.5026399]. \t  \u001b[92m2.2818923667317526\u001b[0m \t 2.2818923667317526\n",
      "13     \t [1.         0.51852758 1.        ]. \t  1.962625236585532 \t 2.2818923667317526\n",
      "14     \t [0.48356163 1.         0.        ]. \t  0.00021066203155861046 \t 2.2818923667317526\n",
      "15     \t [ 5.15564259e-01 -5.55111512e-17  0.00000000e+00]. \t  0.095856883504706 \t 2.2818923667317526\n",
      "16     \t [0.         0.55934002 1.        ]. \t  2.056561164141345 \t 2.2818923667317526\n",
      "17     \t [0.         0.54652816 0.        ]. \t  0.012415677427642971 \t 2.2818923667317526\n",
      "18     \t [1.         1.         0.41858847]. \t  0.13693686784710055 \t 2.2818923667317526\n",
      "19     \t [1.         0.         0.42629816]. \t  0.12848045684343581 \t 2.2818923667317526\n",
      "20     \t [0.         0.71769012 0.40875623]. \t  1.2204125807808461 \t 2.2818923667317526\n",
      "21     \t [0.76401154 0.31993556 0.84494609]. \t  \u001b[92m2.3119854298789684\u001b[0m \t 2.3119854298789684\n",
      "22     \t [0.         0.77033094 0.76628914]. \t  \u001b[92m2.400919721916436\u001b[0m \t 2.400919721916436\n",
      "23     \t [0.7624022  0.69200965 1.        ]. \t  1.7659561475958356 \t 2.400919721916436\n",
      "24     \t [1.         0.47951648 0.        ]. \t  0.00954328730116668 \t 2.400919721916436\n",
      "25     \t [0.718136   1.         0.69684382]. \t  0.6586940578688056 \t 2.400919721916436\n",
      "26     \t [0.64920851 0.         1.        ]. \t  0.09116413943371655 \t 2.400919721916436\n",
      "27     \t [0.33264688 0.64878594 0.        ]. \t  0.007058772262857411 \t 2.400919721916436\n",
      "28     \t [0.         0.35319774 0.83964607]. \t  \u001b[92m2.6372030614700828\u001b[0m \t 2.6372030614700828\n",
      "29     \t [0.26587043 1.         0.44475395]. \t  1.654000609002889 \t 2.6372030614700828\n",
      "30     \t [0.72679367 0.         0.3169051 ]. \t  0.5519325361678922 \t 2.6372030614700828\n",
      "31     \t [0.89341015 0.         0.79023636]. \t  0.246599288495375 \t 2.6372030614700828\n",
      "32     \t [0.77021581 0.26283533 1.        ]. \t  0.8316511839971635 \t 2.6372030614700828\n",
      "33     \t [0.44308908 0.68739187 0.81116634]. \t  \u001b[92m3.123013531063096\u001b[0m \t 3.123013531063096\n",
      "34     \t [7.86613294e-01 2.66901910e-01 2.89050414e-10]. \t  0.05548715062537086 \t 3.123013531063096\n",
      "35     \t [0.87941081 0.77756606 0.        ]. \t  0.0007283666524050162 \t 3.123013531063096\n",
      "36     \t [0.0048933  0.22296955 0.12062001]. \t  0.315128526193098 \t 3.123013531063096\n",
      "37     \t [0.         0.         0.24120358]. \t  0.5681255559376147 \t 3.123013531063096\n",
      "38     \t [1.         0.71369837 0.91083482]. \t  2.6858612110812587 \t 3.123013531063096\n",
      "39     \t [0.30463116 1.         0.73548558]. \t  1.1497445821929033 \t 3.123013531063096\n",
      "40     \t [0.17293064 0.13053941 0.96663048]. \t  0.4328432869258093 \t 3.123013531063096\n",
      "41     \t [5.30072516e-09 9.99999990e-01 2.75306300e-01]. \t  0.23729281251156856 \t 3.123013531063096\n",
      "42     \t [1.         0.30629473 0.90017624]. \t  1.9169006362978656 \t 3.123013531063096\n",
      "43     \t [0.         0.53158511 0.64413525]. \t  1.8758675164618843 \t 3.123013531063096\n",
      "44     \t [0.21382478 0.         0.13676607]. \t  0.4866588351065435 \t 3.123013531063096\n",
      "45     \t [1.         0.18493514 0.20114254]. \t  0.2535940217751877 \t 3.123013531063096\n",
      "46     \t [0.99999994 1.         0.75990722]. \t  0.4128678592141537 \t 3.123013531063096\n",
      "47     \t [0.55927956 0.56466832 0.81471951]. \t  \u001b[92m3.6640042749097628\u001b[0m \t 3.6640042749097628\n",
      "48     \t [0.59444729 0.41929783 0.82225066]. \t  3.20452933803496 \t 3.6640042749097628\n",
      "49     \t [0.48686509 0.51620972 0.85298374]. \t  \u001b[92m3.796244903394979\u001b[0m \t 3.796244903394979\n",
      "50     \t [0.63321551 0.63954343 0.88794371]. \t  3.4705360668408707 \t 3.796244903394979\n",
      "51     \t [0.50414514 0.48087824 0.85076281]. \t  3.6624873112977547 \t 3.796244903394979\n",
      "52     \t [0.59735844 0.55805921 0.84440968]. \t  \u001b[92m3.806154000332966\u001b[0m \t 3.806154000332966\n",
      "53     \t [0.396528   0.42686665 0.80675071]. \t  3.2074880230211438 \t 3.806154000332966\n",
      "54     \t [0.54070252 0.51272961 0.8814412 ]. \t  3.695084672627413 \t 3.806154000332966\n",
      "55     \t [0.98271704 0.79980215 0.13977839]. \t  0.003519085850986699 \t 3.806154000332966\n",
      "56     \t [0.46304042 0.5440475  0.75555107]. \t  3.077044041486413 \t 3.806154000332966\n",
      "57     \t [0.65608179 0.54400001 0.86985675]. \t  3.7805647007027825 \t 3.806154000332966\n",
      "58     \t [0.611414   0.53984876 0.91971928]. \t  3.3880324235569232 \t 3.806154000332966\n",
      "59     \t [0.63973615 0.46924582 0.88074417]. \t  3.490319035153036 \t 3.806154000332966\n",
      "60     \t [0.61633836 0.42055301 0.80264754]. \t  3.1056868560397763 \t 3.806154000332966\n",
      "61     \t [0.75909538 0.58411675 0.82146815]. \t  3.5993533845334995 \t 3.806154000332966\n",
      "62     \t [0.50595243 0.54022854 0.84127314]. \t  \u001b[92m3.81868527406911\u001b[0m \t 3.81868527406911\n",
      "63     \t [0.21495475 0.53049744 0.84370828]. \t  \u001b[92m3.830615631543687\u001b[0m \t 3.830615631543687\n",
      "64     \t [0.2567204  0.56829324 0.88606246]. \t  3.7471135431831017 \t 3.830615631543687\n",
      "65     \t [0.35123867 0.60952623 0.83928291]. \t  3.732490353457081 \t 3.830615631543687\n",
      "66     \t [0.64075583 0.5555534  0.84318382]. \t  3.791697306058163 \t 3.830615631543687\n",
      "67     \t [0.71302735 0.60703304 0.7817489 ]. \t  3.1569999519198646 \t 3.830615631543687\n",
      "68     \t [0.40611305 0.49027668 0.83114452]. \t  3.681167090102849 \t 3.830615631543687\n",
      "69     \t [0.16862475 0.60128923 0.78902059]. \t  3.455753796307925 \t 3.830615631543687\n",
      "70     \t [0.48312972 0.57260284 0.88989549]. \t  3.7092226960718637 \t 3.830615631543687\n",
      "71     \t [0.90775283 0.5254988  0.84242035]. \t  3.6754742689963997 \t 3.830615631543687\n",
      "72     \t [0.25265524 0.58841491 0.71399517]. \t  2.680538334983674 \t 3.830615631543687\n",
      "73     \t [0.84006022 0.97304116 0.17307741]. \t  0.007181229825528524 \t 3.830615631543687\n",
      "74     \t [0.3271853  0.5502691  0.81880321]. \t  3.7518268259368073 \t 3.830615631543687\n",
      "75     \t [0.69988554 0.4932088  0.89125782]. \t  3.5262379775047323 \t 3.830615631543687\n",
      "76     \t [0.74946233 0.54481031 0.80881192]. \t  3.5575423351560125 \t 3.830615631543687\n",
      "77     \t [0.28925227 0.5054252  0.86896067]. \t  3.7448990347871893 \t 3.830615631543687\n",
      "78     \t [0.44689423 0.5154976  0.91149153]. \t  3.453949769823039 \t 3.830615631543687\n",
      "79     \t [0.04767363 0.51142449 0.88248832]. \t  3.6619547321332275 \t 3.830615631543687\n",
      "80     \t [0.64803939 0.53591233 0.87460719]. \t  3.7576396129466323 \t 3.830615631543687\n",
      "81     \t [0.49384576 0.55502891 0.894738  ]. \t  3.6782435875154134 \t 3.830615631543687\n",
      "82     \t [0.87802128 0.59852302 0.83343611]. \t  3.5770021052549623 \t 3.830615631543687\n",
      "83     \t [0.82840687 0.63762571 0.84676579]. \t  3.4705610852977546 \t 3.830615631543687\n",
      "84     \t [0.17953857 0.50849238 0.8244006 ]. \t  3.7157193149805767 \t 3.830615631543687\n",
      "85     \t [0.22755334 0.54861016 0.85031784]. \t  \u001b[92m3.857020729969175\u001b[0m \t 3.857020729969175\n",
      "86     \t [0.2106337  0.55328243 0.89958027]. \t  3.637762942946111 \t 3.857020729969175\n",
      "87     \t [0.59381703 0.56848542 0.8252941 ]. \t  3.7184815457007714 \t 3.857020729969175\n",
      "88     \t [0.60593404 0.57581286 0.83618806]. \t  3.7568026275641158 \t 3.857020729969175\n",
      "89     \t [0.23407233 0.5913158  0.90817576]. \t  3.524220147307459 \t 3.857020729969175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.82299745 0.59586421 0.83557426]. \t  3.620713098074565 \t 3.857020729969175\n",
      "91     \t [0.36324764 0.58854342 0.88497851]. \t  3.7263568879812476 \t 3.857020729969175\n",
      "92     \t [0.21639331 0.63072894 0.86549085]. \t  3.6523227658908377 \t 3.857020729969175\n",
      "93     \t [0.6527905  0.54931601 0.79228307]. \t  3.4342895364589587 \t 3.857020729969175\n",
      "94     \t [0.61708215 0.50320504 0.89741626]. \t  3.5335186666472373 \t 3.857020729969175\n",
      "95     \t [0.52368506 0.50988633 0.85509094]. \t  3.772031349632633 \t 3.857020729969175\n",
      "96     \t [0.29990595 0.59975688 0.90436139]. \t  3.546507907978062 \t 3.857020729969175\n",
      "97     \t [0.38014222 0.54920237 0.89590161]. \t  3.674897938816254 \t 3.857020729969175\n",
      "98     \t [0.32765339 0.51354126 0.83936791]. \t  3.7889009526790947 \t 3.857020729969175\n",
      "99     \t [0.66185242 0.55095181 0.84959036]. \t  3.7985572152190583 \t 3.857020729969175\n",
      "100    \t [0.27165906 0.56077295 0.89527503]. \t  3.682238418941688 \t 3.857020729969175\n"
     ]
    }
   ],
   "source": [
    "### 6(q). Bayesian optimization runs (x20): STP DF1 run number = 17\n",
    "\n",
    "np.random.seed(run_num_17)\n",
    "surrogate_stp_df1_17 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_17 = GPGO(surrogate_stp_df1_17, Acquisition_new(util_stp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_17.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.2612768783466197, -5.1569445430649665)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(q). Training Regret Minimisation: run number = 17\n",
    "\n",
    "gp_output_17 = np.append(np.max(gpgo_gp_17.GP.y[0:n_init]),gpgo_gp_17.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_17 = np.append(np.max(gpgo_stp_df1_17.GP.y[0:n_init]),gpgo_stp_df1_17.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_17 = np.log(y_global_orig - gp_output_17)\n",
    "regret_stp_df1_17 = np.log(y_global_orig - stp_df1_output_17)\n",
    "\n",
    "train_regret_gp_17 = min_max_array(regret_gp_17)\n",
    "train_regret_stp_df1_17 = min_max_array(regret_stp_df1_17)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 17\n",
    "min_train_regret_gp_17 = min(train_regret_gp_17)\n",
    "min_train_regret_stp_df1_17 = min(train_regret_stp_df1_17)\n",
    "\n",
    "min_train_regret_gp_17, min_train_regret_stp_df1_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.56958114 0.99790771 0.07743185]. \t  0.0015921590337453246 \t 3.0090565698308533\n",
      "init   \t [0.66628867 0.59562743 0.76468568]. \t  3.0090565698308533 \t 3.0090565698308533\n",
      "init   \t [0.05445767 0.68881313 0.38234976]. \t  0.8935231400826513 \t 3.0090565698308533\n",
      "init   \t [0.23364499 0.56422664 0.35897164]. \t  0.4542971398621133 \t 3.0090565698308533\n",
      "init   \t [0.89024441 0.76283226 0.72430813]. \t  1.4917908393689505 \t 3.0090565698308533\n",
      "1      \t [0.28008634 0.55367325 1.        ]. \t  2.082284785549663 \t 3.0090565698308533\n",
      "2      \t [1. 0. 1.]. \t  0.08848201872702738 \t 3.0090565698308533\n",
      "3      \t [0.30980449 1.         0.75554544]. \t  1.0223478483667936 \t 3.0090565698308533\n",
      "4      \t [0.74165427 0.61781439 0.96397875]. \t  2.6304171265839646 \t 3.0090565698308533\n",
      "5      \t [0.44974298 0.24642178 0.90868148]. \t  1.3993585419844592 \t 3.0090565698308533\n",
      "6      \t [0.94736043 0.37082045 0.39651852]. \t  0.1461171552715657 \t 3.0090565698308533\n",
      "7      \t [0.1211028  0.92521855 0.0430827 ]. \t  0.0014026188960298879 \t 3.0090565698308533\n",
      "8      \t [3.60600234e-04 7.37036829e-01 9.45333671e-01]. \t  2.256959958336182 \t 3.0090565698308533\n",
      "9      \t [0.97531832 0.51749605 0.94138432]. \t  2.9462713078220513 \t 3.0090565698308533\n",
      "10     \t [0.4289846  0.67646525 0.75295999]. \t  2.7393581185893625 \t 3.0090565698308533\n",
      "11     \t [0.9092181  0.8567462  0.44335697]. \t  0.31267812607441464 \t 3.0090565698308533\n",
      "12     \t [0.38034447 0.1463173  0.24702142]. \t  0.9796956615333223 \t 3.0090565698308533\n",
      "13     \t [0.1285027  0.11916538 0.01387647]. \t  0.12244282075925639 \t 3.0090565698308533\n",
      "14     \t [0.12231091 0.02836898 0.2482283 ]. \t  0.7621207893356235 \t 3.0090565698308533\n",
      "15     \t [0.23297938 0.46259671 0.16933468]. \t  0.22104948781719047 \t 3.0090565698308533\n",
      "16     \t [0.67675227 0.42565427 0.58594748]. \t  0.7609022475540819 \t 3.0090565698308533\n",
      "17     \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.0090565698308533\n",
      "18     \t [0.47202184 0.         0.        ]. \t  0.09903617830144833 \t 3.0090565698308533\n",
      "19     \t [0.27563951 0.2959729  0.73053409]. \t  1.6604441501305622 \t 3.0090565698308533\n",
      "20     \t [0.01162155 0.96109372 0.63266654]. \t  2.3767180637420666 \t 3.0090565698308533\n",
      "21     \t [0.03975978 0.86739271 0.80447467]. \t  1.7027453780197517 \t 3.0090565698308533\n",
      "22     \t [0.26330455 0.78598847 0.55712736]. \t  2.740427263178963 \t 3.0090565698308533\n",
      "23     \t [0.22029939 0.14267445 0.92609964]. \t  0.6256931446062705 \t 3.0090565698308533\n",
      "24     \t [0.83116072 0.04190395 0.44787579]. \t  0.19857864000864098 \t 3.0090565698308533\n",
      "25     \t [0.70636817 0.43799605 0.0406495 ]. \t  0.054360138213480035 \t 3.0090565698308533\n",
      "26     \t [0.7660409  0.73579825 0.79434102]. \t  2.4236000049184483 \t 3.0090565698308533\n",
      "27     \t [0.97241784 0.28545169 0.57321212]. \t  0.39359041322600297 \t 3.0090565698308533\n",
      "28     \t [0.48997929 0.68900139 0.9571813 ]. \t  2.4562334548055906 \t 3.0090565698308533\n",
      "29     \t [0.24795288 0.00651018 0.64468806]. \t  0.1583586902307153 \t 3.0090565698308533\n",
      "30     \t [0.59951893 0.32433631 0.56327121]. \t  0.481939607248694 \t 3.0090565698308533\n",
      "31     \t [0.00457993 0.32830533 0.38450656]. \t  0.357831996835838 \t 3.0090565698308533\n",
      "32     \t [0.3278872  0.55966829 0.77999973]. \t  \u001b[92m3.4132676557436534\u001b[0m \t 3.4132676557436534\n",
      "33     \t [0.52294031 0.6423345  0.97616677]. \t  2.3785268886952666 \t 3.4132676557436534\n",
      "34     \t [0.         0.45891882 0.89178334]. \t  3.3314745621444377 \t 3.4132676557436534\n",
      "35     \t [0.50589959 0.67089043 0.964269  ]. \t  2.4493092350308325 \t 3.4132676557436534\n",
      "36     \t [0.28029822 0.24029676 0.64252897]. \t  0.7413949229253072 \t 3.4132676557436534\n",
      "37     \t [0.17177419 1.         0.33500956]. \t  0.5938762696916396 \t 3.4132676557436534\n",
      "38     \t [0.42926734 0.63528645 0.39694126]. \t  0.646273844252865 \t 3.4132676557436534\n",
      "39     \t [0.07477626 0.77360975 0.38917425]. \t  1.2013800639602776 \t 3.4132676557436534\n",
      "40     \t [0.80227059 0.75341671 0.91276159]. \t  2.3919856748340997 \t 3.4132676557436534\n",
      "41     \t [0.78491565 0.35739703 0.62711739]. \t  0.9244641848496619 \t 3.4132676557436534\n",
      "42     \t [0.65466918 0.74754918 0.01949571]. \t  0.0025215395561674343 \t 3.4132676557436534\n",
      "43     \t [0.76279964 0.32929696 0.90411326]. \t  2.149041531210113 \t 3.4132676557436534\n",
      "44     \t [0.19300337 0.94967478 0.76840814]. \t  1.2828928292176012 \t 3.4132676557436534\n",
      "45     \t [0.53008054 0.64790432 0.50134084]. \t  1.0972019088774791 \t 3.4132676557436534\n",
      "46     \t [0.59266448 0.24682307 0.67848575]. \t  0.9795488017094827 \t 3.4132676557436534\n",
      "47     \t [0.75114758 0.80457091 0.13493513]. \t  0.007581213036165506 \t 3.4132676557436534\n",
      "48     \t [0.74233991 0.94944572 0.41637275]. \t  0.4813946762320609 \t 3.4132676557436534\n",
      "49     \t [0.5636965  0.6510174  0.05472629]. \t  0.013829108866421655 \t 3.4132676557436534\n",
      "50     \t [0.92302544 0.52414868 0.47645789]. \t  0.2153031196004235 \t 3.4132676557436534\n",
      "51     \t [0.52127318 0.52443949 0.51231328]. \t  0.7229427960419332 \t 3.4132676557436534\n",
      "52     \t [0.94340565 0.76658624 0.80043636]. \t  2.1059263478813666 \t 3.4132676557436534\n",
      "53     \t [0.29555102 0.59107645 0.24676945]. \t  0.17373418661612466 \t 3.4132676557436534\n",
      "54     \t [0.7326438  0.5598479  0.62425415]. \t  1.2160989570338687 \t 3.4132676557436534\n",
      "55     \t [0.46886938 0.52456127 0.62974443]. \t  1.5337697028839168 \t 3.4132676557436534\n",
      "56     \t [0.36096211 0.75878865 0.64911016]. \t  2.3098329936417556 \t 3.4132676557436534\n",
      "57     \t [0.22837151 0.51883192 0.68636951]. \t  2.2736794810006926 \t 3.4132676557436534\n",
      "58     \t [0.11717074 0.52627159 0.96506684]. \t  2.663954101260856 \t 3.4132676557436534\n",
      "59     \t [0.97735296 0.82307273 0.39892734]. \t  0.15282815990197485 \t 3.4132676557436534\n",
      "60     \t [0.23292992 0.26818906 0.63447305]. \t  0.7913902831934719 \t 3.4132676557436534\n",
      "61     \t [0.24417413 0.49478719 0.08006683]. \t  0.08079907603873807 \t 3.4132676557436534\n",
      "62     \t [0.41240371 0.21101068 0.45407101]. \t  0.377401670664511 \t 3.4132676557436534\n",
      "63     \t [0.12086981 0.8961889  0.58911822]. \t  2.995093032851842 \t 3.4132676557436534\n",
      "64     \t [0.18466217 0.09111755 0.72358895]. \t  0.4804772803355755 \t 3.4132676557436534\n",
      "65     \t [0.08812088 0.45928621 0.3312873 ]. \t  0.3400112064391388 \t 3.4132676557436534\n",
      "66     \t [0.09005792 0.38493515 0.97914297]. \t  1.7774656879926547 \t 3.4132676557436534\n",
      "67     \t [0.85319644 0.73179477 0.94653199]. \t  2.2473531058556233 \t 3.4132676557436534\n",
      "68     \t [0.641336   0.72745097 0.4791337 ]. \t  0.92934404205747 \t 3.4132676557436534\n",
      "69     \t [0.26783608 0.5420739  0.02426879]. \t  0.02726748743584299 \t 3.4132676557436534\n",
      "70     \t [0.0883217  0.32365508 0.29756509]. \t  0.5222254099693386 \t 3.4132676557436534\n",
      "71     \t [0.86830494 0.79899416 0.30609522]. \t  0.08357964811246296 \t 3.4132676557436534\n",
      "72     \t [0.61605558 0.30366563 0.95602393]. \t  1.4847966351812047 \t 3.4132676557436534\n",
      "73     \t [0.18810049 0.38857521 0.71365624]. \t  2.079098488173578 \t 3.4132676557436534\n",
      "74     \t [0.13543055 0.58202083 0.33201356]. \t  0.37823776067192605 \t 3.4132676557436534\n",
      "75     \t [0.63412211 0.91638367 0.30924331]. \t  0.21275363781100942 \t 3.4132676557436534\n",
      "76     \t [0.99122992 0.73635612 0.52589615]. \t  0.3504044840074514 \t 3.4132676557436534\n",
      "77     \t [0.72345122 0.60271782 0.69856713]. \t  2.0458997904054526 \t 3.4132676557436534\n",
      "78     \t [0.41865705 0.52321142 0.51237803]. \t  0.8462653330976241 \t 3.4132676557436534\n",
      "79     \t [0.95018987 0.37370397 0.12325249]. \t  0.1008586171640197 \t 3.4132676557436534\n",
      "80     \t [0.25528616 0.66298584 0.03835126]. \t  0.010737671001000398 \t 3.4132676557436534\n",
      "81     \t [0.1130526  0.47269295 0.32990745]. \t  0.34133753595721045 \t 3.4132676557436534\n",
      "82     \t [0.25185482 0.93225163 0.38395643]. \t  1.1385690396687391 \t 3.4132676557436534\n",
      "83     \t [0.141627   0.17754181 0.81660786]. \t  1.0751674954949184 \t 3.4132676557436534\n",
      "84     \t [0.54108088 0.89973097 0.51885327]. \t  1.6717389500116397 \t 3.4132676557436534\n",
      "85     \t [0.96834438 0.06913777 0.1092202 ]. \t  0.15714667359216883 \t 3.4132676557436534\n",
      "86     \t [0.30160992 0.63090251 0.05866265]. \t  0.019984159036218085 \t 3.4132676557436534\n",
      "87     \t [0.41557609 0.77520143 0.7255609 ]. \t  2.1363725677365526 \t 3.4132676557436534\n",
      "88     \t [0.92081502 0.58134916 0.38143573]. \t  0.11284698616895959 \t 3.4132676557436534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.84753875 0.21511821 0.05568592]. \t  0.11920202371107667 \t 3.4132676557436534\n",
      "90     \t [0.01605769 0.64969334 0.8253368 ]. \t  \u001b[92m3.462255486199559\u001b[0m \t 3.462255486199559\n",
      "91     \t [0.80018132 0.4295689  0.05506522]. \t  0.055837558689102215 \t 3.462255486199559\n",
      "92     \t [0.28093383 0.88638802 0.38761102]. \t  1.189851121012053 \t 3.462255486199559\n",
      "93     \t [0.4075054  0.02884244 0.95680571]. \t  0.18273496141611212 \t 3.462255486199559\n",
      "94     \t [0.55211689 0.92376801 0.02249878]. \t  0.0005544019318724364 \t 3.462255486199559\n",
      "95     \t [0.50762658 0.70612604 0.79209695]. \t  2.8197596990607865 \t 3.462255486199559\n",
      "96     \t [0.90088452 0.12914778 0.74568912]. \t  0.6751978006645867 \t 3.462255486199559\n",
      "97     \t [0.87613968 0.42051026 0.73011154]. \t  2.2813146524668344 \t 3.462255486199559\n",
      "98     \t [0.23154994 0.35176825 0.09568859]. \t  0.22542703533870015 \t 3.462255486199559\n",
      "99     \t [0.96421056 0.36795867 0.76779511]. \t  2.3757415312134995 \t 3.462255486199559\n",
      "100    \t [0.69264797 0.17959618 0.63737859]. \t  0.500441921988056 \t 3.462255486199559\n"
     ]
    }
   ],
   "source": [
    "### 6(r). Bayesian optimization runs (x20): GP run number = 18\n",
    "\n",
    "np.random.seed(run_num_18)\n",
    "surrogate_gp_18 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "gpgo_gp_18 = GPGO(surrogate_gp_18, Acquisition_new(util_gp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_18.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.56958114 0.99790771 0.07743185]. \t  0.0015921590337453246 \t 3.0090565698308533\n",
      "init   \t [0.66628867 0.59562743 0.76468568]. \t  3.0090565698308533 \t 3.0090565698308533\n",
      "init   \t [0.05445767 0.68881313 0.38234976]. \t  0.8935231400826513 \t 3.0090565698308533\n",
      "init   \t [0.23364499 0.56422664 0.35897164]. \t  0.4542971398621133 \t 3.0090565698308533\n",
      "init   \t [0.89024441 0.76283226 0.72430813]. \t  1.4917908393689505 \t 3.0090565698308533\n",
      "1      \t [0.16798422 0.70067839 1.        ]. \t  1.7483691705776572 \t 3.0090565698308533\n",
      "2      \t [1. 0. 1.]. \t  0.08848201872702748 \t 3.0090565698308533\n",
      "3      \t [0.53743629 1.         1.        ]. \t  0.33161598702916395 \t 3.0090565698308533\n",
      "4      \t [0. 0. 1.]. \t  0.0902894676548261 \t 3.0090565698308533\n",
      "5      \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.0090565698308533\n",
      "6      \t [-5.55111512e-17  1.00000000e+00  0.00000000e+00]. \t  0.0002735367680454459 \t 3.0090565698308533\n",
      "7      \t [0.         1.         0.62777393]. \t  2.183421633578697 \t 3.0090565698308533\n",
      "8      \t [1.         0.58884429 0.        ]. \t  0.0038427974500132435 \t 3.0090565698308533\n",
      "9      \t [1.         0.         0.50845018]. \t  0.07033308738613096 \t 3.0090565698308533\n",
      "10     \t [ 0.00000000e+00  0.00000000e+00 -5.55111512e-17]. \t  0.06797411659013224 \t 3.0090565698308533\n",
      "11     \t [0.58015675 0.32286962 1.        ]. \t  1.1590785158134713 \t 3.0090565698308533\n",
      "12     \t [1.         0.46378613 1.        ]. \t  1.8088336012388704 \t 3.0090565698308533\n",
      "13     \t [0.         0.         0.54103544]. \t  0.10340149157936897 \t 3.0090565698308533\n",
      "14     \t [0. 1. 1.]. \t  0.330219860606422 \t 3.0090565698308533\n",
      "15     \t [0.56097546 0.         0.        ]. \t  0.09153381683019599 \t 3.0090565698308533\n",
      "16     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 3.0090565698308533\n",
      "17     \t [0.         0.44649997 0.81786715]. \t  \u001b[92m3.3538292876233964\u001b[0m \t 3.3538292876233964\n",
      "18     \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.3538292876233964\n",
      "19     \t [0.         0.68581698 0.77864465]. \t  2.9732018369010733 \t 3.3538292876233964\n",
      "20     \t [0.        0.4809997 1.       ]. \t  1.9118507124054176 \t 3.3538292876233964\n",
      "21     \t [0.         0.42220785 0.        ]. \t  0.030744292078595564 \t 3.3538292876233964\n",
      "22     \t [1.         0.29952642 0.76008795]. \t  1.8131366564658415 \t 3.3538292876233964\n",
      "23     \t [0.46663327 0.         0.74145791]. \t  0.23500298419168716 \t 3.3538292876233964\n",
      "24     \t [0.33301698 1.         0.67543521]. \t  1.578081662298369 \t 3.3538292876233964\n",
      "25     \t [0.79476033 0.29394527 0.32336004]. \t  0.3931371494908251 \t 3.3538292876233964\n",
      "26     \t [0.6601374  0.61742435 0.        ]. \t  0.007491807814562949 \t 3.3538292876233964\n",
      "27     \t [0.26684185 0.47398703 0.76894234]. \t  3.1392613736182398 \t 3.3538292876233964\n",
      "28     \t [1.         1.         0.34215559]. \t  0.0611838946593913 \t 3.3538292876233964\n",
      "29     \t [1.         0.65721796 0.29693302]. \t  0.040250417859764624 \t 3.3538292876233964\n",
      "30     \t [2.25165608e-01 9.09981167e-01 6.08608216e-04]. \t  0.0004909669112775518 \t 3.3538292876233964\n",
      "31     \t [2.00630687e-01 2.90134608e-01 1.38777878e-17]. \t  0.0798138393275205 \t 3.3538292876233964\n",
      "32     \t [0.         0.27237239 0.4958608 ]. \t  0.28414598495997184 \t 3.3538292876233964\n",
      "33     \t [0.80682164 0.67320585 0.98692257]. \t  2.04284295970114 \t 3.3538292876233964\n",
      "34     \t [0.77946085 0.03319371 0.72119186]. \t  0.29327748503655215 \t 3.3538292876233964\n",
      "35     \t [0.63796296 0.86844211 0.47721349]. \t  1.1021523261742947 \t 3.3538292876233964\n",
      "36     \t [0.34929271 0.73166791 0.77994001]. \t  2.664558368879188 \t 3.3538292876233964\n",
      "37     \t [0.03661979 1.         0.31039629]. \t  0.4199459166301415 \t 3.3538292876233964\n",
      "38     \t [0.24139175 0.02977541 0.18560471]. \t  0.7225121329538547 \t 3.3538292876233964\n",
      "39     \t [0.75863395 0.99242363 0.75622733]. \t  0.5858005130496777 \t 3.3538292876233964\n",
      "40     \t [0.11327077 0.24474594 0.85200087]. \t  1.5944721859986566 \t 3.3538292876233964\n",
      "41     \t [0.42426031 0.         1.        ]. \t  0.0917316912963759 \t 3.3538292876233964\n",
      "42     \t [2.41398230e-01 6.74503784e-11 0.00000000e+00]. \t  0.09737971063370304 \t 3.3538292876233964\n",
      "43     \t [1.         0.70779291 1.        ]. \t  1.654153704370445 \t 3.3538292876233964\n",
      "44     \t [0.20295899 0.56170917 0.8089928 ]. \t  \u001b[92m3.6863954837037487\u001b[0m \t 3.6863954837037487\n",
      "45     \t [0.06258291 0.56549533 0.80871543]. \t  3.6585114469879825 \t 3.6863954837037487\n",
      "46     \t [1.00000000e+00 2.74476814e-01 2.85203354e-14]. \t  0.027700564797345055 \t 3.6863954837037487\n",
      "47     \t [0.4075965  0.57935555 0.82511748]. \t  \u001b[92m3.7502169665923795\u001b[0m \t 3.7502169665923795\n",
      "48     \t [0.20238247 0.51528408 0.89906196]. \t  3.5755646831095262 \t 3.7502169665923795\n",
      "49     \t [0.44870542 0.53691461 0.85076139]. \t  \u001b[92m3.8397471223665596\u001b[0m \t 3.8397471223665596\n",
      "50     \t [0.21994577 0.51556104 0.89992572]. \t  3.5703409581495658 \t 3.8397471223665596\n",
      "51     \t [0.30299129 0.56869417 0.93460386]. \t  3.219246468364322 \t 3.8397471223665596\n",
      "52     \t [0.         0.67038312 0.        ]. \t  0.00383417880676077 \t 3.8397471223665596\n",
      "53     \t [8.89749534e-01 1.93749948e-08 1.73453836e-01]. \t  0.2967278755253827 \t 3.8397471223665596\n",
      "54     \t [0.29796357 0.58055351 0.84790762]. \t  3.8350834464525487 \t 3.8397471223665596\n",
      "55     \t [0.22566946 0.57621951 0.84910618]. \t  \u001b[92m3.841820814686842\u001b[0m \t 3.841820814686842\n",
      "56     \t [0.14178579 0.51119375 0.8813794 ]. \t  3.6888130127343652 \t 3.841820814686842\n",
      "57     \t [0.36509469 0.46887739 0.79420308]. \t  3.3602535369835755 \t 3.841820814686842\n",
      "58     \t [0.08984291 0.52639569 0.8181767 ]. \t  3.708766395747336 \t 3.841820814686842\n",
      "59     \t [0.41124709 0.52215781 0.78527097]. \t  3.4301547096927294 \t 3.841820814686842\n",
      "60     \t [0.39226241 0.5396153  0.8082729 ]. \t  3.6655443482292736 \t 3.841820814686842\n",
      "61     \t [0.90683991 0.83205063 0.18156279]. \t  0.008737796029499546 \t 3.841820814686842\n",
      "62     \t [0.51110004 0.61467919 0.87575608]. \t  3.673201256058688 \t 3.841820814686842\n",
      "63     \t [0.88984726 0.52398821 0.86547318]. \t  3.6871245629962095 \t 3.841820814686842\n",
      "64     \t [0.23424528 0.59975304 0.8534589 ]. \t  3.7906836179895325 \t 3.841820814686842\n",
      "65     \t [0.47377002 0.48160759 0.87133712]. \t  3.627808165962213 \t 3.841820814686842\n",
      "66     \t [0.18691043 0.54645407 0.85933037]. \t  \u001b[92m3.847055154300714\u001b[0m \t 3.847055154300714\n",
      "67     \t [0.1269224  0.44862513 0.80417022]. \t  3.3197861218982716 \t 3.847055154300714\n",
      "68     \t [0.65650689 0.54733582 0.86463549]. \t  3.795001219823649 \t 3.847055154300714\n",
      "69     \t [0.21178611 0.58619737 0.8845799 ]. \t  3.729133609879204 \t 3.847055154300714\n",
      "70     \t [0.47021768 0.50062427 0.84424766]. \t  3.7453688137448635 \t 3.847055154300714\n",
      "71     \t [0.52405698 0.53514035 0.87981227]. \t  3.757409621603679 \t 3.847055154300714\n",
      "72     \t [0.24918949 0.62037712 0.86522152]. \t  3.7027817694702687 \t 3.847055154300714\n",
      "73     \t [0.20917312 0.44673055 0.93035339]. \t  2.879138449143638 \t 3.847055154300714\n",
      "74     \t [0.53055566 0.52612237 0.87417712]. \t  3.7671203223715195 \t 3.847055154300714\n",
      "75     \t [0.67886232 0.55173383 0.78405353]. \t  3.3285585483117526 \t 3.847055154300714\n",
      "76     \t [0.45205081 0.55187221 0.88622779]. \t  3.7444596553851888 \t 3.847055154300714\n",
      "77     \t [0.29985424 0.55433597 0.83343594]. \t  3.8253008714989414 \t 3.847055154300714\n",
      "78     \t [0.33240257 0.64301388 0.84128524]. \t  3.580489386751603 \t 3.847055154300714\n",
      "79     \t [0.50679355 0.56678738 0.86628861]. \t  3.821310522306952 \t 3.847055154300714\n",
      "80     \t [0.49343039 0.53630388 0.85051916]. \t  3.8318793275479512 \t 3.847055154300714\n",
      "81     \t [0.29162487 0.54883369 0.92548563]. \t  3.3432852973251315 \t 3.847055154300714\n",
      "82     \t [0.52830111 0.58468398 0.95996852]. \t  2.793297253509545 \t 3.847055154300714\n",
      "83     \t [0.24427507 0.62217055 0.81746023]. \t  3.5956449405127784 \t 3.847055154300714\n",
      "84     \t [0.19555481 0.57512716 0.85317403]. \t  3.842245823328109 \t 3.847055154300714\n",
      "85     \t [0.06567418 0.5783517  0.83302779]. \t  3.7768485473010993 \t 3.847055154300714\n",
      "86     \t [0.28176589 0.57605383 0.89268622]. \t  3.6937651521259283 \t 3.847055154300714\n",
      "87     \t [0.23899976 0.57954387 0.8777447 ]. \t  3.780539377589035 \t 3.847055154300714\n",
      "88     \t [0.1306065  0.52067621 0.79548014]. \t  3.539619821628818 \t 3.847055154300714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.22274771 0.65794478 0.83675705]. \t  3.486698789879804 \t 3.847055154300714\n",
      "90     \t [0.13423248 0.56648336 0.89329413]. \t  3.680396759906685 \t 3.847055154300714\n",
      "91     \t [0.22224575 0.55453629 0.82684578]. \t  3.797202974766475 \t 3.847055154300714\n",
      "92     \t [0.53556374 0.517486   0.88284009]. \t  3.70131713189076 \t 3.847055154300714\n",
      "93     \t [0.51316279 0.47097054 0.85518717]. \t  3.6078717008063865 \t 3.847055154300714\n",
      "94     \t [0.50898147 0.57828169 0.87924587]. \t  3.761243286534048 \t 3.847055154300714\n",
      "95     \t [0.40842788 0.51532026 0.83141014]. \t  3.763081368520184 \t 3.847055154300714\n",
      "96     \t [0.58695986 0.51698665 0.9298754 ]. \t  3.2092312687994067 \t 3.847055154300714\n",
      "97     \t [0.66561311 0.55116936 0.85342002]. \t  3.8008287929843227 \t 3.847055154300714\n",
      "98     \t [0.21522556 0.5376407  0.78675443]. \t  3.489755547409568 \t 3.847055154300714\n",
      "99     \t [0.17833895 0.59861214 0.8062845 ]. \t  3.6041854615029925 \t 3.847055154300714\n",
      "100    \t [0.48221564 0.61545514 0.86607905]. \t  3.7023982808124556 \t 3.847055154300714\n"
     ]
    }
   ],
   "source": [
    "### 6(r). Bayesian optimization runs (x20): STP DF1 run number = 18\n",
    "\n",
    "np.random.seed(run_num_18)\n",
    "surrogate_stp_df1_18 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_18 = GPGO(surrogate_stp_df1_18, Acquisition_new(util_stp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_18.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.9149803063557428, -4.15251328888983)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(r). Training Regret Minimisation: run number = 18\n",
    "\n",
    "gp_output_18 = np.append(np.max(gpgo_gp_18.GP.y[0:n_init]),gpgo_gp_18.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_18 = np.append(np.max(gpgo_stp_df1_18.GP.y[0:n_init]),gpgo_stp_df1_18.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_18 = np.log(y_global_orig - gp_output_18)\n",
    "regret_stp_df1_18 = np.log(y_global_orig - stp_df1_output_18)\n",
    "\n",
    "train_regret_gp_18 = min_max_array(regret_gp_18)\n",
    "train_regret_stp_df1_18 = min_max_array(regret_stp_df1_18)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 18\n",
    "min_train_regret_gp_18 = min(train_regret_gp_18)\n",
    "min_train_regret_stp_df1_18 = min(train_regret_stp_df1_18)\n",
    "\n",
    "min_train_regret_gp_18, min_train_regret_stp_df1_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.82452017 0.98462676 0.89314452]. \t  0.6273086368875856 \t 2.1561178330322286\n",
      "init   \t [0.31964691 0.5932567  0.19549232]. \t  0.11304217526683821 \t 2.1561178330322286\n",
      "init   \t [0.86021676 0.86430051 0.07861785]. \t  0.001240365156738587 \t 2.1561178330322286\n",
      "init   \t [0.71691207 0.72534231 0.73504778]. \t  1.9949269324745122 \t 2.1561178330322286\n",
      "init   \t [0.794917   0.3675148  0.93827185]. \t  2.1561178330322286 \t 2.1561178330322286\n",
      "1      \t [0.         0.44063991 1.        ]. \t  1.757139059410569 \t 2.1561178330322286\n",
      "2      \t [0.92466766 0.10949252 0.06066069]. \t  0.10990011162240057 \t 2.1561178330322286\n",
      "3      \t [0.04170108 0.98219237 0.7896226 ]. \t  1.0016884335569913 \t 2.1561178330322286\n",
      "4      \t [0.15932069 0.03349437 0.73144715]. \t  0.30711678059770103 \t 2.1561178330322286\n",
      "5      \t [0.97981285 0.4358843  0.60538292]. \t  0.7978712781394838 \t 2.1561178330322286\n",
      "6      \t [0.30011299 0.80431552 0.9746108 ]. \t  1.466912290720892 \t 2.1561178330322286\n",
      "7      \t [0.10527681 0.95671063 0.13124968]. \t  0.013304464969727137 \t 2.1561178330322286\n",
      "8      \t [0.6952437  0.05566769 0.85858491]. \t  0.388912912916503 \t 2.1561178330322286\n",
      "9      \t [0.03080891 0.21084339 0.03515589]. \t  0.12904129855377816 \t 2.1561178330322286\n",
      "10     \t [0.00700067 0.70256742 0.44223186]. \t  1.5213482859970213 \t 2.1561178330322286\n",
      "11     \t [0.74100638 0.54270201 0.40820676]. \t  0.23938527742763563 \t 2.1561178330322286\n",
      "12     \t [0.98851896 0.1431645  0.93423669]. \t  0.5811817412012339 \t 2.1561178330322286\n",
      "13     \t [0.54862779 0.185083   0.23837768]. \t  0.8458618983836569 \t 2.1561178330322286\n",
      "14     \t [0.82331522 0.77682757 0.04727918]. \t  0.0018813306586089216 \t 2.1561178330322286\n",
      "15     \t [0.25392326 0.01753782 0.00252167]. \t  0.10626584754456266 \t 2.1561178330322286\n",
      "16     \t [0.85407591 0.98205561 0.45570561]. \t  0.38000935949850795 \t 2.1561178330322286\n",
      "17     \t [0.00858381 0.50110871 0.07690017]. \t  0.052985442381924784 \t 2.1561178330322286\n",
      "18     \t [0.73077833 0.13375398 0.91323886]. \t  0.6182697518083271 \t 2.1561178330322286\n",
      "19     \t [0.56196819 0.68703348 0.37773445]. \t  0.4780089117564882 \t 2.1561178330322286\n",
      "20     \t [0.40158187 0.30268337 0.91701004]. \t  1.8329528308924465 \t 2.1561178330322286\n",
      "21     \t [0.36871495 0.46004707 0.16054244]. \t  0.22320285773127965 \t 2.1561178330322286\n",
      "22     \t [0.04489149 0.28179815 0.0115881 ]. \t  0.07823630267797507 \t 2.1561178330322286\n",
      "23     \t [0.91088861 0.61981791 0.77908971]. \t  \u001b[92m2.959961684756654\u001b[0m \t 2.959961684756654\n",
      "24     \t [0.93712544 0.71022677 0.87114589]. \t  2.9090276573431537 \t 2.959961684756654\n",
      "25     \t [0.11404093 0.43629372 0.65164256]. \t  1.6055497333083995 \t 2.959961684756654\n",
      "26     \t [0.01233006 0.06577671 0.73601712]. \t  0.4056355274317041 \t 2.959961684756654\n",
      "27     \t [0.38514377 0.69017885 0.09665574]. \t  0.01876621285465609 \t 2.959961684756654\n",
      "28     \t [0.2033465  0.70532486 0.76821583]. \t  2.8342998696470065 \t 2.959961684756654\n",
      "29     \t [0.28202365 0.35013026 0.89391261]. \t  2.450817775171962 \t 2.959961684756654\n",
      "30     \t [0.10401149 0.27792765 0.28826605]. \t  0.6280332858725718 \t 2.959961684756654\n",
      "31     \t [0.92799152 0.38023417 0.71111359]. \t  1.8625605275184538 \t 2.959961684756654\n",
      "32     \t [0.27767302 0.5687883  0.18554934]. \t  0.1219703748541654 \t 2.959961684756654\n",
      "33     \t [0.2476098  0.57962432 0.58644946]. \t  1.710759148461701 \t 2.959961684756654\n",
      "34     \t [0.84954895 0.98377218 0.44459678]. \t  0.35953044722335087 \t 2.959961684756654\n",
      "35     \t [0.96944121 0.25967193 0.94548794]. \t  1.2150934359491126 \t 2.959961684756654\n",
      "36     \t [0.68352533 0.48634893 0.23100262]. \t  0.19343292927392064 \t 2.959961684756654\n",
      "37     \t [0.49592986 0.28761255 0.2959124 ]. \t  0.7039557375072858 \t 2.959961684756654\n",
      "38     \t [0.3179192  0.95493172 0.23547365]. \t  0.11663614004055346 \t 2.959961684756654\n",
      "39     \t [0.97228205 0.64140687 0.3969392 ]. \t  0.11322972977729617 \t 2.959961684756654\n",
      "40     \t [0.64314525 0.730001   0.07271057]. \t  0.006956919945909067 \t 2.959961684756654\n",
      "41     \t [0.24304423 0.61291063 0.16265232]. \t  0.07305862146027015 \t 2.959961684756654\n",
      "42     \t [0.51266639 0.41566612 0.66039853]. \t  1.5237159496270198 \t 2.959961684756654\n",
      "43     \t [0.66566541 0.23500127 0.08822782]. \t  0.25529354999382875 \t 2.959961684756654\n",
      "44     \t [0.82600957 0.90331192 0.91389402]. \t  1.0922671563309918 \t 2.959961684756654\n",
      "45     \t [0.93932007 0.65238032 0.79451111]. \t  2.950097212828001 \t 2.959961684756654\n",
      "46     \t [0.75293411 0.38339451 0.30025012]. \t  0.31818920883916224 \t 2.959961684756654\n",
      "47     \t [0.70157686 0.63016257 0.53634963]. \t  0.8008431662152257 \t 2.959961684756654\n",
      "48     \t [0.43075992 0.82009187 0.05370632]. \t  0.002941507951922961 \t 2.959961684756654\n",
      "49     \t [0.31122367 0.32924919 0.91391124]. \t  2.099386849622805 \t 2.959961684756654\n",
      "50     \t [0.92192385 0.70580419 0.66306017]. \t  1.1522499395171426 \t 2.959961684756654\n",
      "51     \t [0.27474425 0.83277773 0.91421182]. \t  1.7344454753512668 \t 2.959961684756654\n",
      "52     \t [0.33582286 0.81375587 0.24359218]. \t  0.14375010069093894 \t 2.959961684756654\n",
      "53     \t [0.78344673 0.02269386 0.49889496]. \t  0.13509199866026939 \t 2.959961684756654\n",
      "54     \t [0.64552606 0.45615125 0.41406992]. \t  0.28055661821599553 \t 2.959961684756654\n",
      "55     \t [0.15778737 0.133807   0.54901988]. \t  0.22146481460069783 \t 2.959961684756654\n",
      "56     \t [0.29773348 0.14730146 0.80910623]. \t  0.8763573486276017 \t 2.959961684756654\n",
      "57     \t [0.2330056  0.52981053 0.02774461]. \t  0.030985004710708766 \t 2.959961684756654\n",
      "58     \t [0.47955244 0.62627238 0.19039628]. \t  0.08051203329841061 \t 2.959961684756654\n",
      "59     \t [0.16061019 0.39909351 0.60569867]. \t  1.044716102280384 \t 2.959961684756654\n",
      "60     \t [0.28456939 0.43122317 0.93835915]. \t  2.6748699950962482 \t 2.959961684756654\n",
      "61     \t [0.82254689 0.68047521 0.70700529]. \t  1.7992998526098303 \t 2.959961684756654\n",
      "62     \t [0.62198033 0.69836864 0.3543649 ]. \t  0.32643611040790055 \t 2.959961684756654\n",
      "63     \t [0.44484843 0.43849629 0.37599603]. \t  0.37933887811540856 \t 2.959961684756654\n",
      "64     \t [0.14722805 0.76726103 0.61524724]. \t  2.7909380710347222 \t 2.959961684756654\n",
      "65     \t [0.47234995 0.5575724  0.86815458]. \t  \u001b[92m3.8269738542235388\u001b[0m \t 3.8269738542235388\n",
      "66     \t [0.84478674 0.34698028 0.87553638]. \t  2.4764172196161254 \t 3.8269738542235388\n",
      "67     \t [0.04811051 0.97038791 0.797992  ]. \t  1.0333327526611367 \t 3.8269738542235388\n",
      "68     \t [0.6665721  0.84826806 0.55539779]. \t  1.2708547038753848 \t 3.8269738542235388\n",
      "69     \t [0.0750117  0.64652368 0.1363376 ]. \t  0.037321251194614394 \t 3.8269738542235388\n",
      "70     \t [0.29344204 0.70527252 0.66890516]. \t  2.39995574402558 \t 3.8269738542235388\n",
      "71     \t [0.19834492 0.78906076 0.3039498 ]. \t  0.42380670166047685 \t 3.8269738542235388\n",
      "72     \t [0.16357136 0.58712554 0.83003738]. \t  3.768650368803586 \t 3.8269738542235388\n",
      "73     \t [0.38447446 0.95443476 0.85001824]. \t  0.9478328190082772 \t 3.8269738542235388\n",
      "74     \t [0.9314772  0.9230741  0.55293894]. \t  0.435265789182831 \t 3.8269738542235388\n",
      "75     \t [0.39924202 0.88681341 0.2305992 ]. \t  0.10212738943876787 \t 3.8269738542235388\n",
      "76     \t [0.54456029 0.51524705 0.52298602]. \t  0.7123501387643273 \t 3.8269738542235388\n",
      "77     \t [0.54664014 0.45774453 0.10568885]. \t  0.13081912064011725 \t 3.8269738542235388\n",
      "78     \t [0.13794793 0.01066562 0.45050206]. \t  0.2882633391338061 \t 3.8269738542235388\n",
      "79     \t [0.9527293  0.9700167  0.18849619]. \t  0.0059874456603420225 \t 3.8269738542235388\n",
      "80     \t [0.8551612  0.1846716  0.88321669]. \t  1.0097354438347454 \t 3.8269738542235388\n",
      "81     \t [0.83960871 0.39210795 0.05139815]. \t  0.05963878985264588 \t 3.8269738542235388\n",
      "82     \t [0.18191387 0.03887788 0.36592012]. \t  0.6352067138028651 \t 3.8269738542235388\n",
      "83     \t [0.09379561 0.51048146 0.63853971]. \t  1.776157230254436 \t 3.8269738542235388\n",
      "84     \t [0.44364168 0.58670954 0.83808142]. \t  3.7837276866634295 \t 3.8269738542235388\n",
      "85     \t [0.91590252 0.67218448 0.89704869]. \t  3.1241886128354035 \t 3.8269738542235388\n",
      "86     \t [0.15246606 0.80217966 0.0905167 ]. \t  0.007531805085568865 \t 3.8269738542235388\n",
      "87     \t [0.73879631 0.42337735 0.2420562 ]. \t  0.26114003189011326 \t 3.8269738542235388\n",
      "88     \t [0.16037909 0.80410913 0.96659887]. \t  1.544010081703296 \t 3.8269738542235388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.32044745 0.76418132 0.57247953]. \t  2.528291762410248 \t 3.8269738542235388\n",
      "90     \t [0.06628757 0.21024405 0.3092532 ]. \t  0.667595785873568 \t 3.8269738542235388\n",
      "91     \t [0.24382643 0.93754587 0.4879825 ]. \t  2.398427017330587 \t 3.8269738542235388\n",
      "92     \t [0.99779742 0.93589694 0.20575286]. \t  0.007329119966931042 \t 3.8269738542235388\n",
      "93     \t [0.04652214 0.23639322 0.79984488]. \t  1.5124798634155665 \t 3.8269738542235388\n",
      "94     \t [0.31163102 0.39319888 0.19588464]. \t  0.4018485724090269 \t 3.8269738542235388\n",
      "95     \t [0.96544108 0.75417828 0.52708221]. \t  0.38910572698274426 \t 3.8269738542235388\n",
      "96     \t [0.75142448 0.17975981 0.19436217]. \t  0.5285271376523967 \t 3.8269738542235388\n",
      "97     \t [0.68550126 0.14975151 0.16901277]. \t  0.5482000897490337 \t 3.8269738542235388\n",
      "98     \t [0.58950138 0.6510817  0.77948185]. \t  3.0041939341055026 \t 3.8269738542235388\n",
      "99     \t [0.62665518 0.15434923 0.16051903]. \t  0.5739594449251738 \t 3.8269738542235388\n",
      "100    \t [0.48570979 0.45060687 0.98156858]. \t  2.1256447748103504 \t 3.8269738542235388\n"
     ]
    }
   ],
   "source": [
    "### 6(s). Bayesian optimization runs (x20): GP run number = 19\n",
    "\n",
    "np.random.seed(run_num_19)\n",
    "surrogate_gp_19 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "gpgo_gp_19 = GPGO(surrogate_gp_19, Acquisition_new(util_gp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_19.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.82452017 0.98462676 0.89314452]. \t  0.6273086368875856 \t 2.1561178330322286\n",
      "init   \t [0.31964691 0.5932567  0.19549232]. \t  0.11304217526683821 \t 2.1561178330322286\n",
      "init   \t [0.86021676 0.86430051 0.07861785]. \t  0.001240365156738587 \t 2.1561178330322286\n",
      "init   \t [0.71691207 0.72534231 0.73504778]. \t  1.9949269324745122 \t 2.1561178330322286\n",
      "init   \t [0.794917   0.3675148  0.93827185]. \t  2.1561178330322286 \t 2.1561178330322286\n",
      "1      \t [0.         0.30614302 1.        ]. \t  1.0551814996882805 \t 2.1561178330322286\n",
      "2      \t [1.         0.         0.19167382]. \t  0.2223942074314731 \t 2.1561178330322286\n",
      "3      \t [-5.55111512e-17  1.00000000e+00  1.00000000e+00]. \t  0.330219860606422 \t 2.1561178330322286\n",
      "4      \t [0.48718196 0.         0.77285126]. \t  0.24905108564148723 \t 2.1561178330322286\n",
      "5      \t [-1.11022302e-16  0.00000000e+00  0.00000000e+00]. \t  0.06797411659013226 \t 2.1561178330322286\n",
      "6      \t [1.         0.50170871 0.64043339]. \t  1.184456039715784 \t 2.1561178330322286\n",
      "7      \t [-5.55111512e-17  1.00000000e+00 -5.55111512e-17]. \t  0.0002735367680454458 \t 2.1561178330322286\n",
      "8      \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.1561178330322286\n",
      "9      \t [0.         0.61210536 0.56784242]. \t  1.8394060354382482 \t 2.1561178330322286\n",
      "10     \t [0.38257238 0.63079011 1.        ]. \t  2.008180698468981 \t 2.1561178330322286\n",
      "11     \t [0.21881441 1.         0.49620723]. \t  \u001b[92m2.2320036987032696\u001b[0m \t 2.2320036987032696\n",
      "12     \t [0.         0.         0.55238321]. \t  0.1000923118366574 \t 2.2320036987032696\n",
      "13     \t [1.         0.35040441 0.        ]. \t  0.020588336952240847 \t 2.2320036987032696\n",
      "14     \t [0.56919179 0.         0.        ]. \t  0.09065281728295391 \t 2.2320036987032696\n",
      "15     \t [1.         0.66116568 1.        ]. \t  1.8419948777676316 \t 2.2320036987032696\n",
      "16     \t [1.         1.         0.42771075]. \t  0.14739929803572946 \t 2.2320036987032696\n",
      "17     \t [0. 0. 1.]. \t  0.0902894676548261 \t 2.2320036987032696\n",
      "18     \t [0.        1.        0.4370789]. \t  1.6299065786720701 \t 2.2320036987032696\n",
      "19     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.2320036987032696\n",
      "20     \t [0.47796155 1.         0.        ]. \t  0.00021290157388917177 \t 2.2320036987032696\n",
      "21     \t [0.         0.43795347 0.        ]. \t  0.0278667934833679 \t 2.2320036987032696\n",
      "22     \t [1. 0. 0.]. \t  0.03095471703300515 \t 2.2320036987032696\n",
      "23     \t [0.27825701 0.36670758 0.74711582]. \t  \u001b[92m2.29612385888613\u001b[0m \t 2.29612385888613\n",
      "24     \t [0.         0.26574618 0.3057519 ]. \t  0.5219585059509564 \t 2.29612385888613\n",
      "25     \t [1.         0.         0.68643404]. \t  0.1807068063182819 \t 2.29612385888613\n",
      "26     \t [0.71378245 0.2817185  0.4348743 ]. \t  0.2814932222240624 \t 2.29612385888613\n",
      "27     \t [0.18376949 1.         0.77293914]. \t  0.9928084195906592 \t 2.29612385888613\n",
      "28     \t [0.63526111 1.         0.44207474]. \t  0.7636009333765003 \t 2.29612385888613\n",
      "29     \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.29612385888613\n",
      "30     \t [0.67624937 0.84595313 1.        ]. \t  0.9649216449774537 \t 2.29612385888613\n",
      "31     \t [0.28150815 0.         0.29826157]. \t  0.8284890966765694 \t 2.29612385888613\n",
      "32     \t [0.32677621 0.17367507 1.        ]. \t  0.4605541595787806 \t 2.29612385888613\n",
      "33     \t [0.         0.79224941 0.20374187]. \t  0.07353019163396565 \t 2.29612385888613\n",
      "34     \t [0.        0.7303495 0.9978591]. \t  1.6173915690981697 \t 2.29612385888613\n",
      "35     \t [1.         0.38260412 1.        ]. \t  1.4390381790714764 \t 2.29612385888613\n",
      "36     \t [0.26508634 0.20702187 0.00086576]. \t  0.10614427173273452 \t 2.29612385888613\n",
      "37     \t [0.99709778 0.68530434 0.32414126]. \t  0.05242245220004358 \t 2.29612385888613\n",
      "38     \t [2.46806524e-01 7.92162309e-01 2.24530344e-09]. \t  0.0014348516281734729 \t 2.29612385888613\n",
      "39     \t [0.21787069 1.         0.14497631]. \t  0.016265956734710617 \t 2.29612385888613\n",
      "40     \t [0.21617121 0.75414619 0.61100162]. \t  \u001b[92m2.6925361128588503\u001b[0m \t 2.6925361128588503\n",
      "41     \t [0.99999999 0.74825184 0.        ]. \t  0.0006833409139688669 \t 2.6925361128588503\n",
      "42     \t [0.75094513 0.53315411 0.99999999]. \t  2.0355611918083723 \t 2.6925361128588503\n",
      "43     \t [0.67272994 0.         1.        ]. \t  0.09105179112262868 \t 2.6925361128588503\n",
      "44     \t [0.59520331 0.47976715 0.        ]. \t  0.02699839653616468 \t 2.6925361128588503\n",
      "45     \t [1.         0.81461597 0.83132528]. \t  1.8238962665126057 \t 2.6925361128588503\n",
      "46     \t [0.31099069 1.         0.99999994]. \t  0.33429027321202176 \t 2.6925361128588503\n",
      "47     \t [7.06678233e-01 4.41008230e-04 4.65083326e-01]. \t  0.2032121372034011 \t 2.6925361128588503\n",
      "48     \t [0.44772615 0.54462364 0.71148313]. \t  2.5073358281530105 \t 2.6925361128588503\n",
      "49     \t [0.15077786 0.44927326 0.50508289]. \t  0.6836818194134581 \t 2.6925361128588503\n",
      "50     \t [2.04469310e-07 0.00000000e+00 2.23731126e-01]. \t  0.5477257914212552 \t 2.6925361128588503\n",
      "51     \t [0.03849759 0.57762728 0.82424242]. \t  \u001b[92m3.7323270930215005\u001b[0m \t 3.7323270930215005\n",
      "52     \t [0.14807183 0.49072015 0.9149446 ]. \t  3.3073446325593796 \t 3.7323270930215005\n",
      "53     \t [0.10943097 0.75279067 0.8615937 ]. \t  2.7065463754782106 \t 3.7323270930215005\n",
      "54     \t [0.01809812 0.61418593 0.83784914]. \t  3.6784044440082075 \t 3.7323270930215005\n",
      "55     \t [0.02303494 0.60346035 0.83943113]. \t  3.7223422231731194 \t 3.7323270930215005\n",
      "56     \t [0.01789383 0.41067671 0.90277089]. \t  2.8908698766639884 \t 3.7323270930215005\n",
      "57     \t [0.00359122 0.39755177 0.79675117]. \t  2.892142916607768 \t 3.7323270930215005\n",
      "58     \t [0.143894   0.54090388 0.78719312]. \t  3.4917618227398814 \t 3.7323270930215005\n",
      "59     \t [0.07183724 0.67260645 0.82762672]. \t  3.3449380454025066 \t 3.7323270930215005\n",
      "60     \t [0.11406979 0.46134105 0.90772279]. \t  3.2269214437484957 \t 3.7323270930215005\n",
      "61     \t [0.03976249 0.69453472 0.7858947 ]. \t  2.983439729492635 \t 3.7323270930215005\n",
      "62     \t [0.15719665 0.64953158 0.84907533]. \t  3.5546758604419084 \t 3.7323270930215005\n",
      "63     \t [0.17512496 0.65277427 0.82777772]. \t  3.488078921403836 \t 3.7323270930215005\n",
      "64     \t [0.24175829 0.61535068 0.8572933 ]. \t  \u001b[92m3.7344046056296993\u001b[0m \t 3.7344046056296993\n",
      "65     \t [0.24776403 0.59083657 0.86479939]. \t  \u001b[92m3.803847775857309\u001b[0m \t 3.803847775857309\n",
      "66     \t [0.12316278 0.52848723 0.90616552]. \t  3.527000142757894 \t 3.803847775857309\n",
      "67     \t [0.80708529 0.07058462 0.04978028]. \t  0.1330535399511004 \t 3.803847775857309\n",
      "68     \t [0.4720648  0.54537481 0.87568751]. \t  3.796299712518319 \t 3.803847775857309\n",
      "69     \t [0.38523996 0.68663613 0.73914688]. \t  2.6242072755633266 \t 3.803847775857309\n",
      "70     \t [0.11993722 0.60928845 0.86616544]. \t  3.7297733662719543 \t 3.803847775857309\n",
      "71     \t [0.03540856 0.6535044  0.79856754]. \t  3.298970928111176 \t 3.803847775857309\n",
      "72     \t [0.16357135 0.58712551 0.83003738]. \t  3.768650455350945 \t 3.803847775857309\n",
      "73     \t [0.25328331 0.51490445 0.83992778]. \t  3.7929359050173237 \t 3.803847775857309\n",
      "74     \t [0.3020364  0.61826222 0.90660086]. \t  3.4647459852455733 \t 3.803847775857309\n",
      "75     \t [0.11508218 0.58675571 0.77450671]. \t  3.3412952943214966 \t 3.803847775857309\n",
      "76     \t [0.50584237 0.50577561 0.79790938]. \t  3.4987368873612503 \t 3.803847775857309\n",
      "77     \t [0.28104438 0.64146873 0.81555747]. \t  3.4892850385120404 \t 3.803847775857309\n",
      "78     \t [0.17996712 0.59659327 0.87042855]. \t  3.7675718379934158 \t 3.803847775857309\n",
      "79     \t [0.03280835 0.46636936 0.83291057]. \t  3.536656683369399 \t 3.803847775857309\n",
      "80     \t [0.1340674  0.56738178 0.896877  ]. \t  3.6501605027369073 \t 3.803847775857309\n",
      "81     \t [0.39295988 0.61389899 0.78574279]. \t  3.3450400919283094 \t 3.803847775857309\n",
      "82     \t [0.36404218 0.51735546 0.86442752]. \t  3.795958890783782 \t 3.803847775857309\n",
      "83     \t [0.09888807 0.67184811 0.83977193]. \t  3.388509080393677 \t 3.803847775857309\n",
      "84     \t [0.2261381  0.48482352 0.89970191]. \t  3.4516519382722906 \t 3.803847775857309\n",
      "85     \t [0.24192172 0.71514992 0.79088134]. \t  2.8841006900360484 \t 3.803847775857309\n",
      "86     \t [0.12318134 0.54007328 0.90952018]. \t  3.5113391059839922 \t 3.803847775857309\n",
      "87     \t [0.30738541 0.55126288 0.90907639]. \t  3.547687119108856 \t 3.803847775857309\n",
      "88     \t [0.34250581 0.60181033 0.90591523]. \t  3.5260816835165376 \t 3.803847775857309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.20741385 0.61121061 0.86974692]. \t  3.7254849963604784 \t 3.803847775857309\n",
      "90     \t [0.4492513  0.49852951 0.85718293]. \t  3.741924185473982 \t 3.803847775857309\n",
      "91     \t [0.34247413 0.59654604 0.90316681]. \t  3.5665729840377125 \t 3.803847775857309\n",
      "92     \t [0.36052097 0.63845228 0.80832007]. \t  3.441670500300252 \t 3.803847775857309\n",
      "93     \t [0.28967277 0.62812906 0.82478814]. \t  3.604162926054547 \t 3.803847775857309\n",
      "94     \t [0.30357447 0.41573148 0.87475693]. \t  3.1690990298831907 \t 3.803847775857309\n",
      "95     \t [0.43541353 0.5507373  0.90582716]. \t  3.579614451846245 \t 3.803847775857309\n",
      "96     \t [0.29866055 0.55008445 0.88808113]. \t  3.7364276621466868 \t 3.803847775857309\n",
      "97     \t [0.48164399 0.48150128 0.85199514]. \t  3.6683089648164184 \t 3.803847775857309\n",
      "98     \t [0.31110636 0.58265537 0.8596373 ]. \t  \u001b[92m3.830945013513494\u001b[0m \t 3.830945013513494\n",
      "99     \t [0.50660721 0.59188868 0.84887542]. \t  3.7823044157689236 \t 3.830945013513494\n",
      "100    \t [0.39401855 0.43888218 0.85831825]. \t  3.4082275619806808 \t 3.830945013513494\n"
     ]
    }
   ],
   "source": [
    "### 6(s). Bayesian optimization runs (x20): STP DF1 run number = 19\n",
    "\n",
    "np.random.seed(run_num_19)\n",
    "surrogate_stp_df1_19 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_19 = GPGO(surrogate_stp_df1_19, Acquisition_new(util_stp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_19.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.329635730575494, -3.4471893899986874)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(s). Training Regret Minimisation: run number = 19\n",
    "\n",
    "gp_output_19 = np.append(np.max(gpgo_gp_19.GP.y[0:n_init]),gpgo_gp_19.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_19 = np.append(np.max(gpgo_stp_df1_19.GP.y[0:n_init]),gpgo_stp_df1_19.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_19 = np.log(y_global_orig - gp_output_19)\n",
    "regret_stp_df1_19 = np.log(y_global_orig - stp_df1_output_19)\n",
    "\n",
    "train_regret_gp_19 = min_max_array(regret_gp_19)\n",
    "train_regret_stp_df1_19 = min_max_array(regret_stp_df1_19)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 19\n",
    "min_train_regret_gp_19 = min(train_regret_gp_19)\n",
    "min_train_regret_stp_df1_19 = min(train_regret_stp_df1_19)\n",
    "\n",
    "min_train_regret_gp_19, min_train_regret_stp_df1_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.57051729 0.56452876 0.48844183]. \t  0.675391399411646 \t 0.675391399411646\n",
      "init   \t [0.33647775 0.37586818 0.53203587]. \t  0.5331349538596052 \t 0.675391399411646\n",
      "init   \t [0.06810629 0.58452906 0.23789776]. \t  0.14747335095307315 \t 0.675391399411646\n",
      "init   \t [0.16075658 0.15211915 0.12706922]. \t  0.48089725804912437 \t 0.675391399411646\n",
      "init   \t [0.32744117 0.69415387 0.35896647]. \t  0.6289408047543804 \t 0.675391399411646\n",
      "1      \t [0.54663172 0.95772822 0.99370791]. \t  0.4914614738653129 \t 0.675391399411646\n",
      "2      \t [0.91932094 0.0125958  0.17618825]. \t  0.2816999755052942 \t 0.675391399411646\n",
      "3      \t [0.837611   0.97862966 0.10314233]. \t  0.0013418779384681767 \t 0.675391399411646\n",
      "4      \t [0.83546183 0.15352723 0.88175242]. \t  \u001b[92m0.8118727969835325\u001b[0m \t 0.8118727969835325\n",
      "5      \t [0.04282238 0.98714938 0.99662789]. \t  0.3795921540393995 \t 0.8118727969835325\n",
      "6      \t [0.00657248 0.04994934 0.95603609]. \t  0.22346365116669764 \t 0.8118727969835325\n",
      "7      \t [0.0140605  0.99189634 0.09256457]. \t  0.0043141598608878945 \t 0.8118727969835325\n",
      "8      \t [0.97302872 0.76767204 0.83300021]. \t  \u001b[92m2.2831423836103286\u001b[0m \t 2.2831423836103286\n",
      "9      \t [0.75244603 0.97391009 0.75870158]. \t  0.6651080101915691 \t 2.2831423836103286\n",
      "10     \t [0.00407095 0.01602185 0.22787245]. \t  0.5782528840560536 \t 2.2831423836103286\n",
      "11     \t [0.99078418 0.22121079 0.03507944]. \t  0.05576618977127451 \t 2.2831423836103286\n",
      "12     \t [0.98562056 0.3532819  0.97238955]. \t  1.6129586582258908 \t 2.2831423836103286\n",
      "13     \t [0.72818255 0.04068818 0.51966219]. \t  0.1375604883842715 \t 2.2831423836103286\n",
      "14     \t [0.21561842 0.81296904 0.1185526 ]. \t  0.013073952399992744 \t 2.2831423836103286\n",
      "15     \t [0.11631352 0.02556041 0.60513699]. \t  0.14369463056464749 \t 2.2831423836103286\n",
      "16     \t [0.7664446 0.6360279 0.767858 ]. \t  \u001b[92m2.8246323331682794\u001b[0m \t 2.8246323331682794\n",
      "17     \t [0.48254864 0.61420849 0.90966464]. \t  \u001b[92m3.4400563356463536\u001b[0m \t 3.4400563356463536\n",
      "18     \t [0.02592365 0.78832521 0.71176665]. \t  2.3541425416158104 \t 3.4400563356463536\n",
      "19     \t [0.73772067 0.50014702 0.94045278]. \t  2.9743049960010755 \t 3.4400563356463536\n",
      "20     \t [0.26761052 0.8975585  0.76765919]. \t  1.559581278102397 \t 3.4400563356463536\n",
      "21     \t [0.86095053 0.97464702 0.91172435]. \t  0.6481411249424998 \t 3.4400563356463536\n",
      "22     \t [0.97581581 0.14663182 0.44064067]. \t  0.15356793092299648 \t 3.4400563356463536\n",
      "23     \t [0.02843244 0.5384091  0.99128982]. \t  2.2016692511646556 \t 3.4400563356463536\n",
      "24     \t [0.70238185 0.1683291  0.57459002]. \t  0.2731710807913837 \t 3.4400563356463536\n",
      "25     \t [0.86044025 0.53007868 0.38860755]. \t  0.1435598115427928 \t 3.4400563356463536\n",
      "26     \t [0.01537935 0.26817864 0.34648792]. \t  0.4768440140289208 \t 3.4400563356463536\n",
      "27     \t [0.16104933 0.76869031 0.71439035]. \t  2.4356809957041783 \t 3.4400563356463536\n",
      "28     \t [0.5990851  0.44034166 0.16274615]. \t  0.2182581964935925 \t 3.4400563356463536\n",
      "29     \t [0.46911668 0.41725862 0.78373556]. \t  2.9645303334571738 \t 3.4400563356463536\n",
      "30     \t [0.55919641 0.15706264 0.21489171]. \t  0.8132787973204357 \t 3.4400563356463536\n",
      "31     \t [0.4171144  0.28870291 0.69791646]. \t  1.3512214601488175 \t 3.4400563356463536\n",
      "32     \t [0.36057657 0.23489665 0.43238393]. \t  0.43665360851459445 \t 3.4400563356463536\n",
      "33     \t [0.23169721 0.43114394 0.52938959]. \t  0.7036496618670851 \t 3.4400563356463536\n",
      "34     \t [0.16399227 0.29837935 0.1023087 ]. \t  0.2806156700136531 \t 3.4400563356463536\n",
      "35     \t [0.95184173 0.6875272  0.16795367]. \t  0.01319718155864041 \t 3.4400563356463536\n",
      "36     \t [0.65257082 0.75698146 0.23990171]. \t  0.06810984282763881 \t 3.4400563356463536\n",
      "37     \t [0.39538804 0.9042213  0.87995339]. \t  1.2484154431092795 \t 3.4400563356463536\n",
      "38     \t [0.85638566 0.3105256  0.90613037]. \t  1.9447564882408375 \t 3.4400563356463536\n",
      "39     \t [0.30206208 0.26687395 0.3378858 ]. \t  0.6979162454139326 \t 3.4400563356463536\n",
      "40     \t [0.35788206 0.31794543 0.8975966 ]. \t  2.11756620489918 \t 3.4400563356463536\n",
      "41     \t [0.31241449 0.381452   0.83839052]. \t  2.943967331975257 \t 3.4400563356463536\n",
      "42     \t [0.16182161 0.3510125  0.21914025]. \t  0.481039053682242 \t 3.4400563356463536\n",
      "43     \t [0.36338206 0.66509103 0.51988038]. \t  1.6999027543704892 \t 3.4400563356463536\n",
      "44     \t [0.46310391 0.20153928 0.66258093]. \t  0.7030942743589284 \t 3.4400563356463536\n",
      "45     \t [0.13314688 0.36283131 0.34132913]. \t  0.4524887204207148 \t 3.4400563356463536\n",
      "46     \t [0.66223288 0.96022554 0.91042743]. \t  0.752947491537213 \t 3.4400563356463536\n",
      "47     \t [0.27687362 0.79075161 0.9371752 ]. \t  1.933725641921452 \t 3.4400563356463536\n",
      "48     \t [0.51236514 0.15357309 0.12380179]. \t  0.5001764571324175 \t 3.4400563356463536\n",
      "49     \t [0.70041028 0.53058182 0.02232165]. \t  0.02154604077917894 \t 3.4400563356463536\n",
      "50     \t [0.3855267  0.32194759 0.06225079]. \t  0.186039907957913 \t 3.4400563356463536\n",
      "51     \t [0.614879   0.01897242 0.51500566]. \t  0.15367501249511611 \t 3.4400563356463536\n",
      "52     \t [0.41167536 0.16385165 0.98067855]. \t  0.5078843597974345 \t 3.4400563356463536\n",
      "53     \t [0.57461273 0.4549721  0.31272816]. \t  0.31286063907099904 \t 3.4400563356463536\n",
      "54     \t [0.37499163 0.63554262 0.10271291]. \t  0.03315445338769701 \t 3.4400563356463536\n",
      "55     \t [0.10772508 0.86123022 0.85728881]. \t  1.684811366751597 \t 3.4400563356463536\n",
      "56     \t [0.1392827  0.79366321 0.04088121]. \t  0.0029033334872716663 \t 3.4400563356463536\n",
      "57     \t [0.29673248 0.72120036 0.82671988]. \t  2.97236620024631 \t 3.4400563356463536\n",
      "58     \t [0.79655151 0.05901964 0.00650744]. \t  0.07260977253929579 \t 3.4400563356463536\n",
      "59     \t [0.95046973 0.09796401 0.89088472]. \t  0.49761056322740577 \t 3.4400563356463536\n",
      "60     \t [0.55213051 0.37210301 0.83852039]. \t  2.8470984778881414 \t 3.4400563356463536\n",
      "61     \t [0.19277311 0.22411141 0.23644552]. \t  0.7916688656986456 \t 3.4400563356463536\n",
      "62     \t [0.02302266 0.55759926 0.85307513]. \t  \u001b[92m3.8182835594078135\u001b[0m \t 3.8182835594078135\n",
      "63     \t [0.23443089 0.12249257 0.53249562]. \t  0.2182628279975289 \t 3.8182835594078135\n",
      "64     \t [0.25033324 0.17223292 0.66275429]. \t  0.5976263057593222 \t 3.8182835594078135\n",
      "65     \t [0.88551001 0.19374512 0.28819094]. \t  0.4188322926123537 \t 3.8182835594078135\n",
      "66     \t [0.29894393 0.69254729 0.07318243]. \t  0.013441052540799628 \t 3.8182835594078135\n",
      "67     \t [0.69204271 0.2617181  0.17389616]. \t  0.4567228690017591 \t 3.8182835594078135\n",
      "68     \t [0.09887742 0.80146926 0.92508264]. \t  1.9310060526836044 \t 3.8182835594078135\n",
      "69     \t [0.26022755 0.94512016 0.60619099]. \t  2.55818934338015 \t 3.8182835594078135\n",
      "70     \t [0.0419944  0.272961   0.60340039]. \t  0.6123246864289482 \t 3.8182835594078135\n",
      "71     \t [0.20739069 0.06601235 0.08339898]. \t  0.3266638276141411 \t 3.8182835594078135\n",
      "72     \t [0.79919213 0.55481786 0.02960821]. \t  0.015561981839875073 \t 3.8182835594078135\n",
      "73     \t [0.53323659 0.07847673 0.42246853]. \t  0.4514836825569653 \t 3.8182835594078135\n",
      "74     \t [0.85153165 0.80186875 0.91415992]. \t  1.9319188912520728 \t 3.8182835594078135\n",
      "75     \t [0.18191887 0.98086372 0.22579752]. \t  0.10293195576328541 \t 3.8182835594078135\n",
      "76     \t [0.59223853 0.49019339 0.514645  ]. \t  0.5636584653373499 \t 3.8182835594078135\n",
      "77     \t [0.05313924 0.61417086 0.68369727]. \t  2.45400306920748 \t 3.8182835594078135\n",
      "78     \t [0.29137267 0.69414151 0.36532438]. \t  0.7023140456625757 \t 3.8182835594078135\n",
      "79     \t [0.56143615 0.11198211 0.34216714]. \t  0.7586939620952345 \t 3.8182835594078135\n",
      "80     \t [0.43159082 0.58183171 0.77795281]. \t  3.33220213936571 \t 3.8182835594078135\n",
      "81     \t [0.82268004 0.38351431 0.59432374]. \t  0.6940951772220839 \t 3.8182835594078135\n",
      "82     \t [0.71742585 0.28987715 0.84530822]. \t  2.0254191443497147 \t 3.8182835594078135\n",
      "83     \t [0.87402104 0.06437169 0.84856424]. \t  0.4253112016867315 \t 3.8182835594078135\n",
      "84     \t [0.67455954 0.21569822 0.55133985]. \t  0.28522871971445246 \t 3.8182835594078135\n",
      "85     \t [0.42274541 0.65986703 0.21753009]. \t  0.09514203604510278 \t 3.8182835594078135\n",
      "86     \t [0.57752401 0.80350725 0.18556134]. \t  0.0312731366325119 \t 3.8182835594078135\n",
      "87     \t [0.6027241  0.49761061 0.81673337]. \t  3.597965090574125 \t 3.8182835594078135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.87891146 0.43847854 0.82573944]. \t  3.2745639011521805 \t 3.8182835594078135\n",
      "89     \t [0.07926141 0.02468115 0.92539295]. \t  0.21503412497346253 \t 3.8182835594078135\n",
      "90     \t [0.34679652 0.50690133 0.93526117]. \t  3.116157716524637 \t 3.8182835594078135\n",
      "91     \t [0.16067943 0.02817582 0.75723975]. \t  0.3137567177172803 \t 3.8182835594078135\n",
      "92     \t [0.26933286 0.18897611 0.34563904]. \t  0.7759469617211912 \t 3.8182835594078135\n",
      "93     \t [0.99474059 0.75673791 0.07898431]. \t  0.002059736298411999 \t 3.8182835594078135\n",
      "94     \t [0.05616427 0.79463796 0.3497037 ]. \t  0.8003549758232651 \t 3.8182835594078135\n",
      "95     \t [0.69138323 0.75223357 0.59086131]. \t  1.2089808454992501 \t 3.8182835594078135\n",
      "96     \t [0.47827067 0.1364753  0.25670492]. \t  0.9585843576107357 \t 3.8182835594078135\n",
      "97     \t [0.64636594 0.08580935 0.32084508]. \t  0.7223969230433356 \t 3.8182835594078135\n",
      "98     \t [0.21669174 0.21331674 0.18901403]. \t  0.708106387725855 \t 3.8182835594078135\n",
      "99     \t [0.73369361 0.68157457 0.39638145]. \t  0.3304032399723793 \t 3.8182835594078135\n",
      "100    \t [0.3689577  0.85363475 0.49223597]. \t  2.201032730366967 \t 3.8182835594078135\n"
     ]
    }
   ],
   "source": [
    "### 6(t). Bayesian optimization runs (x20): GP run number = 20\n",
    "\n",
    "np.random.seed(run_num_20)\n",
    "surrogate_gp_20 = GaussianProcess(cov_func, optimize = optimize)\n",
    "\n",
    "gpgo_gp_20 = GPGO(surrogate_gp_20, Acquisition_new(util_gp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_gp_20.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.57051729 0.56452876 0.48844183]. \t  0.675391399411646 \t 0.675391399411646\n",
      "init   \t [0.33647775 0.37586818 0.53203587]. \t  0.5331349538596052 \t 0.675391399411646\n",
      "init   \t [0.06810629 0.58452906 0.23789776]. \t  0.14747335095307315 \t 0.675391399411646\n",
      "init   \t [0.16075658 0.15211915 0.12706922]. \t  0.48089725804912437 \t 0.675391399411646\n",
      "init   \t [0.32744117 0.69415387 0.35896647]. \t  0.6289408047543804 \t 0.675391399411646\n",
      "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 0.675391399411646\n",
      "2      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 0.675391399411646\n",
      "3      \t [ 1.00000000e+00 -5.55111512e-17  0.00000000e+00]. \t  0.030954717033005136 \t 0.675391399411646\n",
      "4      \t [1. 0. 1.]. \t  0.08848201872702738 \t 0.675391399411646\n",
      "5      \t [0. 1. 1.]. \t  0.330219860606422 \t 0.675391399411646\n",
      "6      \t [0. 0. 1.]. \t  0.0902894676548262 \t 0.675391399411646\n",
      "7      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 0.675391399411646\n",
      "8      \t [0.4785379  0.56337249 1.        ]. \t  \u001b[92m2.0856879757978453\u001b[0m \t 2.0856879757978453\n",
      "9      \t [0.5438344  0.49892771 0.        ]. \t  0.024914082230095697 \t 2.0856879757978453\n",
      "10     \t [0.51002886 1.         1.        ]. \t  0.3321410792596228 \t 2.0856879757978453\n",
      "11     \t [1.         0.50240925 1.        ]. \t  1.9277338439381144 \t 2.0856879757978453\n",
      "12     \t [0.         0.49800586 1.        ]. \t  1.9622802795598022 \t 2.0856879757978453\n",
      "13     \t [0.53334746 0.         0.        ]. \t  0.09427900893557363 \t 2.0856879757978453\n",
      "14     \t [1.         0.         0.47920499]. \t  0.08297336514796487 \t 2.0856879757978453\n",
      "15     \t [0.         0.         0.42092871]. \t  0.2906267759324456 \t 2.0856879757978453\n",
      "16     \t [0.53839207 1.         0.        ]. \t  0.00018819460684435632 \t 2.0856879757978453\n",
      "17     \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.0856879757978453\n",
      "18     \t [1.         1.         0.51086501]. \t  0.23401258925714466 \t 2.0856879757978453\n",
      "19     \t [0.         1.         0.48953932]. \t  \u001b[92m2.177335238718867\u001b[0m \t 2.177335238718867\n",
      "20     \t [1.         0.46874973 0.17530788]. \t  0.06888424006815212 \t 2.177335238718867\n",
      "21     \t [0.55056579 0.         1.        ]. \t  0.09152651147273486 \t 2.177335238718867\n",
      "22     \t [0.         0.77593263 0.76904494]. \t  \u001b[92m2.369044961550811\u001b[0m \t 2.369044961550811\n",
      "23     \t [1.         0.70967826 0.77352216]. \t  2.284818044425065 \t 2.369044961550811\n",
      "24     \t [0.29418434 1.         0.68396039]. \t  1.5717293574499214 \t 2.369044961550811\n",
      "25     \t [1.         0.29153893 0.76400223]. \t  1.7775675060563287 \t 2.369044961550811\n",
      "26     \t [0.34241927 0.         0.66582807]. \t  0.16986635358229285 \t 2.369044961550811\n",
      "27     \t [0.18018658 0.74504671 1.        ]. \t  1.5283775905303099 \t 2.369044961550811\n",
      "28     \t [1.         0.67097432 0.        ]. \t  0.0016676659204338323 \t 2.369044961550811\n",
      "29     \t [0.         0.2232225  0.76443267]. \t  1.304781998615345 \t 2.369044961550811\n",
      "30     \t [0.21941312 0.16070368 1.        ]. \t  0.4153527021513833 \t 2.369044961550811\n",
      "31     \t [1.         0.25766917 0.        ]. \t  0.029123866408652225 \t 2.369044961550811\n",
      "32     \t [0.81328308 0.75180579 0.93671795]. \t  2.2037491779627296 \t 2.369044961550811\n",
      "33     \t [0.21989919 0.81930324 0.        ]. \t  0.0010660635088279187 \t 2.369044961550811\n",
      "34     \t [0.77890207 0.2516097  0.99343167]. \t  0.8244787140939894 \t 2.369044961550811\n",
      "35     \t [0.03026521 0.29467021 0.02024727]. \t  0.08286838639977419 \t 2.369044961550811\n",
      "36     \t [0.65385265 0.09894462 0.17890977]. \t  0.6180662678998297 \t 2.369044961550811\n",
      "37     \t [0.63556893 1.         0.75979356]. \t  0.6715484862347876 \t 2.369044961550811\n",
      "38     \t [0.25268916 1.         0.30028778]. \t  0.3448883292380262 \t 2.369044961550811\n",
      "39     \t [0.80666178 0.         0.78413898]. \t  0.24781261142348623 \t 2.369044961550811\n",
      "40     \t [0.75624732 0.96131006 0.18601364]. \t  0.013809767379731485 \t 2.369044961550811\n",
      "41     \t [0.47583585 0.79387923 0.811716  ]. \t  2.2069804289949477 \t 2.369044961550811\n",
      "42     \t [0.13379582 0.60833766 0.77438203]. \t  \u001b[92m3.2993462928891115\u001b[0m \t 3.2993462928891115\n",
      "43     \t [0.32228799 0.         0.12972217]. \t  0.49104197371570457 \t 3.2993462928891115\n",
      "44     \t [2.01609961e-11 9.96428220e-01 8.30320810e-01]. \t  0.7777251418016968 \t 3.2993462928891115\n",
      "45     \t [1.54990954e-14 5.04002719e-01 6.37138837e-01]. \t  1.7069050320231676 \t 3.2993462928891115\n",
      "46     \t [0.         0.80993304 0.        ]. \t  0.0009129710122761459 \t 3.2993462928891115\n",
      "47     \t [8.00507137e-01 7.73439368e-01 3.31666981e-08]. \t  0.0009649093140220573 \t 3.2993462928891115\n",
      "48     \t [0.81514556 0.51451486 0.79642815]. \t  \u001b[92m3.3937014722653984\u001b[0m \t 3.3937014722653984\n",
      "49     \t [0.24065079 0.52530403 0.85336314]. \t  \u001b[92m3.8286131943260293\u001b[0m \t 3.8286131943260293\n",
      "50     \t [0.34207208 0.49306869 0.8042031 ]. \t  3.543007227871553 \t 3.8286131943260293\n",
      "51     \t [0.25350827 0.52250386 0.82797071]. \t  3.7723878003649562 \t 3.8286131943260293\n",
      "52     \t [0.2195121  0.55564682 0.86674711]. \t  \u001b[92m3.8385122588992804\u001b[0m \t 3.8385122588992804\n",
      "53     \t [0.1567221  0.56056193 0.81431246]. \t  3.7193397174275753 \t 3.8385122588992804\n",
      "54     \t [0.57317171 0.57118287 0.7993258 ]. \t  3.5167380737264664 \t 3.8385122588992804\n",
      "55     \t [0.20029739 0.50294994 0.73099325]. \t  2.778764696143357 \t 3.8385122588992804\n",
      "56     \t [0.4978465  0.47891041 0.85380595]. \t  3.6526175340705698 \t 3.8385122588992804\n",
      "57     \t [0.3432237  0.63363807 0.8199083 ]. \t  3.543551082065648 \t 3.8385122588992804\n",
      "58     \t [0.49402048 0.46204414 0.85607966]. \t  3.558174897814689 \t 3.8385122588992804\n",
      "59     \t [0.36621447 0.52112912 0.84083466]. \t  3.808818223685271 \t 3.8385122588992804\n",
      "60     \t [0.3520314  0.49275964 0.89116749]. \t  3.568666882553094 \t 3.8385122588992804\n",
      "61     \t [0.23341524 0.64051597 0.81569222]. \t  3.4995668130797943 \t 3.8385122588992804\n",
      "62     \t [0.2308866  0.42223125 0.90523098]. \t  2.994199928319184 \t 3.8385122588992804\n",
      "63     \t [0.33587703 0.56174159 0.85366103]. \t  \u001b[92m3.8589053663952364\u001b[0m \t 3.8589053663952364\n",
      "64     \t [0.2228989  0.5925615  0.86417633]. \t  3.799315818059108 \t 3.8589053663952364\n",
      "65     \t [0.34129467 0.65034085 0.81731469]. \t  3.4388971867111247 \t 3.8589053663952364\n",
      "66     \t [0.2149589  0.60982363 0.84502804]. \t  3.74881234845812 \t 3.8589053663952364\n",
      "67     \t [0.26362993 0.60575612 0.87257506]. \t  3.7389934734544736 \t 3.8589053663952364\n",
      "68     \t [0.72840082 0.57290497 0.83569913]. \t  3.717327853498455 \t 3.8589053663952364\n",
      "69     \t [0.63821709 0.55635012 0.86257742]. \t  3.804088258153546 \t 3.8589053663952364\n",
      "70     \t [0.20350048 0.66270427 0.79126722]. \t  3.223823185739327 \t 3.8589053663952364\n",
      "71     \t [0.88547272 0.590213   0.88291759]. \t  3.6115287063533854 \t 3.8589053663952364\n",
      "72     \t [0.27737456 0.60625477 0.85750037]. \t  3.76923226809476 \t 3.8589053663952364\n",
      "73     \t [0.20494665 0.54585712 0.90041803]. \t  3.6240236732297015 \t 3.8589053663952364\n",
      "74     \t [0.74975004 0.48650087 0.88508991]. \t  3.5265117451567582 \t 3.8589053663952364\n",
      "75     \t [0.45373321 0.58623462 0.86229639]. \t  3.8072079760902846 \t 3.8589053663952364\n",
      "76     \t [0.81772461 0.49901898 0.86488292]. \t  3.6454770646355463 \t 3.8589053663952364\n",
      "77     \t [0.92178149 0.55376011 0.87906734]. \t  3.6588967936214796 \t 3.8589053663952364\n",
      "78     \t [0.25747755 0.49040563 0.92073648]. \t  3.2477250220258256 \t 3.8589053663952364\n",
      "79     \t [0.39725423 0.5261547  0.77963125]. \t  3.3783721916373786 \t 3.8589053663952364\n",
      "80     \t [0.24144565 0.51696176 0.83242375]. \t  3.776690395212916 \t 3.8589053663952364\n",
      "81     \t [0.72604475 0.52564319 0.84717868]. \t  3.753944089558296 \t 3.8589053663952364\n",
      "82     \t [0.28872666 0.52897984 0.8610687 ]. \t  3.8299370596887785 \t 3.8589053663952364\n",
      "83     \t [0.53236811 0.59647274 0.87525663]. \t  3.733906031019176 \t 3.8589053663952364\n",
      "84     \t [0.42804805 0.57731867 0.85816354]. \t  3.832333311735268 \t 3.8589053663952364\n",
      "85     \t [0.63321061 0.48104984 0.85464696]. \t  3.637771262531312 \t 3.8589053663952364\n",
      "86     \t [0.29440706 0.5688769  0.78444874]. \t  3.460096554296258 \t 3.8589053663952364\n",
      "87     \t [0.6027241  0.49761061 0.81673337]. \t  3.597965090574125 \t 3.8589053663952364\n",
      "88     \t [0.30745707 0.65592443 0.78629863]. \t  3.19988468301404 \t 3.8589053663952364\n",
      "89     \t [0.28248767 0.4988328  0.79023621]. \t  3.4455982212587832 \t 3.8589053663952364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.23058446 0.60650141 0.85150228]. \t  3.767970656591369 \t 3.8589053663952364\n",
      "91     \t [0.45196872 0.52547583 0.85433512]. \t  3.821909675459616 \t 3.8589053663952364\n",
      "92     \t [0.9966977  0.11281528 0.1773188 ]. \t  0.24041387552247068 \t 3.8589053663952364\n",
      "93     \t [0.88018412 0.51826977 0.79802255]. \t  3.3873958799103514 \t 3.8589053663952364\n",
      "94     \t [0.47503959 0.5211625  0.83581041]. \t  3.781987598226865 \t 3.8589053663952364\n",
      "95     \t [0.33365546 0.59553661 0.85193632]. \t  3.8017371361753254 \t 3.8589053663952364\n",
      "96     \t [0.88114279 0.01148466 0.05196021]. \t  0.1013021039918654 \t 3.8589053663952364\n",
      "97     \t [0.24996006 0.55364059 0.89693862]. \t  3.6654548512891054 \t 3.8589053663952364\n",
      "98     \t [0.28094583 0.56704615 0.85348783]. \t  3.8562383580084028 \t 3.8589053663952364\n",
      "99     \t [0.35187913 0.54509214 0.90165918]. \t  3.6197106142506605 \t 3.8589053663952364\n",
      "100    \t [0.37655235 0.5788305  0.80215123]. \t  3.5958026793380116 \t 3.8589053663952364\n"
     ]
    }
   ],
   "source": [
    "### 6(t). Bayesian optimization runs (x20): STP DF1 run number = 20\n",
    "\n",
    "np.random.seed(run_num_20)\n",
    "surrogate_stp_df1_20 = tStudentProcess(cov_func, nu = df1, optimize = optimize)\n",
    "\n",
    "gpgo_stp_df1_20 = GPGO(surrogate_stp_df1_20, Acquisition_new(util_stp), f_syn_polarity, param, n_jobs = -1) # Define Bayesian optimisation;\n",
    "gpgo_stp_df1_20.run(max_iter = max_iter, init_evals = n_init) # Run Bayesian optimisation sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.1123460797014895, -5.553304174256726)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 6(t). Training Regret Minimisation: run number = 20\n",
    "\n",
    "gp_output_20 = np.append(np.max(gpgo_gp_20.GP.y[0:n_init]),gpgo_gp_20.GP.y[n_init:(n_init+max_iter)]) \n",
    "stp_df1_output_20 = np.append(np.max(gpgo_stp_df1_20.GP.y[0:n_init]),gpgo_stp_df1_20.GP.y[n_init:(n_init+max_iter)])\n",
    "\n",
    "regret_gp_20 = np.log(y_global_orig - gp_output_20)\n",
    "regret_stp_df1_20 = np.log(y_global_orig - stp_df1_output_20)\n",
    "\n",
    "train_regret_gp_20 = min_max_array(regret_gp_20)\n",
    "train_regret_stp_df1_20 = min_max_array(regret_stp_df1_20)\n",
    "\n",
    "# GP, STP df1 - training regret minimization: run number = 20\n",
    "min_train_regret_gp_20 = min(train_regret_gp_20)\n",
    "min_train_regret_stp_df1_20 = min(train_regret_stp_df1_20)\n",
    "\n",
    "min_train_regret_gp_20, min_train_regret_stp_df1_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7(a). Sort GP results:\n",
    "\n",
    "gp_regret = [min_train_regret_gp_1,\n",
    "                 min_train_regret_gp_2,\n",
    "                 min_train_regret_gp_3,\n",
    "                 min_train_regret_gp_4,\n",
    "                 min_train_regret_gp_5,\n",
    "                 min_train_regret_gp_6,\n",
    "                 min_train_regret_gp_7,\n",
    "                 min_train_regret_gp_8,\n",
    "                 min_train_regret_gp_9,\n",
    "                 min_train_regret_gp_10,\n",
    "                 min_train_regret_gp_11,\n",
    "                 min_train_regret_gp_12,\n",
    "                 min_train_regret_gp_13,\n",
    "                 min_train_regret_gp_14,\n",
    "                 min_train_regret_gp_15,\n",
    "                 min_train_regret_gp_16,\n",
    "                 min_train_regret_gp_17,\n",
    "                 min_train_regret_gp_18,\n",
    "                 min_train_regret_gp_19,\n",
    "                 min_train_regret_gp_20]\n",
    "\n",
    "fields = [\"Run 1\",\"Run 2\",\"Run 3\",\"Run 4\",\"Run 5\",\"Run 6\",\"Run 7\",\"Run 8\",\"Run 9\",\"Run 10\",\n",
    "          \"Run 11\",\"Run 12\",\"Run 13\",\"Run 14\",\"Run 15\",\"Run 16\",\"Run 17\",\"Run 18\",\"Run 19\",\"Run 20\"]\n",
    "\n",
    "IndexTitle = [\"GP\"]\n",
    "\n",
    "gp_results = pd.DataFrame(gp_regret, fields, IndexTitle).sort_values(by=[\"GP\"], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp = gp_results[4:5]\n",
    "median_gp = gp_results[9:10]\n",
    "upper_gp = gp_results[14:15]\n",
    "best_gp = gp_results[19:20]\n",
    "\n",
    "#gp_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(              GP\n",
       " Run 10 -1.302671,              GP\n",
       " Run 11 -2.24526,              GP\n",
       " Run 9 -3.110239,               GP\n",
       " Run 16 -5.061351)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 7(b). Training regret minimization - GP:\n",
    "lower_gp, median_gp, upper_gp, best_gp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STP DF 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Run 2</th>\n",
       "      <td>-3.142106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 7</th>\n",
       "      <td>-3.220429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 8</th>\n",
       "      <td>-3.410669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 9</th>\n",
       "      <td>-3.424637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 19</th>\n",
       "      <td>-3.447189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 15</th>\n",
       "      <td>-3.575985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 18</th>\n",
       "      <td>-4.152513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 3</th>\n",
       "      <td>-4.386751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 11</th>\n",
       "      <td>-4.487778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 4</th>\n",
       "      <td>-4.528867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 6</th>\n",
       "      <td>-4.541047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 5</th>\n",
       "      <td>-4.614227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 16</th>\n",
       "      <td>-4.760720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 1</th>\n",
       "      <td>-5.152773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 17</th>\n",
       "      <td>-5.156945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 10</th>\n",
       "      <td>-5.234434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 13</th>\n",
       "      <td>-5.397410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 12</th>\n",
       "      <td>-5.431390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 20</th>\n",
       "      <td>-5.553304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 14</th>\n",
       "      <td>-6.036357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        STP DF 1\n",
       "Run 2  -3.142106\n",
       "Run 7  -3.220429\n",
       "Run 8  -3.410669\n",
       "Run 9  -3.424637\n",
       "Run 19 -3.447189\n",
       "Run 15 -3.575985\n",
       "Run 18 -4.152513\n",
       "Run 3  -4.386751\n",
       "Run 11 -4.487778\n",
       "Run 4  -4.528867\n",
       "Run 6  -4.541047\n",
       "Run 5  -4.614227\n",
       "Run 16 -4.760720\n",
       "Run 1  -5.152773\n",
       "Run 17 -5.156945\n",
       "Run 10 -5.234434\n",
       "Run 13 -5.397410\n",
       "Run 12 -5.431390\n",
       "Run 20 -5.553304\n",
       "Run 14 -6.036357"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 7(c). Sort STP DF1 results:\n",
    "\n",
    "stp_df1_regret = [min_train_regret_stp_df1_1,\n",
    "                  min_train_regret_stp_df1_2,\n",
    "                  min_train_regret_stp_df1_3,\n",
    "                  min_train_regret_stp_df1_4,\n",
    "                  min_train_regret_stp_df1_5,\n",
    "                  min_train_regret_stp_df1_6,\n",
    "                  min_train_regret_stp_df1_7,\n",
    "                  min_train_regret_stp_df1_8,\n",
    "                  min_train_regret_stp_df1_9,\n",
    "                  min_train_regret_stp_df1_10,\n",
    "                  min_train_regret_stp_df1_11,\n",
    "                  min_train_regret_stp_df1_12,\n",
    "                  min_train_regret_stp_df1_13,\n",
    "                  min_train_regret_stp_df1_14,\n",
    "                  min_train_regret_stp_df1_15,\n",
    "                  min_train_regret_stp_df1_16,\n",
    "                  min_train_regret_stp_df1_17,\n",
    "                  min_train_regret_stp_df1_18,\n",
    "                  min_train_regret_stp_df1_19,\n",
    "                  min_train_regret_stp_df1_20]\n",
    "\n",
    "fields = [\"Run 1\",\"Run 2\",\"Run 3\",\"Run 4\",\"Run 5\",\"Run 6\",\"Run 7\",\"Run 8\",\"Run 9\",\"Run 10\",\n",
    "          \"Run 11\",\"Run 12\",\"Run 13\",\"Run 14\",\"Run 15\",\"Run 16\",\"Run 17\",\"Run 18\",\"Run 19\",\"Run 20\"]\n",
    "\n",
    "IndexTitle = [\"STP DF 1\"]\n",
    "\n",
    "stp_df1_results = pd.DataFrame(stp_df1_regret, fields, IndexTitle).sort_values(by=[\"STP DF 1\"], ascending=False)\n",
    "\n",
    "### training regret minimization IQR - STP DF1:\n",
    "lower_stp_df1 = stp_df1_results[4:5]\n",
    "median_stp_df1 = stp_df1_results[9:10]\n",
    "upper_stp_df1 = stp_df1_results[14:15]\n",
    "best_stp_df1 = stp_df1_results[19:20]\n",
    "\n",
    "stp_df1_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(        STP DF 1\n",
       " Run 19 -3.447189,        STP DF 1\n",
       " Run 4 -4.528867,         STP DF 1\n",
       " Run 17 -5.156945,         STP DF 1\n",
       " Run 14 -6.036357)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 7(d). Sort STP DF 1 results:\n",
    "\n",
    "### Training regret minimization - STP DF1:\n",
    "lower_stp_df1, median_stp_df1, upper_stp_df1, best_stp_df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(              GP\n",
       " Run 10 -1.302671,              GP\n",
       " Run 9 -3.110239,         STP DF 1\n",
       " Run 19 -3.447189,         STP DF 1\n",
       " Run 17 -5.156945)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 8(a). IQR inputs:\n",
    "\n",
    "lower_gp, upper_gp, lower_stp_df1, upper_stp_df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEWCAYAAABv+EDhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deXxU5dn4/8+VQBLCqiyirGlVdgiSoqgsUUQERNDWqrj3W8pXS2urj9r6KCi1pVX6tFbUH0/r9nWtWJRSqYoGBStKooisoiyCAgKyhyXA9fvjPjNMkkkyk8zkzHK9X695zZlzzpxznZnkXHPf9zn3LaqKMcaY9JPhdwDGGGP8YQnAGGPSlCUAY4xJU5YAjDEmTVkCMMaYNGUJwBhj0pQlAGOMSVOWAExSEpH1IqIiMiZk3hBv3q46bnu+t53r6xxonIlINxH5QER2icghEdkgIn8WkRy/YzOJzxKAMR4Raeh3DLXQEjgCvAT8HWgN/Az4lZ9BmeRgCcCkLBF5TkS+8n4Z7xWRt0WkV8jyQCniLhFZDhwUkfnAYG+VJ7zlk0NLFyJyu4js9H5tDxORm0Rkm4h8LSLXhmz/NhFZIyL7vRg+EZHvhyx/0tvmYyLyTxEpFZGlIpIfso56j5+KyGfecTwjIlkAqrpQVc9R1R+r6jXAX7235sXvkzWpQqwrCJOMRGQ90AmYA3zhzW4PXAbsVtUWIvIesB7YBfQEBgGrVLVbhW0cBWYCmcA7wJ1AO+BNYAXwb+AgUAQosAzYBpwH7AF2A4uBS4FS4BRV3S0i04GTgc24X+aX4n6td1XV9SLyJHCdF/srwOlAd2Chqg70Ygz8g34L/BO4HGgE/B9V/Zu3zonAPbjSwGXAIWCEqr5fu0/XpIsGfgdgTB2NqmbZ5biTbjtgKS4BdBWRU1T165D1fquq9wReeL/S2wHPqeqT3rwhgcXACNz/zjqgGXClqr4mIttxJ+HTcQnhdtwJ+TTgMC5ptAXOxiWmgNdUdayIFAJvA33DHMsEVX1JRAS4tsI6zYCfh7x+B/iyms/FGMASgEl+Y1X1FQiepIu86dOAj4AmYd7TGghNAO9Fsb99qrpJRFqEzFsdWIZLAI29KppFuJJHuP2H+th7DjReNw7znorrBI9LVdcDIiKtgd8DNwBPAhfUdDAmvVkbgElVI3EnySVAC+CkkGVSYd1DFV4f9Z7D/X8cjXBed9zJ/wjwXW9bK6rY/xHvubr62LDriEjTwLSqbsNVW4ErhRhTLSsBmFS11Xs+HfgzkF/NuhVt9J5/LiK9gSdqsf/twDHc/9g0oCmuKijWHhKR7sCnQDYw2pv/ehz2ZVKMlQBMqvo78Dfcr/OhwO+ieO80XJtBd1zdetQnblXdBEzEJaLzgBLgP9FuJwLv40o6VwJjcVVbU7x9G1MtuwrIGGPSlJUAjDEmTVkCMMaYNGUJwBhj0pQlAGOMSVNJdRloq1attHPnzn6HYYwxSaWkpGS7qla8ATG5EkDnzp0pLi72OwxjjEkqIrIh3HyrAjLGmDRlCcAYY9KUJQBjjElTlgCMMSZNWQIwxpg0lRYJYPNmGDwYtmyJ3bQxxiS7pLoMtLamTIGFC+G++9zrWEw/8kj9xW+MMfGQVL2BFhQUaDT3ATRqBAcPxi+enBw4cCB+2zfGmFgQkRJVLag4P6WrgNauhauuggYxLudkZ8O4cbBuXWy3a4wx9SmlE8DJJ0OzZnDsqJLDQdxoenWbBuXQIaXZ3q9oe1ZnKCpyOysqgs4hr40xJsGldAIA2LpsGxMyZrCIM8ljHXmsq9P0AN6nBbvYMmcxbNgAo0bBH//ongOvLQkYY5JASrcBUFTkTsilpTGL4X+4hV/yP6ynE534MvxKubkwZw4UFsZsv8YYU1vp1wYQh5M/wIXeWNuvc2HVK5WWWknAGJPwUjcB3HBDzE/+AN1YSQe+5N8Mr37F0lIXgzHGJKjUTQBPPOGqYmJMgOH8m7c4n7LqbqPIzXUxGGNMgkrdBFBY6Orh45AELuR19tCcRZwVfgVrAzDGJIHUTQBQOQlkZ7tHHafPz1pIBke4mmfY0igPHnyQzbRlMPPZ0iiPzU+9weDJhTHteiIZpyEx4rAuPIwJL3Py5Ml+xxCxGTNmTB4/fnx0b8rLg7PPhvnz4aWX4Oqr6zydM/MZHvtne746cCKlI37AyIcu5I4/tGTWkYvZP+Jy3t3Rg1mzYP9+ePdd0nZ65Ei44w7/46gYkzHp5t577908efLkGRXnp/ZloHEQ7+4lTPxZFx4m3aTfZaBxEuheIifn+LzMTHB3CZtQ7nNJHLm51oWHMaEsAUQp0L3E4cMuCWRkQJcukBHsMsIJTRDpOB38XDISIyYRV3Jr1gzatsUYgyWAWtm6FSZMgEWL3PPOnTCh32IWyQDyOit5eW5ZXh5pOx38XCb4H1OTJtC6tYvFGoKNOc7aAGLl4Ydh4kR3hjnpJL+jMSF+8Qv4299gzx6/IzHGH9YGEG+Bk/7Wrf7GYSpp1gz27oWjR/2OxJjEYgkgViwBJKzmzd3zvn3+xmFMorEEECuWABJWIAHs3u1vHMYkGksAsWIJIGFZAjAmPEsAsdK8OWRlwTff+B2JqaBZM/dsCcCY8iwBxIqIKwVYCSDhBEoAdhWQMeVZAoglSwAJyaqAjAnP1wQgIsNFZLWIfC4id/oZS0xYAkhIVgVkTHi+JQARyQSmAxcB3YErRaS7X/HEhCWAhGRVQMaE52cJoD/wuaquVdXDwAvAJT7GU3cnneQagY8d8zsSEyI313VMZyUAY8rzMwG0AzaGvN7kzStHRMaLSLGIFG/btq3egquVNm3gyBHXCY5JGCKuGsgSgDHlJXwjsKrOUNUCVS1o3bq13+FUz+4FSFjNm1sVkDEV+ZkAvgI6hLxu781LXpYAElbz5lYCMKYiPxPAYuA0EckTkSzgCmC2j/HUnSWAhGUJwJjKfEsAqnoE+CnwOrAS+LuqLvcrnpiwBJCwrA3AmMoa+LlzVX0NeM3PGGLqxBPd5SaWABJO8+awYoXfURiTWBK+ETipZGS4K4EsASQcqwIypjJLALFmN4MlpEAVUBINgGdM3FkCiDVLAAmpeXN3i8bBg35HYkzisAQQa5YAEpJ1CGdMZZYAYu2kk2DzZujcGYqK3CNdpyEx4igqotndPwdg9+uL/I8ngT6XuBybSR6qmjSPfv36acKbMEHVVTWrZme7RzpO5+aqTpvmnhMgpn8yUkH1w4Zn2+cSz2N7+22//wNNGECxhjmniiZRq1hBQYEWFxf7HUbViopg+HA4fNjvSEwFCziXQSzgTYYylLf8Did15ebCnDlQWOh3JCaEiJSoakHF+VYFFCtFRTBqlJ38E1RzXOX/bpr7HEmKKy11/wdWHZQULAHEyg03uD9+k5Ca4XqCswRQD0pL3f+DSXiWAGLliSdc8dckpEAJYA/NfI4kDeTmuv8Hk/AsAcRKYaGr+7QkkJCsBFBPrA0gqVgCiKWKSSA72z3ScTo3F6ZNS5jPIpNjNGYfuzNPtM8lXtMNGtjJP9mEuzQoUR9JcRmoqrsUrlMn95zO0wn2WZzS8qD+aMRX/seTYJ9LTKYbNlQ9++w6/duY+MEuAzXprnt36NEDXnrJ70hS0Pe/D0uXwmef+R2JCcMuAzVpz8YEiKNu3eCLL+DQIb8jMVGwBGDSho0LHEfdusGxY/D5535HYqJgCcCkDRsTII66dnXPK1f6G4eJiiUAkzYsAcRRly7u2RJAUrEEYNKGtQHEUePG0KkTrFrldyQmCpYATNpo3tz1UnDkiN+RpKiuXa0EkGQsAZi0ERgUxhqC46RbN1cCOHbM70hMhCwBmLTRzOsGyKqB4qRbNzhwADZu9DsSEyFLACZtWAkgzrp1c89WDZQ0LAGYtGHjAsdZ4FLQq67yf6hMExHrCsKkjcWLoX9/mD0bLr7Y72hSUFERnHeemw50EHfoUGTTubkwZQrcfbdrqY/2/RW3ZZ3SlWNdQZi0Z1VAcRQYES/g0KHj3UJEMl1aCrfeenxQpWjfX3FbNipZRCwBmLRhVUBxEjj5J9KIeJYEImIJwKQNSwBxkqjDodrQlDWyBGDSRnY2NGxoCSDmEnU4VBuaska+JAAR+YGILBeRYyJSqWHCmHgQsR5B4yIRh0O1huCI+FUCWAZcCrzr0/5NmmrcGGbOhC1bYPNmGDw4ttNpq67DocZiqEyR49uyk39EGvixU1VdCSCBL8yYerJvH+zYAffd514vXBjb6UceqZ/jSEiBJHDDDcerXqKZLiyEvn1r//6f/hTWrrWTfxR8vQ9AROYDt6lqlRf3i8h4YDxAx44d+23YsKGeojOppFEjOHiwfvaVk+N6RDD17KKLYPt2d8OHKafe7wMQkXkisizM45JotqOqM1S1QFULWrduHa9wTYpbu9bdoNqwYfz2kZMD48bBunXx24ephg34ELW4VQGp6tB4bduYaJ18susM7uhRd6IOlAZiNQ3uPqRmzaBt2/gfjwnDEkDU7DJQkza2boUJE2DRIsjLc49YTAcKppdfnuYNwX6zBBA1XxqBRWQs8BegNfAvEVmiqhf6EYtJH//4x/HptWtjN/3qqzBmDNx+O5xxRuziNVFq3vx4lxCBq4NMtfy6CmgWMMuPfRsTa4FzTX01MpsqhN7q3aaNv7EkCasCMqaOcnLcc6AvMuMT6+0vapYAjKmj0J6IjY+ss6eoWQIwpo6sCihBWAKImiUAY+rIqoAShCWAqFkCMKaOrAooQVgCiFqNVwGJSB5wOTAQ6OzN3gC8A7ykqnbfo0lrlgAShCWAqFVbAhCRWcAa4HdAL2AvsM+bngqsEZGX4x2kMYnM2gASRNOm7tkSQMRqKgGcAvwE+KeqfhO6QETaAKOBH8cpNmOSgrUBJIiGDV1X0JYAIlZtAlDVM6tZ9g3wV+9hTNqyKqAEYt1BRCWiRmAROSoil4e8HiEin8UvLGOSR6CHUUsACcASQFSqLQGISEdcw68A3UVkkLfoIuA78Q3NmOQgUrlnUOMTSwBRqakN4AbgHkCBu70HuISwMo5xGZNUsrOtBJAQLAFEpaYE8CHwKHAT8AbuiiAFdgLPxje0yJSVlbFp0yYO2s8v46O//921P65MgJ9FOTk5tG/fnobxHP0mUTVvDl9+6XcUSaOmRuC5wFwRWQzMV9WEG49x06ZNNG3alM6dO9sYw8Y3ZWXuKsS8PH/jUFV27NjBpk2byPM7GD9YCSAqkd4J/C/gQRHZKSJDReQlEflpPAOL1MGDB2nZsqWd/I2vRMDH4bVD4hBatmyZviViSwBRiTQBTAeGA82AY8B63P0BCcFO/sZvGRlw7JjfUThp/f/QvDmUlroimalRpAlgGPBgyOsVQPKWL4uKoHNn9xwDW7du5aqrruI73/kO/fr1Y8CAAcya5ca7mT9/Ps2bNyc/P59u3bpx7733Vnr/+vXradSoEfn5+cHH008/DUDnzp3p1asXvXv3ZvDgwWzYcLwWTkS4+uqrg6+PHDlC69atGTVqVKV9hMbRtWtXbrvttpgce02efPJJvv7664jWExHmzZsXnPfKK68gIsycOTPi/a1fv56ePXsCUFxczM9+9rPog66FRCkBpD0bEyAqkSaA/cBJ3nQmMBTYEZeI4q2oCEaNgg0b3HMdk4CqMmbMGAYNGsTatWspKSnhhRdeYNOmTcF1Bg4cyJIlSyguLuaZZ57ho48+qrSd7373uyxZsiT4uPbaa0NCLmLp0qUMGTKE3/zmN8H5jRs3ZtmyZRw4cACAN998k3bt2lUZayCOjz/+mDlz5vDee+/V6dgDjh49WuWySBMAQK9evXjhhReCr59//nn69OlT67gKCgp46KGHav3+aCRSCSCtWX9AUYk0AbwATPCm5wBXAM/HJaJ4Cpz8S0vd69LSOieBt99+m6ysLCZMmBCc16lTJyZOnFhp3caNG9OvXz8+//zzWu1rwIABfPXVV+XmjRgxgn/961+AO2FeeeWVNW4nUNoIbGv//v3ceOON9O/fn759+/Lqq68CUFpayuWXX0737t0ZO3YsZ555JsXFxQA0adKEW2+9lT59+vD+++9TUlLC4MGD6devHxdeeCGbN29m5syZFBcXM27cOPLz84OJqioDBw7kww8/pKysjH379vH555+Tn58fXB5uH4H5ffr0oU+fPkyfPj24/vz584OloQ8//JABAwbQt29fzj77bFavXg24BHXppZcyfPhwTjvtNG6//fYaP79wrASQICwBRCXSMYF/BewBAnULc3AdxCWWW26BJUvCL9u5E5Ytq/wzrbQUhg6Fnj3hhBMqvy8/H/70pyp3uXz5cs6IcCTwHTt2sGjRIu6+++5Ky7744otyJ7u//OUvDBw4sNw6//73vxkzZky5eVdccQX33Xcfo0aNYunSpdx4440sWLCg2jh27tzJmjVrGDTI3dd3//33c9555/H444+za9cu+vfvz9ChQ3n00Uc54YQTWLFiBcuWLSsX3/79+znzzDOZNm0aZWVlDB48mFdffZXWrVvz4osvctddd/H444/z8MMP8+CDD1JQUADAPffcQ0FBAaNHj64Ul4gwdOhQXn/9dXbv3s3o0aNZt851NltWVsbEiRPD7uOGG27g4YcfZtCgQfzXf/1X2GPu2rUrCxYsoEGDBsybN49f//rXvPyy68cwUCrKzs6mS5cuTJw4kQ4dOlT7GVaUkWHVzgnBEkBUIukOOhP3a/9pVb0v/iHFyerVVZfRjx1zy886q867ufnmm1m4cCFZWVksXrwYgAULFtC3b18yMjK488476dGjR6X3BaqAwiksLOTbb7+lSZMmTJkypdyy3r17s379ep5//nlGjBhRbWwLFiygT58+rFmzhltuuYW2bdsC8MYbbzB79mwefNA18xw8eJAvv/yShQsX8vOf/xyAnj170rt37+C2MjMzueyyywBYvXo1y5Yt44ILLgBcldDJJ58cNob77qv+T+iKK67goYceYvfu3UybNo3f/va31e5j165d7Nq1K5jMrrnmGubOnVtpu7t37+a6665jzZo1iAhlIWfr888/n+beiaN79+5s2LAh6gRgJYAEYQkgKjUmAFU9KiJdgej+I/xQzS/1StU/oXJzYc4cKCyMepc9evQI/pIEmD59Otu3bw/+4gVXtTFnzpyotx1QVFREixYtGDduHJMmTeKPf/xjueWjR4/mtttuY/78+ezYUXXTTCCOdevWcdZZZ3H55ZeTn5+PqvLyyy/TpUuXiGPKyckhMzMTcO0gPXr04P3336/dAYbo378/n376Kbm5uZx++unB+VXtY9euXRFt9+6776awsJBZs2axfv16hgwZElyWHejNDZfYjhw5EnXc1gaQIJo1c8+WACISaRvAMmCKiDwgIr8MPOIZWMwVFrqTfG5u+fl1OPkDnHfeeRw8eJBHH300OK80XJKpowYNGvCnP/2Jp59+mm+//bbcshtvvJFJkybRq1eviLaVl5fHnXfeye9//3sALrzwQv7yl7+g3k/Yjz/+GIBzzjmHv//97wCsWLGCTz/9NOz2unTpwrZt24In57KyMpYvXw5A06ZN2bt3b1THOnXq1OAv/5r20aJFC1q0aMHChQsBePbZ8Deo7969O9hA/uSTT0YVTySsBJAgrAQQlUgTwOVAC+BW3OWgDwIPxCuouKmYBOp48gdXb/3KK6/wzjvvkJeXR//+/bnuuuuCJ9dIBdoAAo9wV6+cfPLJXHnlleUaOgHat28f9eWOEyZM4N1332X9+vXcfffdlJWV0bt3b3r06BFso7jpppvYtm0b3bt357//+7/p0aNHsKokVFZWFjNnzuSOO+6gT58+5Ofn85///AeA66+/ngkTJgQbge+55x5mz55dbWwXXXQRhRW+k+r28cQTT3DzzTcHSzPh3H777fzqV7+ib9++tfqFX5OMDEsACcEuA42KVPUPU24lketxfQCVo6pPxSGmKhUUFGjgKpSAlStX0q1bt+g2VFQEN9wATzxRp5N/qjt69ChlZWXk5OTwxRdfMHToUFavXk1WVpbfoSWcL7+EHTugb1+/I3Fq9X+RKho1gokT4Q9/8DuShCEiJapaUHF+RFcBqeqTMY/IT4WFsH6931EkvNLSUgoLCykrK0NVeeSRR+zkXwVrA0gg1h1ExCJKACKyNszsXcCbwCRVTdOOR1Jb06ZNqVjiMuEF2gBU3bTxkSWAiEV6H0AbIBfXDxC4toMyoA+QBfwi9qEZkzwCJ31LAAnAEkDEIk0A04GWwM24wWAexpUABPg+lgBMmsvwLqewhuAEYAkgYpFeBXQTsEVVD3nVPVuA63HdRJ9U3RvD8S4nXSUiS0Vkloi0iHYbxiSSwK9+awdIAJYAIhZpAlgK/EpEvhSRDbiuIVYD7YDIevoq702gp6r2Bj7ztmdM0rISQAKxBBCxSBPAD4FXgSZAU+AVXIdwS4Grq3lfWKr6hqoGLsZeBLSPdhvGJJLQNgDjM0sAEYv0MtBNwKVhFm2MQQw3Ai9WtVBExgPjATp27BiD3RkTe4ESgFUBJYDmzWHfPjh6FLzuSkx4EZUARKSlNwxkxENCisg8EVkW5nFJyDp3AUeoZoB5VZ2hqgWqWtC6detIj6ve3X///fTo0YPevXuTn5/PBx98ELyrt23btrRr1y74+vDhw2RmZpKfn0/Pnj35wQ9+ELb7iMA6gcfUqVPLze/ZsycXX3xxuf5wohkkpqZtxcuuXbt45JFHIlo32uOpyuTJk4Od3QGcffbZUb2/KgcOHGDw4MEcO+bGRIh1CSAwIFB+fn65/qUOHz7MoEGD4nJXc9Kzu4EjFmkV0KNEOSSkqg5V1Z5hHq9C8O7iUcA4jeR25BjavBkGD4YtW2Kzvffff585c+bw0UcfsXTpUubNm0eHDh2Cg7tMmDCBX/ziF8HXWVlZNGrUiCVLlrBs2TKysrJ47LHHKm03sE7gceedd5abv2zZMk488cRyXUNEO0hMdduqC1XlWBU/h6NJANEeT6QC3UjU1eOPP86ll15Kw4bul2Y8SgBFRUXBAYUCsrKyOP/883nxxSoLz+nL+gOKWKQJ4AJiOCSkiAwHbgdGq2rse06rwZQpsHAh1NAzccQ2b95Mq1atgr1KtmrVilNOOSXi9w8cOND3QWLCbeuZZ56hf//+5Ofn85Of/CQ48teUKVPo0qUL5557LldeeWXwl/X69evp0qUL1157LT179mTjxo1ht3HnnXcG+z6qqv/+aI6nqjjvv/9+Tj/9dM4999zgADABTZo0AWDMmDH069ePHj16MGPGjOBxdOvWjR//+Mf06NGDYcOGVTmYzbPPPssll1yCCOzbt5vvfOf4RXH9+vVjdxxPQmPGjKmy87u0ZiWAiPk1JOTDuMbkN0VkiYhU/vlbC7fcAkOGVP3IzHSNdY8+6n6pPfqoe52ZWfV7brml5v0OGzaMjRs3cvrpp3PTTTfxzjvvRBzzkSNHmDt3btiePA8cOFCuCqjir72jR4/y1ltvVRpc5YorruCFF17g4MGDLF26lDPPPLPGOCpua+XKlbz44ou89957LFmyhMzMTJ599lkWL17Myy+/zCeffMLcuXMr3Sm8Zs0abrrpJpYvX05paWnYbUydOjU4/sEDD7g+BUeMGFHl0JHVHU9VcQaG5lyyZAmvvfZacGyGih5//HFKSkooLi7moYceCnanvWbNGm6++eZgj6OhXX4HHD58mLVr19K5c2cyMqBJk+YcOFAarJbp06cPS5curfS+gQMHlvteA4/Q8ZADRIRhw4bRr1+/YIIK6NmzZ5XHldasBBCxSG8EewH4Ja5DuDne+2rdG6iqnlrb99ZF//6wdi1s3+4SQEYGtGoF3/1u3bbbpEkTSkpKWLBgAUVFRfzwhz9k6tSpXH/99VW+J3ByB3dC+NGPflRpnUD1TFXv/eqrr+jWrVtwkJSAaAaJqWpbb731FiUlJXzve98LrtemTRu+/fZbLrnkEnJycsjJyeHiiy8ut71OnTpxljewTlXbCAzeEuq1116rMsbqjqe6OMeOHUuu1/NruBHIAB566CFmzZoFwMaNG1mzZg1t27YlLy8v+P3069eP9WH6jtq+fTstWrhbWAJXAbVp05bNmzfToUMHVq1aFRx0J1RNI7aFWrhwIe3ateObb77hggsuoGvXrsHPLzMzk6ysLPbu3UvTpk0j3mbKszEBIhbNkJB7gZHe6znAb6te3R/VjQcT8H//L8yYATk5cPgwXHYZRFgdXa3MzEyGDBnCkCFD6NWrF0899VS1CaCqk3skAu8tLS3lwgsvZPr06ZW6g450kJiqtqWqXHfddfzud+VH/vxTDR9y48aNg9NVbSPcybQmVR1PbeMEN2bwvHnzeP/998nNzWXIkCEcPOi6tao4SEy4KqBGjRoF1w9cBdS27Sl8/fXXfPDBB7Rq1YrTTjut0vsGDhwYdoyEBx98kKFDh5abF2jvaNOmDWPHjuXDDz8sl0APHTpETk5OjceaVgIlgOuvh5dectOB3n+TcbqwMH49GKtqrR7AyNq+t7aPfv36aUUrVqyoNK86Y8eq3nST6pIl7nns2KjeHtaqVav0s88+C76+66679Oabbw6+njRpkj7wwAPl3tO4ceMat1vVOqHzP/roI+3YsaOWlZWVW7Zx40b985//rKqqRUVFOnLkyKi2tXz5cj311FN169atqqq6Y8cOXb9+vX744Yfat29fPXDggO7du1dPO+204LGtW7dOe/ToEdxeVdvYvn27duzYscbjj+R4qtpHSUmJ9urVS0tLS3XPnj166qmnlvsOGjdurK+88oqOGjVKVVVXrlyp2dnZWlRUVOk4HnjgAZ00aVLY+Nq3b68HDhzQAwdUFy9WHTv2Cr3nnnv0e9/7nm7bti2iY6zKvn37dM+ePcHpAQMG6Ny5c4PLt2/frl26dAn73mj/L1LKzJmBfvlUs7PdI1mnc3NVp01zz4HXb78d9UcCFGuYc2q1JQARycH1//Md4ANVfdprwL0fyMe1BySVf/zj+HSMLnhh3759TJw4kV27dtGgQQNOPfXUSvW1tRFaTQQwfPjw4KWgAX379qV37/V1gboAABD5SURBVN48//zzXHPNNcH5tRkkpuK2fvOb3zBs2DCOHTtGw4YNmT59OmeddRajR4+md+/enHTSSfTq1SvsIDHgxtetahvnnHMOPXv25KKLLuKBBx5gxIgR/PWvf62y8byq46luHz/84Q/p06cPbdq0CVYRhRo+fDiPPfYY3bp1o0uXLsGqq2gMGzaMhQsXMmiQ++V+0kmn8Nxzz/H222/TqlWrqLcXauvWrYwdOxZwbUVXXXUVw4cPDy4vKipi5MiRVb09PRUVwbXXHn996FByT5eWwq23ln89alSdB7IKCpcVAg/c9flHcZd+HgVmes9HgZnVvTcej1iUAEzd7d27V1VV9+/fr/369dOSkhKfI/JPSUmJXn311Xr4sCsBeAWRejF27FhdvXp12GVp+X/x9tvHfymn+iPKkgBVlABqugpoGK6+/1zgPtzdwB8DfVX1+3VPPyYZjR8/nvz8fM444wwuu+wyzjjjDL9D8s0ZZ5xBYWEhqu7S0/q6E/jw4cOMGTOG008/vX52mAxuuMH9Qk4HpaXueOuopkbglsDzqvofEVkDTAJ+o6qVr20zaeO5557zO4SEcuONNwZP/FpPtzRmZWVxbWhVh3ENpKNGpUcSyM093lhcB5HcB/B7EVkKzMddBjrN68b5kzrv3ZgUYd1BJ4DCQlc37l36m7Jyc2PWBhBJAugA9AS64QaAyfNeV75zyZg0JXJ8WEjjo4pJIDvbPZJ1OjcXpk07fjwxPPlDDQlAVTOqe8QkAmNShA0MnyACSaBTJ5g71z2SdXrOHPjlL48fTwxP/gCi1fxkEZGuqrqq2g1EsE6sFBQUaMWuB1auXEnXrl0RG4jV+GzJEjjhBPd/6idVZdWqVXTr1s3fQEzCEJESVS2oOL+mRuAVIrIQmA0sxo3+JcApQAEwGjgHH+8HyMnJYceOHbRs2dKSgPFVRob/VUCqyo4dO+zuYBORmhLAGOA24A+4BuBQAizw1vFN+/bt2bRpE9u2bfMzDGP45hvX/UwVHYfWm5ycHNq3t0H2TM2qTQCqOhuYLSIdcPcCdPAWfQm8p6qxGBGsTho2bEheXq17pjYmZn7wA+jSBcJ0HGpMQop0SMiNwPNxjsWYpJadXf4ufmMSXUQJQETOASYDnTle36+qWseOlI1JHZYATLKJtDvo54H2wCHcGL7GmApyciwBmOQSzbX8/62qjVS1aeARt6iMSULZ2eAND2BMUoi0BPAKMEJEPgB2Bmaq6kdxicqYJGRVQCbZRJoAfuo9v1FhftKNB2BMvFgVkEk2kSaAp8LMs15PjAlhVUAm2dQ0Itjs+grEmGRnVUAm2dRUAhhVzTIrARgTwhKASTY1JQC7xdaYCFkbgEk2NXUFsaG+AjEm2VkbgEk21qe/MTGSne3GAzhit0qaJGEJwJgYCfTAbNVAJllYAjAmRgKj+FkCMMnCEoAxMRJIANYOYJKFJQBjYsRKACbZ+JIARGSKiCwVkSUi8oaInOJHHMbEkrUBmGTjVwngAVXtrar5wBzgHp/iMCZmrArIJBtfEoCq7gl52Ri7q9ikAKsCMskm0s7gYk5E7geuBXYDhdWsNx4YD9CxY8f6Cc6YWrAqIJNs4lYCEJF5IrIszOMSAFW9S1U7AM9yvLvpSlR1hqoWqGpB69at4xWuMXVmJQCTbOJWAlDVoRGu+izwGjApXrEYUx+sDcAkG7+uAjot5OUlwCo/4jAmlqwEYJKNX20AU0WkC3AM2ABM8CkOY2LG2gBMsvElAajqZX7s15h4siogk2zsTmBjYsSqgEyysQRgTIxYAjDJxhKAMTFibQAm2VgCMCZGsrLcs7UBmGRhCcCYGMnIgIYNrQRgkoclAGNiyAaGN8nEEoAxMZSdbQnAJA9LAMbEUHa2tQGY5GEJwJgYshKASSaWAIyJIWsDMMnEEoAxMWRVQCaZWAIwJoasCsgkE0sAxsSQVQGZZGIJwJgYshKASSaWAIyJIWsDMMnEEoAxMWQlAJNMLAEYE0PWBmCSiSUAY2LISgAmmVgCMCaGrA3AJBNLAMbEkFUBmWRiCcCYGLIqIJNMLAEYE0PZ2XDkCBw96nckxtTMEoAxMWQDw5tkYgnAmBgKnPi//BI2b4bBg2HLlvLTUPWydJg2iaOB3wEYk0reess9T50KubmwcCHcd5+bF5h+5BGYMiX8snSYfuSRyD9PE1+iqn7HELGCggItLi72OwxjKmnUyC7/jEZODhw44HcU6UNESlS1oOJ8qwIyJgbWroWrrnIntppkZsY/nkSVkwPjxsG6dX5HYsASgDExcfLJ0KwZHD5cPglUnM7IgC5d3HN166XiNLg2kmbNoG1bTAKwBGBMjGzdChMmwKJFkJfnHhWnJ0yAnTtrXi/Vpjt2dJ/RuedaQ3AisTYAY0zcHTvmSgO//KVrIDf1KyHbAETkVhFREWnlZxzGmPjKyID27WHjRr8jMaF8SwAi0gEYBnzpVwzGmPrToYMlgETjZwngf4DbgeSpgzLG1JolgMTjSwIQkUuAr1T1kwjWHS8ixSJSvG3btnqIzhgTDx06wFdfWT9JiSRudwKLyDwg3MVedwG/xlX/1EhVZwAzwDUCxyxAY0y96tABysrc1VKnnOJ3NAbimABUdWi4+SLSC8gDPhERgPbARyLSX1XtAjFjUlTgUtCNGy0BJIp6rwJS1U9VtY2qdlbVzsAm4Aw7+RuT2jp0cM/WDpA47EYwY0y9sASQeHzvDdQrBRhjUtwJJ7geUi0BJA4rARhj6oWIXQqaaCwBGGPqjSWAxGIJwBhTbywBJBZLAMaYetOhgxsesqzM70gMWAIwxtSjDh1A1d0RbPxnCcAYU29CbwYz/rMEYIypN3YvQGKxBGCMqTeWABKLJQBjTL1p0gRatLAEkCgsARhj6pVdCpo4LAEYY+qVJYDEYQnAGFOvTjwRPv0Utmxx9wQMHuymofxrmw7/ucRS5uTJk2O7xTiaMWPG5PHjx/sdhjGmDn7/e1cC2LMHFi6EWbNg/34YORLuuOP463fftelwn8vIkdF/5vfee+/myZMnz6g4X1STZ5CtgoICLS4u9jsMY0wtNGoEBw/6HUVqyMmBAwciX19ESlS1oOJ8qwIyxtSLtWvhqqtcIghwgwJCRobrKjojo/KydJ6u+Lnk5sK4cbBuHTFhCcAYUy9OPhmaNYNDh9wvWHDdQgSmO3d2z+GWpfN06Ody8KD7DNuGG229FiwBGGPqzdatMGECLFoEeXnusWiRm7dzZ/hl6Txd8XOZMCG2DcHWBmCMMSnO2gCMMcaUYwnAGGPSlCUAY4xJU5YAjDEmTVkCMMaYNGUJwBhj0lRSXQYqItuADbV8eytgewzDSQZ2zOnBjjk91OWYO6lq64ozkyoB1IWIFIe7DjaV2TGnBzvm9BCPY7YqIGOMSVOWAIwxJk2lUwKo1Bd2GrBjTg92zOkh5secNm0AxhhjykunEoAxxpgQlgCMMSZNpUUCEJHhIrJaRD4XkTv9jifWRKSDiBSJyAoRWS4iP/fmnygib4rIGu/5BL9jjTURyRSRj0Vkjvc6T0Q+8L7rF0Uky+8YY0lEWojITBFZJSIrRWRAqn/PIvIL7+96mYg8LyI5qfY9i8jjIvKNiCwLmRf2exXnIe/Yl4rIGbXdb8onABHJBKYDFwHdgStFpLu/UcXcEeBWVe0OnAXc7B3jncBbqnoa8Jb3OtX8HFgZ8vr3wP+o6qnATuBHvkQVP38G/q2qXYE+uGNP2e9ZRNoBPwMKVLUnkAlcQep9z08CwyvMq+p7vQg4zXuMBx6t7U5TPgEA/YHPVXWtqh4GXgAu8TmmmFLVzar6kTe9F3dSaIc7zqe81Z4CxvgTYXyISHtgJPBX77UA5wEzvVVS6phFpDkwCPgbgKoeVtVdpPj3DDQAGolIAyAX2EyKfc+q+i7wbYXZVX2vlwBPq7MIaCEiJ9dmv+mQANoBG0Neb/LmpSQR6Qz0BT4ATlLVzd6iLcBJPoUVL38CbgeOea9bArtU9Yj3OtW+6zxgG/CEV+31VxFpTAp/z6r6FfAg8CXuxL8bKCG1v+eAqr7XmJ3T0iEBpA0RaQK8DNyiqntCl6m73jdlrvkVkVHAN6pa4ncs9agBcAbwqKr2BfZTobonBb/nE3C/ePOAU4DGVK4qSXnx+l7TIQF8BXQIed3em5dSRKQh7uT/rKr+w5u9NVA09J6/8Su+ODgHGC0i63HVeufh6sdbeFUFkHrf9SZgk6p+4L2eiUsIqfw9DwXWqeo2VS0D/oH77lP5ew6o6nuN2TktHRLAYuA076qBLFwD0myfY4opr+77b8BKVf1jyKLZwHXe9HXAq/UdW7yo6q9Utb2qdsZ9p2+r6jigCPi+t1qqHfMWYKOIdPFmnQ+sIIW/Z1zVz1kikuv9nQeOOWW/5xBVfa+zgWu9q4HOAnaHVBVFR1VT/gGMAD4DvgDu8jueOBzfubji4VJgifcYgasTfwtYA8wDTvQ71jgd/xBgjjf9HeBD4HPgJSDb7/hifKz5QLH3Xb8CnJDq3zNwL7AKWAb8PyA71b5n4HlcG0cZrqT3o6q+V0BwVzZ+AXyKu0KqVvu1riCMMSZNpUMVkDHGmDAsARhjTJqyBGCMMWnKEoAxxqQpSwDGGJOmLAGYhCcinUVEvccQb9713uvbItxGRxF5T0QOeu/7fjXrrheRfd50dxGZHNhvLHnXtk8WketD5kV1XMbUhSUAk2x+XdMK3gm0c4XZ2cBaYEEE+5jI8RtwugOTcPcaRM3rjbYqud62rw+Z9w5wJfDP2uzPmGhYAjDJZA9wgYgURPtGVV2jqtcA70Ww+l+Ap7wk8pI3b1KgBCIi7UTkZRHZKSJfi8jUwIneKz3sF5FHRGQ30EtEXvLWPShuzIax3jaLvefB3rYnA4NxNwVd7G3vXK/f+31e/+/jvfmBUtF/RGSuiOwRkee8u0NzxY0ZsNuLZYmI9Ij2MzOpzxKASSYLcXeDVioFiEgTEWklIq28WSd4r5vUYX/bcD2Ogutn6UpcNwTPABfg+h6aDdwB3BTyvlxcx2W34fpvWYzrtfRX3vKnRSQn5DhWetueGbINRKSlt/1OIdv6/0TkvJDVzgTeBVZ72zgXuBC4DHgWV5qZDzSs1SdgUpolAJNMFJiK6xe9W4VlD+NO2Nu81x950w/Xemeq+zleYlimqi8Apbhf6U1x1Tc/8ZZfUOHt16nq/wJbgR5eHH/04m4CdAbe8Nb9RlVfUNVlFbYxANfVw99U9TFvf+AGBAn4QFV/h0tQeNtdi+siewDQE3gb+CSaYzfpwRKASTYvAOuACRXm/wF3Eg6ciK/2pv9Qx/2F6ytFcCfUC0IeU0KW71fV3d70BcC1uF/pw4F/efNzqth2dTGEWz8wiEigb/xMVf0E6I2rSuqC60Qs2UfMMnHQoOZVjEkcqnpURP4APFZh/gpc9Qyu00jeU9X1geVeVdAVuO6TAc4XkRaq+tcadrnTex4oIlfgTqbzcSNzDcR1w3surrOyxdVspzFuCL9zQubtwf1SP1VExuGquEK97+3/RyKyEbjGm/9adQGLyEBcH/rLgY9xHQOeUt17THqyEoBJRk8CX0f5nlbA/+I1ruJKEP8bwfsW4npkHIj7Rd0SV7r4B/BT3GhV38X1TBnOm7hSSz4uAb0eWKCuf/sHgBa4doWBoW9U1R3AaFyXyH8E2gI/UdWiGmIuxXWb/DBuzOTXqZAwjQGsN1BjjElXVgIwxpg0ZQnAGGPSlCUAY4xJU5YAjDEmTVkCMMaYNGUJwBhj0pQlAGOMSVP/P5tpFmruBoCEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 8(b). Regret minimisation plot: GP v STP DF 1\n",
    "\n",
    "title = obj_func\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(train_regret_gp_11, label = 'GP ERM Regret: Median', marker = 'D', color = 'Red')\n",
    "plt.plot(train_regret_stp_df1_4, label = 'STP ERM Regret: Median ' r'($\\nu$' ' = {})'.format(df1), marker = '*', color = 'Blue')\n",
    "\n",
    "plt.title(title, weight = 'bold')\n",
    "plt.xlabel('N+1 iterations', weight = 'bold') # x-axis label\n",
    "plt.ylabel('ln(Regret)', weight = 'bold') # y-axis label\n",
    "plt.legend(loc=0) # add plot legend\n",
    "\n",
    "plt.show() #visualise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 8(c). Regret minimisation plot: IQR GP v STP DF 1\n",
    "\n",
    "title = obj_func\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(train_regret_gp_11, marker = 'D', color = 'Red')\n",
    "plt.plot(train_regret_stp_df1_4, marker = '*', color = 'Blue')\n",
    "\n",
    "xstar = np.arange(0, 101, step=1)\n",
    "plt.fill_between(xstar, train_regret_gp_10, train_regret_gp_9, facecolor = 'Red', alpha=0.4, label='GP ERM Regret: IQR')\n",
    "plt.fill_between(xstar, train_regret_stp_df1_19, train_regret_stp_df1_17, facecolor = 'Blue', alpha=0.4, label='STP ERM Regret: IQR ' r'($\\nu$' ' = {})'.format(df1))\n",
    "\n",
    "plt.title(title, weight = 'bold')\n",
    "plt.xlabel('N+1 iterations', weight = 'bold') # x-axis label\n",
    "plt.ylabel('ln(Regret)', weight = 'bold') # y-axis label\n",
    "plt.legend(loc=0) # add plot legend\n",
    "\n",
    "plt.show() #visualise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0151804216478102,\n",
       " 0.6437141816336722,\n",
       " 0.1900705128585028,\n",
       " 1.0151804216478102,\n",
       " 0.6437141816336722,\n",
       " 0.1900705128585028)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iteration1 :\n",
    "\n",
    "slice1 = 0\n",
    "\n",
    "gp1 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp1 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp1_results = pd.DataFrame(gp1).sort_values(by=[0], ascending=False)\n",
    "stp1_results = pd.DataFrame(stp1).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp1 = np.asarray(gp1_results[4:5][0])[0]\n",
    "median_gp1 = np.asarray(gp1_results[9:10][0])[0]\n",
    "upper_gp1 = np.asarray(gp1_results[14:15][0])[0]\n",
    "\n",
    "lower_stp1 = np.asarray(stp1_results[4:5][0])[0]\n",
    "median_stp1 = np.asarray(stp1_results[9:10][0])[0]\n",
    "upper_stp1 = np.asarray(stp1_results[14:15][0])[0]\n",
    "\n",
    "lower_gp1, median_gp1, upper_gp1, lower_stp1, median_stp1, upper_stp1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3196962203024675,\n",
       " -0.15814798990598783,\n",
       " -0.7915786790199397,\n",
       " 0.5770669509474404,\n",
       " 0.41057361244770374,\n",
       " 0.12205201674786556)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iteration11 :\n",
    "\n",
    "slice11 = 10\n",
    "\n",
    "gp11 = [train_regret_gp_1[slice11],\n",
    "       train_regret_gp_2[slice11],\n",
    "       train_regret_gp_3[slice11],\n",
    "       train_regret_gp_4[slice11],\n",
    "       train_regret_gp_5[slice11],\n",
    "       train_regret_gp_6[slice11],\n",
    "       train_regret_gp_7[slice11],\n",
    "       train_regret_gp_8[slice11],\n",
    "       train_regret_gp_9[slice11],\n",
    "       train_regret_gp_10[slice11],\n",
    "       train_regret_gp_11[slice11],\n",
    "       train_regret_gp_12[slice11],\n",
    "       train_regret_gp_13[slice11],\n",
    "       train_regret_gp_14[slice11],\n",
    "       train_regret_gp_15[slice11],\n",
    "       train_regret_gp_16[slice11],\n",
    "       train_regret_gp_17[slice11],\n",
    "       train_regret_gp_18[slice11],\n",
    "       train_regret_gp_19[slice11],\n",
    "       train_regret_gp_20[slice11]]\n",
    "\n",
    "stp11 = [train_regret_stp_df1_1[slice11],\n",
    "       train_regret_stp_df1_2[slice11],\n",
    "       train_regret_stp_df1_3[slice11],\n",
    "       train_regret_stp_df1_4[slice11],\n",
    "       train_regret_stp_df1_5[slice11],\n",
    "       train_regret_stp_df1_6[slice11],\n",
    "       train_regret_stp_df1_7[slice11],\n",
    "       train_regret_stp_df1_8[slice11],\n",
    "       train_regret_stp_df1_9[slice11],\n",
    "       train_regret_stp_df1_10[slice11],\n",
    "       train_regret_stp_df1_11[slice11],\n",
    "       train_regret_stp_df1_12[slice11],\n",
    "       train_regret_stp_df1_13[slice11],\n",
    "       train_regret_stp_df1_14[slice11],\n",
    "       train_regret_stp_df1_15[slice11],\n",
    "       train_regret_stp_df1_16[slice11],\n",
    "       train_regret_stp_df1_17[slice11],\n",
    "       train_regret_stp_df1_18[slice11],\n",
    "       train_regret_stp_df1_19[slice11],\n",
    "       train_regret_stp_df1_20[slice11]]\n",
    "\n",
    "gp11_results = pd.DataFrame(gp11).sort_values(by=[0], ascending=False)\n",
    "stp11_results = pd.DataFrame(stp11).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp11 = np.asarray(gp11_results[4:5][0])[0]\n",
    "median_gp11 = np.asarray(gp11_results[9:10][0])[0]\n",
    "upper_gp11 = np.asarray(gp11_results[14:15][0])[0]\n",
    "\n",
    "lower_stp11 = np.asarray(stp11_results[4:5][0])[0]\n",
    "median_stp11 = np.asarray(stp11_results[9:10][0])[0]\n",
    "upper_stp11 = np.asarray(stp11_results[14:15][0])[0]\n",
    "\n",
    "lower_gp11, median_gp11, upper_gp11, lower_stp11, median_stp11, upper_stp11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.04179147794950536,\n",
       " -0.44964261591388205,\n",
       " -0.9608978268107812,\n",
       " 0.4579864824999007,\n",
       " 0.3011905015281919,\n",
       " -0.30186284635778476)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iteration21 :\n",
    "\n",
    "slice21 = 20\n",
    "\n",
    "gp21 = [train_regret_gp_1[slice21],\n",
    "       train_regret_gp_2[slice21],\n",
    "       train_regret_gp_3[slice21],\n",
    "       train_regret_gp_4[slice21],\n",
    "       train_regret_gp_5[slice21],\n",
    "       train_regret_gp_6[slice21],\n",
    "       train_regret_gp_7[slice21],\n",
    "       train_regret_gp_8[slice21],\n",
    "       train_regret_gp_9[slice21],\n",
    "       train_regret_gp_10[slice21],\n",
    "       train_regret_gp_11[slice21],\n",
    "       train_regret_gp_12[slice21],\n",
    "       train_regret_gp_13[slice21],\n",
    "       train_regret_gp_14[slice21],\n",
    "       train_regret_gp_15[slice21],\n",
    "       train_regret_gp_16[slice21],\n",
    "       train_regret_gp_17[slice21],\n",
    "       train_regret_gp_18[slice21],\n",
    "       train_regret_gp_19[slice21],\n",
    "       train_regret_gp_20[slice21]]\n",
    "\n",
    "stp21 = [train_regret_stp_df1_1[slice21],\n",
    "       train_regret_stp_df1_2[slice21],\n",
    "       train_regret_stp_df1_3[slice21],\n",
    "       train_regret_stp_df1_4[slice21],\n",
    "       train_regret_stp_df1_5[slice21],\n",
    "       train_regret_stp_df1_6[slice21],\n",
    "       train_regret_stp_df1_7[slice21],\n",
    "       train_regret_stp_df1_8[slice21],\n",
    "       train_regret_stp_df1_9[slice21],\n",
    "       train_regret_stp_df1_10[slice21],\n",
    "       train_regret_stp_df1_11[slice21],\n",
    "       train_regret_stp_df1_12[slice21],\n",
    "       train_regret_stp_df1_13[slice21],\n",
    "       train_regret_stp_df1_14[slice21],\n",
    "       train_regret_stp_df1_15[slice21],\n",
    "       train_regret_stp_df1_16[slice21],\n",
    "       train_regret_stp_df1_17[slice21],\n",
    "       train_regret_stp_df1_18[slice21],\n",
    "       train_regret_stp_df1_19[slice21],\n",
    "       train_regret_stp_df1_20[slice21]]\n",
    "\n",
    "gp21_results = pd.DataFrame(gp21).sort_values(by=[0], ascending=False)\n",
    "stp21_results = pd.DataFrame(stp21).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp21 = np.asarray(gp21_results[4:5][0])[0]\n",
    "median_gp21 = np.asarray(gp21_results[9:10][0])[0]\n",
    "upper_gp21 = np.asarray(gp21_results[14:15][0])[0]\n",
    "\n",
    "lower_stp21 = np.asarray(stp21_results[4:5][0])[0]\n",
    "median_stp21 = np.asarray(stp21_results[9:10][0])[0]\n",
    "upper_stp21 = np.asarray(stp21_results[14:15][0])[0]\n",
    "\n",
    "lower_gp21, median_gp21, upper_gp21, lower_stp21, median_stp21, upper_stp21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.27939392843174166,\n",
       " -0.8610365891731478,\n",
       " -1.3026705103183764,\n",
       " 0.32264686419759037,\n",
       " 0.07693158808641175,\n",
       " -0.36528771842242713)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iteration31 :\n",
    "\n",
    "slice31 = 30\n",
    "\n",
    "gp31 = [train_regret_gp_1[slice31],\n",
    "       train_regret_gp_2[slice31],\n",
    "       train_regret_gp_3[slice31],\n",
    "       train_regret_gp_4[slice31],\n",
    "       train_regret_gp_5[slice31],\n",
    "       train_regret_gp_6[slice31],\n",
    "       train_regret_gp_7[slice31],\n",
    "       train_regret_gp_8[slice31],\n",
    "       train_regret_gp_9[slice31],\n",
    "       train_regret_gp_10[slice31],\n",
    "       train_regret_gp_11[slice31],\n",
    "       train_regret_gp_12[slice31],\n",
    "       train_regret_gp_13[slice31],\n",
    "       train_regret_gp_14[slice31],\n",
    "       train_regret_gp_15[slice31],\n",
    "       train_regret_gp_16[slice31],\n",
    "       train_regret_gp_17[slice31],\n",
    "       train_regret_gp_18[slice31],\n",
    "       train_regret_gp_19[slice31],\n",
    "       train_regret_gp_20[slice31]]\n",
    "\n",
    "stp31 = [train_regret_stp_df1_1[slice31],\n",
    "       train_regret_stp_df1_2[slice31],\n",
    "       train_regret_stp_df1_3[slice31],\n",
    "       train_regret_stp_df1_4[slice31],\n",
    "       train_regret_stp_df1_5[slice31],\n",
    "       train_regret_stp_df1_6[slice31],\n",
    "       train_regret_stp_df1_7[slice31],\n",
    "       train_regret_stp_df1_8[slice31],\n",
    "       train_regret_stp_df1_9[slice31],\n",
    "       train_regret_stp_df1_10[slice31],\n",
    "       train_regret_stp_df1_11[slice31],\n",
    "       train_regret_stp_df1_12[slice31],\n",
    "       train_regret_stp_df1_13[slice31],\n",
    "       train_regret_stp_df1_14[slice31],\n",
    "       train_regret_stp_df1_15[slice31],\n",
    "       train_regret_stp_df1_16[slice31],\n",
    "       train_regret_stp_df1_17[slice31],\n",
    "       train_regret_stp_df1_18[slice31],\n",
    "       train_regret_stp_df1_19[slice31],\n",
    "       train_regret_stp_df1_20[slice31]]\n",
    "\n",
    "gp31_results = pd.DataFrame(gp31).sort_values(by=[0], ascending=False)\n",
    "stp31_results = pd.DataFrame(stp31).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp31 = np.asarray(gp31_results[4:5][0])[0]\n",
    "median_gp31 = np.asarray(gp31_results[9:10][0])[0]\n",
    "upper_gp31 = np.asarray(gp31_results[14:15][0])[0]\n",
    "\n",
    "lower_stp31 = np.asarray(stp31_results[4:5][0])[0]\n",
    "median_stp31 = np.asarray(stp31_results[9:10][0])[0]\n",
    "upper_stp31 = np.asarray(stp31_results[14:15][0])[0]\n",
    "\n",
    "lower_gp31, median_gp31, upper_gp31, lower_stp31, median_stp31, upper_stp31\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5689096977358219,\n",
       " -0.8610365891731478,\n",
       " -1.3685402458234954,\n",
       " 0.3011905015281919,\n",
       " -0.1426231786285531,\n",
       " -0.6754040993861657)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iteration41 :\n",
    "\n",
    "slice41 = 40\n",
    "\n",
    "gp41 = [train_regret_gp_1[slice41],\n",
    "       train_regret_gp_2[slice41],\n",
    "       train_regret_gp_3[slice41],\n",
    "       train_regret_gp_4[slice41],\n",
    "       train_regret_gp_5[slice41],\n",
    "       train_regret_gp_6[slice41],\n",
    "       train_regret_gp_7[slice41],\n",
    "       train_regret_gp_8[slice41],\n",
    "       train_regret_gp_9[slice41],\n",
    "       train_regret_gp_10[slice41],\n",
    "       train_regret_gp_11[slice41],\n",
    "       train_regret_gp_12[slice41],\n",
    "       train_regret_gp_13[slice41],\n",
    "       train_regret_gp_14[slice41],\n",
    "       train_regret_gp_15[slice41],\n",
    "       train_regret_gp_16[slice41],\n",
    "       train_regret_gp_17[slice41],\n",
    "       train_regret_gp_18[slice41],\n",
    "       train_regret_gp_19[slice41],\n",
    "       train_regret_gp_20[slice41]]\n",
    "\n",
    "stp41 = [train_regret_stp_df1_1[slice41],\n",
    "       train_regret_stp_df1_2[slice41],\n",
    "       train_regret_stp_df1_3[slice41],\n",
    "       train_regret_stp_df1_4[slice41],\n",
    "       train_regret_stp_df1_5[slice41],\n",
    "       train_regret_stp_df1_6[slice41],\n",
    "       train_regret_stp_df1_7[slice41],\n",
    "       train_regret_stp_df1_8[slice41],\n",
    "       train_regret_stp_df1_9[slice41],\n",
    "       train_regret_stp_df1_10[slice41],\n",
    "       train_regret_stp_df1_11[slice41],\n",
    "       train_regret_stp_df1_12[slice41],\n",
    "       train_regret_stp_df1_13[slice41],\n",
    "       train_regret_stp_df1_14[slice41],\n",
    "       train_regret_stp_df1_15[slice41],\n",
    "       train_regret_stp_df1_16[slice41],\n",
    "       train_regret_stp_df1_17[slice41],\n",
    "       train_regret_stp_df1_18[slice41],\n",
    "       train_regret_stp_df1_19[slice41],\n",
    "       train_regret_stp_df1_20[slice41]]\n",
    "\n",
    "gp41_results = pd.DataFrame(gp41).sort_values(by=[0], ascending=False)\n",
    "stp41_results = pd.DataFrame(stp41).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp41 = np.asarray(gp41_results[4:5][0])[0]\n",
    "median_gp41 = np.asarray(gp41_results[9:10][0])[0]\n",
    "upper_gp41 = np.asarray(gp41_results[14:15][0])[0]\n",
    "\n",
    "lower_stp41 = np.asarray(stp41_results[4:5][0])[0]\n",
    "median_stp41 = np.asarray(stp41_results[9:10][0])[0]\n",
    "upper_stp41 = np.asarray(stp41_results[14:15][0])[0]\n",
    "\n",
    "lower_gp41, median_gp41, upper_gp41, lower_stp41, median_stp41, upper_stp41\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5689096977358219,\n",
       " -1.229927882130823,\n",
       " -1.4636762261000498,\n",
       " -0.8883062751467867,\n",
       " -2.4646613494202945,\n",
       " -3.362936333242479)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iteration51 :\n",
    "\n",
    "slice51 = 50\n",
    "\n",
    "gp51 = [train_regret_gp_1[slice51],\n",
    "       train_regret_gp_2[slice51],\n",
    "       train_regret_gp_3[slice51],\n",
    "       train_regret_gp_4[slice51],\n",
    "       train_regret_gp_5[slice51],\n",
    "       train_regret_gp_6[slice51],\n",
    "       train_regret_gp_7[slice51],\n",
    "       train_regret_gp_8[slice51],\n",
    "       train_regret_gp_9[slice51],\n",
    "       train_regret_gp_10[slice51],\n",
    "       train_regret_gp_11[slice51],\n",
    "       train_regret_gp_12[slice51],\n",
    "       train_regret_gp_13[slice51],\n",
    "       train_regret_gp_14[slice51],\n",
    "       train_regret_gp_15[slice51],\n",
    "       train_regret_gp_16[slice51],\n",
    "       train_regret_gp_17[slice51],\n",
    "       train_regret_gp_18[slice51],\n",
    "       train_regret_gp_19[slice51],\n",
    "       train_regret_gp_20[slice51]]\n",
    "\n",
    "stp51 = [train_regret_stp_df1_1[slice51],\n",
    "       train_regret_stp_df1_2[slice51],\n",
    "       train_regret_stp_df1_3[slice51],\n",
    "       train_regret_stp_df1_4[slice51],\n",
    "       train_regret_stp_df1_5[slice51],\n",
    "       train_regret_stp_df1_6[slice51],\n",
    "       train_regret_stp_df1_7[slice51],\n",
    "       train_regret_stp_df1_8[slice51],\n",
    "       train_regret_stp_df1_9[slice51],\n",
    "       train_regret_stp_df1_10[slice51],\n",
    "       train_regret_stp_df1_11[slice51],\n",
    "       train_regret_stp_df1_12[slice51],\n",
    "       train_regret_stp_df1_13[slice51],\n",
    "       train_regret_stp_df1_14[slice51],\n",
    "       train_regret_stp_df1_15[slice51],\n",
    "       train_regret_stp_df1_16[slice51],\n",
    "       train_regret_stp_df1_17[slice51],\n",
    "       train_regret_stp_df1_18[slice51],\n",
    "       train_regret_stp_df1_19[slice51],\n",
    "       train_regret_stp_df1_20[slice51]]\n",
    "\n",
    "gp51_results = pd.DataFrame(gp51).sort_values(by=[0], ascending=False)\n",
    "stp51_results = pd.DataFrame(stp51).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp51 = np.asarray(gp51_results[4:5][0])[0]\n",
    "median_gp51 = np.asarray(gp51_results[9:10][0])[0]\n",
    "upper_gp51 = np.asarray(gp51_results[14:15][0])[0]\n",
    "\n",
    "lower_stp51 = np.asarray(stp51_results[4:5][0])[0]\n",
    "median_stp51 = np.asarray(stp51_results[9:10][0])[0]\n",
    "upper_stp51 = np.asarray(stp51_results[14:15][0])[0]\n",
    "\n",
    "lower_gp51, median_gp51, upper_gp51, lower_stp51, median_stp51, upper_stp51\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5689096977358219,\n",
       " -1.272451658020414,\n",
       " -1.7930777665900184,\n",
       " -2.4569254614048788,\n",
       " -3.3520105093281267,\n",
       " -3.8283661538674836)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iteration61 :\n",
    "\n",
    "slice61 = 60\n",
    "\n",
    "gp61 = [train_regret_gp_1[slice61],\n",
    "       train_regret_gp_2[slice61],\n",
    "       train_regret_gp_3[slice61],\n",
    "       train_regret_gp_4[slice61],\n",
    "       train_regret_gp_5[slice61],\n",
    "       train_regret_gp_6[slice61],\n",
    "       train_regret_gp_7[slice61],\n",
    "       train_regret_gp_8[slice61],\n",
    "       train_regret_gp_9[slice61],\n",
    "       train_regret_gp_10[slice61],\n",
    "       train_regret_gp_11[slice61],\n",
    "       train_regret_gp_12[slice61],\n",
    "       train_regret_gp_13[slice61],\n",
    "       train_regret_gp_14[slice61],\n",
    "       train_regret_gp_15[slice61],\n",
    "       train_regret_gp_16[slice61],\n",
    "       train_regret_gp_17[slice61],\n",
    "       train_regret_gp_18[slice61],\n",
    "       train_regret_gp_19[slice61],\n",
    "       train_regret_gp_20[slice61]]\n",
    "\n",
    "stp61 = [train_regret_stp_df1_1[slice61],\n",
    "       train_regret_stp_df1_2[slice61],\n",
    "       train_regret_stp_df1_3[slice61],\n",
    "       train_regret_stp_df1_4[slice61],\n",
    "       train_regret_stp_df1_5[slice61],\n",
    "       train_regret_stp_df1_6[slice61],\n",
    "       train_regret_stp_df1_7[slice61],\n",
    "       train_regret_stp_df1_8[slice61],\n",
    "       train_regret_stp_df1_9[slice61],\n",
    "       train_regret_stp_df1_10[slice61],\n",
    "       train_regret_stp_df1_11[slice61],\n",
    "       train_regret_stp_df1_12[slice61],\n",
    "       train_regret_stp_df1_13[slice61],\n",
    "       train_regret_stp_df1_14[slice61],\n",
    "       train_regret_stp_df1_15[slice61],\n",
    "       train_regret_stp_df1_16[slice61],\n",
    "       train_regret_stp_df1_17[slice61],\n",
    "       train_regret_stp_df1_18[slice61],\n",
    "       train_regret_stp_df1_19[slice61],\n",
    "       train_regret_stp_df1_20[slice61]]\n",
    "\n",
    "gp61_results = pd.DataFrame(gp61).sort_values(by=[0], ascending=False)\n",
    "stp61_results = pd.DataFrame(stp61).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp61 = np.asarray(gp61_results[4:5][0])[0]\n",
    "median_gp61 = np.asarray(gp61_results[9:10][0])[0]\n",
    "upper_gp61 = np.asarray(gp61_results[14:15][0])[0]\n",
    "\n",
    "lower_stp61 = np.asarray(stp61_results[4:5][0])[0]\n",
    "median_stp61 = np.asarray(stp61_results[9:10][0])[0]\n",
    "upper_stp61 = np.asarray(stp61_results[14:15][0])[0]\n",
    "\n",
    "lower_gp61, median_gp61, upper_gp61, lower_stp61, median_stp61, upper_stp61\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.7995919632532221,\n",
       " -1.4636762261000498,\n",
       " -3.045118598435262,\n",
       " -3.1476953032824833,\n",
       " -3.431979607755546,\n",
       " -4.2553890088954525)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iteration71 :\n",
    "\n",
    "slice71 = 70\n",
    "\n",
    "gp71 = [train_regret_gp_1[slice71],\n",
    "       train_regret_gp_2[slice71],\n",
    "       train_regret_gp_3[slice71],\n",
    "       train_regret_gp_4[slice71],\n",
    "       train_regret_gp_5[slice71],\n",
    "       train_regret_gp_6[slice71],\n",
    "       train_regret_gp_7[slice71],\n",
    "       train_regret_gp_8[slice71],\n",
    "       train_regret_gp_9[slice71],\n",
    "       train_regret_gp_10[slice71],\n",
    "       train_regret_gp_11[slice71],\n",
    "       train_regret_gp_12[slice71],\n",
    "       train_regret_gp_13[slice71],\n",
    "       train_regret_gp_14[slice71],\n",
    "       train_regret_gp_15[slice71],\n",
    "       train_regret_gp_16[slice71],\n",
    "       train_regret_gp_17[slice71],\n",
    "       train_regret_gp_18[slice71],\n",
    "       train_regret_gp_19[slice71],\n",
    "       train_regret_gp_20[slice71]]\n",
    "\n",
    "stp71 = [train_regret_stp_df1_1[slice71],\n",
    "       train_regret_stp_df1_2[slice71],\n",
    "       train_regret_stp_df1_3[slice71],\n",
    "       train_regret_stp_df1_4[slice71],\n",
    "       train_regret_stp_df1_5[slice71],\n",
    "       train_regret_stp_df1_6[slice71],\n",
    "       train_regret_stp_df1_7[slice71],\n",
    "       train_regret_stp_df1_8[slice71],\n",
    "       train_regret_stp_df1_9[slice71],\n",
    "       train_regret_stp_df1_10[slice71],\n",
    "       train_regret_stp_df1_11[slice71],\n",
    "       train_regret_stp_df1_12[slice71],\n",
    "       train_regret_stp_df1_13[slice71],\n",
    "       train_regret_stp_df1_14[slice71],\n",
    "       train_regret_stp_df1_15[slice71],\n",
    "       train_regret_stp_df1_16[slice71],\n",
    "       train_regret_stp_df1_17[slice71],\n",
    "       train_regret_stp_df1_18[slice71],\n",
    "       train_regret_stp_df1_19[slice71],\n",
    "       train_regret_stp_df1_20[slice71]]\n",
    "\n",
    "gp71_results = pd.DataFrame(gp71).sort_values(by=[0], ascending=False)\n",
    "stp71_results = pd.DataFrame(stp71).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp71 = np.asarray(gp71_results[4:5][0])[0]\n",
    "median_gp71 = np.asarray(gp71_results[9:10][0])[0]\n",
    "upper_gp71 = np.asarray(gp71_results[14:15][0])[0]\n",
    "\n",
    "lower_stp71 = np.asarray(stp71_results[4:5][0])[0]\n",
    "median_stp71 = np.asarray(stp71_results[9:10][0])[0]\n",
    "upper_stp71 = np.asarray(stp71_results[14:15][0])[0]\n",
    "\n",
    "lower_gp71, median_gp71, upper_gp71, lower_stp71, median_stp71, upper_stp71\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0114680702273304,\n",
       " -1.4636762261000498,\n",
       " -3.045118598435262,\n",
       " -3.1476953032824833,\n",
       " -3.4368960088117655,\n",
       " -4.5410472444588335)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iteration81 :\n",
    "\n",
    "slice81 = 80\n",
    "\n",
    "gp81 = [train_regret_gp_1[slice81],\n",
    "       train_regret_gp_2[slice81],\n",
    "       train_regret_gp_3[slice81],\n",
    "       train_regret_gp_4[slice81],\n",
    "       train_regret_gp_5[slice81],\n",
    "       train_regret_gp_6[slice81],\n",
    "       train_regret_gp_7[slice81],\n",
    "       train_regret_gp_8[slice81],\n",
    "       train_regret_gp_9[slice81],\n",
    "       train_regret_gp_10[slice81],\n",
    "       train_regret_gp_11[slice81],\n",
    "       train_regret_gp_12[slice81],\n",
    "       train_regret_gp_13[slice81],\n",
    "       train_regret_gp_14[slice81],\n",
    "       train_regret_gp_15[slice81],\n",
    "       train_regret_gp_16[slice81],\n",
    "       train_regret_gp_17[slice81],\n",
    "       train_regret_gp_18[slice81],\n",
    "       train_regret_gp_19[slice81],\n",
    "       train_regret_gp_20[slice81]]\n",
    "\n",
    "stp81 = [train_regret_stp_df1_1[slice81],\n",
    "       train_regret_stp_df1_2[slice81],\n",
    "       train_regret_stp_df1_3[slice81],\n",
    "       train_regret_stp_df1_4[slice81],\n",
    "       train_regret_stp_df1_5[slice81],\n",
    "       train_regret_stp_df1_6[slice81],\n",
    "       train_regret_stp_df1_7[slice81],\n",
    "       train_regret_stp_df1_8[slice81],\n",
    "       train_regret_stp_df1_9[slice81],\n",
    "       train_regret_stp_df1_10[slice81],\n",
    "       train_regret_stp_df1_11[slice81],\n",
    "       train_regret_stp_df1_12[slice81],\n",
    "       train_regret_stp_df1_13[slice81],\n",
    "       train_regret_stp_df1_14[slice81],\n",
    "       train_regret_stp_df1_15[slice81],\n",
    "       train_regret_stp_df1_16[slice81],\n",
    "       train_regret_stp_df1_17[slice81],\n",
    "       train_regret_stp_df1_18[slice81],\n",
    "       train_regret_stp_df1_19[slice81],\n",
    "       train_regret_stp_df1_20[slice81]]\n",
    "\n",
    "gp81_results = pd.DataFrame(gp81).sort_values(by=[0], ascending=False)\n",
    "stp81_results = pd.DataFrame(stp81).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp81 = np.asarray(gp81_results[4:5][0])[0]\n",
    "median_gp81 = np.asarray(gp81_results[9:10][0])[0]\n",
    "upper_gp81 = np.asarray(gp81_results[14:15][0])[0]\n",
    "\n",
    "lower_stp81 = np.asarray(stp81_results[4:5][0])[0]\n",
    "median_stp81 = np.asarray(stp81_results[9:10][0])[0]\n",
    "upper_stp81 = np.asarray(stp81_results[14:15][0])[0]\n",
    "\n",
    "lower_gp81, median_gp81, upper_gp81, lower_stp81, median_stp81, upper_stp81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.2612768783466197,\n",
       " -2.045849485037573,\n",
       " -3.1102391432412486,\n",
       " -3.4246371929521673,\n",
       " -4.15251328888983,\n",
       " -5.152773175058427)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iteration91 :\n",
    "\n",
    "slice1 = 90\n",
    "\n",
    "gp91 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp91 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp91_results = pd.DataFrame(gp91).sort_values(by=[0], ascending=False)\n",
    "stp91_results = pd.DataFrame(stp91).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp91 = np.asarray(gp91_results[4:5][0])[0]\n",
    "median_gp91 = np.asarray(gp91_results[9:10][0])[0]\n",
    "upper_gp91 = np.asarray(gp91_results[14:15][0])[0]\n",
    "\n",
    "lower_stp91 = np.asarray(stp91_results[4:5][0])[0]\n",
    "median_stp91 = np.asarray(stp91_results[9:10][0])[0]\n",
    "upper_stp91 = np.asarray(stp91_results[14:15][0])[0]\n",
    "\n",
    "lower_gp91, median_gp91, upper_gp91, lower_stp91, median_stp91, upper_stp91\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.3026705103183764,\n",
       " -2.245260434028356,\n",
       " -3.1102391432412486,\n",
       " -3.4471893899986874,\n",
       " -4.528867319093919,\n",
       " -5.1569445430649665)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iteration101 :\n",
    "\n",
    "slice1 = 100\n",
    "\n",
    "gp101 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp101 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp101_results = pd.DataFrame(gp101).sort_values(by=[0], ascending=False)\n",
    "stp101_results = pd.DataFrame(stp101).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp101 = np.asarray(gp101_results[4:5][0])[0]\n",
    "median_gp101 = np.asarray(gp101_results[9:10][0])[0]\n",
    "upper_gp101 = np.asarray(gp101_results[14:15][0])[0]\n",
    "\n",
    "lower_stp101 = np.asarray(stp101_results[4:5][0])[0]\n",
    "median_stp101 = np.asarray(stp101_results[9:10][0])[0]\n",
    "upper_stp101 = np.asarray(stp101_results[14:15][0])[0]\n",
    "\n",
    "lower_gp101, median_gp101, upper_gp101, lower_stp101, median_stp101, upper_stp101\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration2 :\n",
    "\n",
    "slice1 = 1\n",
    "\n",
    "gp2 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp2 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp2_results = pd.DataFrame(gp2).sort_values(by=[0], ascending=False)\n",
    "stp2_results = pd.DataFrame(stp2).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp2 = np.asarray(gp2_results[4:5][0])[0]\n",
    "median_gp2 = np.asarray(gp2_results[9:10][0])[0]\n",
    "upper_gp2 = np.asarray(gp2_results[14:15][0])[0]\n",
    "\n",
    "lower_stp2 = np.asarray(stp2_results[4:5][0])[0]\n",
    "median_stp2 = np.asarray(stp2_results[9:10][0])[0]\n",
    "upper_stp2 = np.asarray(stp2_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration12 :\n",
    "\n",
    "slice11 = 11\n",
    "\n",
    "gp12 = [train_regret_gp_1[slice11],\n",
    "       train_regret_gp_2[slice11],\n",
    "       train_regret_gp_3[slice11],\n",
    "       train_regret_gp_4[slice11],\n",
    "       train_regret_gp_5[slice11],\n",
    "       train_regret_gp_6[slice11],\n",
    "       train_regret_gp_7[slice11],\n",
    "       train_regret_gp_8[slice11],\n",
    "       train_regret_gp_9[slice11],\n",
    "       train_regret_gp_10[slice11],\n",
    "       train_regret_gp_11[slice11],\n",
    "       train_regret_gp_12[slice11],\n",
    "       train_regret_gp_13[slice11],\n",
    "       train_regret_gp_14[slice11],\n",
    "       train_regret_gp_15[slice11],\n",
    "       train_regret_gp_16[slice11],\n",
    "       train_regret_gp_17[slice11],\n",
    "       train_regret_gp_18[slice11],\n",
    "       train_regret_gp_19[slice11],\n",
    "       train_regret_gp_20[slice11]]\n",
    "\n",
    "stp12 = [train_regret_stp_df1_1[slice11],\n",
    "       train_regret_stp_df1_2[slice11],\n",
    "       train_regret_stp_df1_3[slice11],\n",
    "       train_regret_stp_df1_4[slice11],\n",
    "       train_regret_stp_df1_5[slice11],\n",
    "       train_regret_stp_df1_6[slice11],\n",
    "       train_regret_stp_df1_7[slice11],\n",
    "       train_regret_stp_df1_8[slice11],\n",
    "       train_regret_stp_df1_9[slice11],\n",
    "       train_regret_stp_df1_10[slice11],\n",
    "       train_regret_stp_df1_11[slice11],\n",
    "       train_regret_stp_df1_12[slice11],\n",
    "       train_regret_stp_df1_13[slice11],\n",
    "       train_regret_stp_df1_14[slice11],\n",
    "       train_regret_stp_df1_15[slice11],\n",
    "       train_regret_stp_df1_16[slice11],\n",
    "       train_regret_stp_df1_17[slice11],\n",
    "       train_regret_stp_df1_18[slice11],\n",
    "       train_regret_stp_df1_19[slice11],\n",
    "       train_regret_stp_df1_20[slice11]]\n",
    "\n",
    "gp12_results = pd.DataFrame(gp12).sort_values(by=[0], ascending=False)\n",
    "stp12_results = pd.DataFrame(stp12).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp12 = np.asarray(gp12_results[4:5][0])[0]\n",
    "median_gp12 = np.asarray(gp12_results[9:10][0])[0]\n",
    "upper_gp12 = np.asarray(gp12_results[14:15][0])[0]\n",
    "\n",
    "lower_stp12 = np.asarray(stp12_results[4:5][0])[0]\n",
    "median_stp12 = np.asarray(stp12_results[9:10][0])[0]\n",
    "upper_stp12 = np.asarray(stp12_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration22 :\n",
    "\n",
    "slice21 = 21\n",
    "\n",
    "gp22 = [train_regret_gp_1[slice21],\n",
    "       train_regret_gp_2[slice21],\n",
    "       train_regret_gp_3[slice21],\n",
    "       train_regret_gp_4[slice21],\n",
    "       train_regret_gp_5[slice21],\n",
    "       train_regret_gp_6[slice21],\n",
    "       train_regret_gp_7[slice21],\n",
    "       train_regret_gp_8[slice21],\n",
    "       train_regret_gp_9[slice21],\n",
    "       train_regret_gp_10[slice21],\n",
    "       train_regret_gp_11[slice21],\n",
    "       train_regret_gp_12[slice21],\n",
    "       train_regret_gp_13[slice21],\n",
    "       train_regret_gp_14[slice21],\n",
    "       train_regret_gp_15[slice21],\n",
    "       train_regret_gp_16[slice21],\n",
    "       train_regret_gp_17[slice21],\n",
    "       train_regret_gp_18[slice21],\n",
    "       train_regret_gp_19[slice21],\n",
    "       train_regret_gp_20[slice21]]\n",
    "\n",
    "stp22 = [train_regret_stp_df1_1[slice21],\n",
    "       train_regret_stp_df1_2[slice21],\n",
    "       train_regret_stp_df1_3[slice21],\n",
    "       train_regret_stp_df1_4[slice21],\n",
    "       train_regret_stp_df1_5[slice21],\n",
    "       train_regret_stp_df1_6[slice21],\n",
    "       train_regret_stp_df1_7[slice21],\n",
    "       train_regret_stp_df1_8[slice21],\n",
    "       train_regret_stp_df1_9[slice21],\n",
    "       train_regret_stp_df1_10[slice21],\n",
    "       train_regret_stp_df1_11[slice21],\n",
    "       train_regret_stp_df1_12[slice21],\n",
    "       train_regret_stp_df1_13[slice21],\n",
    "       train_regret_stp_df1_14[slice21],\n",
    "       train_regret_stp_df1_15[slice21],\n",
    "       train_regret_stp_df1_16[slice21],\n",
    "       train_regret_stp_df1_17[slice21],\n",
    "       train_regret_stp_df1_18[slice21],\n",
    "       train_regret_stp_df1_19[slice21],\n",
    "       train_regret_stp_df1_20[slice21]]\n",
    "\n",
    "gp22_results = pd.DataFrame(gp22).sort_values(by=[0], ascending=False)\n",
    "stp22_results = pd.DataFrame(stp22).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp22 = np.asarray(gp22_results[4:5][0])[0]\n",
    "median_gp22 = np.asarray(gp22_results[9:10][0])[0]\n",
    "upper_gp22 = np.asarray(gp22_results[14:15][0])[0]\n",
    "\n",
    "lower_stp22 = np.asarray(stp22_results[4:5][0])[0]\n",
    "median_stp22 = np.asarray(stp22_results[9:10][0])[0]\n",
    "upper_stp22 = np.asarray(stp22_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration32 :\n",
    "\n",
    "slice31 = 31\n",
    "\n",
    "gp32 = [train_regret_gp_1[slice31],\n",
    "       train_regret_gp_2[slice31],\n",
    "       train_regret_gp_3[slice31],\n",
    "       train_regret_gp_4[slice31],\n",
    "       train_regret_gp_5[slice31],\n",
    "       train_regret_gp_6[slice31],\n",
    "       train_regret_gp_7[slice31],\n",
    "       train_regret_gp_8[slice31],\n",
    "       train_regret_gp_9[slice31],\n",
    "       train_regret_gp_10[slice31],\n",
    "       train_regret_gp_11[slice31],\n",
    "       train_regret_gp_12[slice31],\n",
    "       train_regret_gp_13[slice31],\n",
    "       train_regret_gp_14[slice31],\n",
    "       train_regret_gp_15[slice31],\n",
    "       train_regret_gp_16[slice31],\n",
    "       train_regret_gp_17[slice31],\n",
    "       train_regret_gp_18[slice31],\n",
    "       train_regret_gp_19[slice31],\n",
    "       train_regret_gp_20[slice31]]\n",
    "\n",
    "stp32 = [train_regret_stp_df1_1[slice31],\n",
    "       train_regret_stp_df1_2[slice31],\n",
    "       train_regret_stp_df1_3[slice31],\n",
    "       train_regret_stp_df1_4[slice31],\n",
    "       train_regret_stp_df1_5[slice31],\n",
    "       train_regret_stp_df1_6[slice31],\n",
    "       train_regret_stp_df1_7[slice31],\n",
    "       train_regret_stp_df1_8[slice31],\n",
    "       train_regret_stp_df1_9[slice31],\n",
    "       train_regret_stp_df1_10[slice31],\n",
    "       train_regret_stp_df1_11[slice31],\n",
    "       train_regret_stp_df1_12[slice31],\n",
    "       train_regret_stp_df1_13[slice31],\n",
    "       train_regret_stp_df1_14[slice31],\n",
    "       train_regret_stp_df1_15[slice31],\n",
    "       train_regret_stp_df1_16[slice31],\n",
    "       train_regret_stp_df1_17[slice31],\n",
    "       train_regret_stp_df1_18[slice31],\n",
    "       train_regret_stp_df1_19[slice31],\n",
    "       train_regret_stp_df1_20[slice31]]\n",
    "\n",
    "gp32_results = pd.DataFrame(gp32).sort_values(by=[0], ascending=False)\n",
    "stp32_results = pd.DataFrame(stp32).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp32 = np.asarray(gp32_results[4:5][0])[0]\n",
    "median_gp32 = np.asarray(gp32_results[9:10][0])[0]\n",
    "upper_gp32 = np.asarray(gp32_results[14:15][0])[0]\n",
    "\n",
    "lower_stp32 = np.asarray(stp32_results[4:5][0])[0]\n",
    "median_stp32 = np.asarray(stp32_results[9:10][0])[0]\n",
    "upper_stp32 = np.asarray(stp32_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration42 :\n",
    "\n",
    "slice41 = 41\n",
    "\n",
    "gp42 = [train_regret_gp_1[slice41],\n",
    "       train_regret_gp_2[slice41],\n",
    "       train_regret_gp_3[slice41],\n",
    "       train_regret_gp_4[slice41],\n",
    "       train_regret_gp_5[slice41],\n",
    "       train_regret_gp_6[slice41],\n",
    "       train_regret_gp_7[slice41],\n",
    "       train_regret_gp_8[slice41],\n",
    "       train_regret_gp_9[slice41],\n",
    "       train_regret_gp_10[slice41],\n",
    "       train_regret_gp_11[slice41],\n",
    "       train_regret_gp_12[slice41],\n",
    "       train_regret_gp_13[slice41],\n",
    "       train_regret_gp_14[slice41],\n",
    "       train_regret_gp_15[slice41],\n",
    "       train_regret_gp_16[slice41],\n",
    "       train_regret_gp_17[slice41],\n",
    "       train_regret_gp_18[slice41],\n",
    "       train_regret_gp_19[slice41],\n",
    "       train_regret_gp_20[slice41]]\n",
    "\n",
    "stp42 = [train_regret_stp_df1_1[slice41],\n",
    "       train_regret_stp_df1_2[slice41],\n",
    "       train_regret_stp_df1_3[slice41],\n",
    "       train_regret_stp_df1_4[slice41],\n",
    "       train_regret_stp_df1_5[slice41],\n",
    "       train_regret_stp_df1_6[slice41],\n",
    "       train_regret_stp_df1_7[slice41],\n",
    "       train_regret_stp_df1_8[slice41],\n",
    "       train_regret_stp_df1_9[slice41],\n",
    "       train_regret_stp_df1_10[slice41],\n",
    "       train_regret_stp_df1_11[slice41],\n",
    "       train_regret_stp_df1_12[slice41],\n",
    "       train_regret_stp_df1_13[slice41],\n",
    "       train_regret_stp_df1_14[slice41],\n",
    "       train_regret_stp_df1_15[slice41],\n",
    "       train_regret_stp_df1_16[slice41],\n",
    "       train_regret_stp_df1_17[slice41],\n",
    "       train_regret_stp_df1_18[slice41],\n",
    "       train_regret_stp_df1_19[slice41],\n",
    "       train_regret_stp_df1_20[slice41]]\n",
    "\n",
    "gp42_results = pd.DataFrame(gp42).sort_values(by=[0], ascending=False)\n",
    "stp42_results = pd.DataFrame(stp42).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp42 = np.asarray(gp42_results[4:5][0])[0]\n",
    "median_gp42 = np.asarray(gp42_results[9:10][0])[0]\n",
    "upper_gp42 = np.asarray(gp42_results[14:15][0])[0]\n",
    "\n",
    "lower_stp42 = np.asarray(stp42_results[4:5][0])[0]\n",
    "median_stp42 = np.asarray(stp42_results[9:10][0])[0]\n",
    "upper_stp42 = np.asarray(stp42_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration52 :\n",
    "\n",
    "slice51 = 51\n",
    "\n",
    "gp52 = [train_regret_gp_1[slice51],\n",
    "       train_regret_gp_2[slice51],\n",
    "       train_regret_gp_3[slice51],\n",
    "       train_regret_gp_4[slice51],\n",
    "       train_regret_gp_5[slice51],\n",
    "       train_regret_gp_6[slice51],\n",
    "       train_regret_gp_7[slice51],\n",
    "       train_regret_gp_8[slice51],\n",
    "       train_regret_gp_9[slice51],\n",
    "       train_regret_gp_10[slice51],\n",
    "       train_regret_gp_11[slice51],\n",
    "       train_regret_gp_12[slice51],\n",
    "       train_regret_gp_13[slice51],\n",
    "       train_regret_gp_14[slice51],\n",
    "       train_regret_gp_15[slice51],\n",
    "       train_regret_gp_16[slice51],\n",
    "       train_regret_gp_17[slice51],\n",
    "       train_regret_gp_18[slice51],\n",
    "       train_regret_gp_19[slice51],\n",
    "       train_regret_gp_20[slice51]]\n",
    "\n",
    "stp52 = [train_regret_stp_df1_1[slice51],\n",
    "       train_regret_stp_df1_2[slice51],\n",
    "       train_regret_stp_df1_3[slice51],\n",
    "       train_regret_stp_df1_4[slice51],\n",
    "       train_regret_stp_df1_5[slice51],\n",
    "       train_regret_stp_df1_6[slice51],\n",
    "       train_regret_stp_df1_7[slice51],\n",
    "       train_regret_stp_df1_8[slice51],\n",
    "       train_regret_stp_df1_9[slice51],\n",
    "       train_regret_stp_df1_10[slice51],\n",
    "       train_regret_stp_df1_11[slice51],\n",
    "       train_regret_stp_df1_12[slice51],\n",
    "       train_regret_stp_df1_13[slice51],\n",
    "       train_regret_stp_df1_14[slice51],\n",
    "       train_regret_stp_df1_15[slice51],\n",
    "       train_regret_stp_df1_16[slice51],\n",
    "       train_regret_stp_df1_17[slice51],\n",
    "       train_regret_stp_df1_18[slice51],\n",
    "       train_regret_stp_df1_19[slice51],\n",
    "       train_regret_stp_df1_20[slice51]]\n",
    "\n",
    "gp52_results = pd.DataFrame(gp52).sort_values(by=[0], ascending=False)\n",
    "stp52_results = pd.DataFrame(stp52).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp52 = np.asarray(gp52_results[4:5][0])[0]\n",
    "median_gp52 = np.asarray(gp52_results[9:10][0])[0]\n",
    "upper_gp52 = np.asarray(gp52_results[14:15][0])[0]\n",
    "\n",
    "lower_stp52 = np.asarray(stp52_results[4:5][0])[0]\n",
    "median_stp52 = np.asarray(stp52_results[9:10][0])[0]\n",
    "upper_stp52 = np.asarray(stp52_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration62 :\n",
    "\n",
    "slice61 = 61\n",
    "\n",
    "gp62 = [train_regret_gp_1[slice61],\n",
    "       train_regret_gp_2[slice61],\n",
    "       train_regret_gp_3[slice61],\n",
    "       train_regret_gp_4[slice61],\n",
    "       train_regret_gp_5[slice61],\n",
    "       train_regret_gp_6[slice61],\n",
    "       train_regret_gp_7[slice61],\n",
    "       train_regret_gp_8[slice61],\n",
    "       train_regret_gp_9[slice61],\n",
    "       train_regret_gp_10[slice61],\n",
    "       train_regret_gp_11[slice61],\n",
    "       train_regret_gp_12[slice61],\n",
    "       train_regret_gp_13[slice61],\n",
    "       train_regret_gp_14[slice61],\n",
    "       train_regret_gp_15[slice61],\n",
    "       train_regret_gp_16[slice61],\n",
    "       train_regret_gp_17[slice61],\n",
    "       train_regret_gp_18[slice61],\n",
    "       train_regret_gp_19[slice61],\n",
    "       train_regret_gp_20[slice61]]\n",
    "\n",
    "stp62 = [train_regret_stp_df1_1[slice61],\n",
    "       train_regret_stp_df1_2[slice61],\n",
    "       train_regret_stp_df1_3[slice61],\n",
    "       train_regret_stp_df1_4[slice61],\n",
    "       train_regret_stp_df1_5[slice61],\n",
    "       train_regret_stp_df1_6[slice61],\n",
    "       train_regret_stp_df1_7[slice61],\n",
    "       train_regret_stp_df1_8[slice61],\n",
    "       train_regret_stp_df1_9[slice61],\n",
    "       train_regret_stp_df1_10[slice61],\n",
    "       train_regret_stp_df1_11[slice61],\n",
    "       train_regret_stp_df1_12[slice61],\n",
    "       train_regret_stp_df1_13[slice61],\n",
    "       train_regret_stp_df1_14[slice61],\n",
    "       train_regret_stp_df1_15[slice61],\n",
    "       train_regret_stp_df1_16[slice61],\n",
    "       train_regret_stp_df1_17[slice61],\n",
    "       train_regret_stp_df1_18[slice61],\n",
    "       train_regret_stp_df1_19[slice61],\n",
    "       train_regret_stp_df1_20[slice61]]\n",
    "\n",
    "gp62_results = pd.DataFrame(gp62).sort_values(by=[0], ascending=False)\n",
    "stp62_results = pd.DataFrame(stp62).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp62 = np.asarray(gp62_results[4:5][0])[0]\n",
    "median_gp62 = np.asarray(gp62_results[9:10][0])[0]\n",
    "upper_gp62 = np.asarray(gp62_results[14:15][0])[0]\n",
    "\n",
    "lower_stp62 = np.asarray(stp62_results[4:5][0])[0]\n",
    "median_stp62 = np.asarray(stp62_results[9:10][0])[0]\n",
    "upper_stp62 = np.asarray(stp62_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration72 :\n",
    "\n",
    "slice71 = 71\n",
    "\n",
    "gp72 = [train_regret_gp_1[slice71],\n",
    "       train_regret_gp_2[slice71],\n",
    "       train_regret_gp_3[slice71],\n",
    "       train_regret_gp_4[slice71],\n",
    "       train_regret_gp_5[slice71],\n",
    "       train_regret_gp_6[slice71],\n",
    "       train_regret_gp_7[slice71],\n",
    "       train_regret_gp_8[slice71],\n",
    "       train_regret_gp_9[slice71],\n",
    "       train_regret_gp_10[slice71],\n",
    "       train_regret_gp_11[slice71],\n",
    "       train_regret_gp_12[slice71],\n",
    "       train_regret_gp_13[slice71],\n",
    "       train_regret_gp_14[slice71],\n",
    "       train_regret_gp_15[slice71],\n",
    "       train_regret_gp_16[slice71],\n",
    "       train_regret_gp_17[slice71],\n",
    "       train_regret_gp_18[slice71],\n",
    "       train_regret_gp_19[slice71],\n",
    "       train_regret_gp_20[slice71]]\n",
    "\n",
    "stp72 = [train_regret_stp_df1_1[slice71],\n",
    "       train_regret_stp_df1_2[slice71],\n",
    "       train_regret_stp_df1_3[slice71],\n",
    "       train_regret_stp_df1_4[slice71],\n",
    "       train_regret_stp_df1_5[slice71],\n",
    "       train_regret_stp_df1_6[slice71],\n",
    "       train_regret_stp_df1_7[slice71],\n",
    "       train_regret_stp_df1_8[slice71],\n",
    "       train_regret_stp_df1_9[slice71],\n",
    "       train_regret_stp_df1_10[slice71],\n",
    "       train_regret_stp_df1_11[slice71],\n",
    "       train_regret_stp_df1_12[slice71],\n",
    "       train_regret_stp_df1_13[slice71],\n",
    "       train_regret_stp_df1_14[slice71],\n",
    "       train_regret_stp_df1_15[slice71],\n",
    "       train_regret_stp_df1_16[slice71],\n",
    "       train_regret_stp_df1_17[slice71],\n",
    "       train_regret_stp_df1_18[slice71],\n",
    "       train_regret_stp_df1_19[slice71],\n",
    "       train_regret_stp_df1_20[slice71]]\n",
    "\n",
    "gp72_results = pd.DataFrame(gp72).sort_values(by=[0], ascending=False)\n",
    "stp72_results = pd.DataFrame(stp72).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp72 = np.asarray(gp72_results[4:5][0])[0]\n",
    "median_gp72 = np.asarray(gp72_results[9:10][0])[0]\n",
    "upper_gp72 = np.asarray(gp72_results[14:15][0])[0]\n",
    "\n",
    "lower_stp72 = np.asarray(stp72_results[4:5][0])[0]\n",
    "median_stp72 = np.asarray(stp72_results[9:10][0])[0]\n",
    "upper_stp72 = np.asarray(stp72_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration82 :\n",
    "\n",
    "slice81 = 81\n",
    "\n",
    "gp82 = [train_regret_gp_1[slice81],\n",
    "       train_regret_gp_2[slice81],\n",
    "       train_regret_gp_3[slice81],\n",
    "       train_regret_gp_4[slice81],\n",
    "       train_regret_gp_5[slice81],\n",
    "       train_regret_gp_6[slice81],\n",
    "       train_regret_gp_7[slice81],\n",
    "       train_regret_gp_8[slice81],\n",
    "       train_regret_gp_9[slice81],\n",
    "       train_regret_gp_10[slice81],\n",
    "       train_regret_gp_11[slice81],\n",
    "       train_regret_gp_12[slice81],\n",
    "       train_regret_gp_13[slice81],\n",
    "       train_regret_gp_14[slice81],\n",
    "       train_regret_gp_15[slice81],\n",
    "       train_regret_gp_16[slice81],\n",
    "       train_regret_gp_17[slice81],\n",
    "       train_regret_gp_18[slice81],\n",
    "       train_regret_gp_19[slice81],\n",
    "       train_regret_gp_20[slice81]]\n",
    "\n",
    "stp82 = [train_regret_stp_df1_1[slice81],\n",
    "       train_regret_stp_df1_2[slice81],\n",
    "       train_regret_stp_df1_3[slice81],\n",
    "       train_regret_stp_df1_4[slice81],\n",
    "       train_regret_stp_df1_5[slice81],\n",
    "       train_regret_stp_df1_6[slice81],\n",
    "       train_regret_stp_df1_7[slice81],\n",
    "       train_regret_stp_df1_8[slice81],\n",
    "       train_regret_stp_df1_9[slice81],\n",
    "       train_regret_stp_df1_10[slice81],\n",
    "       train_regret_stp_df1_11[slice81],\n",
    "       train_regret_stp_df1_12[slice81],\n",
    "       train_regret_stp_df1_13[slice81],\n",
    "       train_regret_stp_df1_14[slice81],\n",
    "       train_regret_stp_df1_15[slice81],\n",
    "       train_regret_stp_df1_16[slice81],\n",
    "       train_regret_stp_df1_17[slice81],\n",
    "       train_regret_stp_df1_18[slice81],\n",
    "       train_regret_stp_df1_19[slice81],\n",
    "       train_regret_stp_df1_20[slice81]]\n",
    "\n",
    "gp82_results = pd.DataFrame(gp82).sort_values(by=[0], ascending=False)\n",
    "stp82_results = pd.DataFrame(stp82).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp82 = np.asarray(gp82_results[4:5][0])[0]\n",
    "median_gp82 = np.asarray(gp82_results[9:10][0])[0]\n",
    "upper_gp82 = np.asarray(gp82_results[14:15][0])[0]\n",
    "\n",
    "lower_stp82 = np.asarray(stp82_results[4:5][0])[0]\n",
    "median_stp82 = np.asarray(stp82_results[9:10][0])[0]\n",
    "upper_stp82 = np.asarray(stp82_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration92 :\n",
    "\n",
    "slice1 = 91\n",
    "\n",
    "gp92 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp92 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp92_results = pd.DataFrame(gp92).sort_values(by=[0], ascending=False)\n",
    "stp92_results = pd.DataFrame(stp92).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp92 = np.asarray(gp92_results[4:5][0])[0]\n",
    "median_gp92 = np.asarray(gp92_results[9:10][0])[0]\n",
    "upper_gp92 = np.asarray(gp92_results[14:15][0])[0]\n",
    "\n",
    "lower_stp92 = np.asarray(stp92_results[4:5][0])[0]\n",
    "median_stp92 = np.asarray(stp92_results[9:10][0])[0]\n",
    "upper_stp92 = np.asarray(stp92_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration3 :\n",
    "\n",
    "slice1 = 2\n",
    "\n",
    "gp3 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp3 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp3_results = pd.DataFrame(gp3).sort_values(by=[0], ascending=False)\n",
    "stp3_results = pd.DataFrame(stp3).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp3 = np.asarray(gp3_results[4:5][0])[0]\n",
    "median_gp3 = np.asarray(gp3_results[9:10][0])[0]\n",
    "upper_gp3 = np.asarray(gp3_results[14:15][0])[0]\n",
    "\n",
    "lower_stp3 = np.asarray(stp3_results[4:5][0])[0]\n",
    "median_stp3 = np.asarray(stp3_results[9:10][0])[0]\n",
    "upper_stp3 = np.asarray(stp3_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration13 :\n",
    "\n",
    "slice11 = 12\n",
    "\n",
    "gp13 = [train_regret_gp_1[slice11],\n",
    "       train_regret_gp_2[slice11],\n",
    "       train_regret_gp_3[slice11],\n",
    "       train_regret_gp_4[slice11],\n",
    "       train_regret_gp_5[slice11],\n",
    "       train_regret_gp_6[slice11],\n",
    "       train_regret_gp_7[slice11],\n",
    "       train_regret_gp_8[slice11],\n",
    "       train_regret_gp_9[slice11],\n",
    "       train_regret_gp_10[slice11],\n",
    "       train_regret_gp_11[slice11],\n",
    "       train_regret_gp_12[slice11],\n",
    "       train_regret_gp_13[slice11],\n",
    "       train_regret_gp_14[slice11],\n",
    "       train_regret_gp_15[slice11],\n",
    "       train_regret_gp_16[slice11],\n",
    "       train_regret_gp_17[slice11],\n",
    "       train_regret_gp_18[slice11],\n",
    "       train_regret_gp_19[slice11],\n",
    "       train_regret_gp_20[slice11]]\n",
    "\n",
    "stp13 = [train_regret_stp_df1_1[slice11],\n",
    "       train_regret_stp_df1_2[slice11],\n",
    "       train_regret_stp_df1_3[slice11],\n",
    "       train_regret_stp_df1_4[slice11],\n",
    "       train_regret_stp_df1_5[slice11],\n",
    "       train_regret_stp_df1_6[slice11],\n",
    "       train_regret_stp_df1_7[slice11],\n",
    "       train_regret_stp_df1_8[slice11],\n",
    "       train_regret_stp_df1_9[slice11],\n",
    "       train_regret_stp_df1_10[slice11],\n",
    "       train_regret_stp_df1_11[slice11],\n",
    "       train_regret_stp_df1_12[slice11],\n",
    "       train_regret_stp_df1_13[slice11],\n",
    "       train_regret_stp_df1_14[slice11],\n",
    "       train_regret_stp_df1_15[slice11],\n",
    "       train_regret_stp_df1_16[slice11],\n",
    "       train_regret_stp_df1_17[slice11],\n",
    "       train_regret_stp_df1_18[slice11],\n",
    "       train_regret_stp_df1_19[slice11],\n",
    "       train_regret_stp_df1_20[slice11]]\n",
    "\n",
    "gp13_results = pd.DataFrame(gp12).sort_values(by=[0], ascending=False)\n",
    "stp13_results = pd.DataFrame(stp12).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp13 = np.asarray(gp13_results[4:5][0])[0]\n",
    "median_gp13 = np.asarray(gp13_results[9:10][0])[0]\n",
    "upper_gp13 = np.asarray(gp13_results[14:15][0])[0]\n",
    "\n",
    "lower_stp13 = np.asarray(stp13_results[4:5][0])[0]\n",
    "median_stp13 = np.asarray(stp13_results[9:10][0])[0]\n",
    "upper_stp13 = np.asarray(stp13_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration23 :\n",
    "\n",
    "slice21 = 22\n",
    "\n",
    "gp23 = [train_regret_gp_1[slice21],\n",
    "       train_regret_gp_2[slice21],\n",
    "       train_regret_gp_3[slice21],\n",
    "       train_regret_gp_4[slice21],\n",
    "       train_regret_gp_5[slice21],\n",
    "       train_regret_gp_6[slice21],\n",
    "       train_regret_gp_7[slice21],\n",
    "       train_regret_gp_8[slice21],\n",
    "       train_regret_gp_9[slice21],\n",
    "       train_regret_gp_10[slice21],\n",
    "       train_regret_gp_11[slice21],\n",
    "       train_regret_gp_12[slice21],\n",
    "       train_regret_gp_13[slice21],\n",
    "       train_regret_gp_14[slice21],\n",
    "       train_regret_gp_15[slice21],\n",
    "       train_regret_gp_16[slice21],\n",
    "       train_regret_gp_17[slice21],\n",
    "       train_regret_gp_18[slice21],\n",
    "       train_regret_gp_19[slice21],\n",
    "       train_regret_gp_20[slice21]]\n",
    "\n",
    "stp23 = [train_regret_stp_df1_1[slice21],\n",
    "       train_regret_stp_df1_2[slice21],\n",
    "       train_regret_stp_df1_3[slice21],\n",
    "       train_regret_stp_df1_4[slice21],\n",
    "       train_regret_stp_df1_5[slice21],\n",
    "       train_regret_stp_df1_6[slice21],\n",
    "       train_regret_stp_df1_7[slice21],\n",
    "       train_regret_stp_df1_8[slice21],\n",
    "       train_regret_stp_df1_9[slice21],\n",
    "       train_regret_stp_df1_10[slice21],\n",
    "       train_regret_stp_df1_11[slice21],\n",
    "       train_regret_stp_df1_12[slice21],\n",
    "       train_regret_stp_df1_13[slice21],\n",
    "       train_regret_stp_df1_14[slice21],\n",
    "       train_regret_stp_df1_15[slice21],\n",
    "       train_regret_stp_df1_16[slice21],\n",
    "       train_regret_stp_df1_17[slice21],\n",
    "       train_regret_stp_df1_18[slice21],\n",
    "       train_regret_stp_df1_19[slice21],\n",
    "       train_regret_stp_df1_20[slice21]]\n",
    "\n",
    "gp23_results = pd.DataFrame(gp23).sort_values(by=[0], ascending=False)\n",
    "stp23_results = pd.DataFrame(stp23).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp23 = np.asarray(gp23_results[4:5][0])[0]\n",
    "median_gp23 = np.asarray(gp23_results[9:10][0])[0]\n",
    "upper_gp23 = np.asarray(gp23_results[14:15][0])[0]\n",
    "\n",
    "lower_stp23 = np.asarray(stp23_results[4:5][0])[0]\n",
    "median_stp23 = np.asarray(stp23_results[9:10][0])[0]\n",
    "upper_stp23 = np.asarray(stp23_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration33 :\n",
    "\n",
    "slice31 = 32\n",
    "\n",
    "gp33 = [train_regret_gp_1[slice31],\n",
    "       train_regret_gp_2[slice31],\n",
    "       train_regret_gp_3[slice31],\n",
    "       train_regret_gp_4[slice31],\n",
    "       train_regret_gp_5[slice31],\n",
    "       train_regret_gp_6[slice31],\n",
    "       train_regret_gp_7[slice31],\n",
    "       train_regret_gp_8[slice31],\n",
    "       train_regret_gp_9[slice31],\n",
    "       train_regret_gp_10[slice31],\n",
    "       train_regret_gp_11[slice31],\n",
    "       train_regret_gp_12[slice31],\n",
    "       train_regret_gp_13[slice31],\n",
    "       train_regret_gp_14[slice31],\n",
    "       train_regret_gp_15[slice31],\n",
    "       train_regret_gp_16[slice31],\n",
    "       train_regret_gp_17[slice31],\n",
    "       train_regret_gp_18[slice31],\n",
    "       train_regret_gp_19[slice31],\n",
    "       train_regret_gp_20[slice31]]\n",
    "\n",
    "stp33 = [train_regret_stp_df1_1[slice31],\n",
    "       train_regret_stp_df1_2[slice31],\n",
    "       train_regret_stp_df1_3[slice31],\n",
    "       train_regret_stp_df1_4[slice31],\n",
    "       train_regret_stp_df1_5[slice31],\n",
    "       train_regret_stp_df1_6[slice31],\n",
    "       train_regret_stp_df1_7[slice31],\n",
    "       train_regret_stp_df1_8[slice31],\n",
    "       train_regret_stp_df1_9[slice31],\n",
    "       train_regret_stp_df1_10[slice31],\n",
    "       train_regret_stp_df1_11[slice31],\n",
    "       train_regret_stp_df1_12[slice31],\n",
    "       train_regret_stp_df1_13[slice31],\n",
    "       train_regret_stp_df1_14[slice31],\n",
    "       train_regret_stp_df1_15[slice31],\n",
    "       train_regret_stp_df1_16[slice31],\n",
    "       train_regret_stp_df1_17[slice31],\n",
    "       train_regret_stp_df1_18[slice31],\n",
    "       train_regret_stp_df1_19[slice31],\n",
    "       train_regret_stp_df1_20[slice31]]\n",
    "\n",
    "gp33_results = pd.DataFrame(gp33).sort_values(by=[0], ascending=False)\n",
    "stp33_results = pd.DataFrame(stp33).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp33 = np.asarray(gp33_results[4:5][0])[0]\n",
    "median_gp33 = np.asarray(gp33_results[9:10][0])[0]\n",
    "upper_gp33 = np.asarray(gp33_results[14:15][0])[0]\n",
    "\n",
    "lower_stp33 = np.asarray(stp33_results[4:5][0])[0]\n",
    "median_stp33 = np.asarray(stp33_results[9:10][0])[0]\n",
    "upper_stp33 = np.asarray(stp33_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration43 :\n",
    "\n",
    "slice41 = 42\n",
    "\n",
    "gp43 = [train_regret_gp_1[slice41],\n",
    "       train_regret_gp_2[slice41],\n",
    "       train_regret_gp_3[slice41],\n",
    "       train_regret_gp_4[slice41],\n",
    "       train_regret_gp_5[slice41],\n",
    "       train_regret_gp_6[slice41],\n",
    "       train_regret_gp_7[slice41],\n",
    "       train_regret_gp_8[slice41],\n",
    "       train_regret_gp_9[slice41],\n",
    "       train_regret_gp_10[slice41],\n",
    "       train_regret_gp_11[slice41],\n",
    "       train_regret_gp_12[slice41],\n",
    "       train_regret_gp_13[slice41],\n",
    "       train_regret_gp_14[slice41],\n",
    "       train_regret_gp_15[slice41],\n",
    "       train_regret_gp_16[slice41],\n",
    "       train_regret_gp_17[slice41],\n",
    "       train_regret_gp_18[slice41],\n",
    "       train_regret_gp_19[slice41],\n",
    "       train_regret_gp_20[slice41]]\n",
    "\n",
    "stp43 = [train_regret_stp_df1_1[slice41],\n",
    "       train_regret_stp_df1_2[slice41],\n",
    "       train_regret_stp_df1_3[slice41],\n",
    "       train_regret_stp_df1_4[slice41],\n",
    "       train_regret_stp_df1_5[slice41],\n",
    "       train_regret_stp_df1_6[slice41],\n",
    "       train_regret_stp_df1_7[slice41],\n",
    "       train_regret_stp_df1_8[slice41],\n",
    "       train_regret_stp_df1_9[slice41],\n",
    "       train_regret_stp_df1_10[slice41],\n",
    "       train_regret_stp_df1_11[slice41],\n",
    "       train_regret_stp_df1_12[slice41],\n",
    "       train_regret_stp_df1_13[slice41],\n",
    "       train_regret_stp_df1_14[slice41],\n",
    "       train_regret_stp_df1_15[slice41],\n",
    "       train_regret_stp_df1_16[slice41],\n",
    "       train_regret_stp_df1_17[slice41],\n",
    "       train_regret_stp_df1_18[slice41],\n",
    "       train_regret_stp_df1_19[slice41],\n",
    "       train_regret_stp_df1_20[slice41]]\n",
    "\n",
    "gp43_results = pd.DataFrame(gp43).sort_values(by=[0], ascending=False)\n",
    "stp43_results = pd.DataFrame(stp43).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp43 = np.asarray(gp43_results[4:5][0])[0]\n",
    "median_gp43 = np.asarray(gp43_results[9:10][0])[0]\n",
    "upper_gp43 = np.asarray(gp43_results[14:15][0])[0]\n",
    "\n",
    "lower_stp43 = np.asarray(stp43_results[4:5][0])[0]\n",
    "median_stp43 = np.asarray(stp43_results[9:10][0])[0]\n",
    "upper_stp43 = np.asarray(stp43_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration53 :\n",
    "\n",
    "slice51 = 52\n",
    "\n",
    "gp53 = [train_regret_gp_1[slice51],\n",
    "       train_regret_gp_2[slice51],\n",
    "       train_regret_gp_3[slice51],\n",
    "       train_regret_gp_4[slice51],\n",
    "       train_regret_gp_5[slice51],\n",
    "       train_regret_gp_6[slice51],\n",
    "       train_regret_gp_7[slice51],\n",
    "       train_regret_gp_8[slice51],\n",
    "       train_regret_gp_9[slice51],\n",
    "       train_regret_gp_10[slice51],\n",
    "       train_regret_gp_11[slice51],\n",
    "       train_regret_gp_12[slice51],\n",
    "       train_regret_gp_13[slice51],\n",
    "       train_regret_gp_14[slice51],\n",
    "       train_regret_gp_15[slice51],\n",
    "       train_regret_gp_16[slice51],\n",
    "       train_regret_gp_17[slice51],\n",
    "       train_regret_gp_18[slice51],\n",
    "       train_regret_gp_19[slice51],\n",
    "       train_regret_gp_20[slice51]]\n",
    "\n",
    "stp53 = [train_regret_stp_df1_1[slice51],\n",
    "       train_regret_stp_df1_2[slice51],\n",
    "       train_regret_stp_df1_3[slice51],\n",
    "       train_regret_stp_df1_4[slice51],\n",
    "       train_regret_stp_df1_5[slice51],\n",
    "       train_regret_stp_df1_6[slice51],\n",
    "       train_regret_stp_df1_7[slice51],\n",
    "       train_regret_stp_df1_8[slice51],\n",
    "       train_regret_stp_df1_9[slice51],\n",
    "       train_regret_stp_df1_10[slice51],\n",
    "       train_regret_stp_df1_11[slice51],\n",
    "       train_regret_stp_df1_12[slice51],\n",
    "       train_regret_stp_df1_13[slice51],\n",
    "       train_regret_stp_df1_14[slice51],\n",
    "       train_regret_stp_df1_15[slice51],\n",
    "       train_regret_stp_df1_16[slice51],\n",
    "       train_regret_stp_df1_17[slice51],\n",
    "       train_regret_stp_df1_18[slice51],\n",
    "       train_regret_stp_df1_19[slice51],\n",
    "       train_regret_stp_df1_20[slice51]]\n",
    "\n",
    "gp53_results = pd.DataFrame(gp53).sort_values(by=[0], ascending=False)\n",
    "stp53_results = pd.DataFrame(stp53).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp53 = np.asarray(gp53_results[4:5][0])[0]\n",
    "median_gp53 = np.asarray(gp53_results[9:10][0])[0]\n",
    "upper_gp53 = np.asarray(gp53_results[14:15][0])[0]\n",
    "\n",
    "lower_stp53 = np.asarray(stp53_results[4:5][0])[0]\n",
    "median_stp53 = np.asarray(stp53_results[9:10][0])[0]\n",
    "upper_stp53 = np.asarray(stp53_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration63 :\n",
    "\n",
    "slice61 = 62\n",
    "\n",
    "gp63 = [train_regret_gp_1[slice61],\n",
    "       train_regret_gp_2[slice61],\n",
    "       train_regret_gp_3[slice61],\n",
    "       train_regret_gp_4[slice61],\n",
    "       train_regret_gp_5[slice61],\n",
    "       train_regret_gp_6[slice61],\n",
    "       train_regret_gp_7[slice61],\n",
    "       train_regret_gp_8[slice61],\n",
    "       train_regret_gp_9[slice61],\n",
    "       train_regret_gp_10[slice61],\n",
    "       train_regret_gp_11[slice61],\n",
    "       train_regret_gp_12[slice61],\n",
    "       train_regret_gp_13[slice61],\n",
    "       train_regret_gp_14[slice61],\n",
    "       train_regret_gp_15[slice61],\n",
    "       train_regret_gp_16[slice61],\n",
    "       train_regret_gp_17[slice61],\n",
    "       train_regret_gp_18[slice61],\n",
    "       train_regret_gp_19[slice61],\n",
    "       train_regret_gp_20[slice61]]\n",
    "\n",
    "stp63 = [train_regret_stp_df1_1[slice61],\n",
    "       train_regret_stp_df1_2[slice61],\n",
    "       train_regret_stp_df1_3[slice61],\n",
    "       train_regret_stp_df1_4[slice61],\n",
    "       train_regret_stp_df1_5[slice61],\n",
    "       train_regret_stp_df1_6[slice61],\n",
    "       train_regret_stp_df1_7[slice61],\n",
    "       train_regret_stp_df1_8[slice61],\n",
    "       train_regret_stp_df1_9[slice61],\n",
    "       train_regret_stp_df1_10[slice61],\n",
    "       train_regret_stp_df1_11[slice61],\n",
    "       train_regret_stp_df1_12[slice61],\n",
    "       train_regret_stp_df1_13[slice61],\n",
    "       train_regret_stp_df1_14[slice61],\n",
    "       train_regret_stp_df1_15[slice61],\n",
    "       train_regret_stp_df1_16[slice61],\n",
    "       train_regret_stp_df1_17[slice61],\n",
    "       train_regret_stp_df1_18[slice61],\n",
    "       train_regret_stp_df1_19[slice61],\n",
    "       train_regret_stp_df1_20[slice61]]\n",
    "\n",
    "gp63_results = pd.DataFrame(gp63).sort_values(by=[0], ascending=False)\n",
    "stp63_results = pd.DataFrame(stp63).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp63 = np.asarray(gp63_results[4:5][0])[0]\n",
    "median_gp63 = np.asarray(gp63_results[9:10][0])[0]\n",
    "upper_gp63 = np.asarray(gp63_results[14:15][0])[0]\n",
    "\n",
    "lower_stp63 = np.asarray(stp63_results[4:5][0])[0]\n",
    "median_stp63 = np.asarray(stp63_results[9:10][0])[0]\n",
    "upper_stp63 = np.asarray(stp63_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration73 :\n",
    "\n",
    "slice71 = 72\n",
    "\n",
    "gp73 = [train_regret_gp_1[slice71],\n",
    "       train_regret_gp_2[slice71],\n",
    "       train_regret_gp_3[slice71],\n",
    "       train_regret_gp_4[slice71],\n",
    "       train_regret_gp_5[slice71],\n",
    "       train_regret_gp_6[slice71],\n",
    "       train_regret_gp_7[slice71],\n",
    "       train_regret_gp_8[slice71],\n",
    "       train_regret_gp_9[slice71],\n",
    "       train_regret_gp_10[slice71],\n",
    "       train_regret_gp_11[slice71],\n",
    "       train_regret_gp_12[slice71],\n",
    "       train_regret_gp_13[slice71],\n",
    "       train_regret_gp_14[slice71],\n",
    "       train_regret_gp_15[slice71],\n",
    "       train_regret_gp_16[slice71],\n",
    "       train_regret_gp_17[slice71],\n",
    "       train_regret_gp_18[slice71],\n",
    "       train_regret_gp_19[slice71],\n",
    "       train_regret_gp_20[slice71]]\n",
    "\n",
    "stp73 = [train_regret_stp_df1_1[slice71],\n",
    "       train_regret_stp_df1_2[slice71],\n",
    "       train_regret_stp_df1_3[slice71],\n",
    "       train_regret_stp_df1_4[slice71],\n",
    "       train_regret_stp_df1_5[slice71],\n",
    "       train_regret_stp_df1_6[slice71],\n",
    "       train_regret_stp_df1_7[slice71],\n",
    "       train_regret_stp_df1_8[slice71],\n",
    "       train_regret_stp_df1_9[slice71],\n",
    "       train_regret_stp_df1_10[slice71],\n",
    "       train_regret_stp_df1_11[slice71],\n",
    "       train_regret_stp_df1_12[slice71],\n",
    "       train_regret_stp_df1_13[slice71],\n",
    "       train_regret_stp_df1_14[slice71],\n",
    "       train_regret_stp_df1_15[slice71],\n",
    "       train_regret_stp_df1_16[slice71],\n",
    "       train_regret_stp_df1_17[slice71],\n",
    "       train_regret_stp_df1_18[slice71],\n",
    "       train_regret_stp_df1_19[slice71],\n",
    "       train_regret_stp_df1_20[slice71]]\n",
    "\n",
    "gp73_results = pd.DataFrame(gp73).sort_values(by=[0], ascending=False)\n",
    "stp73_results = pd.DataFrame(stp73).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp73 = np.asarray(gp73_results[4:5][0])[0]\n",
    "median_gp73 = np.asarray(gp73_results[9:10][0])[0]\n",
    "upper_gp73 = np.asarray(gp73_results[14:15][0])[0]\n",
    "\n",
    "lower_stp73 = np.asarray(stp73_results[4:5][0])[0]\n",
    "median_stp73 = np.asarray(stp73_results[9:10][0])[0]\n",
    "upper_stp73 = np.asarray(stp73_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration83 :\n",
    "\n",
    "slice81 = 82\n",
    "\n",
    "gp83 = [train_regret_gp_1[slice81],\n",
    "       train_regret_gp_2[slice81],\n",
    "       train_regret_gp_3[slice81],\n",
    "       train_regret_gp_4[slice81],\n",
    "       train_regret_gp_5[slice81],\n",
    "       train_regret_gp_6[slice81],\n",
    "       train_regret_gp_7[slice81],\n",
    "       train_regret_gp_8[slice81],\n",
    "       train_regret_gp_9[slice81],\n",
    "       train_regret_gp_10[slice81],\n",
    "       train_regret_gp_11[slice81],\n",
    "       train_regret_gp_12[slice81],\n",
    "       train_regret_gp_13[slice81],\n",
    "       train_regret_gp_14[slice81],\n",
    "       train_regret_gp_15[slice81],\n",
    "       train_regret_gp_16[slice81],\n",
    "       train_regret_gp_17[slice81],\n",
    "       train_regret_gp_18[slice81],\n",
    "       train_regret_gp_19[slice81],\n",
    "       train_regret_gp_20[slice81]]\n",
    "\n",
    "stp83 = [train_regret_stp_df1_1[slice81],\n",
    "       train_regret_stp_df1_2[slice81],\n",
    "       train_regret_stp_df1_3[slice81],\n",
    "       train_regret_stp_df1_4[slice81],\n",
    "       train_regret_stp_df1_5[slice81],\n",
    "       train_regret_stp_df1_6[slice81],\n",
    "       train_regret_stp_df1_7[slice81],\n",
    "       train_regret_stp_df1_8[slice81],\n",
    "       train_regret_stp_df1_9[slice81],\n",
    "       train_regret_stp_df1_10[slice81],\n",
    "       train_regret_stp_df1_11[slice81],\n",
    "       train_regret_stp_df1_12[slice81],\n",
    "       train_regret_stp_df1_13[slice81],\n",
    "       train_regret_stp_df1_14[slice81],\n",
    "       train_regret_stp_df1_15[slice81],\n",
    "       train_regret_stp_df1_16[slice81],\n",
    "       train_regret_stp_df1_17[slice81],\n",
    "       train_regret_stp_df1_18[slice81],\n",
    "       train_regret_stp_df1_19[slice81],\n",
    "       train_regret_stp_df1_20[slice81]]\n",
    "\n",
    "gp83_results = pd.DataFrame(gp83).sort_values(by=[0], ascending=False)\n",
    "stp83_results = pd.DataFrame(stp83).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp83 = np.asarray(gp83_results[4:5][0])[0]\n",
    "median_gp83 = np.asarray(gp83_results[9:10][0])[0]\n",
    "upper_gp83 = np.asarray(gp83_results[14:15][0])[0]\n",
    "\n",
    "lower_stp83 = np.asarray(stp83_results[4:5][0])[0]\n",
    "median_stp83 = np.asarray(stp83_results[9:10][0])[0]\n",
    "upper_stp83 = np.asarray(stp83_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration93 :\n",
    "\n",
    "slice1 = 92\n",
    "\n",
    "gp93 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp93 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp93_results = pd.DataFrame(gp93).sort_values(by=[0], ascending=False)\n",
    "stp93_results = pd.DataFrame(stp93).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp93 = np.asarray(gp93_results[4:5][0])[0]\n",
    "median_gp93 = np.asarray(gp93_results[9:10][0])[0]\n",
    "upper_gp93 = np.asarray(gp93_results[14:15][0])[0]\n",
    "\n",
    "lower_stp93 = np.asarray(stp93_results[4:5][0])[0]\n",
    "median_stp93 = np.asarray(stp93_results[9:10][0])[0]\n",
    "upper_stp93 = np.asarray(stp93_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration4 :\n",
    "\n",
    "slice1 = 3\n",
    "\n",
    "gp4 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp4 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp4_results = pd.DataFrame(gp4).sort_values(by=[0], ascending=False)\n",
    "stp4_results = pd.DataFrame(stp4).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp4 = np.asarray(gp4_results[4:5][0])[0]\n",
    "median_gp4 = np.asarray(gp4_results[9:10][0])[0]\n",
    "upper_gp4 = np.asarray(gp4_results[14:15][0])[0]\n",
    "\n",
    "lower_stp4 = np.asarray(stp4_results[4:5][0])[0]\n",
    "median_stp4 = np.asarray(stp4_results[9:10][0])[0]\n",
    "upper_stp4 = np.asarray(stp4_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration14 :\n",
    "\n",
    "slice11 = 13\n",
    "\n",
    "gp14 = [train_regret_gp_1[slice11],\n",
    "       train_regret_gp_2[slice11],\n",
    "       train_regret_gp_3[slice11],\n",
    "       train_regret_gp_4[slice11],\n",
    "       train_regret_gp_5[slice11],\n",
    "       train_regret_gp_6[slice11],\n",
    "       train_regret_gp_7[slice11],\n",
    "       train_regret_gp_8[slice11],\n",
    "       train_regret_gp_9[slice11],\n",
    "       train_regret_gp_10[slice11],\n",
    "       train_regret_gp_11[slice11],\n",
    "       train_regret_gp_12[slice11],\n",
    "       train_regret_gp_13[slice11],\n",
    "       train_regret_gp_14[slice11],\n",
    "       train_regret_gp_15[slice11],\n",
    "       train_regret_gp_16[slice11],\n",
    "       train_regret_gp_17[slice11],\n",
    "       train_regret_gp_18[slice11],\n",
    "       train_regret_gp_19[slice11],\n",
    "       train_regret_gp_20[slice11]]\n",
    "\n",
    "stp14 = [train_regret_stp_df1_1[slice11],\n",
    "       train_regret_stp_df1_2[slice11],\n",
    "       train_regret_stp_df1_3[slice11],\n",
    "       train_regret_stp_df1_4[slice11],\n",
    "       train_regret_stp_df1_5[slice11],\n",
    "       train_regret_stp_df1_6[slice11],\n",
    "       train_regret_stp_df1_7[slice11],\n",
    "       train_regret_stp_df1_8[slice11],\n",
    "       train_regret_stp_df1_9[slice11],\n",
    "       train_regret_stp_df1_10[slice11],\n",
    "       train_regret_stp_df1_11[slice11],\n",
    "       train_regret_stp_df1_12[slice11],\n",
    "       train_regret_stp_df1_13[slice11],\n",
    "       train_regret_stp_df1_14[slice11],\n",
    "       train_regret_stp_df1_15[slice11],\n",
    "       train_regret_stp_df1_16[slice11],\n",
    "       train_regret_stp_df1_17[slice11],\n",
    "       train_regret_stp_df1_18[slice11],\n",
    "       train_regret_stp_df1_19[slice11],\n",
    "       train_regret_stp_df1_20[slice11]]\n",
    "\n",
    "gp14_results = pd.DataFrame(gp14).sort_values(by=[0], ascending=False)\n",
    "stp14_results = pd.DataFrame(stp14).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp14 = np.asarray(gp14_results[4:5][0])[0]\n",
    "median_gp14 = np.asarray(gp14_results[9:10][0])[0]\n",
    "upper_gp14 = np.asarray(gp14_results[14:15][0])[0]\n",
    "\n",
    "lower_stp14 = np.asarray(stp14_results[4:5][0])[0]\n",
    "median_stp14 = np.asarray(stp14_results[9:10][0])[0]\n",
    "upper_stp14 = np.asarray(stp14_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration24 :\n",
    "\n",
    "slice21 = 23\n",
    "\n",
    "gp24 = [train_regret_gp_1[slice21],\n",
    "       train_regret_gp_2[slice21],\n",
    "       train_regret_gp_3[slice21],\n",
    "       train_regret_gp_4[slice21],\n",
    "       train_regret_gp_5[slice21],\n",
    "       train_regret_gp_6[slice21],\n",
    "       train_regret_gp_7[slice21],\n",
    "       train_regret_gp_8[slice21],\n",
    "       train_regret_gp_9[slice21],\n",
    "       train_regret_gp_10[slice21],\n",
    "       train_regret_gp_11[slice21],\n",
    "       train_regret_gp_12[slice21],\n",
    "       train_regret_gp_13[slice21],\n",
    "       train_regret_gp_14[slice21],\n",
    "       train_regret_gp_15[slice21],\n",
    "       train_regret_gp_16[slice21],\n",
    "       train_regret_gp_17[slice21],\n",
    "       train_regret_gp_18[slice21],\n",
    "       train_regret_gp_19[slice21],\n",
    "       train_regret_gp_20[slice21]]\n",
    "\n",
    "stp24 = [train_regret_stp_df1_1[slice21],\n",
    "       train_regret_stp_df1_2[slice21],\n",
    "       train_regret_stp_df1_3[slice21],\n",
    "       train_regret_stp_df1_4[slice21],\n",
    "       train_regret_stp_df1_5[slice21],\n",
    "       train_regret_stp_df1_6[slice21],\n",
    "       train_regret_stp_df1_7[slice21],\n",
    "       train_regret_stp_df1_8[slice21],\n",
    "       train_regret_stp_df1_9[slice21],\n",
    "       train_regret_stp_df1_10[slice21],\n",
    "       train_regret_stp_df1_11[slice21],\n",
    "       train_regret_stp_df1_12[slice21],\n",
    "       train_regret_stp_df1_13[slice21],\n",
    "       train_regret_stp_df1_14[slice21],\n",
    "       train_regret_stp_df1_15[slice21],\n",
    "       train_regret_stp_df1_16[slice21],\n",
    "       train_regret_stp_df1_17[slice21],\n",
    "       train_regret_stp_df1_18[slice21],\n",
    "       train_regret_stp_df1_19[slice21],\n",
    "       train_regret_stp_df1_20[slice21]]\n",
    "\n",
    "gp24_results = pd.DataFrame(gp24).sort_values(by=[0], ascending=False)\n",
    "stp24_results = pd.DataFrame(stp24).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp24 = np.asarray(gp24_results[4:5][0])[0]\n",
    "median_gp24 = np.asarray(gp24_results[9:10][0])[0]\n",
    "upper_gp24 = np.asarray(gp24_results[14:15][0])[0]\n",
    "\n",
    "lower_stp24 = np.asarray(stp24_results[4:5][0])[0]\n",
    "median_stp24 = np.asarray(stp24_results[9:10][0])[0]\n",
    "upper_stp24 = np.asarray(stp24_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration34 :\n",
    "\n",
    "slice31 = 33\n",
    "\n",
    "gp34 = [train_regret_gp_1[slice31],\n",
    "       train_regret_gp_2[slice31],\n",
    "       train_regret_gp_3[slice31],\n",
    "       train_regret_gp_4[slice31],\n",
    "       train_regret_gp_5[slice31],\n",
    "       train_regret_gp_6[slice31],\n",
    "       train_regret_gp_7[slice31],\n",
    "       train_regret_gp_8[slice31],\n",
    "       train_regret_gp_9[slice31],\n",
    "       train_regret_gp_10[slice31],\n",
    "       train_regret_gp_11[slice31],\n",
    "       train_regret_gp_12[slice31],\n",
    "       train_regret_gp_13[slice31],\n",
    "       train_regret_gp_14[slice31],\n",
    "       train_regret_gp_15[slice31],\n",
    "       train_regret_gp_16[slice31],\n",
    "       train_regret_gp_17[slice31],\n",
    "       train_regret_gp_18[slice31],\n",
    "       train_regret_gp_19[slice31],\n",
    "       train_regret_gp_20[slice31]]\n",
    "\n",
    "stp34 = [train_regret_stp_df1_1[slice31],\n",
    "       train_regret_stp_df1_2[slice31],\n",
    "       train_regret_stp_df1_3[slice31],\n",
    "       train_regret_stp_df1_4[slice31],\n",
    "       train_regret_stp_df1_5[slice31],\n",
    "       train_regret_stp_df1_6[slice31],\n",
    "       train_regret_stp_df1_7[slice31],\n",
    "       train_regret_stp_df1_8[slice31],\n",
    "       train_regret_stp_df1_9[slice31],\n",
    "       train_regret_stp_df1_10[slice31],\n",
    "       train_regret_stp_df1_11[slice31],\n",
    "       train_regret_stp_df1_12[slice31],\n",
    "       train_regret_stp_df1_13[slice31],\n",
    "       train_regret_stp_df1_14[slice31],\n",
    "       train_regret_stp_df1_15[slice31],\n",
    "       train_regret_stp_df1_16[slice31],\n",
    "       train_regret_stp_df1_17[slice31],\n",
    "       train_regret_stp_df1_18[slice31],\n",
    "       train_regret_stp_df1_19[slice31],\n",
    "       train_regret_stp_df1_20[slice31]]\n",
    "\n",
    "gp34_results = pd.DataFrame(gp34).sort_values(by=[0], ascending=False)\n",
    "stp34_results = pd.DataFrame(stp34).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp34 = np.asarray(gp34_results[4:5][0])[0]\n",
    "median_gp34 = np.asarray(gp34_results[9:10][0])[0]\n",
    "upper_gp34 = np.asarray(gp34_results[14:15][0])[0]\n",
    "\n",
    "lower_stp34 = np.asarray(stp34_results[4:5][0])[0]\n",
    "median_stp34 = np.asarray(stp34_results[9:10][0])[0]\n",
    "upper_stp34 = np.asarray(stp34_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration44 :\n",
    "\n",
    "slice41 = 43\n",
    "\n",
    "gp44 = [train_regret_gp_1[slice41],\n",
    "       train_regret_gp_2[slice41],\n",
    "       train_regret_gp_3[slice41],\n",
    "       train_regret_gp_4[slice41],\n",
    "       train_regret_gp_5[slice41],\n",
    "       train_regret_gp_6[slice41],\n",
    "       train_regret_gp_7[slice41],\n",
    "       train_regret_gp_8[slice41],\n",
    "       train_regret_gp_9[slice41],\n",
    "       train_regret_gp_10[slice41],\n",
    "       train_regret_gp_11[slice41],\n",
    "       train_regret_gp_12[slice41],\n",
    "       train_regret_gp_13[slice41],\n",
    "       train_regret_gp_14[slice41],\n",
    "       train_regret_gp_15[slice41],\n",
    "       train_regret_gp_16[slice41],\n",
    "       train_regret_gp_17[slice41],\n",
    "       train_regret_gp_18[slice41],\n",
    "       train_regret_gp_19[slice41],\n",
    "       train_regret_gp_20[slice41]]\n",
    "\n",
    "stp44 = [train_regret_stp_df1_1[slice41],\n",
    "       train_regret_stp_df1_2[slice41],\n",
    "       train_regret_stp_df1_3[slice41],\n",
    "       train_regret_stp_df1_4[slice41],\n",
    "       train_regret_stp_df1_5[slice41],\n",
    "       train_regret_stp_df1_6[slice41],\n",
    "       train_regret_stp_df1_7[slice41],\n",
    "       train_regret_stp_df1_8[slice41],\n",
    "       train_regret_stp_df1_9[slice41],\n",
    "       train_regret_stp_df1_10[slice41],\n",
    "       train_regret_stp_df1_11[slice41],\n",
    "       train_regret_stp_df1_12[slice41],\n",
    "       train_regret_stp_df1_13[slice41],\n",
    "       train_regret_stp_df1_14[slice41],\n",
    "       train_regret_stp_df1_15[slice41],\n",
    "       train_regret_stp_df1_16[slice41],\n",
    "       train_regret_stp_df1_17[slice41],\n",
    "       train_regret_stp_df1_18[slice41],\n",
    "       train_regret_stp_df1_19[slice41],\n",
    "       train_regret_stp_df1_20[slice41]]\n",
    "\n",
    "gp44_results = pd.DataFrame(gp44).sort_values(by=[0], ascending=False)\n",
    "stp44_results = pd.DataFrame(stp44).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp44 = np.asarray(gp44_results[4:5][0])[0]\n",
    "median_gp44 = np.asarray(gp44_results[9:10][0])[0]\n",
    "upper_gp44 = np.asarray(gp44_results[14:15][0])[0]\n",
    "\n",
    "lower_stp44 = np.asarray(stp44_results[4:5][0])[0]\n",
    "median_stp44 = np.asarray(stp44_results[9:10][0])[0]\n",
    "upper_stp44 = np.asarray(stp44_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration54 :\n",
    "\n",
    "slice51 = 53\n",
    "\n",
    "gp54 = [train_regret_gp_1[slice51],\n",
    "       train_regret_gp_2[slice51],\n",
    "       train_regret_gp_3[slice51],\n",
    "       train_regret_gp_4[slice51],\n",
    "       train_regret_gp_5[slice51],\n",
    "       train_regret_gp_6[slice51],\n",
    "       train_regret_gp_7[slice51],\n",
    "       train_regret_gp_8[slice51],\n",
    "       train_regret_gp_9[slice51],\n",
    "       train_regret_gp_10[slice51],\n",
    "       train_regret_gp_11[slice51],\n",
    "       train_regret_gp_12[slice51],\n",
    "       train_regret_gp_13[slice51],\n",
    "       train_regret_gp_14[slice51],\n",
    "       train_regret_gp_15[slice51],\n",
    "       train_regret_gp_16[slice51],\n",
    "       train_regret_gp_17[slice51],\n",
    "       train_regret_gp_18[slice51],\n",
    "       train_regret_gp_19[slice51],\n",
    "       train_regret_gp_20[slice51]]\n",
    "\n",
    "stp54 = [train_regret_stp_df1_1[slice51],\n",
    "       train_regret_stp_df1_2[slice51],\n",
    "       train_regret_stp_df1_3[slice51],\n",
    "       train_regret_stp_df1_4[slice51],\n",
    "       train_regret_stp_df1_5[slice51],\n",
    "       train_regret_stp_df1_6[slice51],\n",
    "       train_regret_stp_df1_7[slice51],\n",
    "       train_regret_stp_df1_8[slice51],\n",
    "       train_regret_stp_df1_9[slice51],\n",
    "       train_regret_stp_df1_10[slice51],\n",
    "       train_regret_stp_df1_11[slice51],\n",
    "       train_regret_stp_df1_12[slice51],\n",
    "       train_regret_stp_df1_13[slice51],\n",
    "       train_regret_stp_df1_14[slice51],\n",
    "       train_regret_stp_df1_15[slice51],\n",
    "       train_regret_stp_df1_16[slice51],\n",
    "       train_regret_stp_df1_17[slice51],\n",
    "       train_regret_stp_df1_18[slice51],\n",
    "       train_regret_stp_df1_19[slice51],\n",
    "       train_regret_stp_df1_20[slice51]]\n",
    "\n",
    "gp54_results = pd.DataFrame(gp54).sort_values(by=[0], ascending=False)\n",
    "stp54_results = pd.DataFrame(stp54).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp54 = np.asarray(gp54_results[4:5][0])[0]\n",
    "median_gp54 = np.asarray(gp54_results[9:10][0])[0]\n",
    "upper_gp54 = np.asarray(gp54_results[14:15][0])[0]\n",
    "\n",
    "lower_stp54 = np.asarray(stp54_results[4:5][0])[0]\n",
    "median_stp54 = np.asarray(stp54_results[9:10][0])[0]\n",
    "upper_stp54 = np.asarray(stp54_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration64 :\n",
    "\n",
    "slice61 = 63\n",
    "\n",
    "gp64 = [train_regret_gp_1[slice61],\n",
    "       train_regret_gp_2[slice61],\n",
    "       train_regret_gp_3[slice61],\n",
    "       train_regret_gp_4[slice61],\n",
    "       train_regret_gp_5[slice61],\n",
    "       train_regret_gp_6[slice61],\n",
    "       train_regret_gp_7[slice61],\n",
    "       train_regret_gp_8[slice61],\n",
    "       train_regret_gp_9[slice61],\n",
    "       train_regret_gp_10[slice61],\n",
    "       train_regret_gp_11[slice61],\n",
    "       train_regret_gp_12[slice61],\n",
    "       train_regret_gp_13[slice61],\n",
    "       train_regret_gp_14[slice61],\n",
    "       train_regret_gp_15[slice61],\n",
    "       train_regret_gp_16[slice61],\n",
    "       train_regret_gp_17[slice61],\n",
    "       train_regret_gp_18[slice61],\n",
    "       train_regret_gp_19[slice61],\n",
    "       train_regret_gp_20[slice61]]\n",
    "\n",
    "stp64 = [train_regret_stp_df1_1[slice61],\n",
    "       train_regret_stp_df1_2[slice61],\n",
    "       train_regret_stp_df1_3[slice61],\n",
    "       train_regret_stp_df1_4[slice61],\n",
    "       train_regret_stp_df1_5[slice61],\n",
    "       train_regret_stp_df1_6[slice61],\n",
    "       train_regret_stp_df1_7[slice61],\n",
    "       train_regret_stp_df1_8[slice61],\n",
    "       train_regret_stp_df1_9[slice61],\n",
    "       train_regret_stp_df1_10[slice61],\n",
    "       train_regret_stp_df1_11[slice61],\n",
    "       train_regret_stp_df1_12[slice61],\n",
    "       train_regret_stp_df1_13[slice61],\n",
    "       train_regret_stp_df1_14[slice61],\n",
    "       train_regret_stp_df1_15[slice61],\n",
    "       train_regret_stp_df1_16[slice61],\n",
    "       train_regret_stp_df1_17[slice61],\n",
    "       train_regret_stp_df1_18[slice61],\n",
    "       train_regret_stp_df1_19[slice61],\n",
    "       train_regret_stp_df1_20[slice61]]\n",
    "\n",
    "gp64_results = pd.DataFrame(gp64).sort_values(by=[0], ascending=False)\n",
    "stp64_results = pd.DataFrame(stp64).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp64 = np.asarray(gp64_results[4:5][0])[0]\n",
    "median_gp64 = np.asarray(gp64_results[9:10][0])[0]\n",
    "upper_gp64 = np.asarray(gp64_results[14:15][0])[0]\n",
    "\n",
    "lower_stp64 = np.asarray(stp64_results[4:5][0])[0]\n",
    "median_stp64 = np.asarray(stp64_results[9:10][0])[0]\n",
    "upper_stp64 = np.asarray(stp64_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration74 :\n",
    "\n",
    "slice71 = 73\n",
    "\n",
    "gp74 = [train_regret_gp_1[slice71],\n",
    "       train_regret_gp_2[slice71],\n",
    "       train_regret_gp_3[slice71],\n",
    "       train_regret_gp_4[slice71],\n",
    "       train_regret_gp_5[slice71],\n",
    "       train_regret_gp_6[slice71],\n",
    "       train_regret_gp_7[slice71],\n",
    "       train_regret_gp_8[slice71],\n",
    "       train_regret_gp_9[slice71],\n",
    "       train_regret_gp_10[slice71],\n",
    "       train_regret_gp_11[slice71],\n",
    "       train_regret_gp_12[slice71],\n",
    "       train_regret_gp_13[slice71],\n",
    "       train_regret_gp_14[slice71],\n",
    "       train_regret_gp_15[slice71],\n",
    "       train_regret_gp_16[slice71],\n",
    "       train_regret_gp_17[slice71],\n",
    "       train_regret_gp_18[slice71],\n",
    "       train_regret_gp_19[slice71],\n",
    "       train_regret_gp_20[slice71]]\n",
    "\n",
    "stp74 = [train_regret_stp_df1_1[slice71],\n",
    "       train_regret_stp_df1_2[slice71],\n",
    "       train_regret_stp_df1_3[slice71],\n",
    "       train_regret_stp_df1_4[slice71],\n",
    "       train_regret_stp_df1_5[slice71],\n",
    "       train_regret_stp_df1_6[slice71],\n",
    "       train_regret_stp_df1_7[slice71],\n",
    "       train_regret_stp_df1_8[slice71],\n",
    "       train_regret_stp_df1_9[slice71],\n",
    "       train_regret_stp_df1_10[slice71],\n",
    "       train_regret_stp_df1_11[slice71],\n",
    "       train_regret_stp_df1_12[slice71],\n",
    "       train_regret_stp_df1_13[slice71],\n",
    "       train_regret_stp_df1_14[slice71],\n",
    "       train_regret_stp_df1_15[slice71],\n",
    "       train_regret_stp_df1_16[slice71],\n",
    "       train_regret_stp_df1_17[slice71],\n",
    "       train_regret_stp_df1_18[slice71],\n",
    "       train_regret_stp_df1_19[slice71],\n",
    "       train_regret_stp_df1_20[slice71]]\n",
    "\n",
    "gp74_results = pd.DataFrame(gp74).sort_values(by=[0], ascending=False)\n",
    "stp74_results = pd.DataFrame(stp74).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp74 = np.asarray(gp74_results[4:5][0])[0]\n",
    "median_gp74 = np.asarray(gp74_results[9:10][0])[0]\n",
    "upper_gp74 = np.asarray(gp74_results[14:15][0])[0]\n",
    "\n",
    "lower_stp74 = np.asarray(stp74_results[4:5][0])[0]\n",
    "median_stp74 = np.asarray(stp74_results[9:10][0])[0]\n",
    "upper_stp74 = np.asarray(stp74_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration84 :\n",
    "\n",
    "slice81 = 83\n",
    "\n",
    "gp84 = [train_regret_gp_1[slice81],\n",
    "       train_regret_gp_2[slice81],\n",
    "       train_regret_gp_3[slice81],\n",
    "       train_regret_gp_4[slice81],\n",
    "       train_regret_gp_5[slice81],\n",
    "       train_regret_gp_6[slice81],\n",
    "       train_regret_gp_7[slice81],\n",
    "       train_regret_gp_8[slice81],\n",
    "       train_regret_gp_9[slice81],\n",
    "       train_regret_gp_10[slice81],\n",
    "       train_regret_gp_11[slice81],\n",
    "       train_regret_gp_12[slice81],\n",
    "       train_regret_gp_13[slice81],\n",
    "       train_regret_gp_14[slice81],\n",
    "       train_regret_gp_15[slice81],\n",
    "       train_regret_gp_16[slice81],\n",
    "       train_regret_gp_17[slice81],\n",
    "       train_regret_gp_18[slice81],\n",
    "       train_regret_gp_19[slice81],\n",
    "       train_regret_gp_20[slice81]]\n",
    "\n",
    "stp84 = [train_regret_stp_df1_1[slice81],\n",
    "       train_regret_stp_df1_2[slice81],\n",
    "       train_regret_stp_df1_3[slice81],\n",
    "       train_regret_stp_df1_4[slice81],\n",
    "       train_regret_stp_df1_5[slice81],\n",
    "       train_regret_stp_df1_6[slice81],\n",
    "       train_regret_stp_df1_7[slice81],\n",
    "       train_regret_stp_df1_8[slice81],\n",
    "       train_regret_stp_df1_9[slice81],\n",
    "       train_regret_stp_df1_10[slice81],\n",
    "       train_regret_stp_df1_11[slice81],\n",
    "       train_regret_stp_df1_12[slice81],\n",
    "       train_regret_stp_df1_13[slice81],\n",
    "       train_regret_stp_df1_14[slice81],\n",
    "       train_regret_stp_df1_15[slice81],\n",
    "       train_regret_stp_df1_16[slice81],\n",
    "       train_regret_stp_df1_17[slice81],\n",
    "       train_regret_stp_df1_18[slice81],\n",
    "       train_regret_stp_df1_19[slice81],\n",
    "       train_regret_stp_df1_20[slice81]]\n",
    "\n",
    "gp84_results = pd.DataFrame(gp84).sort_values(by=[0], ascending=False)\n",
    "stp84_results = pd.DataFrame(stp84).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp84 = np.asarray(gp84_results[4:5][0])[0]\n",
    "median_gp84 = np.asarray(gp84_results[9:10][0])[0]\n",
    "upper_gp84 = np.asarray(gp84_results[14:15][0])[0]\n",
    "\n",
    "lower_stp84 = np.asarray(stp84_results[4:5][0])[0]\n",
    "median_stp84 = np.asarray(stp84_results[9:10][0])[0]\n",
    "upper_stp84 = np.asarray(stp84_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration94 :\n",
    "\n",
    "slice1 = 93\n",
    "\n",
    "gp94 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp94 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp94_results = pd.DataFrame(gp94).sort_values(by=[0], ascending=False)\n",
    "stp94_results = pd.DataFrame(stp94).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp94 = np.asarray(gp94_results[4:5][0])[0]\n",
    "median_gp94 = np.asarray(gp94_results[9:10][0])[0]\n",
    "upper_gp94 = np.asarray(gp94_results[14:15][0])[0]\n",
    "\n",
    "lower_stp94 = np.asarray(stp94_results[4:5][0])[0]\n",
    "median_stp94 = np.asarray(stp94_results[9:10][0])[0]\n",
    "upper_stp94 = np.asarray(stp94_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration5 :\n",
    "\n",
    "slice1 = 4\n",
    "\n",
    "gp5 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp5 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp5_results = pd.DataFrame(gp5).sort_values(by=[0], ascending=False)\n",
    "stp5_results = pd.DataFrame(stp5).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp5 = np.asarray(gp5_results[4:5][0])[0]\n",
    "median_gp5 = np.asarray(gp5_results[9:10][0])[0]\n",
    "upper_gp5 = np.asarray(gp5_results[14:15][0])[0]\n",
    "\n",
    "lower_stp5 = np.asarray(stp5_results[4:5][0])[0]\n",
    "median_stp5 = np.asarray(stp5_results[9:10][0])[0]\n",
    "upper_stp5 = np.asarray(stp5_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration15 :\n",
    "\n",
    "slice11 = 14\n",
    "\n",
    "gp15 = [train_regret_gp_1[slice11],\n",
    "       train_regret_gp_2[slice11],\n",
    "       train_regret_gp_3[slice11],\n",
    "       train_regret_gp_4[slice11],\n",
    "       train_regret_gp_5[slice11],\n",
    "       train_regret_gp_6[slice11],\n",
    "       train_regret_gp_7[slice11],\n",
    "       train_regret_gp_8[slice11],\n",
    "       train_regret_gp_9[slice11],\n",
    "       train_regret_gp_10[slice11],\n",
    "       train_regret_gp_11[slice11],\n",
    "       train_regret_gp_12[slice11],\n",
    "       train_regret_gp_13[slice11],\n",
    "       train_regret_gp_14[slice11],\n",
    "       train_regret_gp_15[slice11],\n",
    "       train_regret_gp_16[slice11],\n",
    "       train_regret_gp_17[slice11],\n",
    "       train_regret_gp_18[slice11],\n",
    "       train_regret_gp_19[slice11],\n",
    "       train_regret_gp_20[slice11]]\n",
    "\n",
    "stp15 = [train_regret_stp_df1_1[slice11],\n",
    "       train_regret_stp_df1_2[slice11],\n",
    "       train_regret_stp_df1_3[slice11],\n",
    "       train_regret_stp_df1_4[slice11],\n",
    "       train_regret_stp_df1_5[slice11],\n",
    "       train_regret_stp_df1_6[slice11],\n",
    "       train_regret_stp_df1_7[slice11],\n",
    "       train_regret_stp_df1_8[slice11],\n",
    "       train_regret_stp_df1_9[slice11],\n",
    "       train_regret_stp_df1_10[slice11],\n",
    "       train_regret_stp_df1_11[slice11],\n",
    "       train_regret_stp_df1_12[slice11],\n",
    "       train_regret_stp_df1_13[slice11],\n",
    "       train_regret_stp_df1_14[slice11],\n",
    "       train_regret_stp_df1_15[slice11],\n",
    "       train_regret_stp_df1_16[slice11],\n",
    "       train_regret_stp_df1_17[slice11],\n",
    "       train_regret_stp_df1_18[slice11],\n",
    "       train_regret_stp_df1_19[slice11],\n",
    "       train_regret_stp_df1_20[slice11]]\n",
    "\n",
    "gp15_results = pd.DataFrame(gp15).sort_values(by=[0], ascending=False)\n",
    "stp15_results = pd.DataFrame(stp15).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp15 = np.asarray(gp15_results[4:5][0])[0]\n",
    "median_gp15 = np.asarray(gp15_results[9:10][0])[0]\n",
    "upper_gp15 = np.asarray(gp15_results[14:15][0])[0]\n",
    "\n",
    "lower_stp15 = np.asarray(stp15_results[4:5][0])[0]\n",
    "median_stp15 = np.asarray(stp15_results[9:10][0])[0]\n",
    "upper_stp15 = np.asarray(stp15_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration25 :\n",
    "\n",
    "slice21 = 24\n",
    "\n",
    "gp25 = [train_regret_gp_1[slice21],\n",
    "       train_regret_gp_2[slice21],\n",
    "       train_regret_gp_3[slice21],\n",
    "       train_regret_gp_4[slice21],\n",
    "       train_regret_gp_5[slice21],\n",
    "       train_regret_gp_6[slice21],\n",
    "       train_regret_gp_7[slice21],\n",
    "       train_regret_gp_8[slice21],\n",
    "       train_regret_gp_9[slice21],\n",
    "       train_regret_gp_10[slice21],\n",
    "       train_regret_gp_11[slice21],\n",
    "       train_regret_gp_12[slice21],\n",
    "       train_regret_gp_13[slice21],\n",
    "       train_regret_gp_14[slice21],\n",
    "       train_regret_gp_15[slice21],\n",
    "       train_regret_gp_16[slice21],\n",
    "       train_regret_gp_17[slice21],\n",
    "       train_regret_gp_18[slice21],\n",
    "       train_regret_gp_19[slice21],\n",
    "       train_regret_gp_20[slice21]]\n",
    "\n",
    "stp25 = [train_regret_stp_df1_1[slice21],\n",
    "       train_regret_stp_df1_2[slice21],\n",
    "       train_regret_stp_df1_3[slice21],\n",
    "       train_regret_stp_df1_4[slice21],\n",
    "       train_regret_stp_df1_5[slice21],\n",
    "       train_regret_stp_df1_6[slice21],\n",
    "       train_regret_stp_df1_7[slice21],\n",
    "       train_regret_stp_df1_8[slice21],\n",
    "       train_regret_stp_df1_9[slice21],\n",
    "       train_regret_stp_df1_10[slice21],\n",
    "       train_regret_stp_df1_11[slice21],\n",
    "       train_regret_stp_df1_12[slice21],\n",
    "       train_regret_stp_df1_13[slice21],\n",
    "       train_regret_stp_df1_14[slice21],\n",
    "       train_regret_stp_df1_15[slice21],\n",
    "       train_regret_stp_df1_16[slice21],\n",
    "       train_regret_stp_df1_17[slice21],\n",
    "       train_regret_stp_df1_18[slice21],\n",
    "       train_regret_stp_df1_19[slice21],\n",
    "       train_regret_stp_df1_20[slice21]]\n",
    "\n",
    "gp25_results = pd.DataFrame(gp25).sort_values(by=[0], ascending=False)\n",
    "stp25_results = pd.DataFrame(stp25).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp25 = np.asarray(gp25_results[4:5][0])[0]\n",
    "median_gp25 = np.asarray(gp25_results[9:10][0])[0]\n",
    "upper_gp25 = np.asarray(gp25_results[14:15][0])[0]\n",
    "\n",
    "lower_stp25 = np.asarray(stp25_results[4:5][0])[0]\n",
    "median_stp25 = np.asarray(stp25_results[9:10][0])[0]\n",
    "upper_stp25= np.asarray(stp25_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration35 :\n",
    "\n",
    "slice31 = 34\n",
    "\n",
    "gp35 = [train_regret_gp_1[slice31],\n",
    "       train_regret_gp_2[slice31],\n",
    "       train_regret_gp_3[slice31],\n",
    "       train_regret_gp_4[slice31],\n",
    "       train_regret_gp_5[slice31],\n",
    "       train_regret_gp_6[slice31],\n",
    "       train_regret_gp_7[slice31],\n",
    "       train_regret_gp_8[slice31],\n",
    "       train_regret_gp_9[slice31],\n",
    "       train_regret_gp_10[slice31],\n",
    "       train_regret_gp_11[slice31],\n",
    "       train_regret_gp_12[slice31],\n",
    "       train_regret_gp_13[slice31],\n",
    "       train_regret_gp_14[slice31],\n",
    "       train_regret_gp_15[slice31],\n",
    "       train_regret_gp_16[slice31],\n",
    "       train_regret_gp_17[slice31],\n",
    "       train_regret_gp_18[slice31],\n",
    "       train_regret_gp_19[slice31],\n",
    "       train_regret_gp_20[slice31]]\n",
    "\n",
    "stp35 = [train_regret_stp_df1_1[slice31],\n",
    "       train_regret_stp_df1_2[slice31],\n",
    "       train_regret_stp_df1_3[slice31],\n",
    "       train_regret_stp_df1_4[slice31],\n",
    "       train_regret_stp_df1_5[slice31],\n",
    "       train_regret_stp_df1_6[slice31],\n",
    "       train_regret_stp_df1_7[slice31],\n",
    "       train_regret_stp_df1_8[slice31],\n",
    "       train_regret_stp_df1_9[slice31],\n",
    "       train_regret_stp_df1_10[slice31],\n",
    "       train_regret_stp_df1_11[slice31],\n",
    "       train_regret_stp_df1_12[slice31],\n",
    "       train_regret_stp_df1_13[slice31],\n",
    "       train_regret_stp_df1_14[slice31],\n",
    "       train_regret_stp_df1_15[slice31],\n",
    "       train_regret_stp_df1_16[slice31],\n",
    "       train_regret_stp_df1_17[slice31],\n",
    "       train_regret_stp_df1_18[slice31],\n",
    "       train_regret_stp_df1_19[slice31],\n",
    "       train_regret_stp_df1_20[slice31]]\n",
    "\n",
    "gp35_results = pd.DataFrame(gp35).sort_values(by=[0], ascending=False)\n",
    "stp35_results = pd.DataFrame(stp35).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp35 = np.asarray(gp35_results[4:5][0])[0]\n",
    "median_gp35 = np.asarray(gp35_results[9:10][0])[0]\n",
    "upper_gp35 = np.asarray(gp35_results[14:15][0])[0]\n",
    "\n",
    "lower_stp35 = np.asarray(stp35_results[4:5][0])[0]\n",
    "median_stp35 = np.asarray(stp35_results[9:10][0])[0]\n",
    "upper_stp35 = np.asarray(stp35_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration45 :\n",
    "\n",
    "slice41 = 44\n",
    "\n",
    "gp45 = [train_regret_gp_1[slice41],\n",
    "       train_regret_gp_2[slice41],\n",
    "       train_regret_gp_3[slice41],\n",
    "       train_regret_gp_4[slice41],\n",
    "       train_regret_gp_5[slice41],\n",
    "       train_regret_gp_6[slice41],\n",
    "       train_regret_gp_7[slice41],\n",
    "       train_regret_gp_8[slice41],\n",
    "       train_regret_gp_9[slice41],\n",
    "       train_regret_gp_10[slice41],\n",
    "       train_regret_gp_11[slice41],\n",
    "       train_regret_gp_12[slice41],\n",
    "       train_regret_gp_13[slice41],\n",
    "       train_regret_gp_14[slice41],\n",
    "       train_regret_gp_15[slice41],\n",
    "       train_regret_gp_16[slice41],\n",
    "       train_regret_gp_17[slice41],\n",
    "       train_regret_gp_18[slice41],\n",
    "       train_regret_gp_19[slice41],\n",
    "       train_regret_gp_20[slice41]]\n",
    "\n",
    "stp45 = [train_regret_stp_df1_1[slice41],\n",
    "       train_regret_stp_df1_2[slice41],\n",
    "       train_regret_stp_df1_3[slice41],\n",
    "       train_regret_stp_df1_4[slice41],\n",
    "       train_regret_stp_df1_5[slice41],\n",
    "       train_regret_stp_df1_6[slice41],\n",
    "       train_regret_stp_df1_7[slice41],\n",
    "       train_regret_stp_df1_8[slice41],\n",
    "       train_regret_stp_df1_9[slice41],\n",
    "       train_regret_stp_df1_10[slice41],\n",
    "       train_regret_stp_df1_11[slice41],\n",
    "       train_regret_stp_df1_12[slice41],\n",
    "       train_regret_stp_df1_13[slice41],\n",
    "       train_regret_stp_df1_14[slice41],\n",
    "       train_regret_stp_df1_15[slice41],\n",
    "       train_regret_stp_df1_16[slice41],\n",
    "       train_regret_stp_df1_17[slice41],\n",
    "       train_regret_stp_df1_18[slice41],\n",
    "       train_regret_stp_df1_19[slice41],\n",
    "       train_regret_stp_df1_20[slice41]]\n",
    "\n",
    "gp45_results = pd.DataFrame(gp45).sort_values(by=[0], ascending=False)\n",
    "stp45_results = pd.DataFrame(stp45).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp45 = np.asarray(gp45_results[4:5][0])[0]\n",
    "median_gp45 = np.asarray(gp45_results[9:10][0])[0]\n",
    "upper_gp45 = np.asarray(gp45_results[14:15][0])[0]\n",
    "\n",
    "lower_stp45 = np.asarray(stp45_results[4:5][0])[0]\n",
    "median_stp45 = np.asarray(stp45_results[9:10][0])[0]\n",
    "upper_stp45 = np.asarray(stp45_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration55 :\n",
    "\n",
    "slice51 = 54\n",
    "\n",
    "gp55 = [train_regret_gp_1[slice51],\n",
    "       train_regret_gp_2[slice51],\n",
    "       train_regret_gp_3[slice51],\n",
    "       train_regret_gp_4[slice51],\n",
    "       train_regret_gp_5[slice51],\n",
    "       train_regret_gp_6[slice51],\n",
    "       train_regret_gp_7[slice51],\n",
    "       train_regret_gp_8[slice51],\n",
    "       train_regret_gp_9[slice51],\n",
    "       train_regret_gp_10[slice51],\n",
    "       train_regret_gp_11[slice51],\n",
    "       train_regret_gp_12[slice51],\n",
    "       train_regret_gp_13[slice51],\n",
    "       train_regret_gp_14[slice51],\n",
    "       train_regret_gp_15[slice51],\n",
    "       train_regret_gp_16[slice51],\n",
    "       train_regret_gp_17[slice51],\n",
    "       train_regret_gp_18[slice51],\n",
    "       train_regret_gp_19[slice51],\n",
    "       train_regret_gp_20[slice51]]\n",
    "\n",
    "stp55 = [train_regret_stp_df1_1[slice51],\n",
    "       train_regret_stp_df1_2[slice51],\n",
    "       train_regret_stp_df1_3[slice51],\n",
    "       train_regret_stp_df1_4[slice51],\n",
    "       train_regret_stp_df1_5[slice51],\n",
    "       train_regret_stp_df1_6[slice51],\n",
    "       train_regret_stp_df1_7[slice51],\n",
    "       train_regret_stp_df1_8[slice51],\n",
    "       train_regret_stp_df1_9[slice51],\n",
    "       train_regret_stp_df1_10[slice51],\n",
    "       train_regret_stp_df1_11[slice51],\n",
    "       train_regret_stp_df1_12[slice51],\n",
    "       train_regret_stp_df1_13[slice51],\n",
    "       train_regret_stp_df1_14[slice51],\n",
    "       train_regret_stp_df1_15[slice51],\n",
    "       train_regret_stp_df1_16[slice51],\n",
    "       train_regret_stp_df1_17[slice51],\n",
    "       train_regret_stp_df1_18[slice51],\n",
    "       train_regret_stp_df1_19[slice51],\n",
    "       train_regret_stp_df1_20[slice51]]\n",
    "\n",
    "gp55_results = pd.DataFrame(gp55).sort_values(by=[0], ascending=False)\n",
    "stp55_results = pd.DataFrame(stp55).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp55 = np.asarray(gp55_results[4:5][0])[0]\n",
    "median_gp55 = np.asarray(gp55_results[9:10][0])[0]\n",
    "upper_gp55 = np.asarray(gp55_results[14:15][0])[0]\n",
    "\n",
    "lower_stp55 = np.asarray(stp55_results[4:5][0])[0]\n",
    "median_stp55 = np.asarray(stp55_results[9:10][0])[0]\n",
    "upper_stp55 = np.asarray(stp55_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration65 :\n",
    "\n",
    "slice61 = 64\n",
    "\n",
    "gp65 = [train_regret_gp_1[slice61],\n",
    "       train_regret_gp_2[slice61],\n",
    "       train_regret_gp_3[slice61],\n",
    "       train_regret_gp_4[slice61],\n",
    "       train_regret_gp_5[slice61],\n",
    "       train_regret_gp_6[slice61],\n",
    "       train_regret_gp_7[slice61],\n",
    "       train_regret_gp_8[slice61],\n",
    "       train_regret_gp_9[slice61],\n",
    "       train_regret_gp_10[slice61],\n",
    "       train_regret_gp_11[slice61],\n",
    "       train_regret_gp_12[slice61],\n",
    "       train_regret_gp_13[slice61],\n",
    "       train_regret_gp_14[slice61],\n",
    "       train_regret_gp_15[slice61],\n",
    "       train_regret_gp_16[slice61],\n",
    "       train_regret_gp_17[slice61],\n",
    "       train_regret_gp_18[slice61],\n",
    "       train_regret_gp_19[slice61],\n",
    "       train_regret_gp_20[slice61]]\n",
    "\n",
    "stp65 = [train_regret_stp_df1_1[slice61],\n",
    "       train_regret_stp_df1_2[slice61],\n",
    "       train_regret_stp_df1_3[slice61],\n",
    "       train_regret_stp_df1_4[slice61],\n",
    "       train_regret_stp_df1_5[slice61],\n",
    "       train_regret_stp_df1_6[slice61],\n",
    "       train_regret_stp_df1_7[slice61],\n",
    "       train_regret_stp_df1_8[slice61],\n",
    "       train_regret_stp_df1_9[slice61],\n",
    "       train_regret_stp_df1_10[slice61],\n",
    "       train_regret_stp_df1_11[slice61],\n",
    "       train_regret_stp_df1_12[slice61],\n",
    "       train_regret_stp_df1_13[slice61],\n",
    "       train_regret_stp_df1_14[slice61],\n",
    "       train_regret_stp_df1_15[slice61],\n",
    "       train_regret_stp_df1_16[slice61],\n",
    "       train_regret_stp_df1_17[slice61],\n",
    "       train_regret_stp_df1_18[slice61],\n",
    "       train_regret_stp_df1_19[slice61],\n",
    "       train_regret_stp_df1_20[slice61]]\n",
    "\n",
    "gp65_results = pd.DataFrame(gp65).sort_values(by=[0], ascending=False)\n",
    "stp65_results = pd.DataFrame(stp65).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp65 = np.asarray(gp65_results[4:5][0])[0]\n",
    "median_gp65 = np.asarray(gp65_results[9:10][0])[0]\n",
    "upper_gp65 = np.asarray(gp65_results[14:15][0])[0]\n",
    "\n",
    "lower_stp65 = np.asarray(stp65_results[4:5][0])[0]\n",
    "median_stp65 = np.asarray(stp65_results[9:10][0])[0]\n",
    "upper_stp65 = np.asarray(stp65_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration75 :\n",
    "\n",
    "slice71 = 74\n",
    "\n",
    "gp75 = [train_regret_gp_1[slice71],\n",
    "       train_regret_gp_2[slice71],\n",
    "       train_regret_gp_3[slice71],\n",
    "       train_regret_gp_4[slice71],\n",
    "       train_regret_gp_5[slice71],\n",
    "       train_regret_gp_6[slice71],\n",
    "       train_regret_gp_7[slice71],\n",
    "       train_regret_gp_8[slice71],\n",
    "       train_regret_gp_9[slice71],\n",
    "       train_regret_gp_10[slice71],\n",
    "       train_regret_gp_11[slice71],\n",
    "       train_regret_gp_12[slice71],\n",
    "       train_regret_gp_13[slice71],\n",
    "       train_regret_gp_14[slice71],\n",
    "       train_regret_gp_15[slice71],\n",
    "       train_regret_gp_16[slice71],\n",
    "       train_regret_gp_17[slice71],\n",
    "       train_regret_gp_18[slice71],\n",
    "       train_regret_gp_19[slice71],\n",
    "       train_regret_gp_20[slice71]]\n",
    "\n",
    "stp75 = [train_regret_stp_df1_1[slice71],\n",
    "       train_regret_stp_df1_2[slice71],\n",
    "       train_regret_stp_df1_3[slice71],\n",
    "       train_regret_stp_df1_4[slice71],\n",
    "       train_regret_stp_df1_5[slice71],\n",
    "       train_regret_stp_df1_6[slice71],\n",
    "       train_regret_stp_df1_7[slice71],\n",
    "       train_regret_stp_df1_8[slice71],\n",
    "       train_regret_stp_df1_9[slice71],\n",
    "       train_regret_stp_df1_10[slice71],\n",
    "       train_regret_stp_df1_11[slice71],\n",
    "       train_regret_stp_df1_12[slice71],\n",
    "       train_regret_stp_df1_13[slice71],\n",
    "       train_regret_stp_df1_14[slice71],\n",
    "       train_regret_stp_df1_15[slice71],\n",
    "       train_regret_stp_df1_16[slice71],\n",
    "       train_regret_stp_df1_17[slice71],\n",
    "       train_regret_stp_df1_18[slice71],\n",
    "       train_regret_stp_df1_19[slice71],\n",
    "       train_regret_stp_df1_20[slice71]]\n",
    "\n",
    "gp75_results = pd.DataFrame(gp75).sort_values(by=[0], ascending=False)\n",
    "stp75_results = pd.DataFrame(stp75).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp75 = np.asarray(gp75_results[4:5][0])[0]\n",
    "median_gp75 = np.asarray(gp75_results[9:10][0])[0]\n",
    "upper_gp75 = np.asarray(gp75_results[14:15][0])[0]\n",
    "\n",
    "lower_stp75 = np.asarray(stp75_results[4:5][0])[0]\n",
    "median_stp75 = np.asarray(stp75_results[9:10][0])[0]\n",
    "upper_stp75 = np.asarray(stp75_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration85 :\n",
    "\n",
    "slice81 = 84\n",
    "\n",
    "gp85 = [train_regret_gp_1[slice81],\n",
    "       train_regret_gp_2[slice81],\n",
    "       train_regret_gp_3[slice81],\n",
    "       train_regret_gp_4[slice81],\n",
    "       train_regret_gp_5[slice81],\n",
    "       train_regret_gp_6[slice81],\n",
    "       train_regret_gp_7[slice81],\n",
    "       train_regret_gp_8[slice81],\n",
    "       train_regret_gp_9[slice81],\n",
    "       train_regret_gp_10[slice81],\n",
    "       train_regret_gp_11[slice81],\n",
    "       train_regret_gp_12[slice81],\n",
    "       train_regret_gp_13[slice81],\n",
    "       train_regret_gp_14[slice81],\n",
    "       train_regret_gp_15[slice81],\n",
    "       train_regret_gp_16[slice81],\n",
    "       train_regret_gp_17[slice81],\n",
    "       train_regret_gp_18[slice81],\n",
    "       train_regret_gp_19[slice81],\n",
    "       train_regret_gp_20[slice81]]\n",
    "\n",
    "stp85 = [train_regret_stp_df1_1[slice81],\n",
    "       train_regret_stp_df1_2[slice81],\n",
    "       train_regret_stp_df1_3[slice81],\n",
    "       train_regret_stp_df1_4[slice81],\n",
    "       train_regret_stp_df1_5[slice81],\n",
    "       train_regret_stp_df1_6[slice81],\n",
    "       train_regret_stp_df1_7[slice81],\n",
    "       train_regret_stp_df1_8[slice81],\n",
    "       train_regret_stp_df1_9[slice81],\n",
    "       train_regret_stp_df1_10[slice81],\n",
    "       train_regret_stp_df1_11[slice81],\n",
    "       train_regret_stp_df1_12[slice81],\n",
    "       train_regret_stp_df1_13[slice81],\n",
    "       train_regret_stp_df1_14[slice81],\n",
    "       train_regret_stp_df1_15[slice81],\n",
    "       train_regret_stp_df1_16[slice81],\n",
    "       train_regret_stp_df1_17[slice81],\n",
    "       train_regret_stp_df1_18[slice81],\n",
    "       train_regret_stp_df1_19[slice81],\n",
    "       train_regret_stp_df1_20[slice81]]\n",
    "\n",
    "gp85_results = pd.DataFrame(gp85).sort_values(by=[0], ascending=False)\n",
    "stp85_results = pd.DataFrame(stp85).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp85 = np.asarray(gp85_results[4:5][0])[0]\n",
    "median_gp85 = np.asarray(gp85_results[9:10][0])[0]\n",
    "upper_gp85 = np.asarray(gp85_results[14:15][0])[0]\n",
    "\n",
    "lower_stp85 = np.asarray(stp85_results[4:5][0])[0]\n",
    "median_stp85 = np.asarray(stp85_results[9:10][0])[0]\n",
    "upper_stp85 = np.asarray(stp85_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration95 :\n",
    "\n",
    "slice1 = 94\n",
    "\n",
    "gp95 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp95 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp95_results = pd.DataFrame(gp95).sort_values(by=[0], ascending=False)\n",
    "stp95_results = pd.DataFrame(stp95).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp95 = np.asarray(gp95_results[4:5][0])[0]\n",
    "median_gp95 = np.asarray(gp95_results[9:10][0])[0]\n",
    "upper_gp95 = np.asarray(gp95_results[14:15][0])[0]\n",
    "\n",
    "lower_stp95 = np.asarray(stp95_results[4:5][0])[0]\n",
    "median_stp95 = np.asarray(stp95_results[9:10][0])[0]\n",
    "upper_stp95 = np.asarray(stp95_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration6 :\n",
    "\n",
    "slice1 = 5\n",
    "\n",
    "gp6 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp6 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp6_results = pd.DataFrame(gp6).sort_values(by=[0], ascending=False)\n",
    "stp6_results = pd.DataFrame(stp6).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp6 = np.asarray(gp6_results[4:5][0])[0]\n",
    "median_gp6 = np.asarray(gp6_results[9:10][0])[0]\n",
    "upper_gp6 = np.asarray(gp6_results[14:15][0])[0]\n",
    "\n",
    "lower_stp6 = np.asarray(stp6_results[4:5][0])[0]\n",
    "median_stp6 = np.asarray(stp6_results[9:10][0])[0]\n",
    "upper_stp6 = np.asarray(stp6_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration16 :\n",
    "\n",
    "slice11 = 15\n",
    "\n",
    "gp16 = [train_regret_gp_1[slice11],\n",
    "       train_regret_gp_2[slice11],\n",
    "       train_regret_gp_3[slice11],\n",
    "       train_regret_gp_4[slice11],\n",
    "       train_regret_gp_5[slice11],\n",
    "       train_regret_gp_6[slice11],\n",
    "       train_regret_gp_7[slice11],\n",
    "       train_regret_gp_8[slice11],\n",
    "       train_regret_gp_9[slice11],\n",
    "       train_regret_gp_10[slice11],\n",
    "       train_regret_gp_11[slice11],\n",
    "       train_regret_gp_12[slice11],\n",
    "       train_regret_gp_13[slice11],\n",
    "       train_regret_gp_14[slice11],\n",
    "       train_regret_gp_15[slice11],\n",
    "       train_regret_gp_16[slice11],\n",
    "       train_regret_gp_17[slice11],\n",
    "       train_regret_gp_18[slice11],\n",
    "       train_regret_gp_19[slice11],\n",
    "       train_regret_gp_20[slice11]]\n",
    "\n",
    "stp16 = [train_regret_stp_df1_1[slice11],\n",
    "       train_regret_stp_df1_2[slice11],\n",
    "       train_regret_stp_df1_3[slice11],\n",
    "       train_regret_stp_df1_4[slice11],\n",
    "       train_regret_stp_df1_5[slice11],\n",
    "       train_regret_stp_df1_6[slice11],\n",
    "       train_regret_stp_df1_7[slice11],\n",
    "       train_regret_stp_df1_8[slice11],\n",
    "       train_regret_stp_df1_9[slice11],\n",
    "       train_regret_stp_df1_10[slice11],\n",
    "       train_regret_stp_df1_11[slice11],\n",
    "       train_regret_stp_df1_12[slice11],\n",
    "       train_regret_stp_df1_13[slice11],\n",
    "       train_regret_stp_df1_14[slice11],\n",
    "       train_regret_stp_df1_15[slice11],\n",
    "       train_regret_stp_df1_16[slice11],\n",
    "       train_regret_stp_df1_17[slice11],\n",
    "       train_regret_stp_df1_18[slice11],\n",
    "       train_regret_stp_df1_19[slice11],\n",
    "       train_regret_stp_df1_20[slice11]]\n",
    "\n",
    "gp16_results = pd.DataFrame(gp16).sort_values(by=[0], ascending=False)\n",
    "stp16_results = pd.DataFrame(stp16).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp16 = np.asarray(gp16_results[4:5][0])[0]\n",
    "median_gp16 = np.asarray(gp16_results[9:10][0])[0]\n",
    "upper_gp16 = np.asarray(gp16_results[14:15][0])[0]\n",
    "\n",
    "lower_stp16 = np.asarray(stp16_results[4:5][0])[0]\n",
    "median_stp16 = np.asarray(stp16_results[9:10][0])[0]\n",
    "upper_stp16 = np.asarray(stp16_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration26 :\n",
    "\n",
    "slice21 = 25\n",
    "\n",
    "gp26 = [train_regret_gp_1[slice21],\n",
    "       train_regret_gp_2[slice21],\n",
    "       train_regret_gp_3[slice21],\n",
    "       train_regret_gp_4[slice21],\n",
    "       train_regret_gp_5[slice21],\n",
    "       train_regret_gp_6[slice21],\n",
    "       train_regret_gp_7[slice21],\n",
    "       train_regret_gp_8[slice21],\n",
    "       train_regret_gp_9[slice21],\n",
    "       train_regret_gp_10[slice21],\n",
    "       train_regret_gp_11[slice21],\n",
    "       train_regret_gp_12[slice21],\n",
    "       train_regret_gp_13[slice21],\n",
    "       train_regret_gp_14[slice21],\n",
    "       train_regret_gp_15[slice21],\n",
    "       train_regret_gp_16[slice21],\n",
    "       train_regret_gp_17[slice21],\n",
    "       train_regret_gp_18[slice21],\n",
    "       train_regret_gp_19[slice21],\n",
    "       train_regret_gp_20[slice21]]\n",
    "\n",
    "stp26 = [train_regret_stp_df1_1[slice21],\n",
    "       train_regret_stp_df1_2[slice21],\n",
    "       train_regret_stp_df1_3[slice21],\n",
    "       train_regret_stp_df1_4[slice21],\n",
    "       train_regret_stp_df1_5[slice21],\n",
    "       train_regret_stp_df1_6[slice21],\n",
    "       train_regret_stp_df1_7[slice21],\n",
    "       train_regret_stp_df1_8[slice21],\n",
    "       train_regret_stp_df1_9[slice21],\n",
    "       train_regret_stp_df1_10[slice21],\n",
    "       train_regret_stp_df1_11[slice21],\n",
    "       train_regret_stp_df1_12[slice21],\n",
    "       train_regret_stp_df1_13[slice21],\n",
    "       train_regret_stp_df1_14[slice21],\n",
    "       train_regret_stp_df1_15[slice21],\n",
    "       train_regret_stp_df1_16[slice21],\n",
    "       train_regret_stp_df1_17[slice21],\n",
    "       train_regret_stp_df1_18[slice21],\n",
    "       train_regret_stp_df1_19[slice21],\n",
    "       train_regret_stp_df1_20[slice21]]\n",
    "\n",
    "gp26_results = pd.DataFrame(gp26).sort_values(by=[0], ascending=False)\n",
    "stp26_results = pd.DataFrame(stp26).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp26 = np.asarray(gp26_results[4:5][0])[0]\n",
    "median_gp26 = np.asarray(gp26_results[9:10][0])[0]\n",
    "upper_gp26 = np.asarray(gp26_results[14:15][0])[0]\n",
    "\n",
    "lower_stp26 = np.asarray(stp26_results[4:5][0])[0]\n",
    "median_stp26 = np.asarray(stp26_results[9:10][0])[0]\n",
    "upper_stp26 = np.asarray(stp26_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration36 :\n",
    "\n",
    "slice31 = 35\n",
    "\n",
    "gp36 = [train_regret_gp_1[slice31],\n",
    "       train_regret_gp_2[slice31],\n",
    "       train_regret_gp_3[slice31],\n",
    "       train_regret_gp_4[slice31],\n",
    "       train_regret_gp_5[slice31],\n",
    "       train_regret_gp_6[slice31],\n",
    "       train_regret_gp_7[slice31],\n",
    "       train_regret_gp_8[slice31],\n",
    "       train_regret_gp_9[slice31],\n",
    "       train_regret_gp_10[slice31],\n",
    "       train_regret_gp_11[slice31],\n",
    "       train_regret_gp_12[slice31],\n",
    "       train_regret_gp_13[slice31],\n",
    "       train_regret_gp_14[slice31],\n",
    "       train_regret_gp_15[slice31],\n",
    "       train_regret_gp_16[slice31],\n",
    "       train_regret_gp_17[slice31],\n",
    "       train_regret_gp_18[slice31],\n",
    "       train_regret_gp_19[slice31],\n",
    "       train_regret_gp_20[slice31]]\n",
    "\n",
    "stp36 = [train_regret_stp_df1_1[slice31],\n",
    "       train_regret_stp_df1_2[slice31],\n",
    "       train_regret_stp_df1_3[slice31],\n",
    "       train_regret_stp_df1_4[slice31],\n",
    "       train_regret_stp_df1_5[slice31],\n",
    "       train_regret_stp_df1_6[slice31],\n",
    "       train_regret_stp_df1_7[slice31],\n",
    "       train_regret_stp_df1_8[slice31],\n",
    "       train_regret_stp_df1_9[slice31],\n",
    "       train_regret_stp_df1_10[slice31],\n",
    "       train_regret_stp_df1_11[slice31],\n",
    "       train_regret_stp_df1_12[slice31],\n",
    "       train_regret_stp_df1_13[slice31],\n",
    "       train_regret_stp_df1_14[slice31],\n",
    "       train_regret_stp_df1_15[slice31],\n",
    "       train_regret_stp_df1_16[slice31],\n",
    "       train_regret_stp_df1_17[slice31],\n",
    "       train_regret_stp_df1_18[slice31],\n",
    "       train_regret_stp_df1_19[slice31],\n",
    "       train_regret_stp_df1_20[slice31]]\n",
    "\n",
    "gp36_results = pd.DataFrame(gp36).sort_values(by=[0], ascending=False)\n",
    "stp36_results = pd.DataFrame(stp36).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp36 = np.asarray(gp36_results[4:5][0])[0]\n",
    "median_gp36 = np.asarray(gp36_results[9:10][0])[0]\n",
    "upper_gp36 = np.asarray(gp36_results[14:15][0])[0]\n",
    "\n",
    "lower_stp36 = np.asarray(stp36_results[4:5][0])[0]\n",
    "median_stp36 = np.asarray(stp36_results[9:10][0])[0]\n",
    "upper_stp36 = np.asarray(stp36_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration46 :\n",
    "\n",
    "slice41 = 45\n",
    "\n",
    "gp46 = [train_regret_gp_1[slice41],\n",
    "       train_regret_gp_2[slice41],\n",
    "       train_regret_gp_3[slice41],\n",
    "       train_regret_gp_4[slice41],\n",
    "       train_regret_gp_5[slice41],\n",
    "       train_regret_gp_6[slice41],\n",
    "       train_regret_gp_7[slice41],\n",
    "       train_regret_gp_8[slice41],\n",
    "       train_regret_gp_9[slice41],\n",
    "       train_regret_gp_10[slice41],\n",
    "       train_regret_gp_11[slice41],\n",
    "       train_regret_gp_12[slice41],\n",
    "       train_regret_gp_13[slice41],\n",
    "       train_regret_gp_14[slice41],\n",
    "       train_regret_gp_15[slice41],\n",
    "       train_regret_gp_16[slice41],\n",
    "       train_regret_gp_17[slice41],\n",
    "       train_regret_gp_18[slice41],\n",
    "       train_regret_gp_19[slice41],\n",
    "       train_regret_gp_20[slice41]]\n",
    "\n",
    "stp46 = [train_regret_stp_df1_1[slice41],\n",
    "       train_regret_stp_df1_2[slice41],\n",
    "       train_regret_stp_df1_3[slice41],\n",
    "       train_regret_stp_df1_4[slice41],\n",
    "       train_regret_stp_df1_5[slice41],\n",
    "       train_regret_stp_df1_6[slice41],\n",
    "       train_regret_stp_df1_7[slice41],\n",
    "       train_regret_stp_df1_8[slice41],\n",
    "       train_regret_stp_df1_9[slice41],\n",
    "       train_regret_stp_df1_10[slice41],\n",
    "       train_regret_stp_df1_11[slice41],\n",
    "       train_regret_stp_df1_12[slice41],\n",
    "       train_regret_stp_df1_13[slice41],\n",
    "       train_regret_stp_df1_14[slice41],\n",
    "       train_regret_stp_df1_15[slice41],\n",
    "       train_regret_stp_df1_16[slice41],\n",
    "       train_regret_stp_df1_17[slice41],\n",
    "       train_regret_stp_df1_18[slice41],\n",
    "       train_regret_stp_df1_19[slice41],\n",
    "       train_regret_stp_df1_20[slice41]]\n",
    "\n",
    "gp46_results = pd.DataFrame(gp46).sort_values(by=[0], ascending=False)\n",
    "stp46_results = pd.DataFrame(stp46).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp46 = np.asarray(gp46_results[4:5][0])[0]\n",
    "median_gp46 = np.asarray(gp46_results[9:10][0])[0]\n",
    "upper_gp46 = np.asarray(gp46_results[14:15][0])[0]\n",
    "\n",
    "lower_stp46 = np.asarray(stp46_results[4:5][0])[0]\n",
    "median_stp46 = np.asarray(stp46_results[9:10][0])[0]\n",
    "upper_stp46 = np.asarray(stp46_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration56 :\n",
    "\n",
    "slice51 = 55\n",
    "\n",
    "gp56 = [train_regret_gp_1[slice51],\n",
    "       train_regret_gp_2[slice51],\n",
    "       train_regret_gp_3[slice51],\n",
    "       train_regret_gp_4[slice51],\n",
    "       train_regret_gp_5[slice51],\n",
    "       train_regret_gp_6[slice51],\n",
    "       train_regret_gp_7[slice51],\n",
    "       train_regret_gp_8[slice51],\n",
    "       train_regret_gp_9[slice51],\n",
    "       train_regret_gp_10[slice51],\n",
    "       train_regret_gp_11[slice51],\n",
    "       train_regret_gp_12[slice51],\n",
    "       train_regret_gp_13[slice51],\n",
    "       train_regret_gp_14[slice51],\n",
    "       train_regret_gp_15[slice51],\n",
    "       train_regret_gp_16[slice51],\n",
    "       train_regret_gp_17[slice51],\n",
    "       train_regret_gp_18[slice51],\n",
    "       train_regret_gp_19[slice51],\n",
    "       train_regret_gp_20[slice51]]\n",
    "\n",
    "stp56 = [train_regret_stp_df1_1[slice51],\n",
    "       train_regret_stp_df1_2[slice51],\n",
    "       train_regret_stp_df1_3[slice51],\n",
    "       train_regret_stp_df1_4[slice51],\n",
    "       train_regret_stp_df1_5[slice51],\n",
    "       train_regret_stp_df1_6[slice51],\n",
    "       train_regret_stp_df1_7[slice51],\n",
    "       train_regret_stp_df1_8[slice51],\n",
    "       train_regret_stp_df1_9[slice51],\n",
    "       train_regret_stp_df1_10[slice51],\n",
    "       train_regret_stp_df1_11[slice51],\n",
    "       train_regret_stp_df1_12[slice51],\n",
    "       train_regret_stp_df1_13[slice51],\n",
    "       train_regret_stp_df1_14[slice51],\n",
    "       train_regret_stp_df1_15[slice51],\n",
    "       train_regret_stp_df1_16[slice51],\n",
    "       train_regret_stp_df1_17[slice51],\n",
    "       train_regret_stp_df1_18[slice51],\n",
    "       train_regret_stp_df1_19[slice51],\n",
    "       train_regret_stp_df1_20[slice51]]\n",
    "\n",
    "gp56_results = pd.DataFrame(gp56).sort_values(by=[0], ascending=False)\n",
    "stp56_results = pd.DataFrame(stp56).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp56 = np.asarray(gp56_results[4:5][0])[0]\n",
    "median_gp56 = np.asarray(gp56_results[9:10][0])[0]\n",
    "upper_gp56 = np.asarray(gp56_results[14:15][0])[0]\n",
    "\n",
    "lower_stp56 = np.asarray(stp56_results[4:5][0])[0]\n",
    "median_stp56 = np.asarray(stp56_results[9:10][0])[0]\n",
    "upper_stp56 = np.asarray(stp56_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration66 :\n",
    "\n",
    "slice61 = 65\n",
    "\n",
    "gp66 = [train_regret_gp_1[slice61],\n",
    "       train_regret_gp_2[slice61],\n",
    "       train_regret_gp_3[slice61],\n",
    "       train_regret_gp_4[slice61],\n",
    "       train_regret_gp_5[slice61],\n",
    "       train_regret_gp_6[slice61],\n",
    "       train_regret_gp_7[slice61],\n",
    "       train_regret_gp_8[slice61],\n",
    "       train_regret_gp_9[slice61],\n",
    "       train_regret_gp_10[slice61],\n",
    "       train_regret_gp_11[slice61],\n",
    "       train_regret_gp_12[slice61],\n",
    "       train_regret_gp_13[slice61],\n",
    "       train_regret_gp_14[slice61],\n",
    "       train_regret_gp_15[slice61],\n",
    "       train_regret_gp_16[slice61],\n",
    "       train_regret_gp_17[slice61],\n",
    "       train_regret_gp_18[slice61],\n",
    "       train_regret_gp_19[slice61],\n",
    "       train_regret_gp_20[slice61]]\n",
    "\n",
    "stp66 = [train_regret_stp_df1_1[slice61],\n",
    "       train_regret_stp_df1_2[slice61],\n",
    "       train_regret_stp_df1_3[slice61],\n",
    "       train_regret_stp_df1_4[slice61],\n",
    "       train_regret_stp_df1_5[slice61],\n",
    "       train_regret_stp_df1_6[slice61],\n",
    "       train_regret_stp_df1_7[slice61],\n",
    "       train_regret_stp_df1_8[slice61],\n",
    "       train_regret_stp_df1_9[slice61],\n",
    "       train_regret_stp_df1_10[slice61],\n",
    "       train_regret_stp_df1_11[slice61],\n",
    "       train_regret_stp_df1_12[slice61],\n",
    "       train_regret_stp_df1_13[slice61],\n",
    "       train_regret_stp_df1_14[slice61],\n",
    "       train_regret_stp_df1_15[slice61],\n",
    "       train_regret_stp_df1_16[slice61],\n",
    "       train_regret_stp_df1_17[slice61],\n",
    "       train_regret_stp_df1_18[slice61],\n",
    "       train_regret_stp_df1_19[slice61],\n",
    "       train_regret_stp_df1_20[slice61]]\n",
    "\n",
    "gp66_results = pd.DataFrame(gp66).sort_values(by=[0], ascending=False)\n",
    "stp66_results = pd.DataFrame(stp66).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp66 = np.asarray(gp66_results[4:5][0])[0]\n",
    "median_gp66 = np.asarray(gp66_results[9:10][0])[0]\n",
    "upper_gp66 = np.asarray(gp66_results[14:15][0])[0]\n",
    "\n",
    "lower_stp66 = np.asarray(stp66_results[4:5][0])[0]\n",
    "median_stp66 = np.asarray(stp66_results[9:10][0])[0]\n",
    "upper_stp66 = np.asarray(stp66_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration76 :\n",
    "\n",
    "slice71 = 75\n",
    "\n",
    "gp76 = [train_regret_gp_1[slice71],\n",
    "       train_regret_gp_2[slice71],\n",
    "       train_regret_gp_3[slice71],\n",
    "       train_regret_gp_4[slice71],\n",
    "       train_regret_gp_5[slice71],\n",
    "       train_regret_gp_6[slice71],\n",
    "       train_regret_gp_7[slice71],\n",
    "       train_regret_gp_8[slice71],\n",
    "       train_regret_gp_9[slice71],\n",
    "       train_regret_gp_10[slice71],\n",
    "       train_regret_gp_11[slice71],\n",
    "       train_regret_gp_12[slice71],\n",
    "       train_regret_gp_13[slice71],\n",
    "       train_regret_gp_14[slice71],\n",
    "       train_regret_gp_15[slice71],\n",
    "       train_regret_gp_16[slice71],\n",
    "       train_regret_gp_17[slice71],\n",
    "       train_regret_gp_18[slice71],\n",
    "       train_regret_gp_19[slice71],\n",
    "       train_regret_gp_20[slice71]]\n",
    "\n",
    "stp76 = [train_regret_stp_df1_1[slice71],\n",
    "       train_regret_stp_df1_2[slice71],\n",
    "       train_regret_stp_df1_3[slice71],\n",
    "       train_regret_stp_df1_4[slice71],\n",
    "       train_regret_stp_df1_5[slice71],\n",
    "       train_regret_stp_df1_6[slice71],\n",
    "       train_regret_stp_df1_7[slice71],\n",
    "       train_regret_stp_df1_8[slice71],\n",
    "       train_regret_stp_df1_9[slice71],\n",
    "       train_regret_stp_df1_10[slice71],\n",
    "       train_regret_stp_df1_11[slice71],\n",
    "       train_regret_stp_df1_12[slice71],\n",
    "       train_regret_stp_df1_13[slice71],\n",
    "       train_regret_stp_df1_14[slice71],\n",
    "       train_regret_stp_df1_15[slice71],\n",
    "       train_regret_stp_df1_16[slice71],\n",
    "       train_regret_stp_df1_17[slice71],\n",
    "       train_regret_stp_df1_18[slice71],\n",
    "       train_regret_stp_df1_19[slice71],\n",
    "       train_regret_stp_df1_20[slice71]]\n",
    "\n",
    "gp76_results = pd.DataFrame(gp76).sort_values(by=[0], ascending=False)\n",
    "stp76_results = pd.DataFrame(stp76).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp76 = np.asarray(gp76_results[4:5][0])[0]\n",
    "median_gp76 = np.asarray(gp76_results[9:10][0])[0]\n",
    "upper_gp76 = np.asarray(gp76_results[14:15][0])[0]\n",
    "\n",
    "lower_stp76 = np.asarray(stp76_results[4:5][0])[0]\n",
    "median_stp76 = np.asarray(stp76_results[9:10][0])[0]\n",
    "upper_stp76 = np.asarray(stp76_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration86 :\n",
    "\n",
    "slice81 = 85\n",
    "\n",
    "gp86 = [train_regret_gp_1[slice81],\n",
    "       train_regret_gp_2[slice81],\n",
    "       train_regret_gp_3[slice81],\n",
    "       train_regret_gp_4[slice81],\n",
    "       train_regret_gp_5[slice81],\n",
    "       train_regret_gp_6[slice81],\n",
    "       train_regret_gp_7[slice81],\n",
    "       train_regret_gp_8[slice81],\n",
    "       train_regret_gp_9[slice81],\n",
    "       train_regret_gp_10[slice81],\n",
    "       train_regret_gp_11[slice81],\n",
    "       train_regret_gp_12[slice81],\n",
    "       train_regret_gp_13[slice81],\n",
    "       train_regret_gp_14[slice81],\n",
    "       train_regret_gp_15[slice81],\n",
    "       train_regret_gp_16[slice81],\n",
    "       train_regret_gp_17[slice81],\n",
    "       train_regret_gp_18[slice81],\n",
    "       train_regret_gp_19[slice81],\n",
    "       train_regret_gp_20[slice81]]\n",
    "\n",
    "stp86 = [train_regret_stp_df1_1[slice81],\n",
    "       train_regret_stp_df1_2[slice81],\n",
    "       train_regret_stp_df1_3[slice81],\n",
    "       train_regret_stp_df1_4[slice81],\n",
    "       train_regret_stp_df1_5[slice81],\n",
    "       train_regret_stp_df1_6[slice81],\n",
    "       train_regret_stp_df1_7[slice81],\n",
    "       train_regret_stp_df1_8[slice81],\n",
    "       train_regret_stp_df1_9[slice81],\n",
    "       train_regret_stp_df1_10[slice81],\n",
    "       train_regret_stp_df1_11[slice81],\n",
    "       train_regret_stp_df1_12[slice81],\n",
    "       train_regret_stp_df1_13[slice81],\n",
    "       train_regret_stp_df1_14[slice81],\n",
    "       train_regret_stp_df1_15[slice81],\n",
    "       train_regret_stp_df1_16[slice81],\n",
    "       train_regret_stp_df1_17[slice81],\n",
    "       train_regret_stp_df1_18[slice81],\n",
    "       train_regret_stp_df1_19[slice81],\n",
    "       train_regret_stp_df1_20[slice81]]\n",
    "\n",
    "gp86_results = pd.DataFrame(gp86).sort_values(by=[0], ascending=False)\n",
    "stp86_results = pd.DataFrame(stp86).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp86 = np.asarray(gp86_results[4:5][0])[0]\n",
    "median_gp86 = np.asarray(gp86_results[9:10][0])[0]\n",
    "upper_gp86 = np.asarray(gp86_results[14:15][0])[0]\n",
    "\n",
    "lower_stp86 = np.asarray(stp86_results[4:5][0])[0]\n",
    "median_stp86 = np.asarray(stp86_results[9:10][0])[0]\n",
    "upper_stp86 = np.asarray(stp86_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration96 :\n",
    "\n",
    "slice1 = 95\n",
    "\n",
    "gp96 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp96 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp96_results = pd.DataFrame(gp96).sort_values(by=[0], ascending=False)\n",
    "stp96_results = pd.DataFrame(stp96).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp96 = np.asarray(gp96_results[4:5][0])[0]\n",
    "median_gp96 = np.asarray(gp96_results[9:10][0])[0]\n",
    "upper_gp96 = np.asarray(gp96_results[14:15][0])[0]\n",
    "\n",
    "lower_stp96 = np.asarray(stp96_results[4:5][0])[0]\n",
    "median_stp96 = np.asarray(stp96_results[9:10][0])[0]\n",
    "upper_stp96 = np.asarray(stp96_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration7 :\n",
    "\n",
    "slice1 = 6\n",
    "\n",
    "gp7 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp7 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp7_results = pd.DataFrame(gp7).sort_values(by=[0], ascending=False)\n",
    "stp7_results = pd.DataFrame(stp7).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp7 = np.asarray(gp7_results[4:5][0])[0]\n",
    "median_gp7 = np.asarray(gp7_results[9:10][0])[0]\n",
    "upper_gp7 = np.asarray(gp7_results[14:15][0])[0]\n",
    "\n",
    "lower_stp7 = np.asarray(stp7_results[4:5][0])[0]\n",
    "median_stp7 = np.asarray(stp7_results[9:10][0])[0]\n",
    "upper_stp7 = np.asarray(stp7_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration17 :\n",
    "\n",
    "slice11 = 16\n",
    "\n",
    "gp17 = [train_regret_gp_1[slice11],\n",
    "       train_regret_gp_2[slice11],\n",
    "       train_regret_gp_3[slice11],\n",
    "       train_regret_gp_4[slice11],\n",
    "       train_regret_gp_5[slice11],\n",
    "       train_regret_gp_6[slice11],\n",
    "       train_regret_gp_7[slice11],\n",
    "       train_regret_gp_8[slice11],\n",
    "       train_regret_gp_9[slice11],\n",
    "       train_regret_gp_10[slice11],\n",
    "       train_regret_gp_11[slice11],\n",
    "       train_regret_gp_12[slice11],\n",
    "       train_regret_gp_13[slice11],\n",
    "       train_regret_gp_14[slice11],\n",
    "       train_regret_gp_15[slice11],\n",
    "       train_regret_gp_16[slice11],\n",
    "       train_regret_gp_17[slice11],\n",
    "       train_regret_gp_18[slice11],\n",
    "       train_regret_gp_19[slice11],\n",
    "       train_regret_gp_20[slice11]]\n",
    "\n",
    "stp17 = [train_regret_stp_df1_1[slice11],\n",
    "       train_regret_stp_df1_2[slice11],\n",
    "       train_regret_stp_df1_3[slice11],\n",
    "       train_regret_stp_df1_4[slice11],\n",
    "       train_regret_stp_df1_5[slice11],\n",
    "       train_regret_stp_df1_6[slice11],\n",
    "       train_regret_stp_df1_7[slice11],\n",
    "       train_regret_stp_df1_8[slice11],\n",
    "       train_regret_stp_df1_9[slice11],\n",
    "       train_regret_stp_df1_10[slice11],\n",
    "       train_regret_stp_df1_11[slice11],\n",
    "       train_regret_stp_df1_12[slice11],\n",
    "       train_regret_stp_df1_13[slice11],\n",
    "       train_regret_stp_df1_14[slice11],\n",
    "       train_regret_stp_df1_15[slice11],\n",
    "       train_regret_stp_df1_16[slice11],\n",
    "       train_regret_stp_df1_17[slice11],\n",
    "       train_regret_stp_df1_18[slice11],\n",
    "       train_regret_stp_df1_19[slice11],\n",
    "       train_regret_stp_df1_20[slice11]]\n",
    "\n",
    "gp17_results = pd.DataFrame(gp17).sort_values(by=[0], ascending=False)\n",
    "stp17_results = pd.DataFrame(stp17).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp17 = np.asarray(gp17_results[4:5][0])[0]\n",
    "median_gp17 = np.asarray(gp17_results[9:10][0])[0]\n",
    "upper_gp17 = np.asarray(gp17_results[14:15][0])[0]\n",
    "\n",
    "lower_stp17 = np.asarray(stp17_results[4:5][0])[0]\n",
    "median_stp17 = np.asarray(stp17_results[9:10][0])[0]\n",
    "upper_stp17 = np.asarray(stp17_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration27 :\n",
    "\n",
    "slice21 = 26\n",
    "\n",
    "gp27 = [train_regret_gp_1[slice21],\n",
    "       train_regret_gp_2[slice21],\n",
    "       train_regret_gp_3[slice21],\n",
    "       train_regret_gp_4[slice21],\n",
    "       train_regret_gp_5[slice21],\n",
    "       train_regret_gp_6[slice21],\n",
    "       train_regret_gp_7[slice21],\n",
    "       train_regret_gp_8[slice21],\n",
    "       train_regret_gp_9[slice21],\n",
    "       train_regret_gp_10[slice21],\n",
    "       train_regret_gp_11[slice21],\n",
    "       train_regret_gp_12[slice21],\n",
    "       train_regret_gp_13[slice21],\n",
    "       train_regret_gp_14[slice21],\n",
    "       train_regret_gp_15[slice21],\n",
    "       train_regret_gp_16[slice21],\n",
    "       train_regret_gp_17[slice21],\n",
    "       train_regret_gp_18[slice21],\n",
    "       train_regret_gp_19[slice21],\n",
    "       train_regret_gp_20[slice21]]\n",
    "\n",
    "stp27 = [train_regret_stp_df1_1[slice21],\n",
    "       train_regret_stp_df1_2[slice21],\n",
    "       train_regret_stp_df1_3[slice21],\n",
    "       train_regret_stp_df1_4[slice21],\n",
    "       train_regret_stp_df1_5[slice21],\n",
    "       train_regret_stp_df1_6[slice21],\n",
    "       train_regret_stp_df1_7[slice21],\n",
    "       train_regret_stp_df1_8[slice21],\n",
    "       train_regret_stp_df1_9[slice21],\n",
    "       train_regret_stp_df1_10[slice21],\n",
    "       train_regret_stp_df1_11[slice21],\n",
    "       train_regret_stp_df1_12[slice21],\n",
    "       train_regret_stp_df1_13[slice21],\n",
    "       train_regret_stp_df1_14[slice21],\n",
    "       train_regret_stp_df1_15[slice21],\n",
    "       train_regret_stp_df1_16[slice21],\n",
    "       train_regret_stp_df1_17[slice21],\n",
    "       train_regret_stp_df1_18[slice21],\n",
    "       train_regret_stp_df1_19[slice21],\n",
    "       train_regret_stp_df1_20[slice21]]\n",
    "\n",
    "gp27_results = pd.DataFrame(gp27).sort_values(by=[0], ascending=False)\n",
    "stp27_results = pd.DataFrame(stp27).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp27 = np.asarray(gp27_results[4:5][0])[0]\n",
    "median_gp27 = np.asarray(gp27_results[9:10][0])[0]\n",
    "upper_gp27 = np.asarray(gp27_results[14:15][0])[0]\n",
    "\n",
    "lower_stp27 = np.asarray(stp27_results[4:5][0])[0]\n",
    "median_stp27 = np.asarray(stp27_results[9:10][0])[0]\n",
    "upper_stp27 = np.asarray(stp27_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration37 :\n",
    "\n",
    "slice31 = 36\n",
    "\n",
    "gp37 = [train_regret_gp_1[slice31],\n",
    "       train_regret_gp_2[slice31],\n",
    "       train_regret_gp_3[slice31],\n",
    "       train_regret_gp_4[slice31],\n",
    "       train_regret_gp_5[slice31],\n",
    "       train_regret_gp_6[slice31],\n",
    "       train_regret_gp_7[slice31],\n",
    "       train_regret_gp_8[slice31],\n",
    "       train_regret_gp_9[slice31],\n",
    "       train_regret_gp_10[slice31],\n",
    "       train_regret_gp_11[slice31],\n",
    "       train_regret_gp_12[slice31],\n",
    "       train_regret_gp_13[slice31],\n",
    "       train_regret_gp_14[slice31],\n",
    "       train_regret_gp_15[slice31],\n",
    "       train_regret_gp_16[slice31],\n",
    "       train_regret_gp_17[slice31],\n",
    "       train_regret_gp_18[slice31],\n",
    "       train_regret_gp_19[slice31],\n",
    "       train_regret_gp_20[slice31]]\n",
    "\n",
    "stp37 = [train_regret_stp_df1_1[slice31],\n",
    "       train_regret_stp_df1_2[slice31],\n",
    "       train_regret_stp_df1_3[slice31],\n",
    "       train_regret_stp_df1_4[slice31],\n",
    "       train_regret_stp_df1_5[slice31],\n",
    "       train_regret_stp_df1_6[slice31],\n",
    "       train_regret_stp_df1_7[slice31],\n",
    "       train_regret_stp_df1_8[slice31],\n",
    "       train_regret_stp_df1_9[slice31],\n",
    "       train_regret_stp_df1_10[slice31],\n",
    "       train_regret_stp_df1_11[slice31],\n",
    "       train_regret_stp_df1_12[slice31],\n",
    "       train_regret_stp_df1_13[slice31],\n",
    "       train_regret_stp_df1_14[slice31],\n",
    "       train_regret_stp_df1_15[slice31],\n",
    "       train_regret_stp_df1_16[slice31],\n",
    "       train_regret_stp_df1_17[slice31],\n",
    "       train_regret_stp_df1_18[slice31],\n",
    "       train_regret_stp_df1_19[slice31],\n",
    "       train_regret_stp_df1_20[slice31]]\n",
    "\n",
    "gp37_results = pd.DataFrame(gp37).sort_values(by=[0], ascending=False)\n",
    "stp37_results = pd.DataFrame(stp37).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp37 = np.asarray(gp37_results[4:5][0])[0]\n",
    "median_gp37 = np.asarray(gp37_results[9:10][0])[0]\n",
    "upper_gp37 = np.asarray(gp37_results[14:15][0])[0]\n",
    "\n",
    "lower_stp37 = np.asarray(stp37_results[4:5][0])[0]\n",
    "median_stp37 = np.asarray(stp37_results[9:10][0])[0]\n",
    "upper_stp37 = np.asarray(stp37_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration47 :\n",
    "\n",
    "slice41 = 46\n",
    "\n",
    "gp47 = [train_regret_gp_1[slice41],\n",
    "       train_regret_gp_2[slice41],\n",
    "       train_regret_gp_3[slice41],\n",
    "       train_regret_gp_4[slice41],\n",
    "       train_regret_gp_5[slice41],\n",
    "       train_regret_gp_6[slice41],\n",
    "       train_regret_gp_7[slice41],\n",
    "       train_regret_gp_8[slice41],\n",
    "       train_regret_gp_9[slice41],\n",
    "       train_regret_gp_10[slice41],\n",
    "       train_regret_gp_11[slice41],\n",
    "       train_regret_gp_12[slice41],\n",
    "       train_regret_gp_13[slice41],\n",
    "       train_regret_gp_14[slice41],\n",
    "       train_regret_gp_15[slice41],\n",
    "       train_regret_gp_16[slice41],\n",
    "       train_regret_gp_17[slice41],\n",
    "       train_regret_gp_18[slice41],\n",
    "       train_regret_gp_19[slice41],\n",
    "       train_regret_gp_20[slice41]]\n",
    "\n",
    "stp47 = [train_regret_stp_df1_1[slice41],\n",
    "       train_regret_stp_df1_2[slice41],\n",
    "       train_regret_stp_df1_3[slice41],\n",
    "       train_regret_stp_df1_4[slice41],\n",
    "       train_regret_stp_df1_5[slice41],\n",
    "       train_regret_stp_df1_6[slice41],\n",
    "       train_regret_stp_df1_7[slice41],\n",
    "       train_regret_stp_df1_8[slice41],\n",
    "       train_regret_stp_df1_9[slice41],\n",
    "       train_regret_stp_df1_10[slice41],\n",
    "       train_regret_stp_df1_11[slice41],\n",
    "       train_regret_stp_df1_12[slice41],\n",
    "       train_regret_stp_df1_13[slice41],\n",
    "       train_regret_stp_df1_14[slice41],\n",
    "       train_regret_stp_df1_15[slice41],\n",
    "       train_regret_stp_df1_16[slice41],\n",
    "       train_regret_stp_df1_17[slice41],\n",
    "       train_regret_stp_df1_18[slice41],\n",
    "       train_regret_stp_df1_19[slice41],\n",
    "       train_regret_stp_df1_20[slice41]]\n",
    "\n",
    "gp47_results = pd.DataFrame(gp47).sort_values(by=[0], ascending=False)\n",
    "stp47_results = pd.DataFrame(stp47).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp47 = np.asarray(gp47_results[4:5][0])[0]\n",
    "median_gp47 = np.asarray(gp47_results[9:10][0])[0]\n",
    "upper_gp47 = np.asarray(gp47_results[14:15][0])[0]\n",
    "\n",
    "lower_stp47 = np.asarray(stp47_results[4:5][0])[0]\n",
    "median_stp47 = np.asarray(stp47_results[9:10][0])[0]\n",
    "upper_stp47 = np.asarray(stp47_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration57 :\n",
    "\n",
    "slice51 = 56\n",
    "\n",
    "gp57 = [train_regret_gp_1[slice51],\n",
    "       train_regret_gp_2[slice51],\n",
    "       train_regret_gp_3[slice51],\n",
    "       train_regret_gp_4[slice51],\n",
    "       train_regret_gp_5[slice51],\n",
    "       train_regret_gp_6[slice51],\n",
    "       train_regret_gp_7[slice51],\n",
    "       train_regret_gp_8[slice51],\n",
    "       train_regret_gp_9[slice51],\n",
    "       train_regret_gp_10[slice51],\n",
    "       train_regret_gp_11[slice51],\n",
    "       train_regret_gp_12[slice51],\n",
    "       train_regret_gp_13[slice51],\n",
    "       train_regret_gp_14[slice51],\n",
    "       train_regret_gp_15[slice51],\n",
    "       train_regret_gp_16[slice51],\n",
    "       train_regret_gp_17[slice51],\n",
    "       train_regret_gp_18[slice51],\n",
    "       train_regret_gp_19[slice51],\n",
    "       train_regret_gp_20[slice51]]\n",
    "\n",
    "stp57 = [train_regret_stp_df1_1[slice51],\n",
    "       train_regret_stp_df1_2[slice51],\n",
    "       train_regret_stp_df1_3[slice51],\n",
    "       train_regret_stp_df1_4[slice51],\n",
    "       train_regret_stp_df1_5[slice51],\n",
    "       train_regret_stp_df1_6[slice51],\n",
    "       train_regret_stp_df1_7[slice51],\n",
    "       train_regret_stp_df1_8[slice51],\n",
    "       train_regret_stp_df1_9[slice51],\n",
    "       train_regret_stp_df1_10[slice51],\n",
    "       train_regret_stp_df1_11[slice51],\n",
    "       train_regret_stp_df1_12[slice51],\n",
    "       train_regret_stp_df1_13[slice51],\n",
    "       train_regret_stp_df1_14[slice51],\n",
    "       train_regret_stp_df1_15[slice51],\n",
    "       train_regret_stp_df1_16[slice51],\n",
    "       train_regret_stp_df1_17[slice51],\n",
    "       train_regret_stp_df1_18[slice51],\n",
    "       train_regret_stp_df1_19[slice51],\n",
    "       train_regret_stp_df1_20[slice51]]\n",
    "\n",
    "gp57_results = pd.DataFrame(gp57).sort_values(by=[0], ascending=False)\n",
    "stp57_results = pd.DataFrame(stp57).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp57 = np.asarray(gp57_results[4:5][0])[0]\n",
    "median_gp57 = np.asarray(gp57_results[9:10][0])[0]\n",
    "upper_gp57 = np.asarray(gp57_results[14:15][0])[0]\n",
    "\n",
    "lower_stp57 = np.asarray(stp57_results[4:5][0])[0]\n",
    "median_stp57 = np.asarray(stp57_results[9:10][0])[0]\n",
    "upper_stp57 = np.asarray(stp57_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration67 :\n",
    "\n",
    "slice61 = 66\n",
    "\n",
    "gp67 = [train_regret_gp_1[slice61],\n",
    "       train_regret_gp_2[slice61],\n",
    "       train_regret_gp_3[slice61],\n",
    "       train_regret_gp_4[slice61],\n",
    "       train_regret_gp_5[slice61],\n",
    "       train_regret_gp_6[slice61],\n",
    "       train_regret_gp_7[slice61],\n",
    "       train_regret_gp_8[slice61],\n",
    "       train_regret_gp_9[slice61],\n",
    "       train_regret_gp_10[slice61],\n",
    "       train_regret_gp_11[slice61],\n",
    "       train_regret_gp_12[slice61],\n",
    "       train_regret_gp_13[slice61],\n",
    "       train_regret_gp_14[slice61],\n",
    "       train_regret_gp_15[slice61],\n",
    "       train_regret_gp_16[slice61],\n",
    "       train_regret_gp_17[slice61],\n",
    "       train_regret_gp_18[slice61],\n",
    "       train_regret_gp_19[slice61],\n",
    "       train_regret_gp_20[slice61]]\n",
    "\n",
    "stp67 = [train_regret_stp_df1_1[slice61],\n",
    "       train_regret_stp_df1_2[slice61],\n",
    "       train_regret_stp_df1_3[slice61],\n",
    "       train_regret_stp_df1_4[slice61],\n",
    "       train_regret_stp_df1_5[slice61],\n",
    "       train_regret_stp_df1_6[slice61],\n",
    "       train_regret_stp_df1_7[slice61],\n",
    "       train_regret_stp_df1_8[slice61],\n",
    "       train_regret_stp_df1_9[slice61],\n",
    "       train_regret_stp_df1_10[slice61],\n",
    "       train_regret_stp_df1_11[slice61],\n",
    "       train_regret_stp_df1_12[slice61],\n",
    "       train_regret_stp_df1_13[slice61],\n",
    "       train_regret_stp_df1_14[slice61],\n",
    "       train_regret_stp_df1_15[slice61],\n",
    "       train_regret_stp_df1_16[slice61],\n",
    "       train_regret_stp_df1_17[slice61],\n",
    "       train_regret_stp_df1_18[slice61],\n",
    "       train_regret_stp_df1_19[slice61],\n",
    "       train_regret_stp_df1_20[slice61]]\n",
    "\n",
    "gp67_results = pd.DataFrame(gp67).sort_values(by=[0], ascending=False)\n",
    "stp67_results = pd.DataFrame(stp67).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp67 = np.asarray(gp67_results[4:5][0])[0]\n",
    "median_gp67 = np.asarray(gp67_results[9:10][0])[0]\n",
    "upper_gp67 = np.asarray(gp67_results[14:15][0])[0]\n",
    "\n",
    "lower_stp67 = np.asarray(stp67_results[4:5][0])[0]\n",
    "median_stp67 = np.asarray(stp67_results[9:10][0])[0]\n",
    "upper_stp67 = np.asarray(stp67_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration77 :\n",
    "\n",
    "slice71 = 76\n",
    "\n",
    "gp77 = [train_regret_gp_1[slice71],\n",
    "       train_regret_gp_2[slice71],\n",
    "       train_regret_gp_3[slice71],\n",
    "       train_regret_gp_4[slice71],\n",
    "       train_regret_gp_5[slice71],\n",
    "       train_regret_gp_6[slice71],\n",
    "       train_regret_gp_7[slice71],\n",
    "       train_regret_gp_8[slice71],\n",
    "       train_regret_gp_9[slice71],\n",
    "       train_regret_gp_10[slice71],\n",
    "       train_regret_gp_11[slice71],\n",
    "       train_regret_gp_12[slice71],\n",
    "       train_regret_gp_13[slice71],\n",
    "       train_regret_gp_14[slice71],\n",
    "       train_regret_gp_15[slice71],\n",
    "       train_regret_gp_16[slice71],\n",
    "       train_regret_gp_17[slice71],\n",
    "       train_regret_gp_18[slice71],\n",
    "       train_regret_gp_19[slice71],\n",
    "       train_regret_gp_20[slice71]]\n",
    "\n",
    "stp77 = [train_regret_stp_df1_1[slice71],\n",
    "       train_regret_stp_df1_2[slice71],\n",
    "       train_regret_stp_df1_3[slice71],\n",
    "       train_regret_stp_df1_4[slice71],\n",
    "       train_regret_stp_df1_5[slice71],\n",
    "       train_regret_stp_df1_6[slice71],\n",
    "       train_regret_stp_df1_7[slice71],\n",
    "       train_regret_stp_df1_8[slice71],\n",
    "       train_regret_stp_df1_9[slice71],\n",
    "       train_regret_stp_df1_10[slice71],\n",
    "       train_regret_stp_df1_11[slice71],\n",
    "       train_regret_stp_df1_12[slice71],\n",
    "       train_regret_stp_df1_13[slice71],\n",
    "       train_regret_stp_df1_14[slice71],\n",
    "       train_regret_stp_df1_15[slice71],\n",
    "       train_regret_stp_df1_16[slice71],\n",
    "       train_regret_stp_df1_17[slice71],\n",
    "       train_regret_stp_df1_18[slice71],\n",
    "       train_regret_stp_df1_19[slice71],\n",
    "       train_regret_stp_df1_20[slice71]]\n",
    "\n",
    "gp77_results = pd.DataFrame(gp77).sort_values(by=[0], ascending=False)\n",
    "stp77_results = pd.DataFrame(stp77).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp77 = np.asarray(gp77_results[4:5][0])[0]\n",
    "median_gp77 = np.asarray(gp77_results[9:10][0])[0]\n",
    "upper_gp77 = np.asarray(gp77_results[14:15][0])[0]\n",
    "\n",
    "lower_stp77 = np.asarray(stp77_results[4:5][0])[0]\n",
    "median_stp77 = np.asarray(stp77_results[9:10][0])[0]\n",
    "upper_stp77 = np.asarray(stp77_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration87 :\n",
    "\n",
    "slice81 = 86\n",
    "\n",
    "gp87 = [train_regret_gp_1[slice81],\n",
    "       train_regret_gp_2[slice81],\n",
    "       train_regret_gp_3[slice81],\n",
    "       train_regret_gp_4[slice81],\n",
    "       train_regret_gp_5[slice81],\n",
    "       train_regret_gp_6[slice81],\n",
    "       train_regret_gp_7[slice81],\n",
    "       train_regret_gp_8[slice81],\n",
    "       train_regret_gp_9[slice81],\n",
    "       train_regret_gp_10[slice81],\n",
    "       train_regret_gp_11[slice81],\n",
    "       train_regret_gp_12[slice81],\n",
    "       train_regret_gp_13[slice81],\n",
    "       train_regret_gp_14[slice81],\n",
    "       train_regret_gp_15[slice81],\n",
    "       train_regret_gp_16[slice81],\n",
    "       train_regret_gp_17[slice81],\n",
    "       train_regret_gp_18[slice81],\n",
    "       train_regret_gp_19[slice81],\n",
    "       train_regret_gp_20[slice81]]\n",
    "\n",
    "stp87 = [train_regret_stp_df1_1[slice81],\n",
    "       train_regret_stp_df1_2[slice81],\n",
    "       train_regret_stp_df1_3[slice81],\n",
    "       train_regret_stp_df1_4[slice81],\n",
    "       train_regret_stp_df1_5[slice81],\n",
    "       train_regret_stp_df1_6[slice81],\n",
    "       train_regret_stp_df1_7[slice81],\n",
    "       train_regret_stp_df1_8[slice81],\n",
    "       train_regret_stp_df1_9[slice81],\n",
    "       train_regret_stp_df1_10[slice81],\n",
    "       train_regret_stp_df1_11[slice81],\n",
    "       train_regret_stp_df1_12[slice81],\n",
    "       train_regret_stp_df1_13[slice81],\n",
    "       train_regret_stp_df1_14[slice81],\n",
    "       train_regret_stp_df1_15[slice81],\n",
    "       train_regret_stp_df1_16[slice81],\n",
    "       train_regret_stp_df1_17[slice81],\n",
    "       train_regret_stp_df1_18[slice81],\n",
    "       train_regret_stp_df1_19[slice81],\n",
    "       train_regret_stp_df1_20[slice81]]\n",
    "\n",
    "gp87_results = pd.DataFrame(gp87).sort_values(by=[0], ascending=False)\n",
    "stp87_results = pd.DataFrame(stp87).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp87 = np.asarray(gp87_results[4:5][0])[0]\n",
    "median_gp87 = np.asarray(gp87_results[9:10][0])[0]\n",
    "upper_gp87 = np.asarray(gp87_results[14:15][0])[0]\n",
    "\n",
    "lower_stp87 = np.asarray(stp87_results[4:5][0])[0]\n",
    "median_stp87 = np.asarray(stp87_results[9:10][0])[0]\n",
    "upper_stp87 = np.asarray(stp87_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration97 :\n",
    "\n",
    "slice1 = 96\n",
    "\n",
    "gp97 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp97 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp97_results = pd.DataFrame(gp97).sort_values(by=[0], ascending=False)\n",
    "stp97_results = pd.DataFrame(stp97).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp97 = np.asarray(gp97_results[4:5][0])[0]\n",
    "median_gp97 = np.asarray(gp97_results[9:10][0])[0]\n",
    "upper_gp97 = np.asarray(gp97_results[14:15][0])[0]\n",
    "\n",
    "lower_stp97 = np.asarray(stp97_results[4:5][0])[0]\n",
    "median_stp97 = np.asarray(stp97_results[9:10][0])[0]\n",
    "upper_stp97 = np.asarray(stp97_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration8 :\n",
    "\n",
    "slice1 = 7\n",
    "\n",
    "gp8 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp8 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp8_results = pd.DataFrame(gp8).sort_values(by=[0], ascending=False)\n",
    "stp8_results = pd.DataFrame(stp8).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp8 = np.asarray(gp8_results[4:5][0])[0]\n",
    "median_gp8 = np.asarray(gp8_results[9:10][0])[0]\n",
    "upper_gp8 = np.asarray(gp8_results[14:15][0])[0]\n",
    "\n",
    "lower_stp8 = np.asarray(stp8_results[4:5][0])[0]\n",
    "median_stp8 = np.asarray(stp8_results[9:10][0])[0]\n",
    "upper_stp8 = np.asarray(stp8_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration18 :\n",
    "\n",
    "slice11 = 17\n",
    "\n",
    "gp18 = [train_regret_gp_1[slice11],\n",
    "       train_regret_gp_2[slice11],\n",
    "       train_regret_gp_3[slice11],\n",
    "       train_regret_gp_4[slice11],\n",
    "       train_regret_gp_5[slice11],\n",
    "       train_regret_gp_6[slice11],\n",
    "       train_regret_gp_7[slice11],\n",
    "       train_regret_gp_8[slice11],\n",
    "       train_regret_gp_9[slice11],\n",
    "       train_regret_gp_10[slice11],\n",
    "       train_regret_gp_11[slice11],\n",
    "       train_regret_gp_12[slice11],\n",
    "       train_regret_gp_13[slice11],\n",
    "       train_regret_gp_14[slice11],\n",
    "       train_regret_gp_15[slice11],\n",
    "       train_regret_gp_16[slice11],\n",
    "       train_regret_gp_17[slice11],\n",
    "       train_regret_gp_18[slice11],\n",
    "       train_regret_gp_19[slice11],\n",
    "       train_regret_gp_20[slice11]]\n",
    "\n",
    "stp18 = [train_regret_stp_df1_1[slice11],\n",
    "       train_regret_stp_df1_2[slice11],\n",
    "       train_regret_stp_df1_3[slice11],\n",
    "       train_regret_stp_df1_4[slice11],\n",
    "       train_regret_stp_df1_5[slice11],\n",
    "       train_regret_stp_df1_6[slice11],\n",
    "       train_regret_stp_df1_7[slice11],\n",
    "       train_regret_stp_df1_8[slice11],\n",
    "       train_regret_stp_df1_9[slice11],\n",
    "       train_regret_stp_df1_10[slice11],\n",
    "       train_regret_stp_df1_11[slice11],\n",
    "       train_regret_stp_df1_12[slice11],\n",
    "       train_regret_stp_df1_13[slice11],\n",
    "       train_regret_stp_df1_14[slice11],\n",
    "       train_regret_stp_df1_15[slice11],\n",
    "       train_regret_stp_df1_16[slice11],\n",
    "       train_regret_stp_df1_17[slice11],\n",
    "       train_regret_stp_df1_18[slice11],\n",
    "       train_regret_stp_df1_19[slice11],\n",
    "       train_regret_stp_df1_20[slice11]]\n",
    "\n",
    "gp18_results = pd.DataFrame(gp18).sort_values(by=[0], ascending=False)\n",
    "stp18_results = pd.DataFrame(stp18).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp18 = np.asarray(gp18_results[4:5][0])[0]\n",
    "median_gp18 = np.asarray(gp18_results[9:10][0])[0]\n",
    "upper_gp18 = np.asarray(gp18_results[14:15][0])[0]\n",
    "\n",
    "lower_stp18 = np.asarray(stp18_results[4:5][0])[0]\n",
    "median_stp18 = np.asarray(stp18_results[9:10][0])[0]\n",
    "upper_stp18 = np.asarray(stp18_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration28 :\n",
    "\n",
    "slice21 = 27\n",
    "\n",
    "gp28 = [train_regret_gp_1[slice21],\n",
    "       train_regret_gp_2[slice21],\n",
    "       train_regret_gp_3[slice21],\n",
    "       train_regret_gp_4[slice21],\n",
    "       train_regret_gp_5[slice21],\n",
    "       train_regret_gp_6[slice21],\n",
    "       train_regret_gp_7[slice21],\n",
    "       train_regret_gp_8[slice21],\n",
    "       train_regret_gp_9[slice21],\n",
    "       train_regret_gp_10[slice21],\n",
    "       train_regret_gp_11[slice21],\n",
    "       train_regret_gp_12[slice21],\n",
    "       train_regret_gp_13[slice21],\n",
    "       train_regret_gp_14[slice21],\n",
    "       train_regret_gp_15[slice21],\n",
    "       train_regret_gp_16[slice21],\n",
    "       train_regret_gp_17[slice21],\n",
    "       train_regret_gp_18[slice21],\n",
    "       train_regret_gp_19[slice21],\n",
    "       train_regret_gp_20[slice21]]\n",
    "\n",
    "stp28 = [train_regret_stp_df1_1[slice21],\n",
    "       train_regret_stp_df1_2[slice21],\n",
    "       train_regret_stp_df1_3[slice21],\n",
    "       train_regret_stp_df1_4[slice21],\n",
    "       train_regret_stp_df1_5[slice21],\n",
    "       train_regret_stp_df1_6[slice21],\n",
    "       train_regret_stp_df1_7[slice21],\n",
    "       train_regret_stp_df1_8[slice21],\n",
    "       train_regret_stp_df1_9[slice21],\n",
    "       train_regret_stp_df1_10[slice21],\n",
    "       train_regret_stp_df1_11[slice21],\n",
    "       train_regret_stp_df1_12[slice21],\n",
    "       train_regret_stp_df1_13[slice21],\n",
    "       train_regret_stp_df1_14[slice21],\n",
    "       train_regret_stp_df1_15[slice21],\n",
    "       train_regret_stp_df1_16[slice21],\n",
    "       train_regret_stp_df1_17[slice21],\n",
    "       train_regret_stp_df1_18[slice21],\n",
    "       train_regret_stp_df1_19[slice21],\n",
    "       train_regret_stp_df1_20[slice21]]\n",
    "\n",
    "gp28_results = pd.DataFrame(gp28).sort_values(by=[0], ascending=False)\n",
    "stp28_results = pd.DataFrame(stp28).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp28 = np.asarray(gp28_results[4:5][0])[0]\n",
    "median_gp28 = np.asarray(gp28_results[9:10][0])[0]\n",
    "upper_gp28 = np.asarray(gp28_results[14:15][0])[0]\n",
    "\n",
    "lower_stp28 = np.asarray(stp28_results[4:5][0])[0]\n",
    "median_stp28 = np.asarray(stp28_results[9:10][0])[0]\n",
    "upper_stp28 = np.asarray(stp28_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration38 :\n",
    "\n",
    "slice31 = 37\n",
    "\n",
    "gp38 = [train_regret_gp_1[slice31],\n",
    "       train_regret_gp_2[slice31],\n",
    "       train_regret_gp_3[slice31],\n",
    "       train_regret_gp_4[slice31],\n",
    "       train_regret_gp_5[slice31],\n",
    "       train_regret_gp_6[slice31],\n",
    "       train_regret_gp_7[slice31],\n",
    "       train_regret_gp_8[slice31],\n",
    "       train_regret_gp_9[slice31],\n",
    "       train_regret_gp_10[slice31],\n",
    "       train_regret_gp_11[slice31],\n",
    "       train_regret_gp_12[slice31],\n",
    "       train_regret_gp_13[slice31],\n",
    "       train_regret_gp_14[slice31],\n",
    "       train_regret_gp_15[slice31],\n",
    "       train_regret_gp_16[slice31],\n",
    "       train_regret_gp_17[slice31],\n",
    "       train_regret_gp_18[slice31],\n",
    "       train_regret_gp_19[slice31],\n",
    "       train_regret_gp_20[slice31]]\n",
    "\n",
    "stp38 = [train_regret_stp_df1_1[slice31],\n",
    "       train_regret_stp_df1_2[slice31],\n",
    "       train_regret_stp_df1_3[slice31],\n",
    "       train_regret_stp_df1_4[slice31],\n",
    "       train_regret_stp_df1_5[slice31],\n",
    "       train_regret_stp_df1_6[slice31],\n",
    "       train_regret_stp_df1_7[slice31],\n",
    "       train_regret_stp_df1_8[slice31],\n",
    "       train_regret_stp_df1_9[slice31],\n",
    "       train_regret_stp_df1_10[slice31],\n",
    "       train_regret_stp_df1_11[slice31],\n",
    "       train_regret_stp_df1_12[slice31],\n",
    "       train_regret_stp_df1_13[slice31],\n",
    "       train_regret_stp_df1_14[slice31],\n",
    "       train_regret_stp_df1_15[slice31],\n",
    "       train_regret_stp_df1_16[slice31],\n",
    "       train_regret_stp_df1_17[slice31],\n",
    "       train_regret_stp_df1_18[slice31],\n",
    "       train_regret_stp_df1_19[slice31],\n",
    "       train_regret_stp_df1_20[slice31]]\n",
    "\n",
    "gp38_results = pd.DataFrame(gp38).sort_values(by=[0], ascending=False)\n",
    "stp38_results = pd.DataFrame(stp38).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp38 = np.asarray(gp38_results[4:5][0])[0]\n",
    "median_gp38 = np.asarray(gp38_results[9:10][0])[0]\n",
    "upper_gp38 = np.asarray(gp38_results[14:15][0])[0]\n",
    "\n",
    "lower_stp38 = np.asarray(stp38_results[4:5][0])[0]\n",
    "median_stp38 = np.asarray(stp38_results[9:10][0])[0]\n",
    "upper_stp38 = np.asarray(stp38_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration48 :\n",
    "\n",
    "slice41 = 47\n",
    "\n",
    "gp48 = [train_regret_gp_1[slice41],\n",
    "       train_regret_gp_2[slice41],\n",
    "       train_regret_gp_3[slice41],\n",
    "       train_regret_gp_4[slice41],\n",
    "       train_regret_gp_5[slice41],\n",
    "       train_regret_gp_6[slice41],\n",
    "       train_regret_gp_7[slice41],\n",
    "       train_regret_gp_8[slice41],\n",
    "       train_regret_gp_9[slice41],\n",
    "       train_regret_gp_10[slice41],\n",
    "       train_regret_gp_11[slice41],\n",
    "       train_regret_gp_12[slice41],\n",
    "       train_regret_gp_13[slice41],\n",
    "       train_regret_gp_14[slice41],\n",
    "       train_regret_gp_15[slice41],\n",
    "       train_regret_gp_16[slice41],\n",
    "       train_regret_gp_17[slice41],\n",
    "       train_regret_gp_18[slice41],\n",
    "       train_regret_gp_19[slice41],\n",
    "       train_regret_gp_20[slice41]]\n",
    "\n",
    "stp48 = [train_regret_stp_df1_1[slice41],\n",
    "       train_regret_stp_df1_2[slice41],\n",
    "       train_regret_stp_df1_3[slice41],\n",
    "       train_regret_stp_df1_4[slice41],\n",
    "       train_regret_stp_df1_5[slice41],\n",
    "       train_regret_stp_df1_6[slice41],\n",
    "       train_regret_stp_df1_7[slice41],\n",
    "       train_regret_stp_df1_8[slice41],\n",
    "       train_regret_stp_df1_9[slice41],\n",
    "       train_regret_stp_df1_10[slice41],\n",
    "       train_regret_stp_df1_11[slice41],\n",
    "       train_regret_stp_df1_12[slice41],\n",
    "       train_regret_stp_df1_13[slice41],\n",
    "       train_regret_stp_df1_14[slice41],\n",
    "       train_regret_stp_df1_15[slice41],\n",
    "       train_regret_stp_df1_16[slice41],\n",
    "       train_regret_stp_df1_17[slice41],\n",
    "       train_regret_stp_df1_18[slice41],\n",
    "       train_regret_stp_df1_19[slice41],\n",
    "       train_regret_stp_df1_20[slice41]]\n",
    "\n",
    "gp48_results = pd.DataFrame(gp48).sort_values(by=[0], ascending=False)\n",
    "stp48_results = pd.DataFrame(stp48).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp48 = np.asarray(gp48_results[4:5][0])[0]\n",
    "median_gp48 = np.asarray(gp48_results[9:10][0])[0]\n",
    "upper_gp48 = np.asarray(gp48_results[14:15][0])[0]\n",
    "\n",
    "lower_stp48 = np.asarray(stp48_results[4:5][0])[0]\n",
    "median_stp48 = np.asarray(stp48_results[9:10][0])[0]\n",
    "upper_stp48 = np.asarray(stp48_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration58 :\n",
    "\n",
    "slice51 = 57\n",
    "\n",
    "gp58 = [train_regret_gp_1[slice51],\n",
    "       train_regret_gp_2[slice51],\n",
    "       train_regret_gp_3[slice51],\n",
    "       train_regret_gp_4[slice51],\n",
    "       train_regret_gp_5[slice51],\n",
    "       train_regret_gp_6[slice51],\n",
    "       train_regret_gp_7[slice51],\n",
    "       train_regret_gp_8[slice51],\n",
    "       train_regret_gp_9[slice51],\n",
    "       train_regret_gp_10[slice51],\n",
    "       train_regret_gp_11[slice51],\n",
    "       train_regret_gp_12[slice51],\n",
    "       train_regret_gp_13[slice51],\n",
    "       train_regret_gp_14[slice51],\n",
    "       train_regret_gp_15[slice51],\n",
    "       train_regret_gp_16[slice51],\n",
    "       train_regret_gp_17[slice51],\n",
    "       train_regret_gp_18[slice51],\n",
    "       train_regret_gp_19[slice51],\n",
    "       train_regret_gp_20[slice51]]\n",
    "\n",
    "stp58 = [train_regret_stp_df1_1[slice51],\n",
    "       train_regret_stp_df1_2[slice51],\n",
    "       train_regret_stp_df1_3[slice51],\n",
    "       train_regret_stp_df1_4[slice51],\n",
    "       train_regret_stp_df1_5[slice51],\n",
    "       train_regret_stp_df1_6[slice51],\n",
    "       train_regret_stp_df1_7[slice51],\n",
    "       train_regret_stp_df1_8[slice51],\n",
    "       train_regret_stp_df1_9[slice51],\n",
    "       train_regret_stp_df1_10[slice51],\n",
    "       train_regret_stp_df1_11[slice51],\n",
    "       train_regret_stp_df1_12[slice51],\n",
    "       train_regret_stp_df1_13[slice51],\n",
    "       train_regret_stp_df1_14[slice51],\n",
    "       train_regret_stp_df1_15[slice51],\n",
    "       train_regret_stp_df1_16[slice51],\n",
    "       train_regret_stp_df1_17[slice51],\n",
    "       train_regret_stp_df1_18[slice51],\n",
    "       train_regret_stp_df1_19[slice51],\n",
    "       train_regret_stp_df1_20[slice51]]\n",
    "\n",
    "gp58_results = pd.DataFrame(gp58).sort_values(by=[0], ascending=False)\n",
    "stp58_results = pd.DataFrame(stp58).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp58 = np.asarray(gp58_results[4:5][0])[0]\n",
    "median_gp58 = np.asarray(gp58_results[9:10][0])[0]\n",
    "upper_gp58 = np.asarray(gp58_results[14:15][0])[0]\n",
    "\n",
    "lower_stp58 = np.asarray(stp58_results[4:5][0])[0]\n",
    "median_stp58 = np.asarray(stp58_results[9:10][0])[0]\n",
    "upper_stp58 = np.asarray(stp58_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration68 :\n",
    "\n",
    "slice61 = 67\n",
    "\n",
    "gp68 = [train_regret_gp_1[slice61],\n",
    "       train_regret_gp_2[slice61],\n",
    "       train_regret_gp_3[slice61],\n",
    "       train_regret_gp_4[slice61],\n",
    "       train_regret_gp_5[slice61],\n",
    "       train_regret_gp_6[slice61],\n",
    "       train_regret_gp_7[slice61],\n",
    "       train_regret_gp_8[slice61],\n",
    "       train_regret_gp_9[slice61],\n",
    "       train_regret_gp_10[slice61],\n",
    "       train_regret_gp_11[slice61],\n",
    "       train_regret_gp_12[slice61],\n",
    "       train_regret_gp_13[slice61],\n",
    "       train_regret_gp_14[slice61],\n",
    "       train_regret_gp_15[slice61],\n",
    "       train_regret_gp_16[slice61],\n",
    "       train_regret_gp_17[slice61],\n",
    "       train_regret_gp_18[slice61],\n",
    "       train_regret_gp_19[slice61],\n",
    "       train_regret_gp_20[slice61]]\n",
    "\n",
    "stp68 = [train_regret_stp_df1_1[slice61],\n",
    "       train_regret_stp_df1_2[slice61],\n",
    "       train_regret_stp_df1_3[slice61],\n",
    "       train_regret_stp_df1_4[slice61],\n",
    "       train_regret_stp_df1_5[slice61],\n",
    "       train_regret_stp_df1_6[slice61],\n",
    "       train_regret_stp_df1_7[slice61],\n",
    "       train_regret_stp_df1_8[slice61],\n",
    "       train_regret_stp_df1_9[slice61],\n",
    "       train_regret_stp_df1_10[slice61],\n",
    "       train_regret_stp_df1_11[slice61],\n",
    "       train_regret_stp_df1_12[slice61],\n",
    "       train_regret_stp_df1_13[slice61],\n",
    "       train_regret_stp_df1_14[slice61],\n",
    "       train_regret_stp_df1_15[slice61],\n",
    "       train_regret_stp_df1_16[slice61],\n",
    "       train_regret_stp_df1_17[slice61],\n",
    "       train_regret_stp_df1_18[slice61],\n",
    "       train_regret_stp_df1_19[slice61],\n",
    "       train_regret_stp_df1_20[slice61]]\n",
    "\n",
    "gp68_results = pd.DataFrame(gp68).sort_values(by=[0], ascending=False)\n",
    "stp68_results = pd.DataFrame(stp68).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp68 = np.asarray(gp68_results[4:5][0])[0]\n",
    "median_gp68 = np.asarray(gp68_results[9:10][0])[0]\n",
    "upper_gp68 = np.asarray(gp68_results[14:15][0])[0]\n",
    "\n",
    "lower_stp68 = np.asarray(stp68_results[4:5][0])[0]\n",
    "median_stp68 = np.asarray(stp68_results[9:10][0])[0]\n",
    "upper_stp68 = np.asarray(stp68_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration78 :\n",
    "\n",
    "slice71 = 77\n",
    "\n",
    "gp78 = [train_regret_gp_1[slice71],\n",
    "       train_regret_gp_2[slice71],\n",
    "       train_regret_gp_3[slice71],\n",
    "       train_regret_gp_4[slice71],\n",
    "       train_regret_gp_5[slice71],\n",
    "       train_regret_gp_6[slice71],\n",
    "       train_regret_gp_7[slice71],\n",
    "       train_regret_gp_8[slice71],\n",
    "       train_regret_gp_9[slice71],\n",
    "       train_regret_gp_10[slice71],\n",
    "       train_regret_gp_11[slice71],\n",
    "       train_regret_gp_12[slice71],\n",
    "       train_regret_gp_13[slice71],\n",
    "       train_regret_gp_14[slice71],\n",
    "       train_regret_gp_15[slice71],\n",
    "       train_regret_gp_16[slice71],\n",
    "       train_regret_gp_17[slice71],\n",
    "       train_regret_gp_18[slice71],\n",
    "       train_regret_gp_19[slice71],\n",
    "       train_regret_gp_20[slice71]]\n",
    "\n",
    "stp78 = [train_regret_stp_df1_1[slice71],\n",
    "       train_regret_stp_df1_2[slice71],\n",
    "       train_regret_stp_df1_3[slice71],\n",
    "       train_regret_stp_df1_4[slice71],\n",
    "       train_regret_stp_df1_5[slice71],\n",
    "       train_regret_stp_df1_6[slice71],\n",
    "       train_regret_stp_df1_7[slice71],\n",
    "       train_regret_stp_df1_8[slice71],\n",
    "       train_regret_stp_df1_9[slice71],\n",
    "       train_regret_stp_df1_10[slice71],\n",
    "       train_regret_stp_df1_11[slice71],\n",
    "       train_regret_stp_df1_12[slice71],\n",
    "       train_regret_stp_df1_13[slice71],\n",
    "       train_regret_stp_df1_14[slice71],\n",
    "       train_regret_stp_df1_15[slice71],\n",
    "       train_regret_stp_df1_16[slice71],\n",
    "       train_regret_stp_df1_17[slice71],\n",
    "       train_regret_stp_df1_18[slice71],\n",
    "       train_regret_stp_df1_19[slice71],\n",
    "       train_regret_stp_df1_20[slice71]]\n",
    "\n",
    "gp78_results = pd.DataFrame(gp78).sort_values(by=[0], ascending=False)\n",
    "stp78_results = pd.DataFrame(stp78).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp78 = np.asarray(gp78_results[4:5][0])[0]\n",
    "median_gp78 = np.asarray(gp78_results[9:10][0])[0]\n",
    "upper_gp78 = np.asarray(gp78_results[14:15][0])[0]\n",
    "\n",
    "lower_stp78 = np.asarray(stp78_results[4:5][0])[0]\n",
    "median_stp78 = np.asarray(stp78_results[9:10][0])[0]\n",
    "upper_stp78 = np.asarray(stp78_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration88 :\n",
    "\n",
    "slice81 = 87\n",
    "\n",
    "gp88 = [train_regret_gp_1[slice81],\n",
    "       train_regret_gp_2[slice81],\n",
    "       train_regret_gp_3[slice81],\n",
    "       train_regret_gp_4[slice81],\n",
    "       train_regret_gp_5[slice81],\n",
    "       train_regret_gp_6[slice81],\n",
    "       train_regret_gp_7[slice81],\n",
    "       train_regret_gp_8[slice81],\n",
    "       train_regret_gp_9[slice81],\n",
    "       train_regret_gp_10[slice81],\n",
    "       train_regret_gp_11[slice81],\n",
    "       train_regret_gp_12[slice81],\n",
    "       train_regret_gp_13[slice81],\n",
    "       train_regret_gp_14[slice81],\n",
    "       train_regret_gp_15[slice81],\n",
    "       train_regret_gp_16[slice81],\n",
    "       train_regret_gp_17[slice81],\n",
    "       train_regret_gp_18[slice81],\n",
    "       train_regret_gp_19[slice81],\n",
    "       train_regret_gp_20[slice81]]\n",
    "\n",
    "stp88 = [train_regret_stp_df1_1[slice81],\n",
    "       train_regret_stp_df1_2[slice81],\n",
    "       train_regret_stp_df1_3[slice81],\n",
    "       train_regret_stp_df1_4[slice81],\n",
    "       train_regret_stp_df1_5[slice81],\n",
    "       train_regret_stp_df1_6[slice81],\n",
    "       train_regret_stp_df1_7[slice81],\n",
    "       train_regret_stp_df1_8[slice81],\n",
    "       train_regret_stp_df1_9[slice81],\n",
    "       train_regret_stp_df1_10[slice81],\n",
    "       train_regret_stp_df1_11[slice81],\n",
    "       train_regret_stp_df1_12[slice81],\n",
    "       train_regret_stp_df1_13[slice81],\n",
    "       train_regret_stp_df1_14[slice81],\n",
    "       train_regret_stp_df1_15[slice81],\n",
    "       train_regret_stp_df1_16[slice81],\n",
    "       train_regret_stp_df1_17[slice81],\n",
    "       train_regret_stp_df1_18[slice81],\n",
    "       train_regret_stp_df1_19[slice81],\n",
    "       train_regret_stp_df1_20[slice81]]\n",
    "\n",
    "gp88_results = pd.DataFrame(gp88).sort_values(by=[0], ascending=False)\n",
    "stp88_results = pd.DataFrame(stp88).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp88 = np.asarray(gp88_results[4:5][0])[0]\n",
    "median_gp88 = np.asarray(gp88_results[9:10][0])[0]\n",
    "upper_gp88 = np.asarray(gp88_results[14:15][0])[0]\n",
    "\n",
    "lower_stp88 = np.asarray(stp88_results[4:5][0])[0]\n",
    "median_stp88 = np.asarray(stp88_results[9:10][0])[0]\n",
    "upper_stp88 = np.asarray(stp88_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration98 :\n",
    "\n",
    "slice1 = 97\n",
    "\n",
    "gp98 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp98 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp98_results = pd.DataFrame(gp98).sort_values(by=[0], ascending=False)\n",
    "stp98_results = pd.DataFrame(stp98).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp98 = np.asarray(gp98_results[4:5][0])[0]\n",
    "median_gp98 = np.asarray(gp98_results[9:10][0])[0]\n",
    "upper_gp98 = np.asarray(gp98_results[14:15][0])[0]\n",
    "\n",
    "lower_stp98 = np.asarray(stp98_results[4:5][0])[0]\n",
    "median_stp98 = np.asarray(stp98_results[9:10][0])[0]\n",
    "upper_stp98 = np.asarray(stp98_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration9 :\n",
    "\n",
    "slice1 = 8\n",
    "\n",
    "gp9 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp9 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp9_results = pd.DataFrame(gp9).sort_values(by=[0], ascending=False)\n",
    "stp9_results = pd.DataFrame(stp9).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp9 = np.asarray(gp9_results[4:5][0])[0]\n",
    "median_gp9 = np.asarray(gp9_results[9:10][0])[0]\n",
    "upper_gp9 = np.asarray(gp9_results[14:15][0])[0]\n",
    "\n",
    "lower_stp9 = np.asarray(stp9_results[4:5][0])[0]\n",
    "median_stp9 = np.asarray(stp9_results[9:10][0])[0]\n",
    "upper_stp9 = np.asarray(stp9_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration19 :\n",
    "\n",
    "slice11 = 18\n",
    "\n",
    "gp19 = [train_regret_gp_1[slice11],\n",
    "       train_regret_gp_2[slice11],\n",
    "       train_regret_gp_3[slice11],\n",
    "       train_regret_gp_4[slice11],\n",
    "       train_regret_gp_5[slice11],\n",
    "       train_regret_gp_6[slice11],\n",
    "       train_regret_gp_7[slice11],\n",
    "       train_regret_gp_8[slice11],\n",
    "       train_regret_gp_9[slice11],\n",
    "       train_regret_gp_10[slice11],\n",
    "       train_regret_gp_11[slice11],\n",
    "       train_regret_gp_12[slice11],\n",
    "       train_regret_gp_13[slice11],\n",
    "       train_regret_gp_14[slice11],\n",
    "       train_regret_gp_15[slice11],\n",
    "       train_regret_gp_16[slice11],\n",
    "       train_regret_gp_17[slice11],\n",
    "       train_regret_gp_18[slice11],\n",
    "       train_regret_gp_19[slice11],\n",
    "       train_regret_gp_20[slice11]]\n",
    "\n",
    "stp19 = [train_regret_stp_df1_1[slice11],\n",
    "       train_regret_stp_df1_2[slice11],\n",
    "       train_regret_stp_df1_3[slice11],\n",
    "       train_regret_stp_df1_4[slice11],\n",
    "       train_regret_stp_df1_5[slice11],\n",
    "       train_regret_stp_df1_6[slice11],\n",
    "       train_regret_stp_df1_7[slice11],\n",
    "       train_regret_stp_df1_8[slice11],\n",
    "       train_regret_stp_df1_9[slice11],\n",
    "       train_regret_stp_df1_10[slice11],\n",
    "       train_regret_stp_df1_11[slice11],\n",
    "       train_regret_stp_df1_12[slice11],\n",
    "       train_regret_stp_df1_13[slice11],\n",
    "       train_regret_stp_df1_14[slice11],\n",
    "       train_regret_stp_df1_15[slice11],\n",
    "       train_regret_stp_df1_16[slice11],\n",
    "       train_regret_stp_df1_17[slice11],\n",
    "       train_regret_stp_df1_18[slice11],\n",
    "       train_regret_stp_df1_19[slice11],\n",
    "       train_regret_stp_df1_20[slice11]]\n",
    "\n",
    "gp19_results = pd.DataFrame(gp19).sort_values(by=[0], ascending=False)\n",
    "stp19_results = pd.DataFrame(stp19).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp19 = np.asarray(gp19_results[4:5][0])[0]\n",
    "median_gp19 = np.asarray(gp19_results[9:10][0])[0]\n",
    "upper_gp19 = np.asarray(gp19_results[14:15][0])[0]\n",
    "\n",
    "lower_stp19 = np.asarray(stp19_results[4:5][0])[0]\n",
    "median_stp19 = np.asarray(stp19_results[9:10][0])[0]\n",
    "upper_stp19 = np.asarray(stp19_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration29 :\n",
    "\n",
    "slice21 = 28\n",
    "\n",
    "gp29 = [train_regret_gp_1[slice21],\n",
    "       train_regret_gp_2[slice21],\n",
    "       train_regret_gp_3[slice21],\n",
    "       train_regret_gp_4[slice21],\n",
    "       train_regret_gp_5[slice21],\n",
    "       train_regret_gp_6[slice21],\n",
    "       train_regret_gp_7[slice21],\n",
    "       train_regret_gp_8[slice21],\n",
    "       train_regret_gp_9[slice21],\n",
    "       train_regret_gp_10[slice21],\n",
    "       train_regret_gp_11[slice21],\n",
    "       train_regret_gp_12[slice21],\n",
    "       train_regret_gp_13[slice21],\n",
    "       train_regret_gp_14[slice21],\n",
    "       train_regret_gp_15[slice21],\n",
    "       train_regret_gp_16[slice21],\n",
    "       train_regret_gp_17[slice21],\n",
    "       train_regret_gp_18[slice21],\n",
    "       train_regret_gp_19[slice21],\n",
    "       train_regret_gp_20[slice21]]\n",
    "\n",
    "stp29 = [train_regret_stp_df1_1[slice21],\n",
    "       train_regret_stp_df1_2[slice21],\n",
    "       train_regret_stp_df1_3[slice21],\n",
    "       train_regret_stp_df1_4[slice21],\n",
    "       train_regret_stp_df1_5[slice21],\n",
    "       train_regret_stp_df1_6[slice21],\n",
    "       train_regret_stp_df1_7[slice21],\n",
    "       train_regret_stp_df1_8[slice21],\n",
    "       train_regret_stp_df1_9[slice21],\n",
    "       train_regret_stp_df1_10[slice21],\n",
    "       train_regret_stp_df1_11[slice21],\n",
    "       train_regret_stp_df1_12[slice21],\n",
    "       train_regret_stp_df1_13[slice21],\n",
    "       train_regret_stp_df1_14[slice21],\n",
    "       train_regret_stp_df1_15[slice21],\n",
    "       train_regret_stp_df1_16[slice21],\n",
    "       train_regret_stp_df1_17[slice21],\n",
    "       train_regret_stp_df1_18[slice21],\n",
    "       train_regret_stp_df1_19[slice21],\n",
    "       train_regret_stp_df1_20[slice21]]\n",
    "\n",
    "gp29_results = pd.DataFrame(gp29).sort_values(by=[0], ascending=False)\n",
    "stp29_results = pd.DataFrame(stp29).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp29 = np.asarray(gp29_results[4:5][0])[0]\n",
    "median_gp29 = np.asarray(gp29_results[9:10][0])[0]\n",
    "upper_gp29 = np.asarray(gp29_results[14:15][0])[0]\n",
    "\n",
    "lower_stp29 = np.asarray(stp29_results[4:5][0])[0]\n",
    "median_stp29 = np.asarray(stp29_results[9:10][0])[0]\n",
    "upper_stp29 = np.asarray(stp29_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration39 :\n",
    "\n",
    "slice31 = 38\n",
    "\n",
    "gp39 = [train_regret_gp_1[slice31],\n",
    "       train_regret_gp_2[slice31],\n",
    "       train_regret_gp_3[slice31],\n",
    "       train_regret_gp_4[slice31],\n",
    "       train_regret_gp_5[slice31],\n",
    "       train_regret_gp_6[slice31],\n",
    "       train_regret_gp_7[slice31],\n",
    "       train_regret_gp_8[slice31],\n",
    "       train_regret_gp_9[slice31],\n",
    "       train_regret_gp_10[slice31],\n",
    "       train_regret_gp_11[slice31],\n",
    "       train_regret_gp_12[slice31],\n",
    "       train_regret_gp_13[slice31],\n",
    "       train_regret_gp_14[slice31],\n",
    "       train_regret_gp_15[slice31],\n",
    "       train_regret_gp_16[slice31],\n",
    "       train_regret_gp_17[slice31],\n",
    "       train_regret_gp_18[slice31],\n",
    "       train_regret_gp_19[slice31],\n",
    "       train_regret_gp_20[slice31]]\n",
    "\n",
    "stp39 = [train_regret_stp_df1_1[slice31],\n",
    "       train_regret_stp_df1_2[slice31],\n",
    "       train_regret_stp_df1_3[slice31],\n",
    "       train_regret_stp_df1_4[slice31],\n",
    "       train_regret_stp_df1_5[slice31],\n",
    "       train_regret_stp_df1_6[slice31],\n",
    "       train_regret_stp_df1_7[slice31],\n",
    "       train_regret_stp_df1_8[slice31],\n",
    "       train_regret_stp_df1_9[slice31],\n",
    "       train_regret_stp_df1_10[slice31],\n",
    "       train_regret_stp_df1_11[slice31],\n",
    "       train_regret_stp_df1_12[slice31],\n",
    "       train_regret_stp_df1_13[slice31],\n",
    "       train_regret_stp_df1_14[slice31],\n",
    "       train_regret_stp_df1_15[slice31],\n",
    "       train_regret_stp_df1_16[slice31],\n",
    "       train_regret_stp_df1_17[slice31],\n",
    "       train_regret_stp_df1_18[slice31],\n",
    "       train_regret_stp_df1_19[slice31],\n",
    "       train_regret_stp_df1_20[slice31]]\n",
    "\n",
    "gp39_results = pd.DataFrame(gp39).sort_values(by=[0], ascending=False)\n",
    "stp39_results = pd.DataFrame(stp39).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp39 = np.asarray(gp39_results[4:5][0])[0]\n",
    "median_gp39 = np.asarray(gp39_results[9:10][0])[0]\n",
    "upper_gp39 = np.asarray(gp39_results[14:15][0])[0]\n",
    "\n",
    "lower_stp39 = np.asarray(stp39_results[4:5][0])[0]\n",
    "median_stp39 = np.asarray(stp39_results[9:10][0])[0]\n",
    "upper_stp39 = np.asarray(stp39_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration49 :\n",
    "\n",
    "slice41 = 48\n",
    "\n",
    "gp49 = [train_regret_gp_1[slice41],\n",
    "       train_regret_gp_2[slice41],\n",
    "       train_regret_gp_3[slice41],\n",
    "       train_regret_gp_4[slice41],\n",
    "       train_regret_gp_5[slice41],\n",
    "       train_regret_gp_6[slice41],\n",
    "       train_regret_gp_7[slice41],\n",
    "       train_regret_gp_8[slice41],\n",
    "       train_regret_gp_9[slice41],\n",
    "       train_regret_gp_10[slice41],\n",
    "       train_regret_gp_11[slice41],\n",
    "       train_regret_gp_12[slice41],\n",
    "       train_regret_gp_13[slice41],\n",
    "       train_regret_gp_14[slice41],\n",
    "       train_regret_gp_15[slice41],\n",
    "       train_regret_gp_16[slice41],\n",
    "       train_regret_gp_17[slice41],\n",
    "       train_regret_gp_18[slice41],\n",
    "       train_regret_gp_19[slice41],\n",
    "       train_regret_gp_20[slice41]]\n",
    "\n",
    "stp49 = [train_regret_stp_df1_1[slice41],\n",
    "       train_regret_stp_df1_2[slice41],\n",
    "       train_regret_stp_df1_3[slice41],\n",
    "       train_regret_stp_df1_4[slice41],\n",
    "       train_regret_stp_df1_5[slice41],\n",
    "       train_regret_stp_df1_6[slice41],\n",
    "       train_regret_stp_df1_7[slice41],\n",
    "       train_regret_stp_df1_8[slice41],\n",
    "       train_regret_stp_df1_9[slice41],\n",
    "       train_regret_stp_df1_10[slice41],\n",
    "       train_regret_stp_df1_11[slice41],\n",
    "       train_regret_stp_df1_12[slice41],\n",
    "       train_regret_stp_df1_13[slice41],\n",
    "       train_regret_stp_df1_14[slice41],\n",
    "       train_regret_stp_df1_15[slice41],\n",
    "       train_regret_stp_df1_16[slice41],\n",
    "       train_regret_stp_df1_17[slice41],\n",
    "       train_regret_stp_df1_18[slice41],\n",
    "       train_regret_stp_df1_19[slice41],\n",
    "       train_regret_stp_df1_20[slice41]]\n",
    "\n",
    "gp49_results = pd.DataFrame(gp49).sort_values(by=[0], ascending=False)\n",
    "stp49_results = pd.DataFrame(stp49).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp49 = np.asarray(gp49_results[4:5][0])[0]\n",
    "median_gp49 = np.asarray(gp49_results[9:10][0])[0]\n",
    "upper_gp49 = np.asarray(gp49_results[14:15][0])[0]\n",
    "\n",
    "lower_stp49 = np.asarray(stp49_results[4:5][0])[0]\n",
    "median_stp49 = np.asarray(stp49_results[9:10][0])[0]\n",
    "upper_stp49 = np.asarray(stp49_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration59 :\n",
    "\n",
    "slice51 = 58\n",
    "\n",
    "gp59 = [train_regret_gp_1[slice51],\n",
    "       train_regret_gp_2[slice51],\n",
    "       train_regret_gp_3[slice51],\n",
    "       train_regret_gp_4[slice51],\n",
    "       train_regret_gp_5[slice51],\n",
    "       train_regret_gp_6[slice51],\n",
    "       train_regret_gp_7[slice51],\n",
    "       train_regret_gp_8[slice51],\n",
    "       train_regret_gp_9[slice51],\n",
    "       train_regret_gp_10[slice51],\n",
    "       train_regret_gp_11[slice51],\n",
    "       train_regret_gp_12[slice51],\n",
    "       train_regret_gp_13[slice51],\n",
    "       train_regret_gp_14[slice51],\n",
    "       train_regret_gp_15[slice51],\n",
    "       train_regret_gp_16[slice51],\n",
    "       train_regret_gp_17[slice51],\n",
    "       train_regret_gp_18[slice51],\n",
    "       train_regret_gp_19[slice51],\n",
    "       train_regret_gp_20[slice51]]\n",
    "\n",
    "stp59 = [train_regret_stp_df1_1[slice51],\n",
    "       train_regret_stp_df1_2[slice51],\n",
    "       train_regret_stp_df1_3[slice51],\n",
    "       train_regret_stp_df1_4[slice51],\n",
    "       train_regret_stp_df1_5[slice51],\n",
    "       train_regret_stp_df1_6[slice51],\n",
    "       train_regret_stp_df1_7[slice51],\n",
    "       train_regret_stp_df1_8[slice51],\n",
    "       train_regret_stp_df1_9[slice51],\n",
    "       train_regret_stp_df1_10[slice51],\n",
    "       train_regret_stp_df1_11[slice51],\n",
    "       train_regret_stp_df1_12[slice51],\n",
    "       train_regret_stp_df1_13[slice51],\n",
    "       train_regret_stp_df1_14[slice51],\n",
    "       train_regret_stp_df1_15[slice51],\n",
    "       train_regret_stp_df1_16[slice51],\n",
    "       train_regret_stp_df1_17[slice51],\n",
    "       train_regret_stp_df1_18[slice51],\n",
    "       train_regret_stp_df1_19[slice51],\n",
    "       train_regret_stp_df1_20[slice51]]\n",
    "\n",
    "gp59_results = pd.DataFrame(gp59).sort_values(by=[0], ascending=False)\n",
    "stp59_results = pd.DataFrame(stp59).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp59 = np.asarray(gp59_results[4:5][0])[0]\n",
    "median_gp59 = np.asarray(gp59_results[9:10][0])[0]\n",
    "upper_gp59 = np.asarray(gp59_results[14:15][0])[0]\n",
    "\n",
    "lower_stp59 = np.asarray(stp59_results[4:5][0])[0]\n",
    "median_stp59 = np.asarray(stp59_results[9:10][0])[0]\n",
    "upper_stp59 = np.asarray(stp59_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration69 :\n",
    "\n",
    "slice61 = 68\n",
    "\n",
    "gp69 = [train_regret_gp_1[slice61],\n",
    "       train_regret_gp_2[slice61],\n",
    "       train_regret_gp_3[slice61],\n",
    "       train_regret_gp_4[slice61],\n",
    "       train_regret_gp_5[slice61],\n",
    "       train_regret_gp_6[slice61],\n",
    "       train_regret_gp_7[slice61],\n",
    "       train_regret_gp_8[slice61],\n",
    "       train_regret_gp_9[slice61],\n",
    "       train_regret_gp_10[slice61],\n",
    "       train_regret_gp_11[slice61],\n",
    "       train_regret_gp_12[slice61],\n",
    "       train_regret_gp_13[slice61],\n",
    "       train_regret_gp_14[slice61],\n",
    "       train_regret_gp_15[slice61],\n",
    "       train_regret_gp_16[slice61],\n",
    "       train_regret_gp_17[slice61],\n",
    "       train_regret_gp_18[slice61],\n",
    "       train_regret_gp_19[slice61],\n",
    "       train_regret_gp_20[slice61]]\n",
    "\n",
    "stp69 = [train_regret_stp_df1_1[slice61],\n",
    "       train_regret_stp_df1_2[slice61],\n",
    "       train_regret_stp_df1_3[slice61],\n",
    "       train_regret_stp_df1_4[slice61],\n",
    "       train_regret_stp_df1_5[slice61],\n",
    "       train_regret_stp_df1_6[slice61],\n",
    "       train_regret_stp_df1_7[slice61],\n",
    "       train_regret_stp_df1_8[slice61],\n",
    "       train_regret_stp_df1_9[slice61],\n",
    "       train_regret_stp_df1_10[slice61],\n",
    "       train_regret_stp_df1_11[slice61],\n",
    "       train_regret_stp_df1_12[slice61],\n",
    "       train_regret_stp_df1_13[slice61],\n",
    "       train_regret_stp_df1_14[slice61],\n",
    "       train_regret_stp_df1_15[slice61],\n",
    "       train_regret_stp_df1_16[slice61],\n",
    "       train_regret_stp_df1_17[slice61],\n",
    "       train_regret_stp_df1_18[slice61],\n",
    "       train_regret_stp_df1_19[slice61],\n",
    "       train_regret_stp_df1_20[slice61]]\n",
    "\n",
    "gp69_results = pd.DataFrame(gp69).sort_values(by=[0], ascending=False)\n",
    "stp69_results = pd.DataFrame(stp69).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp69 = np.asarray(gp69_results[4:5][0])[0]\n",
    "median_gp69 = np.asarray(gp69_results[9:10][0])[0]\n",
    "upper_gp69 = np.asarray(gp69_results[14:15][0])[0]\n",
    "\n",
    "lower_stp69 = np.asarray(stp69_results[4:5][0])[0]\n",
    "median_stp69 = np.asarray(stp69_results[9:10][0])[0]\n",
    "upper_stp69 = np.asarray(stp69_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration79 :\n",
    "\n",
    "slice71 = 78\n",
    "\n",
    "gp79 = [train_regret_gp_1[slice71],\n",
    "       train_regret_gp_2[slice71],\n",
    "       train_regret_gp_3[slice71],\n",
    "       train_regret_gp_4[slice71],\n",
    "       train_regret_gp_5[slice71],\n",
    "       train_regret_gp_6[slice71],\n",
    "       train_regret_gp_7[slice71],\n",
    "       train_regret_gp_8[slice71],\n",
    "       train_regret_gp_9[slice71],\n",
    "       train_regret_gp_10[slice71],\n",
    "       train_regret_gp_11[slice71],\n",
    "       train_regret_gp_12[slice71],\n",
    "       train_regret_gp_13[slice71],\n",
    "       train_regret_gp_14[slice71],\n",
    "       train_regret_gp_15[slice71],\n",
    "       train_regret_gp_16[slice71],\n",
    "       train_regret_gp_17[slice71],\n",
    "       train_regret_gp_18[slice71],\n",
    "       train_regret_gp_19[slice71],\n",
    "       train_regret_gp_20[slice71]]\n",
    "\n",
    "stp79 = [train_regret_stp_df1_1[slice71],\n",
    "       train_regret_stp_df1_2[slice71],\n",
    "       train_regret_stp_df1_3[slice71],\n",
    "       train_regret_stp_df1_4[slice71],\n",
    "       train_regret_stp_df1_5[slice71],\n",
    "       train_regret_stp_df1_6[slice71],\n",
    "       train_regret_stp_df1_7[slice71],\n",
    "       train_regret_stp_df1_8[slice71],\n",
    "       train_regret_stp_df1_9[slice71],\n",
    "       train_regret_stp_df1_10[slice71],\n",
    "       train_regret_stp_df1_11[slice71],\n",
    "       train_regret_stp_df1_12[slice71],\n",
    "       train_regret_stp_df1_13[slice71],\n",
    "       train_regret_stp_df1_14[slice71],\n",
    "       train_regret_stp_df1_15[slice71],\n",
    "       train_regret_stp_df1_16[slice71],\n",
    "       train_regret_stp_df1_17[slice71],\n",
    "       train_regret_stp_df1_18[slice71],\n",
    "       train_regret_stp_df1_19[slice71],\n",
    "       train_regret_stp_df1_20[slice71]]\n",
    "\n",
    "gp79_results = pd.DataFrame(gp79).sort_values(by=[0], ascending=False)\n",
    "stp79_results = pd.DataFrame(stp79).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp79 = np.asarray(gp79_results[4:5][0])[0]\n",
    "median_gp79 = np.asarray(gp79_results[9:10][0])[0]\n",
    "upper_gp79 = np.asarray(gp79_results[14:15][0])[0]\n",
    "\n",
    "lower_stp79 = np.asarray(stp79_results[4:5][0])[0]\n",
    "median_stp79 = np.asarray(stp79_results[9:10][0])[0]\n",
    "upper_stp79 = np.asarray(stp79_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration89 :\n",
    "\n",
    "slice81 = 88\n",
    "\n",
    "gp89 = [train_regret_gp_1[slice81],\n",
    "       train_regret_gp_2[slice81],\n",
    "       train_regret_gp_3[slice81],\n",
    "       train_regret_gp_4[slice81],\n",
    "       train_regret_gp_5[slice81],\n",
    "       train_regret_gp_6[slice81],\n",
    "       train_regret_gp_7[slice81],\n",
    "       train_regret_gp_8[slice81],\n",
    "       train_regret_gp_9[slice81],\n",
    "       train_regret_gp_10[slice81],\n",
    "       train_regret_gp_11[slice81],\n",
    "       train_regret_gp_12[slice81],\n",
    "       train_regret_gp_13[slice81],\n",
    "       train_regret_gp_14[slice81],\n",
    "       train_regret_gp_15[slice81],\n",
    "       train_regret_gp_16[slice81],\n",
    "       train_regret_gp_17[slice81],\n",
    "       train_regret_gp_18[slice81],\n",
    "       train_regret_gp_19[slice81],\n",
    "       train_regret_gp_20[slice81]]\n",
    "\n",
    "stp89 = [train_regret_stp_df1_1[slice81],\n",
    "       train_regret_stp_df1_2[slice81],\n",
    "       train_regret_stp_df1_3[slice81],\n",
    "       train_regret_stp_df1_4[slice81],\n",
    "       train_regret_stp_df1_5[slice81],\n",
    "       train_regret_stp_df1_6[slice81],\n",
    "       train_regret_stp_df1_7[slice81],\n",
    "       train_regret_stp_df1_8[slice81],\n",
    "       train_regret_stp_df1_9[slice81],\n",
    "       train_regret_stp_df1_10[slice81],\n",
    "       train_regret_stp_df1_11[slice81],\n",
    "       train_regret_stp_df1_12[slice81],\n",
    "       train_regret_stp_df1_13[slice81],\n",
    "       train_regret_stp_df1_14[slice81],\n",
    "       train_regret_stp_df1_15[slice81],\n",
    "       train_regret_stp_df1_16[slice81],\n",
    "       train_regret_stp_df1_17[slice81],\n",
    "       train_regret_stp_df1_18[slice81],\n",
    "       train_regret_stp_df1_19[slice81],\n",
    "       train_regret_stp_df1_20[slice81]]\n",
    "\n",
    "gp89_results = pd.DataFrame(gp89).sort_values(by=[0], ascending=False)\n",
    "stp89_results = pd.DataFrame(stp89).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp89 = np.asarray(gp89_results[4:5][0])[0]\n",
    "median_gp89 = np.asarray(gp89_results[9:10][0])[0]\n",
    "upper_gp89 = np.asarray(gp89_results[14:15][0])[0]\n",
    "\n",
    "lower_stp89 = np.asarray(stp89_results[4:5][0])[0]\n",
    "median_stp89 = np.asarray(stp89_results[9:10][0])[0]\n",
    "upper_stp89 = np.asarray(stp89_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.3026705103183764, -3.431979607755546)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iteration99 :\n",
    "\n",
    "slice1 = 98\n",
    "\n",
    "gp99 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp99 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp99_results = pd.DataFrame(gp99).sort_values(by=[0], ascending=False)\n",
    "stp99_results = pd.DataFrame(stp99).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp99 = np.asarray(gp99_results[4:5][0])[0]\n",
    "median_gp99 = np.asarray(gp99_results[9:10][0])[0]\n",
    "upper_gp99 = np.asarray(gp99_results[14:15][0])[0]\n",
    "\n",
    "lower_stp99 = np.asarray(stp99_results[4:5][0])[0]\n",
    "median_stp99 = np.asarray(stp99_results[9:10][0])[0]\n",
    "upper_stp99 = np.asarray(stp99_results[14:15][0])[0]\n",
    "\n",
    "lower_gp99, lower_stp99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration10 :\n",
    "\n",
    "slice1 = 9\n",
    "\n",
    "gp10 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp10 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp10_results = pd.DataFrame(gp10).sort_values(by=[0], ascending=False)\n",
    "stp10_results = pd.DataFrame(stp10).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp10 = np.asarray(gp10_results[4:5][0])[0]\n",
    "median_gp10 = np.asarray(gp10_results[9:10][0])[0]\n",
    "upper_gp10 = np.asarray(gp10_results[14:15][0])[0]\n",
    "\n",
    "lower_stp10 = np.asarray(stp10_results[4:5][0])[0]\n",
    "median_stp10 = np.asarray(stp10_results[9:10][0])[0]\n",
    "upper_stp10 = np.asarray(stp10_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration20 :\n",
    "\n",
    "slice1 = 19\n",
    "\n",
    "gp20 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp20 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp20_results = pd.DataFrame(gp20).sort_values(by=[0], ascending=False)\n",
    "stp20_results = pd.DataFrame(stp20).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp20 = np.asarray(gp20_results[4:5][0])[0]\n",
    "median_gp20 = np.asarray(gp20_results[9:10][0])[0]\n",
    "upper_gp20 = np.asarray(gp20_results[14:15][0])[0]\n",
    "\n",
    "lower_stp20 = np.asarray(stp20_results[4:5][0])[0]\n",
    "median_stp20 = np.asarray(stp20_results[9:10][0])[0]\n",
    "upper_stp20 = np.asarray(stp20_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration30 :\n",
    "\n",
    "slice1 = 29\n",
    "\n",
    "gp30 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp30 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp30_results = pd.DataFrame(gp30).sort_values(by=[0], ascending=False)\n",
    "stp30_results = pd.DataFrame(stp30).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp30 = np.asarray(gp30_results[4:5][0])[0]\n",
    "median_gp30 = np.asarray(gp30_results[9:10][0])[0]\n",
    "upper_gp30 = np.asarray(gp30_results[14:15][0])[0]\n",
    "\n",
    "lower_stp30 = np.asarray(stp30_results[4:5][0])[0]\n",
    "median_stp30 = np.asarray(stp30_results[9:10][0])[0]\n",
    "upper_stp30 = np.asarray(stp30_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration40 :\n",
    "\n",
    "slice1 = 39\n",
    "\n",
    "gp40 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp40 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp40_results = pd.DataFrame(gp40).sort_values(by=[0], ascending=False)\n",
    "stp40_results = pd.DataFrame(stp40).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp40 = np.asarray(gp40_results[4:5][0])[0]\n",
    "median_gp40 = np.asarray(gp40_results[9:10][0])[0]\n",
    "upper_gp40 = np.asarray(gp40_results[14:15][0])[0]\n",
    "\n",
    "lower_stp40 = np.asarray(stp40_results[4:5][0])[0]\n",
    "median_stp40 = np.asarray(stp40_results[9:10][0])[0]\n",
    "upper_stp40 = np.asarray(stp40_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration50 :\n",
    "\n",
    "slice1 = 49\n",
    "\n",
    "gp50 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp50 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp50_results = pd.DataFrame(gp50).sort_values(by=[0], ascending=False)\n",
    "stp50_results = pd.DataFrame(stp50).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp50 = np.asarray(gp50_results[4:5][0])[0]\n",
    "median_gp50 = np.asarray(gp50_results[9:10][0])[0]\n",
    "upper_gp50 = np.asarray(gp50_results[14:15][0])[0]\n",
    "\n",
    "lower_stp50 = np.asarray(stp50_results[4:5][0])[0]\n",
    "median_stp50 = np.asarray(stp50_results[9:10][0])[0]\n",
    "upper_stp50 = np.asarray(stp50_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration60 :\n",
    "\n",
    "slice1 = 59\n",
    "\n",
    "gp60 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp60 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp60_results = pd.DataFrame(gp60).sort_values(by=[0], ascending=False)\n",
    "stp60_results = pd.DataFrame(stp60).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp60 = np.asarray(gp60_results[4:5][0])[0]\n",
    "median_gp60 = np.asarray(gp60_results[9:10][0])[0]\n",
    "upper_gp60 = np.asarray(gp60_results[14:15][0])[0]\n",
    "\n",
    "lower_stp60 = np.asarray(stp60_results[4:5][0])[0]\n",
    "median_stp60 = np.asarray(stp60_results[9:10][0])[0]\n",
    "upper_stp60 = np.asarray(stp60_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration70 :\n",
    "\n",
    "slice1 = 69\n",
    "\n",
    "gp70 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp70 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp70_results = pd.DataFrame(gp70).sort_values(by=[0], ascending=False)\n",
    "stp70_results = pd.DataFrame(stp70).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp70 = np.asarray(gp70_results[4:5][0])[0]\n",
    "median_gp70 = np.asarray(gp70_results[9:10][0])[0]\n",
    "upper_gp70 = np.asarray(gp70_results[14:15][0])[0]\n",
    "\n",
    "lower_stp70 = np.asarray(stp70_results[4:5][0])[0]\n",
    "median_stp70 = np.asarray(stp70_results[9:10][0])[0]\n",
    "upper_stp70 = np.asarray(stp70_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration80 :\n",
    "\n",
    "slice1 = 79\n",
    "\n",
    "gp80 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp80 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp80_results = pd.DataFrame(gp80).sort_values(by=[0], ascending=False)\n",
    "stp80_results = pd.DataFrame(stp80).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp80 = np.asarray(gp80_results[4:5][0])[0]\n",
    "median_gp80 = np.asarray(gp80_results[9:10][0])[0]\n",
    "upper_gp80 = np.asarray(gp80_results[14:15][0])[0]\n",
    "\n",
    "lower_stp80 = np.asarray(stp80_results[4:5][0])[0]\n",
    "median_stp80 = np.asarray(stp80_results[9:10][0])[0]\n",
    "upper_stp80 = np.asarray(stp80_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration90 :\n",
    "\n",
    "slice1 = 89\n",
    "\n",
    "gp90 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp90 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp90_results = pd.DataFrame(gp90).sort_values(by=[0], ascending=False)\n",
    "stp90_results = pd.DataFrame(stp90).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp90 = np.asarray(gp90_results[4:5][0])[0]\n",
    "median_gp90 = np.asarray(gp90_results[9:10][0])[0]\n",
    "upper_gp90 = np.asarray(gp90_results[14:15][0])[0]\n",
    "\n",
    "lower_stp90 = np.asarray(stp90_results[4:5][0])[0]\n",
    "median_stp90 = np.asarray(stp90_results[9:10][0])[0]\n",
    "upper_stp90 = np.asarray(stp90_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration100 :\n",
    "\n",
    "slice1 = 99\n",
    "\n",
    "gp100 = [train_regret_gp_1[slice1],\n",
    "       train_regret_gp_2[slice1],\n",
    "       train_regret_gp_3[slice1],\n",
    "       train_regret_gp_4[slice1],\n",
    "       train_regret_gp_5[slice1],\n",
    "       train_regret_gp_6[slice1],\n",
    "       train_regret_gp_7[slice1],\n",
    "       train_regret_gp_8[slice1],\n",
    "       train_regret_gp_9[slice1],\n",
    "       train_regret_gp_10[slice1],\n",
    "       train_regret_gp_11[slice1],\n",
    "       train_regret_gp_12[slice1],\n",
    "       train_regret_gp_13[slice1],\n",
    "       train_regret_gp_14[slice1],\n",
    "       train_regret_gp_15[slice1],\n",
    "       train_regret_gp_16[slice1],\n",
    "       train_regret_gp_17[slice1],\n",
    "       train_regret_gp_18[slice1],\n",
    "       train_regret_gp_19[slice1],\n",
    "       train_regret_gp_20[slice1]]\n",
    "\n",
    "stp100 = [train_regret_stp_df1_1[slice1],\n",
    "       train_regret_stp_df1_2[slice1],\n",
    "       train_regret_stp_df1_3[slice1],\n",
    "       train_regret_stp_df1_4[slice1],\n",
    "       train_regret_stp_df1_5[slice1],\n",
    "       train_regret_stp_df1_6[slice1],\n",
    "       train_regret_stp_df1_7[slice1],\n",
    "       train_regret_stp_df1_8[slice1],\n",
    "       train_regret_stp_df1_9[slice1],\n",
    "       train_regret_stp_df1_10[slice1],\n",
    "       train_regret_stp_df1_11[slice1],\n",
    "       train_regret_stp_df1_12[slice1],\n",
    "       train_regret_stp_df1_13[slice1],\n",
    "       train_regret_stp_df1_14[slice1],\n",
    "       train_regret_stp_df1_15[slice1],\n",
    "       train_regret_stp_df1_16[slice1],\n",
    "       train_regret_stp_df1_17[slice1],\n",
    "       train_regret_stp_df1_18[slice1],\n",
    "       train_regret_stp_df1_19[slice1],\n",
    "       train_regret_stp_df1_20[slice1]]\n",
    "\n",
    "gp100_results = pd.DataFrame(gp100).sort_values(by=[0], ascending=False)\n",
    "stp100_results = pd.DataFrame(stp100).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - GP:\n",
    "lower_gp100 = np.asarray(gp100_results[4:5][0])[0]\n",
    "median_gp100 = np.asarray(gp100_results[9:10][0])[0]\n",
    "upper_gp100 = np.asarray(gp100_results[14:15][0])[0]\n",
    "\n",
    "lower_stp100 = np.asarray(stp100_results[4:5][0])[0]\n",
    "median_stp100 = np.asarray(stp100_results[9:10][0])[0]\n",
    "upper_stp100 = np.asarray(stp100_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9(a). Summarize Arrays: GPs\n",
    "\n",
    "lower_gp = [lower_gp1,\n",
    "            lower_gp2,\n",
    "            lower_gp3,\n",
    "            lower_gp4,\n",
    "            lower_gp5,\n",
    "            lower_gp6,\n",
    "            lower_gp7,\n",
    "            lower_gp8,\n",
    "            lower_gp9,\n",
    "            lower_gp10,\n",
    "            lower_gp11,\n",
    "            lower_gp12,\n",
    "            lower_gp13,\n",
    "            lower_gp14,\n",
    "            lower_gp15,\n",
    "            lower_gp16,\n",
    "            lower_gp17,\n",
    "            lower_gp18,\n",
    "            lower_gp19,\n",
    "            lower_gp20,\n",
    "            lower_gp21,\n",
    "            lower_gp22,\n",
    "            lower_gp23,\n",
    "            lower_gp24,\n",
    "            lower_gp25,\n",
    "            lower_gp26,\n",
    "            lower_gp27,\n",
    "            lower_gp28,\n",
    "            lower_gp29,\n",
    "            lower_gp30,\n",
    "            lower_gp31,\n",
    "            lower_gp32,\n",
    "            lower_gp33,\n",
    "            lower_gp34,\n",
    "            lower_gp35,\n",
    "            lower_gp36,\n",
    "            lower_gp37,\n",
    "            lower_gp38,\n",
    "            lower_gp39,\n",
    "            lower_gp40,\n",
    "            lower_gp41,\n",
    "            lower_gp42,\n",
    "            lower_gp43,\n",
    "            lower_gp44,\n",
    "            lower_gp45,\n",
    "            lower_gp46,\n",
    "            lower_gp47,\n",
    "            lower_gp48,\n",
    "            lower_gp49,\n",
    "            lower_gp50,\n",
    "            lower_gp51,\n",
    "            lower_gp52,\n",
    "            lower_gp53,\n",
    "            lower_gp54,\n",
    "            lower_gp55,\n",
    "            lower_gp56,\n",
    "            lower_gp57,\n",
    "            lower_gp58,\n",
    "            lower_gp59,\n",
    "            lower_gp60,\n",
    "            lower_gp61,\n",
    "            lower_gp62,\n",
    "            lower_gp63,\n",
    "            lower_gp64,\n",
    "            lower_gp65,\n",
    "            lower_gp66,\n",
    "            lower_gp67,\n",
    "            lower_gp68,\n",
    "            lower_gp69,\n",
    "            lower_gp70,\n",
    "            lower_gp71,\n",
    "            lower_gp72,\n",
    "            lower_gp73,\n",
    "            lower_gp74,\n",
    "            lower_gp75,\n",
    "            lower_gp76,\n",
    "            lower_gp77,\n",
    "            lower_gp78,\n",
    "            lower_gp79,\n",
    "            lower_gp80,\n",
    "            lower_gp81,\n",
    "            lower_gp82,\n",
    "            lower_gp83,\n",
    "            lower_gp84,\n",
    "            lower_gp85,\n",
    "            lower_gp86,\n",
    "            lower_gp87,\n",
    "            lower_gp88,\n",
    "            lower_gp89,\n",
    "            lower_gp90,\n",
    "            lower_gp91,\n",
    "            lower_gp92,\n",
    "            lower_gp93,\n",
    "            lower_gp94,\n",
    "            lower_gp95,\n",
    "            lower_gp96,\n",
    "            lower_gp97,\n",
    "            lower_gp98,\n",
    "            lower_gp99,\n",
    "            lower_gp100,\n",
    "            lower_gp101]\n",
    "\n",
    "median_gp = [median_gp1,\n",
    "            median_gp2,\n",
    "            median_gp3,\n",
    "            median_gp4,\n",
    "            median_gp5,\n",
    "            median_gp6,\n",
    "            median_gp7,\n",
    "            median_gp8,\n",
    "            median_gp9,\n",
    "            median_gp10,\n",
    "            median_gp11,\n",
    "            median_gp12,\n",
    "            median_gp13,\n",
    "            median_gp14,\n",
    "            median_gp15,\n",
    "            median_gp16,\n",
    "            median_gp17,\n",
    "            median_gp18,\n",
    "            median_gp19,\n",
    "            median_gp20,\n",
    "            median_gp21,\n",
    "            median_gp22,\n",
    "            median_gp23,\n",
    "            median_gp24,\n",
    "            median_gp25,\n",
    "            median_gp26,\n",
    "            median_gp27,\n",
    "            median_gp28,\n",
    "            median_gp29,\n",
    "            median_gp30,\n",
    "            median_gp31,\n",
    "            median_gp32,\n",
    "            median_gp33,\n",
    "            median_gp34,\n",
    "            median_gp35,\n",
    "            median_gp36,\n",
    "            median_gp37,\n",
    "            median_gp38,\n",
    "            median_gp39,\n",
    "            median_gp40,\n",
    "            median_gp41,\n",
    "            median_gp42,\n",
    "            median_gp43,\n",
    "            median_gp44,\n",
    "            median_gp45,\n",
    "            median_gp46,\n",
    "            median_gp47,\n",
    "            median_gp48,\n",
    "            median_gp49,\n",
    "            median_gp50,\n",
    "            median_gp51,\n",
    "            median_gp52,\n",
    "            median_gp53,\n",
    "            median_gp54,\n",
    "            median_gp55,\n",
    "            median_gp56,\n",
    "            median_gp57,\n",
    "            median_gp58,\n",
    "            median_gp59,\n",
    "            median_gp60,\n",
    "            median_gp61,\n",
    "            median_gp62,\n",
    "            median_gp63,\n",
    "            median_gp64,\n",
    "            median_gp65,\n",
    "            median_gp66,\n",
    "            median_gp67,\n",
    "            median_gp68,\n",
    "            median_gp69,\n",
    "            median_gp70,\n",
    "            median_gp71,\n",
    "            median_gp72,\n",
    "            median_gp73,\n",
    "            median_gp74,\n",
    "            median_gp75,\n",
    "            median_gp76,\n",
    "            median_gp77,\n",
    "            median_gp78,\n",
    "            median_gp79,\n",
    "            median_gp80,\n",
    "            median_gp81,\n",
    "            median_gp82,\n",
    "            median_gp83,\n",
    "            median_gp84,\n",
    "            median_gp85,\n",
    "            median_gp86,\n",
    "            median_gp87,\n",
    "            median_gp88,\n",
    "            median_gp89,\n",
    "            median_gp90,\n",
    "            median_gp91,\n",
    "            median_gp92,\n",
    "            median_gp93,\n",
    "            median_gp94,\n",
    "            median_gp95,\n",
    "            median_gp96,\n",
    "            median_gp97,\n",
    "            median_gp98,\n",
    "            median_gp99,\n",
    "            median_gp100,\n",
    "            median_gp101]\n",
    "\n",
    "upper_gp = [upper_gp1,\n",
    "            upper_gp2,\n",
    "            upper_gp3,\n",
    "            upper_gp4,\n",
    "            upper_gp5,\n",
    "            upper_gp6,\n",
    "            upper_gp7,\n",
    "            upper_gp8,\n",
    "            upper_gp9,\n",
    "            upper_gp10,\n",
    "            upper_gp11,\n",
    "            upper_gp12,\n",
    "            upper_gp13,\n",
    "            upper_gp14,\n",
    "            upper_gp15,\n",
    "            upper_gp16,\n",
    "            upper_gp17,\n",
    "            upper_gp18,\n",
    "            upper_gp19,\n",
    "            upper_gp20,\n",
    "            upper_gp21,\n",
    "            upper_gp22,\n",
    "            upper_gp23,\n",
    "            upper_gp24,\n",
    "            upper_gp25,\n",
    "            upper_gp26,\n",
    "            upper_gp27,\n",
    "            upper_gp28,\n",
    "            upper_gp29,\n",
    "            upper_gp30,\n",
    "            upper_gp31,\n",
    "            upper_gp32,\n",
    "            upper_gp33,\n",
    "            upper_gp34,\n",
    "            upper_gp35,\n",
    "            upper_gp36,\n",
    "            upper_gp37,\n",
    "            upper_gp38,\n",
    "            upper_gp39,\n",
    "            upper_gp40,\n",
    "            upper_gp41,\n",
    "            upper_gp42,\n",
    "            upper_gp43,\n",
    "            upper_gp44,\n",
    "            upper_gp45,\n",
    "            upper_gp46,\n",
    "            upper_gp47,\n",
    "            upper_gp48,\n",
    "            upper_gp49,\n",
    "            upper_gp50,\n",
    "            upper_gp51,\n",
    "            upper_gp52,\n",
    "            upper_gp53,\n",
    "            upper_gp54,\n",
    "            upper_gp55,\n",
    "            upper_gp56,\n",
    "            upper_gp57,\n",
    "            upper_gp58,\n",
    "            upper_gp59,\n",
    "            upper_gp60,\n",
    "            upper_gp61,\n",
    "            upper_gp62,\n",
    "            upper_gp63,\n",
    "            upper_gp64,\n",
    "            upper_gp65,\n",
    "            upper_gp66,\n",
    "            upper_gp67,\n",
    "            upper_gp68,\n",
    "            upper_gp69,\n",
    "            upper_gp70,\n",
    "            upper_gp71,\n",
    "            upper_gp72,\n",
    "            upper_gp73,\n",
    "            upper_gp74,\n",
    "            upper_gp75,\n",
    "            upper_gp76,\n",
    "            upper_gp77,\n",
    "            upper_gp78,\n",
    "            upper_gp79,\n",
    "            upper_gp80,\n",
    "            upper_gp81,\n",
    "            upper_gp82,\n",
    "            upper_gp83,\n",
    "            upper_gp84,\n",
    "            upper_gp85,\n",
    "            upper_gp86,\n",
    "            upper_gp87,\n",
    "            upper_gp88,\n",
    "            upper_gp89,\n",
    "            upper_gp90,\n",
    "            upper_gp91,\n",
    "            upper_gp92,\n",
    "            upper_gp93,\n",
    "            upper_gp94,\n",
    "            upper_gp95,\n",
    "            upper_gp96,\n",
    "            upper_gp97,\n",
    "            upper_gp98,\n",
    "            upper_gp99,\n",
    "            upper_gp100,\n",
    "            upper_gp101]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9(a). Summarize Arrays: STPs\n",
    "\n",
    "lower_stp = [lower_stp1,\n",
    "            lower_stp2,\n",
    "            lower_stp3,\n",
    "            lower_stp4,\n",
    "            lower_stp5,\n",
    "            lower_stp6,\n",
    "            lower_stp7,\n",
    "            lower_stp8,\n",
    "            lower_stp9,\n",
    "            lower_stp10,\n",
    "            lower_stp11,\n",
    "            lower_stp12,\n",
    "            lower_stp13,\n",
    "            lower_stp14,\n",
    "            lower_stp15,\n",
    "            lower_stp16,\n",
    "            lower_stp17,\n",
    "            lower_stp18,\n",
    "            lower_stp19,\n",
    "            lower_stp20,\n",
    "            lower_stp21,\n",
    "            lower_stp22,\n",
    "            lower_stp23,\n",
    "            lower_stp24,\n",
    "            lower_stp25,\n",
    "            lower_stp26,\n",
    "            lower_stp27,\n",
    "            lower_stp28,\n",
    "            lower_stp29,\n",
    "            lower_stp30,\n",
    "            lower_stp31,\n",
    "            lower_stp32,\n",
    "            lower_stp33,\n",
    "            lower_stp34,\n",
    "            lower_stp35,\n",
    "            lower_stp36,\n",
    "            lower_stp37,\n",
    "            lower_stp38,\n",
    "            lower_stp39,\n",
    "            lower_stp40,\n",
    "            lower_stp41,\n",
    "            lower_stp42,\n",
    "            lower_stp43,\n",
    "            lower_stp44,\n",
    "            lower_stp45,\n",
    "            lower_stp46,\n",
    "            lower_stp47,\n",
    "            lower_stp48,\n",
    "            lower_stp49,\n",
    "            lower_stp50,\n",
    "            lower_stp51,\n",
    "            lower_stp52,\n",
    "            lower_stp53,\n",
    "            lower_stp54,\n",
    "            lower_stp55,\n",
    "            lower_stp56,\n",
    "            lower_stp57,\n",
    "            lower_stp58,\n",
    "            lower_stp59,\n",
    "            lower_stp60,\n",
    "            lower_stp61,\n",
    "            lower_stp62,\n",
    "            lower_stp63,\n",
    "            lower_stp64,\n",
    "            lower_stp65,\n",
    "            lower_stp66,\n",
    "            lower_stp67,\n",
    "            lower_stp68,\n",
    "            lower_stp69,\n",
    "            lower_stp70,\n",
    "            lower_stp71,\n",
    "            lower_stp72,\n",
    "            lower_stp73,\n",
    "            lower_stp74,\n",
    "            lower_stp75,\n",
    "            lower_stp76,\n",
    "            lower_stp77,\n",
    "            lower_stp78,\n",
    "            lower_stp79,\n",
    "            lower_stp80,\n",
    "            lower_stp81,\n",
    "            lower_stp82,\n",
    "            lower_stp83,\n",
    "            lower_stp84,\n",
    "            lower_stp85,\n",
    "            lower_stp86,\n",
    "            lower_stp87,\n",
    "            lower_stp88,\n",
    "            lower_stp89,\n",
    "            lower_stp90,\n",
    "            lower_stp91,\n",
    "            lower_stp92,\n",
    "            lower_stp93,\n",
    "            lower_stp94,\n",
    "            lower_stp95,\n",
    "            lower_stp96,\n",
    "            lower_stp97,\n",
    "            lower_stp98,\n",
    "            lower_stp99,\n",
    "            lower_stp100,\n",
    "            lower_stp101]\n",
    "\n",
    "median_stp = [median_stp1,\n",
    "            median_stp2,\n",
    "            median_stp3,\n",
    "            median_stp4,\n",
    "            median_stp5,\n",
    "            median_stp6,\n",
    "            median_stp7,\n",
    "            median_stp8,\n",
    "            median_stp9,\n",
    "            median_stp10,\n",
    "            median_stp11,\n",
    "            median_stp12,\n",
    "            median_stp13,\n",
    "            median_stp14,\n",
    "            median_stp15,\n",
    "            median_stp16,\n",
    "            median_stp17,\n",
    "            median_stp18,\n",
    "            median_stp19,\n",
    "            median_stp20,\n",
    "            median_stp21,\n",
    "            median_stp22,\n",
    "            median_stp23,\n",
    "            median_stp24,\n",
    "            median_stp25,\n",
    "            median_stp26,\n",
    "            median_stp27,\n",
    "            median_stp28,\n",
    "            median_stp29,\n",
    "            median_stp30,\n",
    "            median_stp31,\n",
    "            median_stp32,\n",
    "            median_stp33,\n",
    "            median_stp34,\n",
    "            median_stp35,\n",
    "            median_stp36,\n",
    "            median_stp37,\n",
    "            median_stp38,\n",
    "            median_stp39,\n",
    "            median_stp40,\n",
    "            median_stp41,\n",
    "            median_stp42,\n",
    "            median_stp43,\n",
    "            median_stp44,\n",
    "            median_stp45,\n",
    "            median_stp46,\n",
    "            median_stp47,\n",
    "            median_stp48,\n",
    "            median_stp49,\n",
    "            median_stp50,\n",
    "            median_stp51,\n",
    "            median_stp52,\n",
    "            median_stp53,\n",
    "            median_stp54,\n",
    "            median_stp55,\n",
    "            median_stp56,\n",
    "            median_stp57,\n",
    "            median_stp58,\n",
    "            median_stp59,\n",
    "            median_stp60,\n",
    "            median_stp61,\n",
    "            median_stp62,\n",
    "            median_stp63,\n",
    "            median_stp64,\n",
    "            median_stp65,\n",
    "            median_stp66,\n",
    "            median_stp67,\n",
    "            median_stp68,\n",
    "            median_stp69,\n",
    "            median_stp70,\n",
    "            median_stp71,\n",
    "            median_stp72,\n",
    "            median_stp73,\n",
    "            median_stp74,\n",
    "            median_stp75,\n",
    "            median_stp76,\n",
    "            median_stp77,\n",
    "            median_stp78,\n",
    "            median_stp79,\n",
    "            median_stp80,\n",
    "            median_stp81,\n",
    "            median_stp82,\n",
    "            median_stp83,\n",
    "            median_stp84,\n",
    "            median_stp85,\n",
    "            median_stp86,\n",
    "            median_stp87,\n",
    "            median_stp88,\n",
    "            median_stp89,\n",
    "            median_stp90,\n",
    "            median_stp91,\n",
    "            median_stp92,\n",
    "            median_stp93,\n",
    "            median_stp94,\n",
    "            median_stp95,\n",
    "            median_stp96,\n",
    "            median_stp97,\n",
    "            median_stp98,\n",
    "            median_stp99,\n",
    "            median_stp100,\n",
    "            median_stp101]\n",
    "\n",
    "upper_stp = [upper_stp1,\n",
    "            upper_stp2,\n",
    "            upper_stp3,\n",
    "            upper_stp4,\n",
    "            upper_stp5,\n",
    "            upper_stp6,\n",
    "            upper_stp7,\n",
    "            upper_stp8,\n",
    "            upper_stp9,\n",
    "            upper_stp10,\n",
    "            upper_stp11,\n",
    "            upper_stp12,\n",
    "            upper_stp13,\n",
    "            upper_stp14,\n",
    "            upper_stp15,\n",
    "            upper_stp16,\n",
    "            upper_stp17,\n",
    "            upper_stp18,\n",
    "            upper_stp19,\n",
    "            upper_stp20,\n",
    "            upper_stp21,\n",
    "            upper_stp22,\n",
    "            upper_stp23,\n",
    "            upper_stp24,\n",
    "            upper_stp25,\n",
    "            upper_stp26,\n",
    "            upper_stp27,\n",
    "            upper_stp28,\n",
    "            upper_stp29,\n",
    "            upper_stp30,\n",
    "            upper_stp31,\n",
    "            upper_stp32,\n",
    "            upper_stp33,\n",
    "            upper_stp34,\n",
    "            upper_stp35,\n",
    "            upper_stp36,\n",
    "            upper_stp37,\n",
    "            upper_stp38,\n",
    "            upper_stp39,\n",
    "            upper_stp40,\n",
    "            upper_stp41,\n",
    "            upper_stp42,\n",
    "            upper_stp43,\n",
    "            upper_stp44,\n",
    "            upper_stp45,\n",
    "            upper_stp46,\n",
    "            upper_stp47,\n",
    "            upper_stp48,\n",
    "            upper_stp49,\n",
    "            upper_stp50,\n",
    "            upper_stp51,\n",
    "            upper_stp52,\n",
    "            upper_stp53,\n",
    "            upper_stp54,\n",
    "            upper_stp55,\n",
    "            upper_stp56,\n",
    "            upper_stp57,\n",
    "            upper_stp58,\n",
    "            upper_stp59,\n",
    "            upper_stp60,\n",
    "            upper_stp61,\n",
    "            upper_stp62,\n",
    "            upper_stp63,\n",
    "            upper_stp64,\n",
    "            upper_stp65,\n",
    "            upper_stp66,\n",
    "            upper_stp67,\n",
    "            upper_stp68,\n",
    "            upper_stp69,\n",
    "            upper_stp70,\n",
    "            upper_stp71,\n",
    "            upper_stp72,\n",
    "            upper_stp73,\n",
    "            upper_stp74,\n",
    "            upper_stp75,\n",
    "            upper_stp76,\n",
    "            upper_stp77,\n",
    "            upper_stp78,\n",
    "            upper_stp79,\n",
    "            upper_stp80,\n",
    "            upper_stp81,\n",
    "            upper_stp82,\n",
    "            upper_stp83,\n",
    "            upper_stp84,\n",
    "            upper_stp85,\n",
    "            upper_stp86,\n",
    "            upper_stp87,\n",
    "            upper_stp88,\n",
    "            upper_stp89,\n",
    "            upper_stp90,\n",
    "            upper_stp91,\n",
    "            upper_stp92,\n",
    "            upper_stp93,\n",
    "            upper_stp94,\n",
    "            upper_stp95,\n",
    "            upper_stp96,\n",
    "            upper_stp97,\n",
    "            upper_stp98,\n",
    "            upper_stp99,\n",
    "            upper_stp100,\n",
    "            upper_stp101]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 10. Visualise!\n",
    "\n",
    "title = obj_func\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(median_gp, color = 'Red')\n",
    "plt.plot(median_stp, color = 'Blue')\n",
    "\n",
    "xstar = np.arange(0, 101, step=1)\n",
    "plt.fill_between(xstar, lower_gp, upper_gp, facecolor = 'Red', alpha=0.4, label='GP ERM Regret: IQR')\n",
    "plt.fill_between(xstar, lower_stp, upper_stp, facecolor = 'Blue', alpha=0.4, label='STP ERM Regret: IQR ' r'($\\nu$' ' = {})'.format(df1))\n",
    "\n",
    "plt.title(title, weight = 'bold')\n",
    "plt.xlabel('(post-initialization) iteration $\\it{k}$', weight = 'bold') # x-axis label\n",
    "plt.ylabel('ln(Regret)', weight = 'bold') # y-axis label\n",
    "plt.legend(loc=0) # add plot legend\n",
    "\n",
    "plt.show() #visualise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
