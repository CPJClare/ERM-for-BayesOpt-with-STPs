{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hartmann3 synthetic function:\n",
    "\n",
    "GP ERM versus STP nu = 5 ERM (winner)\n",
    "\n",
    "https://www.sfu.ca/~ssurjano/sumsqu.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import modules:\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "from pyGPGO.logger import EventLogger\n",
    "from pyGPGO.GPGO import GPGO\n",
    "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\n",
    "from pyGPGO.surrogates.tStudentProcess import tStudentProcess, logpdf\n",
    "from pyGPGO.acquisition import Acquisition\n",
    "from pyGPGO.covfunc import squaredExponential\n",
    "\n",
    "from collections import OrderedDict\n",
    "from joblib import Parallel, delayed\n",
    "from numpy.linalg import slogdet, inv, cholesky, solve\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.special import gamma\n",
    "from scipy.stats import norm, t\n",
    "from matplotlib.pyplot import rc\n",
    "\n",
    "rc('text', usetex=False)\n",
    "plt.rcParams['text.latex.preamble']=[r'\\usepackage{amsmath}']\n",
    "plt.rcParams['text.latex.preamble'] = [r'\\boldmath']\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inputs:\n",
    "\n",
    "obj_func = 'Hartmann3'\n",
    "n_test = 50 # test points\n",
    "df = 5 # nu\n",
    "\n",
    "util_loser = 'RegretMinimized'\n",
    "util_winner = 'tRegretMinimized'\n",
    "n_init = 5 # random initialisations\n",
    "\n",
    "cov_func = squaredExponential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Objective function:\n",
    "\n",
    "if obj_func == 'Hartmann3':\n",
    "            \n",
    "    # True y bounds:\n",
    "    y_lb = -3.86278\n",
    "    operator = -1 # targets global minimum \n",
    "    y_global_orig = y_lb * operator # targets global minimum\n",
    "            \n",
    "# Constraints:\n",
    "    lb = 0\n",
    "    ub = 1\n",
    "    \n",
    "# Input array dimension(s):\n",
    "    dim = 3\n",
    "\n",
    "# 3-D inputs' parameter bounds:\n",
    "    param = {'x1_training': ('cont', [lb, ub]),\n",
    "             'x2_training': ('cont', [lb, ub]),\n",
    "             'x3_training': ('cont', [lb, ub])}\n",
    "    \n",
    "    max_iter = (10 * dim)*0 + 100  # iterations of Bayesian optimisation\n",
    "    \n",
    "# Test data:\n",
    "    x1_test = np.linspace(lb, ub, n_test) \n",
    "    x2_test = np.linspace(lb, ub, n_test)\n",
    "    x3_test = np.linspace(lb, ub, n_test)\n",
    "    Xstar_d = np.column_stack((x1_test, x2_test, x3_test))\n",
    "    \n",
    "    def f_syn_polarity(x1_training, x2_training, x3_training):\n",
    "       \n",
    "        value = np.array([x1_training, x2_training, x3_training])\n",
    "      \n",
    "        a = np.array([[3.0, 10, 30],\n",
    "                      [0.1, 10, 35],\n",
    "                      [3.0, 10, 30],\n",
    "                      [0.1, 10, 35]])\n",
    "        \n",
    "        alpha = np.array([1.0, 1.2, 3.0, 3.2])\n",
    "      \n",
    "        p = np.array([[.3689, .1170, .2673],\n",
    "                      [.4699, .4387, .7470],\n",
    "                      [.1091, .8732, .5547],\n",
    "                      [.3810, .5743, .8828]])\n",
    "  \n",
    "        s = 0\n",
    "        for i in [0,1,2,3]:\n",
    "            sm = a[i,0]*(value[0]-p[i,0])**2\n",
    "            sm += a[i,1]*(value[1]-p[i,1])**2\n",
    "            sm += a[i,2]*(value[2]-p[i,2])**2\n",
    "            s += alpha[i]*np.exp(-sm)\n",
    "        result = -s\n",
    "        \n",
    "        return operator * result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cumulative Regret Calculator:\n",
    "\n",
    "def min_max_array(x):\n",
    "    new_list = []\n",
    "    for i, num in enumerate(x):\n",
    "            new_list.append(np.min(x[0:i+1]))\n",
    "    return new_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set-seeds:\n",
    "\n",
    "run_num_1 = 111\n",
    "run_num_2 = 113\n",
    "run_num_3 = 3333\n",
    "run_num_4 = 444\n",
    "run_num_5 = 5555\n",
    "run_num_6 = 6\n",
    "run_num_7 = 777\n",
    "run_num_8 = 887\n",
    "run_num_9 = 99\n",
    "run_num_10 = 1000\n",
    "run_num_11 = 1113\n",
    "run_num_12 = 1234\n",
    "run_num_13 = 2345\n",
    "run_num_14 = 88\n",
    "run_num_15 = 1556\n",
    "run_num_16 = 1666\n",
    "run_num_17 = 717\n",
    "run_num_18 = 8\n",
    "run_num_19 = 1998\n",
    "run_num_20 = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Acquisition function - ERM:\n",
    "\n",
    "class Acquisition_new(Acquisition):    \n",
    "    def __init__(self, mode, eps=1e-06, **params):\n",
    "        \n",
    "        self.params = params\n",
    "        self.eps = eps\n",
    "\n",
    "        mode_dict = {\n",
    "            'RegretMinimized': self.RegretMinimized,\n",
    "            'tRegretMinimized': self.tRegretMinimized\n",
    "        }\n",
    "\n",
    "        self.f = mode_dict[mode]\n",
    "   \n",
    "    def RegretMinimized(self, tau, mean, std):\n",
    "        \n",
    "        z = (mean - y_global_orig - self.eps) / (std + self.eps)\n",
    "        return z * (std + self.eps) * norm.cdf(z) + (std + self.eps) * norm.pdf(z)[0]\n",
    "    \n",
    "    def tRegretMinimized(self, tau, mean, std, nu=3.0):\n",
    "        \n",
    "        gamma = (mean - y_global_orig - self.eps) / (std + self.eps)\n",
    "        return gamma * (std + self.eps) * t.cdf(gamma, df=nu) + (std + self.eps) * (nu + gamma ** 2)/(nu - 1) * t.pdf(gamma, df=nu)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.61217018 0.16906975 0.43605902]. \t  0.37345684462559386 \t 2.3951473341797507\n",
      "init   \t [0.76926247 0.2953253  0.14916296]. \t  0.296162062221701 \t 2.3951473341797507\n",
      "init   \t [0.02247832 0.42022449 0.23868214]. \t  0.2904113287153621 \t 2.3951473341797507\n",
      "init   \t [0.33765619 0.99071246 0.23772645]. \t  0.11013785080555143 \t 2.3951473341797507\n",
      "init   \t [0.08119266 0.66960024 0.62124292]. \t  2.3951473341797507 \t 2.3951473341797507\n",
      "1      \t [0.         0.86482736 1.        ]. \t  0.8663380764047253 \t 2.3951473341797507\n",
      "2      \t [0.9694624  0.81729872 0.76150236]. \t  1.3879340688336017 \t 2.3951473341797507\n",
      "3      \t [0.00746842 0.39300358 0.94016277]. \t  2.339565033428072 \t 2.3951473341797507\n",
      "4      \t [0.99271916 0.97953791 0.19194934]. \t  0.0051249034030868 \t 2.3951473341797507\n",
      "5      \t [0.89539121 0.31310218 0.96935805]. \t  1.3903958518762796 \t 2.3951473341797507\n",
      "6      \t [0.43154031 0.53910525 0.95905966]. \t  \u001b[92m2.8079980209670032\u001b[0m \t 2.8079980209670032\n",
      "7      \t [0.48961894 0.53008348 0.60431267]. \t  1.3107116768279226 \t 2.8079980209670032\n",
      "8      \t [0.08393622 0.02257993 0.82276374]. \t  0.30471528635060674 \t 2.8079980209670032\n",
      "9      \t [0.85262173 0.90853957 0.974701  ]. \t  0.7859874906235119 \t 2.8079980209670032\n",
      "10     \t [0.97309633 0.19016164 0.29778079]. \t  0.3092980434831704 \t 2.8079980209670032\n",
      "11     \t [0.41453023 0.189136   0.98671538]. \t  0.5835678811098508 \t 2.8079980209670032\n",
      "12     \t [0.38250177 0.04518189 0.08113646]. \t  0.3356017180695572 \t 2.8079980209670032\n",
      "13     \t [0.13178792 0.01843852 0.37992013]. \t  0.5266166458834225 \t 2.8079980209670032\n",
      "14     \t [0.29631203 0.21583422 0.16238708]. \t  0.6420229022556283 \t 2.8079980209670032\n",
      "15     \t [0.27903694 0.29800729 0.85858851]. \t  2.1003956761271243 \t 2.8079980209670032\n",
      "16     \t [0.61325549 0.30243981 0.90825932]. \t  1.8874160789018661 \t 2.8079980209670032\n",
      "17     \t [0.2483362  0.96867155 0.62386134]. \t  2.345384616481004 \t 2.8079980209670032\n",
      "18     \t [0.75511073 0.31098202 0.11627379]. \t  0.22145948662279805 \t 2.8079980209670032\n",
      "19     \t [0.6988929  0.82224792 0.81285555]. \t  1.8184288957347945 \t 2.8079980209670032\n",
      "20     \t [0.74629129 0.39196569 0.04424157]. \t  0.06887424994128499 \t 2.8079980209670032\n",
      "21     \t [0.65530889 0.99070857 0.91077714]. \t  0.5916856864635031 \t 2.8079980209670032\n",
      "22     \t [0.085087   0.97222274 0.28108833]. \t  0.28790122550111297 \t 2.8079980209670032\n",
      "23     \t [0.83604037 0.55025856 0.68514336]. \t  1.8388292114924272 \t 2.8079980209670032\n",
      "24     \t [0.57095976 0.81673629 0.74317659]. \t  1.7102203992439355 \t 2.8079980209670032\n",
      "25     \t [0.63342171 0.0610053  0.61277941]. \t  0.1942697040063976 \t 2.8079980209670032\n",
      "26     \t [0.34353819 0.78447438 0.42476058]. \t  1.4333216348280542 \t 2.8079980209670032\n",
      "27     \t [0.51315413 0.21560717 0.66828498]. \t  0.7868197231909237 \t 2.8079980209670032\n",
      "28     \t [0.52769422 0.79558163 0.20534277]. \t  0.05118149121556392 \t 2.8079980209670032\n",
      "29     \t [0.65439246 0.40648609 0.80296072]. \t  \u001b[92m3.0000629220142074\u001b[0m \t 3.0000629220142074\n",
      "30     \t [0.31703275 0.38679538 0.87263802]. \t  2.925033424807816 \t 3.0000629220142074\n",
      "31     \t [0.20163824 0.26175231 0.28072878]. \t  0.7494070337353412 \t 3.0000629220142074\n",
      "32     \t [0.80015172 0.95292565 0.34818642]. \t  0.1877165275170739 \t 3.0000629220142074\n",
      "33     \t [0.19810728 0.12835955 0.30677975]. \t  0.8755510102119044 \t 3.0000629220142074\n",
      "34     \t [0.92713944 0.37449199 0.1992468 ]. \t  0.17686171776661236 \t 3.0000629220142074\n",
      "35     \t [0.97646817 0.72199139 0.23044572]. \t  0.01886879752069358 \t 3.0000629220142074\n",
      "36     \t [0.7132736  0.45084692 0.77419227]. \t  2.9988847266872862 \t 3.0000629220142074\n",
      "37     \t [0.62059475 0.19878146 0.39672362]. \t  0.48416330717961403 \t 3.0000629220142074\n",
      "38     \t [0.93771887 0.12502626 0.69952327]. \t  0.5349600897796973 \t 3.0000629220142074\n",
      "39     \t [0.91155269 0.4824978  0.5395311 ]. \t  0.4077509563328468 \t 3.0000629220142074\n",
      "40     \t [0.10508875 0.32971212 0.91216338]. \t  2.1021775737118262 \t 3.0000629220142074\n",
      "41     \t [0.44206035 0.23963528 0.53817297]. \t  0.32404118535996623 \t 3.0000629220142074\n",
      "42     \t [0.33387899 0.53342683 0.11634561]. \t  0.09134636202859761 \t 3.0000629220142074\n",
      "43     \t [0.10933948 0.39434738 0.48843668]. \t  0.47463495201539446 \t 3.0000629220142074\n",
      "44     \t [0.18147797 0.00427161 0.93758596]. \t  0.16191639832850643 \t 3.0000629220142074\n",
      "45     \t [0.64603072 0.98356685 0.85169221]. \t  0.6965671981222739 \t 3.0000629220142074\n",
      "46     \t [0.60435642 0.84482559 0.20250671]. \t  0.03825490005082811 \t 3.0000629220142074\n",
      "47     \t [0.31632777 0.08667041 0.17033392]. \t  0.7412096226284234 \t 3.0000629220142074\n",
      "48     \t [0.11224833 0.81636102 0.05510213]. \t  0.0032231561413997357 \t 3.0000629220142074\n",
      "49     \t [0.18856892 0.58701826 0.71088235]. \t  2.6686946187175598 \t 3.0000629220142074\n",
      "50     \t [0.27851208 0.8787004  0.92779283]. \t  1.2761988007569391 \t 3.0000629220142074\n",
      "51     \t [0.86535767 0.22722627 0.11850734]. \t  0.2176305506090892 \t 3.0000629220142074\n",
      "52     \t [0.63427989 0.87306528 0.75324288]. \t  1.306823750539479 \t 3.0000629220142074\n",
      "53     \t [0.75308494 0.52011    0.17899753]. \t  0.10370341809742749 \t 3.0000629220142074\n",
      "54     \t [0.56886202 0.04790407 0.91984523]. \t  0.2817905816351073 \t 3.0000629220142074\n",
      "55     \t [0.72579525 0.35976761 0.33217317]. \t  0.3519326230068731 \t 3.0000629220142074\n",
      "56     \t [0.96154876 0.6577064  0.49507711]. \t  0.28915685733921626 \t 3.0000629220142074\n",
      "57     \t [0.55035505 0.35769172 0.58668171]. \t  0.6870939250800232 \t 3.0000629220142074\n",
      "58     \t [0.02752459 0.61563494 0.42928929]. \t  0.9991490514822108 \t 3.0000629220142074\n",
      "59     \t [0.85815424 0.3766133  0.43959801]. \t  0.17764358099336147 \t 3.0000629220142074\n",
      "60     \t [0.66539115 0.70870415 0.14928367]. \t  0.02179028217114431 \t 3.0000629220142074\n",
      "61     \t [0.68751657 0.93769228 0.33449312]. \t  0.24729420645382472 \t 3.0000629220142074\n",
      "62     \t [0.61867025 0.21017493 0.90488381]. \t  1.1280061567678494 \t 3.0000629220142074\n",
      "63     \t [0.45139881 0.75764658 0.1536751 ]. \t  0.025809063494270642 \t 3.0000629220142074\n",
      "64     \t [0.90472964 0.9160513  0.18428633]. \t  0.007771975311035936 \t 3.0000629220142074\n",
      "65     \t [0.58464209 0.25549428 0.4077631 ]. \t  0.4304226878949818 \t 3.0000629220142074\n",
      "66     \t [0.73167071 0.04646788 0.63602023]. \t  0.20109147747427045 \t 3.0000629220142074\n",
      "67     \t [0.50159174 0.39568288 0.3605373 ]. \t  0.4049837552433268 \t 3.0000629220142074\n",
      "68     \t [0.284838   0.7672172  0.22291601]. \t  0.1034113167291186 \t 3.0000629220142074\n",
      "69     \t [0.1213989  0.52283173 0.43351905]. \t  0.6735992321504352 \t 3.0000629220142074\n",
      "70     \t [0.98892067 0.72767522 0.86826642]. \t  2.734855490734007 \t 3.0000629220142074\n",
      "71     \t [0.33554799 0.1960815  0.29285402]. \t  0.9219313064285296 \t 3.0000629220142074\n",
      "72     \t [0.98804586 0.0449322  0.30230819]. \t  0.29004380755883535 \t 3.0000629220142074\n",
      "73     \t [0.95901186 0.25076518 0.23941641]. \t  0.28784139865955954 \t 3.0000629220142074\n",
      "74     \t [0.19916068 0.6271735  0.40368678]. \t  0.8597929973599497 \t 3.0000629220142074\n",
      "75     \t [0.11165879 0.28524441 0.48207772]. \t  0.3208186689144355 \t 3.0000629220142074\n",
      "76     \t [0.34104117 0.1003848  0.35929984]. \t  0.7758886749514444 \t 3.0000629220142074\n",
      "77     \t [0.4699236  0.89020772 0.37975838]. \t  0.8113985516385133 \t 3.0000629220142074\n",
      "78     \t [0.4392073  0.78519199 0.10636872]. \t  0.010029344977685532 \t 3.0000629220142074\n",
      "79     \t [0.21961907 0.6015288  0.09956847]. \t  0.04121021759401271 \t 3.0000629220142074\n",
      "80     \t [0.95655451 0.88808101 0.58809126]. \t  0.4553290610022084 \t 3.0000629220142074\n",
      "81     \t [0.78657803 0.00727757 0.24876982]. \t  0.5199892114983524 \t 3.0000629220142074\n",
      "82     \t [0.78933727 0.90472402 0.82215516]. \t  1.126411336305204 \t 3.0000629220142074\n",
      "83     \t [0.8777519  0.14790529 0.76351869]. \t  0.8108427181301892 \t 3.0000629220142074\n",
      "84     \t [0.33935565 0.09674464 0.31095635]. \t  0.9396204683691252 \t 3.0000629220142074\n",
      "85     \t [0.43361283 0.20523394 0.62461828]. \t  0.5329662580521853 \t 3.0000629220142074\n",
      "86     \t [0.35345103 0.44407979 0.5372887 ]. \t  0.7310458138475027 \t 3.0000629220142074\n",
      "87     \t [0.39871357 0.2735528  0.87510818]. \t  1.8095336899165377 \t 3.0000629220142074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.52645571 0.75007053 0.81331628]. \t  2.5756383452991445 \t 3.0000629220142074\n",
      "89     \t [0.67162765 0.16588182 0.85316793]. \t  0.9636196694046057 \t 3.0000629220142074\n",
      "90     \t [0.1353118  0.41816142 0.57977477]. \t  0.9330594111440559 \t 3.0000629220142074\n",
      "91     \t [0.84515891 0.7984235  0.27681832]. \t  0.06007456863337144 \t 3.0000629220142074\n",
      "92     \t [0.65259893 0.40543277 0.61169084]. \t  0.9410805313922265 \t 3.0000629220142074\n",
      "93     \t [0.89712384 0.86740939 0.57199769]. \t  0.5705142395729424 \t 3.0000629220142074\n",
      "94     \t [0.10050002 0.70316347 0.1260103 ]. \t  0.02331172194873839 \t 3.0000629220142074\n",
      "95     \t [0.49266886 0.21165411 0.91177336]. \t  1.110705187350084 \t 3.0000629220142074\n",
      "96     \t [0.02291614 0.84016591 0.0677154 ]. \t  0.0034917025704461944 \t 3.0000629220142074\n",
      "97     \t [0.09047156 0.52056546 0.37167556]. \t  0.43681458744751706 \t 3.0000629220142074\n",
      "98     \t [0.30024739 0.21118253 0.95814266]. \t  0.8514100172171297 \t 3.0000629220142074\n",
      "99     \t [0.05766926 0.62078636 0.81547445]. \t  \u001b[92m3.5677144595940287\u001b[0m \t 3.5677144595940287\n",
      "100    \t [0.31882033 0.71137107 0.41807763]. \t  1.1847704621319368 \t 3.5677144595940287\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\n",
    "\n",
    "np.random.seed(run_num_1)\n",
    "surrogate_loser_1 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_1 = GPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_1.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.85198549 0.0739036  0.89493176]. \t  0.3999083566189884 \t 2.6229838112516717\n",
      "init   \t [0.43649355 0.12767773 0.57585787]. \t  0.24461577848211966 \t 2.6229838112516717\n",
      "init   \t [0.84047092 0.43512055 0.69591056]. \t  1.8897715258798413 \t 2.6229838112516717\n",
      "init   \t [0.6846381  0.70064837 0.77969426]. \t  2.6229838112516717 \t 2.6229838112516717\n",
      "init   \t [0.64274937 0.96102617 0.10846489]. \t  0.003309399496320042 \t 2.6229838112516717\n",
      "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.6229838112516717\n",
      "2      \t [0.         0.88842232 1.        ]. \t  0.7510512573785252 \t 2.6229838112516717\n",
      "3      \t [0.03171049 0.81766905 0.30766729]. \t  0.4633094918758506 \t 2.6229838112516717\n",
      "4      \t [0.30290203 0.50085936 0.98998561]. \t  2.1749524963713047 \t 2.6229838112516717\n",
      "5      \t [0.92029033 0.04265395 0.04009595]. \t  0.08078082202939513 \t 2.6229838112516717\n",
      "6      \t [0.08503078 0.11774937 0.01943383]. \t  0.12432388056630428 \t 2.6229838112516717\n",
      "7      \t [0.42498819 0.6784016  0.00833422]. \t  0.005863578553280072 \t 2.6229838112516717\n",
      "8      \t [0.68025983 0.57148002 0.98342386]. \t  2.3683751921405296 \t 2.6229838112516717\n",
      "9      \t [0.00287034 0.00960506 0.94199974]. \t  0.16425706845727325 \t 2.6229838112516717\n",
      "10     \t [0.34967458 0.9663794  0.97964562]. \t  0.5167517317875341 \t 2.6229838112516717\n",
      "11     \t [0.9449635  0.59504613 0.16844385]. \t  0.029987329500278628 \t 2.6229838112516717\n",
      "12     \t [0.07723703 0.42935057 0.68862569]. \t  1.979554528781844 \t 2.6229838112516717\n",
      "13     \t [0.81284857 0.45466138 0.81303408]. \t  \u001b[92m3.327086283949069\u001b[0m \t 3.327086283949069\n",
      "14     \t [0.06392242 1.         0.67266609]. \t  1.824308008802697 \t 3.327086283949069\n",
      "15     \t [1.         0.52461502 0.92322125]. \t  3.204514986898651 \t 3.327086283949069\n",
      "16     \t [0.49889293 0.38898862 0.80587178]. \t  2.907130697178981 \t 3.327086283949069\n",
      "17     \t [0.96647132 0.75221483 0.78488307]. \t  2.0857966270751653 \t 3.327086283949069\n",
      "18     \t [0.84647935 0.85606054 0.01601542]. \t  0.0004190810508713835 \t 3.327086283949069\n",
      "19     \t [0.02101621 0.3103691  0.98987519]. \t  1.1806830067459866 \t 3.327086283949069\n",
      "20     \t [0.67074769 0.50407663 0.10950069]. \t  0.08134994605282261 \t 3.327086283949069\n",
      "21     \t [0.99776615 0.20786608 0.19673105]. \t  0.24219398588023455 \t 3.327086283949069\n",
      "22     \t [0.18451048 0.43115052 0.82357876]. \t  3.313692032179021 \t 3.327086283949069\n",
      "23     \t [0.17455699 0.98486952 0.17098058]. \t  0.031913330206764436 \t 3.327086283949069\n",
      "24     \t [0.95239638 0.36144772 0.94554421]. \t  1.9936188205498406 \t 3.327086283949069\n",
      "25     \t [0.311088   0.10086237 0.85501238]. \t  0.5855724326415652 \t 3.327086283949069\n",
      "26     \t [0.17618559 0.68440006 0.8314362 ]. \t  3.2889860093354377 \t 3.327086283949069\n",
      "27     \t [0.64697655 0.9959314  0.73510199]. \t  0.7117122301240999 \t 3.327086283949069\n",
      "28     \t [0.71430163 0.98804758 0.82488168]. \t  0.6532725282053791 \t 3.327086283949069\n",
      "29     \t [0.24906671 0.42368708 0.15161783]. \t  0.25316392589076253 \t 3.327086283949069\n",
      "30     \t [0.51674399 0.65932498 0.64410161]. \t  1.821659570287034 \t 3.327086283949069\n",
      "31     \t [0.73365766 0.53991925 0.82607642]. \t  \u001b[92m3.68852662283979\u001b[0m \t 3.68852662283979\n",
      "32     \t [0.1268655  0.41178033 0.26678082]. \t  0.38181628623354774 \t 3.68852662283979\n",
      "33     \t [0.64809123 0.52606378 0.86501151]. \t  \u001b[92m3.771634489655683\u001b[0m \t 3.771634489655683\n",
      "34     \t [0.4115952  0.00448034 0.06480005]. \t  0.256081721264955 \t 3.771634489655683\n",
      "35     \t [0.68065875 0.06935073 0.27964859]. \t  0.727306180729379 \t 3.771634489655683\n",
      "36     \t [0.80834889 0.13656897 0.75847908]. \t  0.7445052723203085 \t 3.771634489655683\n",
      "37     \t [0.73667086 0.41211989 0.68574058]. \t  1.7285172452558117 \t 3.771634489655683\n",
      "38     \t [0.14154474 0.71312893 0.82279243]. \t  3.038390692600929 \t 3.771634489655683\n",
      "39     \t [0.85618534 0.25398188 0.5061637 ]. \t  0.2029365073720305 \t 3.771634489655683\n",
      "40     \t [0.68891349 0.07170848 0.91273754]. \t  0.3644836785822796 \t 3.771634489655683\n",
      "41     \t [0.70664598 0.00359851 0.80572512]. \t  0.2584865256522639 \t 3.771634489655683\n",
      "42     \t [0.28937801 0.46416963 0.24158263]. \t  0.3153279295858197 \t 3.771634489655683\n",
      "43     \t [0.57580235 0.03307013 0.7990242 ]. \t  0.34400695897946443 \t 3.771634489655683\n",
      "44     \t [0.09739252 0.48744649 0.98239158]. \t  2.249316846427416 \t 3.771634489655683\n",
      "45     \t [0.57215748 0.81772148 0.08081551]. \t  0.004107945210182036 \t 3.771634489655683\n",
      "46     \t [0.25029555 0.03218495 0.41040645]. \t  0.48831139871669466 \t 3.771634489655683\n",
      "47     \t [0.48144191 0.66129613 0.32669206]. \t  0.3118766076901997 \t 3.771634489655683\n",
      "48     \t [0.54383562 0.68843494 0.39754072]. \t  0.6071596355594916 \t 3.771634489655683\n",
      "49     \t [0.61632653 0.80762409 0.11658522]. \t  0.007764003637543907 \t 3.771634489655683\n",
      "50     \t [0.11943269 0.79208637 0.67105988]. \t  2.5604362371269542 \t 3.771634489655683\n",
      "51     \t [0.22895586 0.32638851 0.44386193]. \t  0.38289739220068636 \t 3.771634489655683\n",
      "52     \t [0.35694243 0.13628735 0.97680919]. \t  0.4204855119916151 \t 3.771634489655683\n",
      "53     \t [0.16520131 0.15697552 0.28504892]. \t  0.8630790546054824 \t 3.771634489655683\n",
      "54     \t [0.89807939 0.20808284 0.13597905]. \t  0.2368611982249034 \t 3.771634489655683\n",
      "55     \t [0.14556332 0.74958193 0.86294748]. \t  2.7389053935290635 \t 3.771634489655683\n",
      "56     \t [0.09778912 0.09784369 0.99987747]. \t  0.24248164089538843 \t 3.771634489655683\n",
      "57     \t [0.93793433 0.96289914 0.62095157]. \t  0.41431461357234256 \t 3.771634489655683\n",
      "58     \t [0.73769945 0.64403488 0.34053379]. \t  0.1746857730921407 \t 3.771634489655683\n",
      "59     \t [0.17637354 0.45353782 0.90834784]. \t  3.181216311396711 \t 3.771634489655683\n",
      "60     \t [0.1102904  0.72905619 0.7105759 ]. \t  2.5478252541413138 \t 3.771634489655683\n",
      "61     \t [0.38506754 0.63776631 0.47773801]. \t  1.2393662793255629 \t 3.771634489655683\n",
      "62     \t [0.92199931 0.75913036 0.59764878]. \t  0.6646765953627011 \t 3.771634489655683\n",
      "63     \t [0.19705171 0.73435699 0.7803774 ]. \t  2.712353150515283 \t 3.771634489655683\n",
      "64     \t [0.75147761 0.48911937 0.01747507]. \t  0.024852224218635303 \t 3.771634489655683\n",
      "65     \t [0.41907116 0.64224203 0.25228123]. \t  0.14748024004701676 \t 3.771634489655683\n",
      "66     \t [0.09918691 0.51251405 0.82103803]. \t  3.6962924579227248 \t 3.771634489655683\n",
      "67     \t [0.42471671 0.88158675 0.23989252]. \t  0.11653903631853006 \t 3.771634489655683\n",
      "68     \t [0.48388965 0.08673052 0.61339832]. \t  0.2393791775025995 \t 3.771634489655683\n",
      "69     \t [0.0713518  0.62234148 0.90537674]. \t  3.43256432178603 \t 3.771634489655683\n",
      "70     \t [0.86144225 0.25484801 0.5945089 ]. \t  0.4624660965502681 \t 3.771634489655683\n",
      "71     \t [0.27452343 0.49067769 0.19135809]. \t  0.21490083226807472 \t 3.771634489655683\n",
      "72     \t [0.40849331 0.37194864 0.5961946 ]. \t  0.8341827724971607 \t 3.771634489655683\n",
      "73     \t [0.89971495 0.71543465 0.28623959]. \t  0.053432034071667286 \t 3.771634489655683\n",
      "74     \t [0.5574833  0.60674016 0.63907891]. \t  1.6491914725936576 \t 3.771634489655683\n",
      "75     \t [0.52544185 0.57976028 0.5353924 ]. \t  1.0100049618567128 \t 3.771634489655683\n",
      "76     \t [0.2030483  0.60445376 0.6118741 ]. \t  2.008738081126147 \t 3.771634489655683\n",
      "77     \t [0.83960497 0.7178898  0.51860813]. \t  0.5713437535535434 \t 3.771634489655683\n",
      "78     \t [0.12568486 0.48898317 0.39101566]. \t  0.4535341902267533 \t 3.771634489655683\n",
      "79     \t [0.40356815 0.91222831 0.4853063 ]. \t  1.9875464421362756 \t 3.771634489655683\n",
      "80     \t [0.30448895 0.14236495 0.57839827]. \t  0.26961305933473323 \t 3.771634489655683\n",
      "81     \t [0.85132587 0.65806184 0.68313386]. \t  1.5772789254037718 \t 3.771634489655683\n",
      "82     \t [0.98963774 0.39128212 0.0483968 ]. \t  0.03524480693066017 \t 3.771634489655683\n",
      "83     \t [0.46545686 0.78536145 0.41128135]. \t  1.0375274839054927 \t 3.771634489655683\n",
      "84     \t [0.15183179 0.68326864 0.73319422]. \t  2.740138846743 \t 3.771634489655683\n",
      "85     \t [0.76625153 0.90636263 0.16262998]. \t  0.008953763578063236 \t 3.771634489655683\n",
      "86     \t [0.38871972 0.08593287 0.46946098]. \t  0.31822054482285544 \t 3.771634489655683\n",
      "87     \t [0.88177672 0.84891331 0.85058481]. \t  1.6023093149208276 \t 3.771634489655683\n",
      "88     \t [0.7690034  0.94956963 0.54386002]. \t  0.7981680382972645 \t 3.771634489655683\n",
      "89     \t [0.42584837 0.27642666 0.84476741]. \t  1.91730519849934 \t 3.771634489655683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.59454665 0.45537968 0.58468917]. \t  0.8630606019833844 \t 3.771634489655683\n",
      "91     \t [0.40142707 0.84353954 0.4657078 ]. \t  1.8342725319658606 \t 3.771634489655683\n",
      "92     \t [0.84081615 0.99488463 0.97315247]. \t  0.41314251161269006 \t 3.771634489655683\n",
      "93     \t [0.87104222 0.54797113 0.12798409]. \t  0.04169874636833335 \t 3.771634489655683\n",
      "94     \t [0.45601705 0.61857631 0.12320264]. \t  0.04646537162221157 \t 3.771634489655683\n",
      "95     \t [0.92791424 0.4076477  0.77780579]. \t  2.735659286442517 \t 3.771634489655683\n",
      "96     \t [0.59449511 0.41620195 0.91113527]. \t  2.8805701809722963 \t 3.771634489655683\n",
      "97     \t [0.37566745 0.03245529 0.93029623]. \t  0.22800922636108983 \t 3.771634489655683\n",
      "98     \t [0.02696673 0.47445665 0.50263357]. \t  0.7518362739769882 \t 3.771634489655683\n",
      "99     \t [0.32353418 0.39124557 0.82877661]. \t  3.019316440408473 \t 3.771634489655683\n",
      "100    \t [0.13645963 0.68909868 0.32313889]. \t  0.45748233319260295 \t 3.771634489655683\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\n",
    "\n",
    "np.random.seed(run_num_2)\n",
    "surrogate_loser_2 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_2 = GPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_2.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.75157561 0.10925128 0.48612128]. \t  0.19395982560567754 \t 0.5647137279144399\n",
      "init   \t [0.49983118 0.65711    0.23588471]. \t  0.10633623603471726 \t 0.5647137279144399\n",
      "init   \t [0.61279489 0.1196524  0.7122023 ]. \t  0.5647137279144399 \t 0.5647137279144399\n",
      "init   \t [0.37256054 0.52476827 0.26328708]. \t  0.24645583811739444 \t 0.5647137279144399\n",
      "init   \t [0.62163122 0.44909976 0.21906361]. \t  0.2633149120115605 \t 0.5647137279144399\n",
      "1      \t [0.84818564 0.94085197 0.99354084]. \t  0.5448385422791876 \t 0.5647137279144399\n",
      "2      \t [0.00227319 0.27141826 0.95291977]. \t  \u001b[92m1.263004128201619\u001b[0m \t 1.263004128201619\n",
      "3      \t [0.00798031 0.03065721 0.14136081]. \t  0.39018974818183505 \t 1.263004128201619\n",
      "4      \t [0.08579033 0.76749811 0.9456053 ]. \t  \u001b[92m2.0306218951148267\u001b[0m \t 2.0306218951148267\n",
      "5      \t [0.04738879 0.88300606 0.04039215]. \t  0.0015031669884217013 \t 2.0306218951148267\n",
      "6      \t [0.9395453  0.92040294 0.01364733]. \t  0.0001428288445807452 \t 2.0306218951148267\n",
      "7      \t [0.77850495 0.03166667 0.08677142]. \t  0.21143052308139257 \t 2.0306218951148267\n",
      "8      \t [0.93601229 0.21307273 0.95402798]. \t  0.8621344872119134 \t 2.0306218951148267\n",
      "9      \t [0.05132907 0.95144077 0.74570392]. \t  1.4155608156504336 \t 2.0306218951148267\n",
      "10     \t [0.36440183 0.64658727 0.9861336 ]. \t  \u001b[92m2.200649424947822\u001b[0m \t 2.200649424947822\n",
      "11     \t [0.01606995 0.70223025 0.52824604]. \t  \u001b[92m2.2826705169024453\u001b[0m \t 2.2826705169024453\n",
      "12     \t [0.99009257 0.71650847 0.39851076]. \t  0.12354186056992493 \t 2.2826705169024453\n",
      "13     \t [0.98320704 0.25377295 0.0649609 ]. \t  0.0782882581977021 \t 2.2826705169024453\n",
      "14     \t [0.83729419 0.97610451 0.06943465]. \t  0.0005698155093462204 \t 2.2826705169024453\n",
      "15     \t [0.7906421  0.54259562 0.28658985]. \t  0.12432963415046257 \t 2.2826705169024453\n",
      "16     \t [0.79490539 0.06382176 0.19744044]. \t  0.4872052488139809 \t 2.2826705169024453\n",
      "17     \t [0.6838507  0.23022748 0.79615579]. \t  1.4602972829198642 \t 2.2826705169024453\n",
      "18     \t [0.22636183 0.64061672 0.198285  ]. \t  0.08967756666680651 \t 2.2826705169024453\n",
      "19     \t [0.0691621  0.46326016 0.50833668]. \t  0.7423073923540788 \t 2.2826705169024453\n",
      "20     \t [0.83170682 0.17888937 0.36366687]. \t  0.3883592928907606 \t 2.2826705169024453\n",
      "21     \t [0.16170007 0.68023358 0.6872927 ]. \t  \u001b[92m2.542547919759233\u001b[0m \t 2.542547919759233\n",
      "22     \t [0.2639102  0.23353625 0.79583519]. \t  1.497806538156057 \t 2.542547919759233\n",
      "23     \t [0.28682474 0.64334271 0.66487252]. \t  2.317991104262553 \t 2.542547919759233\n",
      "24     \t [0.23133169 0.39329039 0.65341517]. \t  1.444128581846464 \t 2.542547919759233\n",
      "25     \t [0.96725291 0.67978249 0.99568319]. \t  1.8467876437501514 \t 2.542547919759233\n",
      "26     \t [0.94842633 0.89564128 0.87267219]. \t  1.200400456562727 \t 2.542547919759233\n",
      "27     \t [0.09099856 0.64101224 0.05424805]. \t  0.01399937644003779 \t 2.542547919759233\n",
      "28     \t [0.59874655 0.29218002 0.45897895]. \t  0.3021734390962647 \t 2.542547919759233\n",
      "29     \t [0.64496072 0.22245524 0.27928371]. \t  0.7110696406898894 \t 2.542547919759233\n",
      "30     \t [0.52697739 0.7663314  0.85444265]. \t  2.527873327885665 \t 2.542547919759233\n",
      "31     \t [0.08489481 0.18980925 0.71654586]. \t  0.9054809583985677 \t 2.542547919759233\n",
      "32     \t [0.27760652 0.85506997 0.97266512]. \t  1.145464938616533 \t 2.542547919759233\n",
      "33     \t [0.79321136 0.08824052 0.45244894]. \t  0.22491499230957876 \t 2.542547919759233\n",
      "34     \t [0.15568853 0.49689242 0.63048503]. \t  1.6499720569649918 \t 2.542547919759233\n",
      "35     \t [0.23233499 0.26917355 0.48381664]. \t  0.33202393393512286 \t 2.542547919759233\n",
      "36     \t [0.88845153 0.69037472 0.38714919]. \t  0.1676020084418643 \t 2.542547919759233\n",
      "37     \t [0.97764727 0.19556086 0.15078478]. \t  0.2058548030755244 \t 2.542547919759233\n",
      "38     \t [0.10534516 0.35382056 0.37061701]. \t  0.41746701641257455 \t 2.542547919759233\n",
      "39     \t [0.53849731 0.10961919 0.44318802]. \t  0.3823976204649163 \t 2.542547919759233\n",
      "40     \t [0.76599538 0.12989459 0.05405983]. \t  0.15900068899346548 \t 2.542547919759233\n",
      "41     \t [0.76260918 0.64093003 0.56663145]. \t  0.8307494998265441 \t 2.542547919759233\n",
      "42     \t [0.36382114 0.14709907 0.68928492]. \t  0.6070017544805645 \t 2.542547919759233\n",
      "43     \t [0.70167635 0.07015801 0.52279586]. \t  0.1561225370415103 \t 2.542547919759233\n",
      "44     \t [0.05170108 0.1134196  0.63952085]. \t  0.33990734323546096 \t 2.542547919759233\n",
      "45     \t [0.839324   0.06403955 0.40024224]. \t  0.2994111813182178 \t 2.542547919759233\n",
      "46     \t [0.45411088 0.59508763 0.96650425]. \t  \u001b[92m2.671951945047823\u001b[0m \t 2.671951945047823\n",
      "47     \t [0.78906737 0.38659812 0.15240793]. \t  0.1921299403310432 \t 2.671951945047823\n",
      "48     \t [0.38384218 0.0751017  0.74184332]. \t  0.45404500777371515 \t 2.671951945047823\n",
      "49     \t [0.79007954 0.40272769 0.71645872]. \t  2.062790581082308 \t 2.671951945047823\n",
      "50     \t [0.03878896 0.35923014 0.02009634]. \t  0.06416205097137362 \t 2.671951945047823\n",
      "51     \t [0.20506765 0.03084034 0.10531936]. \t  0.3898960934927274 \t 2.671951945047823\n",
      "52     \t [0.26498722 0.11267496 0.61787615]. \t  0.2946864112690853 \t 2.671951945047823\n",
      "53     \t [0.09033347 0.62724944 0.40843099]. \t  0.909831714731411 \t 2.671951945047823\n",
      "54     \t [0.62065724 0.12402137 0.97715591]. \t  0.37643420052178506 \t 2.671951945047823\n",
      "55     \t [0.61411986 0.75466171 0.15501918]. \t  0.01986646070580702 \t 2.671951945047823\n",
      "56     \t [0.57568572 0.03836061 0.34427041]. \t  0.6934393647900274 \t 2.671951945047823\n",
      "57     \t [0.59188961 0.39128668 0.67530293]. \t  1.580913872707931 \t 2.671951945047823\n",
      "58     \t [0.42475792 0.97387134 0.00902368]. \t  0.00035204927402394324 \t 2.671951945047823\n",
      "59     \t [0.15778838 0.18300772 0.09020225]. \t  0.32691265266507724 \t 2.671951945047823\n",
      "60     \t [0.87275814 0.84250628 0.84964085]. \t  1.6618514972948977 \t 2.671951945047823\n",
      "61     \t [0.14574635 0.38726702 0.24731617]. \t  0.42664930173697285 \t 2.671951945047823\n",
      "62     \t [0.81638382 0.34391524 0.37149758]. \t  0.2594376062979833 \t 2.671951945047823\n",
      "63     \t [0.02564778 0.55046707 0.80272242]. \t  \u001b[92m3.6055589342778753\u001b[0m \t 3.6055589342778753\n",
      "64     \t [0.29217694 0.60262025 0.33532167]. \t  0.39133002431244923 \t 3.6055589342778753\n",
      "65     \t [0.39043185 0.52941375 0.10714522]. \t  0.08622169483458463 \t 3.6055589342778753\n",
      "66     \t [0.63444916 0.36426605 0.85539499]. \t  2.7493518034550775 \t 3.6055589342778753\n",
      "67     \t [0.0465056  0.60854331 0.92882691]. \t  3.2037911489156845 \t 3.6055589342778753\n",
      "68     \t [0.47191162 0.13818727 0.7444622 ]. \t  0.7346908833612771 \t 3.6055589342778753\n",
      "69     \t [0.55406155 0.09043208 0.11680137]. \t  0.45412533126015486 \t 3.6055589342778753\n",
      "70     \t [0.34222285 0.83045299 0.18503117]. \t  0.04650710902298375 \t 3.6055589342778753\n",
      "71     \t [0.43584131 0.50778002 0.05015357]. \t  0.05234924350693408 \t 3.6055589342778753\n",
      "72     \t [0.1067718  0.54409883 0.40585873]. \t  0.6154702295468619 \t 3.6055589342778753\n",
      "73     \t [0.11137856 0.94406589 0.09120133]. \t  0.004877999350758647 \t 3.6055589342778753\n",
      "74     \t [0.47318103 0.36048154 0.0183971 ]. \t  0.08343197041809045 \t 3.6055589342778753\n",
      "75     \t [0.67612549 0.38549353 0.03872676]. \t  0.07646283841794146 \t 3.6055589342778753\n",
      "76     \t [0.42596168 0.35197751 0.56469788]. \t  0.5906940593598009 \t 3.6055589342778753\n",
      "77     \t [0.60053513 0.61777627 0.11920594]. \t  0.03847212463219365 \t 3.6055589342778753\n",
      "78     \t [0.20200377 0.47480006 0.44549866]. \t  0.5690413955300141 \t 3.6055589342778753\n",
      "79     \t [0.27845204 0.01145598 0.09828696]. \t  0.3705038669599746 \t 3.6055589342778753\n",
      "80     \t [0.12125842 0.59471263 0.86922824]. \t  \u001b[92m3.766861277872451\u001b[0m \t 3.766861277872451\n",
      "81     \t [0.66688846 0.22554472 0.58509531]. \t  0.3956915884135331 \t 3.766861277872451\n",
      "82     \t [0.70037105 0.44356148 0.04236935]. \t  0.05432916266670234 \t 3.766861277872451\n",
      "83     \t [0.83545233 0.46071611 0.8940389 ]. \t  3.2994723316848176 \t 3.766861277872451\n",
      "84     \t [0.66383305 0.13344965 0.87018013]. \t  0.7291879962461709 \t 3.766861277872451\n",
      "85     \t [0.64278847 0.51956449 0.17011568]. \t  0.1232935205513317 \t 3.766861277872451\n",
      "86     \t [0.46624611 0.63255562 0.52234503]. \t  1.2945035042624384 \t 3.766861277872451\n",
      "87     \t [0.46295686 0.00718677 0.2675899 ]. \t  0.8633361727835598 \t 3.766861277872451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.63661989 0.6262248  0.89263547]. \t  3.507866049799997 \t 3.766861277872451\n",
      "89     \t [0.50620969 0.60759504 0.05899758]. \t  0.023746032625237397 \t 3.766861277872451\n",
      "90     \t [0.69190312 0.33390545 0.79015211]. \t  2.330472349785727 \t 3.766861277872451\n",
      "91     \t [0.13401842 0.59394694 0.07117342]. \t  0.02871568488814516 \t 3.766861277872451\n",
      "92     \t [0.2571393  0.48793247 0.20039213]. \t  0.2275105901066279 \t 3.766861277872451\n",
      "93     \t [0.70093933 0.69291867 0.53900539]. \t  0.936698712797758 \t 3.766861277872451\n",
      "94     \t [0.65575744 0.69334793 0.64502505]. \t  1.509331797337941 \t 3.766861277872451\n",
      "95     \t [0.03451085 0.63602022 0.93854795]. \t  2.971034254042888 \t 3.766861277872451\n",
      "96     \t [0.79172764 0.90784428 0.70914857]. \t  0.8431319054674943 \t 3.766861277872451\n",
      "97     \t [0.16505998 0.56790177 0.200132  ]. \t  0.127912546004578 \t 3.766861277872451\n",
      "98     \t [0.42483603 0.22137435 0.08625916]. \t  0.3323829907804243 \t 3.766861277872451\n",
      "99     \t [0.81538527 0.49290493 0.42253205]. \t  0.18928231253683095 \t 3.766861277872451\n",
      "100    \t [0.77252383 0.70572381 0.59796455]. \t  0.9956774010817171 \t 3.766861277872451\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\n",
    "\n",
    "np.random.seed(run_num_3)\n",
    "surrogate_loser_3 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_3 = GPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_3.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.83936105 0.82129497 0.64220716]. \t  0.8796727111154451 \t 1.9592421489197056\n",
      "init   \t [0.66722262 0.03913991 0.06567239]. \t  0.21283931850025994 \t 1.9592421489197056\n",
      "init   \t [0.27650064 0.3163605  0.19359241]. \t  0.5590312289203825 \t 1.9592421489197056\n",
      "init   \t [0.19709288 0.84920429 0.84235809]. \t  1.818649590001362 \t 1.9592421489197056\n",
      "init   \t [0.34736473 0.7397423  0.49340914]. \t  1.9592421489197056 \t 1.9592421489197056\n",
      "1      \t [0.47647277 0.92393606 0.15672912]. \t  0.017844770068977132 \t 1.9592421489197056\n",
      "2      \t [0.72176492 0.         0.96425917]. \t  0.12600618381348344 \t 1.9592421489197056\n",
      "3      \t [0.12891537 0.10450196 0.76439878]. \t  0.6009821728805325 \t 1.9592421489197056\n",
      "4      \t [0.07213246 0.95861543 0.39059898]. \t  1.2397133678239307 \t 1.9592421489197056\n",
      "5      \t [0.98597041 0.2856864  0.41065736]. \t  0.15284929019920618 \t 1.9592421489197056\n",
      "6      \t [0.35984016 0.39345196 0.95312626]. \t  \u001b[92m2.2080405720154364\u001b[0m \t 2.2080405720154364\n",
      "7      \t [0.80886206 0.5371506  0.97065275]. \t  \u001b[92m2.5535236020960204\u001b[0m \t 2.5535236020960204\n",
      "8      \t [0.78616262 0.98348255 0.89699467]. \t  0.6336596985860946 \t 2.5535236020960204\n",
      "9      \t [0.04681794 0.56517096 0.99093683]. \t  2.2288293321682677 \t 2.5535236020960204\n",
      "10     \t [0.92671631 0.30850458 0.73208369]. \t  1.6831956823568888 \t 2.5535236020960204\n",
      "11     \t [0.99251564 0.53200473 0.97139017]. \t  2.4848356404702816 \t 2.5535236020960204\n",
      "12     \t [0.46379372 0.38690137 0.37719231]. \t  0.4121445056585975 \t 2.5535236020960204\n",
      "13     \t [0.01989029 0.49631073 0.04396718]. \t  0.03714686443394807 \t 2.5535236020960204\n",
      "14     \t [0.004404   0.09896688 0.094363  ]. \t  0.27280620176167747 \t 2.5535236020960204\n",
      "15     \t [0.00680257 0.83486728 0.96935618]. \t  1.2909852498742977 \t 2.5535236020960204\n",
      "16     \t [0.45769784 0.80757882 0.94089557]. \t  1.7544268191834966 \t 2.5535236020960204\n",
      "17     \t [0.95230129 0.38134913 0.4858654 ]. \t  0.18305807391691747 \t 2.5535236020960204\n",
      "18     \t [0.6537332  0.60354243 0.75153684]. \t  \u001b[92m2.8201029650726914\u001b[0m \t 2.8201029650726914\n",
      "19     \t [0.969245   0.14050121 0.02213768]. \t  0.055581951405792945 \t 2.8201029650726914\n",
      "20     \t [0.61043042 0.76901763 0.23691638]. \t  0.07288343079196587 \t 2.8201029650726914\n",
      "21     \t [0.72091588 0.81569726 0.01051573]. \t  0.0008541583076826821 \t 2.8201029650726914\n",
      "22     \t [0.71083172 0.23505423 0.94873069]. \t  1.0498890486494339 \t 2.8201029650726914\n",
      "23     \t [0.72226889 0.20156952 0.09222259]. \t  0.25522171878427014 \t 2.8201029650726914\n",
      "24     \t [0.39269752 0.54762669 0.83297827]. \t  \u001b[92m3.8151893325258737\u001b[0m \t 3.8151893325258737\n",
      "25     \t [0.15506875 0.7398081  0.79992413]. \t  2.7493877272612255 \t 3.8151893325258737\n",
      "26     \t [0.47922685 0.39281578 0.00439121]. \t  0.056670893979183526 \t 3.8151893325258737\n",
      "27     \t [0.93341652 0.98195286 0.01379805]. \t  8.502821513842029e-05 \t 3.8151893325258737\n",
      "28     \t [0.58161244 0.85353531 0.4330298 ]. \t  0.9907374713535583 \t 3.8151893325258737\n",
      "29     \t [0.36698684 0.71903546 0.38399462]. \t  0.8319079231681563 \t 3.8151893325258737\n",
      "30     \t [0.96710442 0.0449519  0.9665492 ]. \t  0.19275441918756014 \t 3.8151893325258737\n",
      "31     \t [0.47944205 0.33257806 0.96390037]. \t  1.623099451158217 \t 3.8151893325258737\n",
      "32     \t [0.09745536 0.94251266 0.16993469]. \t  0.03433223941386708 \t 3.8151893325258737\n",
      "33     \t [0.71625866 0.87709437 0.30436503]. \t  0.1537142166541831 \t 3.8151893325258737\n",
      "34     \t [0.19012126 0.12058599 0.11795816]. \t  0.4653194865578239 \t 3.8151893325258737\n",
      "35     \t [0.74774734 0.04970054 0.42040843]. \t  0.31451243264143736 \t 3.8151893325258737\n",
      "36     \t [0.84958006 0.02224249 0.22693705]. \t  0.4352870206613002 \t 3.8151893325258737\n",
      "37     \t [0.73814083 0.75071711 0.80438852]. \t  2.3887537283639095 \t 3.8151893325258737\n",
      "38     \t [0.64913429 0.33518378 0.30617113]. \t  0.48114562793001553 \t 3.8151893325258737\n",
      "39     \t [0.79436457 0.7512464  0.20992399]. \t  0.02730621755927814 \t 3.8151893325258737\n",
      "40     \t [0.76449008 0.18849949 0.41987   ]. \t  0.3153807348262885 \t 3.8151893325258737\n",
      "41     \t [0.07141796 0.17239542 0.26121577]. \t  0.7446444819706365 \t 3.8151893325258737\n",
      "42     \t [0.38180059 0.43843081 0.22180373]. \t  0.3474035754294986 \t 3.8151893325258737\n",
      "43     \t [0.08482858 0.82164654 0.49044133]. \t  2.6127388005142196 \t 3.8151893325258737\n",
      "44     \t [0.         0.59972128 0.65899086]. \t  2.222863886254104 \t 3.8151893325258737\n",
      "45     \t [0.32959084 0.36001994 0.87349593]. \t  2.667005212854527 \t 3.8151893325258737\n",
      "46     \t [0.99747198 0.15794026 0.84858456]. \t  0.8922482324898482 \t 3.8151893325258737\n",
      "47     \t [0.59205156 0.12922306 0.02366767]. \t  0.14491954357998088 \t 3.8151893325258737\n",
      "48     \t [0.99623678 0.57079417 0.8806454 ]. \t  3.6095994722521034 \t 3.8151893325258737\n",
      "49     \t [0.49316125 0.64947007 0.46837257]. \t  1.0091456887671393 \t 3.8151893325258737\n",
      "50     \t [0.77070132 0.78203712 0.49392271]. \t  0.7154307154874395 \t 3.8151893325258737\n",
      "51     \t [0.08966898 0.76237058 0.22546117]. \t  0.11425353032662568 \t 3.8151893325258737\n",
      "52     \t [0.85449185 0.45695142 0.28387514]. \t  0.16567414047062834 \t 3.8151893325258737\n",
      "53     \t [0.78613408 0.08167713 0.18378513]. \t  0.47525089404355164 \t 3.8151893325258737\n",
      "54     \t [0.29687815 0.87933742 0.87351696]. \t  1.4831575364258622 \t 3.8151893325258737\n",
      "55     \t [0.65080677 0.50894956 0.83584544]. \t  3.7124054670165427 \t 3.8151893325258737\n",
      "56     \t [0.25056999 0.23593882 0.47352493]. \t  0.33300666000299306 \t 3.8151893325258737\n",
      "57     \t [0.95419104 0.99391944 0.31777697]. \t  0.05673756586560111 \t 3.8151893325258737\n",
      "58     \t [0.68517702 0.04633665 0.7821367 ]. \t  0.3828038218861167 \t 3.8151893325258737\n",
      "59     \t [0.2365173  0.19077569 0.7414871 ]. \t  1.0200235283282992 \t 3.8151893325258737\n",
      "60     \t [0.6926002  0.01666735 0.52810415]. \t  0.12583141797854358 \t 3.8151893325258737\n",
      "61     \t [0.48323297 0.97953335 0.58107143]. \t  1.7743492083378223 \t 3.8151893325258737\n",
      "62     \t [0.29621598 0.79790783 0.02054715]. \t  0.002024858861289507 \t 3.8151893325258737\n",
      "63     \t [0.97243547 0.39776351 0.90290875]. \t  2.7231173646354363 \t 3.8151893325258737\n",
      "64     \t [0.67144269 0.06370543 0.88091737]. \t  0.3904208308121737 \t 3.8151893325258737\n",
      "65     \t [0.25392813 0.87188832 0.77817404]. \t  1.7048137935058674 \t 3.8151893325258737\n",
      "66     \t [0.43391942 0.96545355 0.6232254 ]. \t  1.8531866038798177 \t 3.8151893325258737\n",
      "67     \t [0.02004482 0.15447473 0.24703621]. \t  0.6771287708568413 \t 3.8151893325258737\n",
      "68     \t [0.03434399 0.61503807 0.84049126]. \t  3.6869648987519668 \t 3.8151893325258737\n",
      "69     \t [0.05458561 0.38780484 0.02342249]. \t  0.06002266829451044 \t 3.8151893325258737\n",
      "70     \t [0.93801884 0.55742704 0.78516778]. \t  3.2134915628582204 \t 3.8151893325258737\n",
      "71     \t [0.34682653 0.0185079  0.84163519]. \t  0.2873547834147019 \t 3.8151893325258737\n",
      "72     \t [0.04159497 0.06039998 0.78206446]. \t  0.4289808497990568 \t 3.8151893325258737\n",
      "73     \t [0.74001869 0.56057665 0.76221524]. \t  3.0080293794656545 \t 3.8151893325258737\n",
      "74     \t [0.78632777 0.4286754  0.5041643 ]. \t  0.3065319733912303 \t 3.8151893325258737\n",
      "75     \t [0.57431136 0.18655666 0.37108816]. \t  0.6173683076972666 \t 3.8151893325258737\n",
      "76     \t [0.26793312 0.20835569 0.39616386]. \t  0.5675650435515753 \t 3.8151893325258737\n",
      "77     \t [0.63036437 0.10444753 0.50685531]. \t  0.2032777599608422 \t 3.8151893325258737\n",
      "78     \t [0.94754388 0.93130913 0.02533518]. \t  0.00016201526987493124 \t 3.8151893325258737\n",
      "79     \t [0.72749109 0.29778109 0.07816875]. \t  0.16771807928609878 \t 3.8151893325258737\n",
      "80     \t [0.24104627 0.05784681 0.35761507]. \t  0.7223868418715442 \t 3.8151893325258737\n",
      "81     \t [0.22224128 0.58835294 0.54227105]. \t  1.5620019707976203 \t 3.8151893325258737\n",
      "82     \t [0.50645353 0.96469724 0.40230175]. \t  0.8577685518309991 \t 3.8151893325258737\n",
      "83     \t [0.47760278 0.03970655 0.12418723]. \t  0.4918340278072209 \t 3.8151893325258737\n",
      "84     \t [0.7042768  0.90279546 0.76947643]. \t  1.0802503344142702 \t 3.8151893325258737\n",
      "85     \t [0.93029315 0.72624329 0.18884353]. \t  0.013666408812849307 \t 3.8151893325258737\n",
      "86     \t [0.38226498 0.56576795 0.31580928]. \t  0.2940891935717929 \t 3.8151893325258737\n",
      "87     \t [0.36123269 0.05714129 0.24442103]. \t  0.9498213276855749 \t 3.8151893325258737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.42658279 0.36008033 0.44964381]. \t  0.37058270938451565 \t 3.8151893325258737\n",
      "89     \t [0.07821525 0.28395456 0.59400979]. \t  0.5962106261528902 \t 3.8151893325258737\n",
      "90     \t [0.35700609 0.85474401 0.67761676]. \t  2.0935319664822267 \t 3.8151893325258737\n",
      "91     \t [0.95577186 0.2331745  0.18906347]. \t  0.25888078497782724 \t 3.8151893325258737\n",
      "92     \t [0.89335318 0.48621021 0.25719843]. \t  0.11947728525669717 \t 3.8151893325258737\n",
      "93     \t [0.25767767 0.32707301 0.11423834]. \t  0.30731655934584096 \t 3.8151893325258737\n",
      "94     \t [0.08343446 0.38720798 0.21858599]. \t  0.3610006026372162 \t 3.8151893325258737\n",
      "95     \t [0.74268025 0.72842831 0.68601941]. \t  1.5289709164819008 \t 3.8151893325258737\n",
      "96     \t [0.02572599 0.03736815 0.77135148]. \t  0.34568347538666316 \t 3.8151893325258737\n",
      "97     \t [0.69612463 0.41825352 0.06053865]. \t  0.08125363518274566 \t 3.8151893325258737\n",
      "98     \t [0.17078721 0.27754479 0.64585559]. \t  0.9021637496207571 \t 3.8151893325258737\n",
      "99     \t [0.67232233 0.97100767 0.99615183]. \t  0.4304963204316718 \t 3.8151893325258737\n",
      "100    \t [0.33960335 0.24229867 0.9555809 ]. \t  1.0608480606353172 \t 3.8151893325258737\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\n",
    "\n",
    "np.random.seed(run_num_4)\n",
    "surrogate_loser_4 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_4 = GPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_4.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.55735327 0.54176063 0.56042378]. \t  0.9599610140567721 \t 0.9810564697651996\n",
      "init   \t [0.38204809 0.11909418 0.84599912]. \t  0.6913364116577386 \t 0.9810564697651996\n",
      "init   \t [0.73974161 0.93235756 0.85932801]. \t  0.9810564697651996 \t 0.9810564697651996\n",
      "init   \t [0.70810463 0.68096875 0.4086023 ]. \t  0.4017443293787304 \t 0.9810564697651996\n",
      "init   \t [0.77458462 0.9844762  0.96548486]. \t  0.4769494155906823 \t 0.9810564697651996\n",
      "1      \t [0.         1.         0.70996449]. \t  \u001b[92m1.4248573182815316\u001b[0m \t 1.4248573182815316\n",
      "2      \t [0.92779976 0.07015106 0.89872933]. \t  0.3773619776486492 \t 1.4248573182815316\n",
      "3      \t [0.02311239 0.85121629 0.01672445]. \t  0.000979175972205631 \t 1.4248573182815316\n",
      "4      \t [0.12010582 0.59106076 0.95578026]. \t  \u001b[92m2.8456094195030626\u001b[0m \t 2.8456094195030626\n",
      "5      \t [0.46435384 0.46360957 0.98170214]. \t  2.1839422331213445 \t 2.8456094195030626\n",
      "6      \t [0.00198933 0.4672197  0.7091874 ]. \t  2.360146381618021 \t 2.8456094195030626\n",
      "7      \t [0.10422717 0.07186274 0.09377813]. \t  0.32180875456045027 \t 2.8456094195030626\n",
      "8      \t [0.9288332  0.06863405 0.08101552]. \t  0.13465766405489876 \t 2.8456094195030626\n",
      "9      \t [0.95865942 0.72495028 0.83845002]. \t  2.712478201608403 \t 2.8456094195030626\n",
      "10     \t [0.95644514 0.36678771 0.46830963]. \t  0.15636536592706055 \t 2.8456094195030626\n",
      "11     \t [0.97048148 0.5725626  0.97381238]. \t  2.4750997190639303 \t 2.8456094195030626\n",
      "12     \t [0.98431778 0.97728845 0.65410684]. \t  0.3460698616746582 \t 2.8456094195030626\n",
      "13     \t [0.39684022 0.46533085 0.29798335]. \t  0.35068722282708686 \t 2.8456094195030626\n",
      "14     \t [0.06927026 0.24993547 0.9685129 ]. \t  1.004531629071045 \t 2.8456094195030626\n",
      "15     \t [0.29117701 0.79906396 0.21189735]. \t  0.08424253943987578 \t 2.8456094195030626\n",
      "16     \t [0.73542419 0.17154116 0.13117851]. \t  0.3721146732362513 \t 2.8456094195030626\n",
      "17     \t [0.03538692 0.87260147 0.91229935]. \t  1.3922845520042249 \t 2.8456094195030626\n",
      "18     \t [0.0097183  0.73342815 0.65880477]. \t  2.5293795831783044 \t 2.8456094195030626\n",
      "19     \t [0.16605575 0.31484971 0.61033204]. \t  0.7886849227311512 \t 2.8456094195030626\n",
      "20     \t [0.78937418 0.94870826 0.39243625]. \t  0.3225235715036547 \t 2.8456094195030626\n",
      "21     \t [0.95838034 0.79362036 0.08587379]. \t  0.0017922585961232876 \t 2.8456094195030626\n",
      "22     \t [0.32959478 0.97977578 0.31699981]. \t  0.42556300209858455 \t 2.8456094195030626\n",
      "23     \t [0.12594363 0.68422837 0.07078373]. \t  0.012399148292579262 \t 2.8456094195030626\n",
      "24     \t [0.40475859 0.4658762  0.25740992]. \t  0.3253303343023408 \t 2.8456094195030626\n",
      "25     \t [0.71548507 0.98750527 0.46697302]. \t  0.6987097690521482 \t 2.8456094195030626\n",
      "26     \t [0.97590941 0.99579231 0.53571642]. \t  0.2867894657820969 \t 2.8456094195030626\n",
      "27     \t [0.64593789 0.48116769 0.76663233]. \t  \u001b[92m3.046702602216973\u001b[0m \t 3.046702602216973\n",
      "28     \t [0.67537018 0.05927558 0.00145966]. \t  0.08757814118957351 \t 3.046702602216973\n",
      "29     \t [0.84624275 0.55502505 0.96251073]. \t  2.7026173611483757 \t 3.046702602216973\n",
      "30     \t [0.86200592 0.93431391 0.21993853]. \t  0.0188642484337634 \t 3.046702602216973\n",
      "31     \t [0.28519781 0.40292405 0.13698136]. \t  0.26134883725309527 \t 3.046702602216973\n",
      "32     \t [0.5252679  0.54306335 0.56921457]. \t  1.0631090833926988 \t 3.046702602216973\n",
      "33     \t [0.04496167 0.15128783 0.07163235]. \t  0.22876159277394795 \t 3.046702602216973\n",
      "34     \t [0.17807663 0.98302126 0.19239364]. \t  0.05150638057792035 \t 3.046702602216973\n",
      "35     \t [0.10563847 0.51965299 0.35410946]. \t  0.3902774762175974 \t 3.046702602216973\n",
      "36     \t [0.36961533 0.38808606 0.64315993]. \t  1.2951624379659934 \t 3.046702602216973\n",
      "37     \t [0.75607284 0.43117565 0.85118749]. \t  \u001b[92m3.304700873877996\u001b[0m \t 3.304700873877996\n",
      "38     \t [0.08613721 0.892057   0.89648729]. \t  1.3071904162918941 \t 3.304700873877996\n",
      "39     \t [0.19581017 0.82604754 0.02575648]. \t  0.0016902112440418018 \t 3.304700873877996\n",
      "40     \t [0.9628041  0.00165349 0.86583751]. \t  0.22112418189446642 \t 3.304700873877996\n",
      "41     \t [0.61891633 0.22574784 0.45757517]. \t  0.3065004557945397 \t 3.304700873877996\n",
      "42     \t [0.05983348 0.37532816 0.45309878]. \t  0.3784370368464777 \t 3.304700873877996\n",
      "43     \t [0.70141011 0.22837892 0.54719061]. \t  0.28500028019394896 \t 3.304700873877996\n",
      "44     \t [0.90068885 0.51601282 0.81339203]. \t  \u001b[92m3.5116799583321114\u001b[0m \t 3.5116799583321114\n",
      "45     \t [0.99817048 0.97270388 0.19931396]. \t  0.005914829519934096 \t 3.5116799583321114\n",
      "46     \t [0.9406422  0.3104256  0.12201668]. \t  0.1370253403613483 \t 3.5116799583321114\n",
      "47     \t [0.78008751 0.91217743 0.09891658]. \t  0.001965946817253484 \t 3.5116799583321114\n",
      "48     \t [0.54025253 0.58494373 0.35184001]. \t  0.3047095942676279 \t 3.5116799583321114\n",
      "49     \t [0.34329903 0.88702102 0.03721005]. \t  0.0013660761576769008 \t 3.5116799583321114\n",
      "50     \t [0.03360824 0.00802081 0.36067209]. \t  0.48946990766554466 \t 3.5116799583321114\n",
      "51     \t [0.20787217 0.61004851 0.74845814]. \t  3.0360388299352037 \t 3.5116799583321114\n",
      "52     \t [0.89340077 0.85748575 0.59961431]. \t  0.6248569322835986 \t 3.5116799583321114\n",
      "53     \t [0.18926891 0.95140736 0.7658503 ]. \t  1.2879568215548085 \t 3.5116799583321114\n",
      "54     \t [0.40381616 0.65269767 0.05450903]. \t  0.015308395907646804 \t 3.5116799583321114\n",
      "55     \t [0.45580338 0.64567726 0.65293851]. \t  1.9857258482938995 \t 3.5116799583321114\n",
      "56     \t [0.54251355 0.85179015 0.45787374]. \t  1.2987563774658661 \t 3.5116799583321114\n",
      "57     \t [0.98766017 0.04640037 0.82327905]. \t  0.3724708599416452 \t 3.5116799583321114\n",
      "58     \t [0.11902505 0.69838178 0.02175544]. \t  0.00506596019128309 \t 3.5116799583321114\n",
      "59     \t [0.24986741 0.16910196 0.38781282]. \t  0.6183420024255487 \t 3.5116799583321114\n",
      "60     \t [0.03513873 0.21439434 0.78423176]. \t  1.3025923811717548 \t 3.5116799583321114\n",
      "61     \t [0.6361924  0.27829844 0.47523219]. \t  0.27509536117455013 \t 3.5116799583321114\n",
      "62     \t [0.59626604 0.05648247 0.91943951]. \t  0.3063033169372537 \t 3.5116799583321114\n",
      "63     \t [0.70429363 0.00312427 0.24408351]. \t  0.6167915982559065 \t 3.5116799583321114\n",
      "64     \t [0.88808978 0.22863736 0.34092134]. \t  0.33856711999431305 \t 3.5116799583321114\n",
      "65     \t [0.09824742 0.23858652 0.2844683 ]. \t  0.6927292643999329 \t 3.5116799583321114\n",
      "66     \t [0.14501681 0.47075112 0.91461001]. \t  3.2110042352024024 \t 3.5116799583321114\n",
      "67     \t [0.78434316 0.3956329  0.08473852]. \t  0.10096567793274311 \t 3.5116799583321114\n",
      "68     \t [0.18659218 0.45043702 0.89531113]. \t  3.285246583751448 \t 3.5116799583321114\n",
      "69     \t [0.03261414 0.66221918 0.05884548]. \t  0.011079641212285937 \t 3.5116799583321114\n",
      "70     \t [0.54468173 0.29590807 0.9311682 ]. \t  1.6539672441123188 \t 3.5116799583321114\n",
      "71     \t [0.01404798 0.87419782 0.65725577]. \t  2.4792575908001058 \t 3.5116799583321114\n",
      "72     \t [0.78235941 0.60061819 0.19284479]. \t  0.0561269513606585 \t 3.5116799583321114\n",
      "73     \t [0.34871399 0.92072751 0.39317276]. \t  1.1313363399092695 \t 3.5116799583321114\n",
      "74     \t [0.61380648 0.96105248 0.51387535]. \t  1.2481539430553115 \t 3.5116799583321114\n",
      "75     \t [0.7139411  0.86961409 0.66357915]. \t  1.0935382974348675 \t 3.5116799583321114\n",
      "76     \t [0.10004329 0.77380573 0.36903565]. \t  0.9767257832757625 \t 3.5116799583321114\n",
      "77     \t [0.32555747 0.87119144 0.02391585]. \t  0.001125942403551703 \t 3.5116799583321114\n",
      "78     \t [0.76932015 0.93434117 0.12890602]. \t  0.003831929221092921 \t 3.5116799583321114\n",
      "79     \t [0.20831485 0.14504721 0.92687713]. \t  0.6345433658676212 \t 3.5116799583321114\n",
      "80     \t [0.86204741 0.58837699 0.24105523]. \t  0.06403281689301106 \t 3.5116799583321114\n",
      "81     \t [0.46085152 0.56645311 0.57553687]. \t  1.2863944466686195 \t 3.5116799583321114\n",
      "82     \t [0.21069057 0.77407452 0.23344649]. \t  0.13121712040533615 \t 3.5116799583321114\n",
      "83     \t [0.1001701  0.47295882 0.12013719]. \t  0.12052624207518559 \t 3.5116799583321114\n",
      "84     \t [0.43359098 0.97200977 0.13762089]. \t  0.011141220909150746 \t 3.5116799583321114\n",
      "85     \t [0.73812931 0.77157111 0.42711039]. \t  0.5232747468742454 \t 3.5116799583321114\n",
      "86     \t [0.27448468 0.17200083 0.38396597]. \t  0.6423027740941983 \t 3.5116799583321114\n",
      "87     \t [0.62358364 0.8260888  0.13237028]. \t  0.009415497102037882 \t 3.5116799583321114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.75099457 0.65441938 0.12255032]. \t  0.021155966226623357 \t 3.5116799583321114\n",
      "89     \t [0.77697783 0.89252243 0.43042056]. \t  0.4993825044385206 \t 3.5116799583321114\n",
      "90     \t [0.10858734 0.34765346 0.2913764 ]. \t  0.4955393359039446 \t 3.5116799583321114\n",
      "91     \t [0.06013269 0.35026807 0.49586356]. \t  0.3954287956437817 \t 3.5116799583321114\n",
      "92     \t [0.57018049 0.66606921 0.11723059]. \t  0.02542004122614448 \t 3.5116799583321114\n",
      "93     \t [0.22195332 0.64359576 0.87193586]. \t  \u001b[92m3.5668333327348387\u001b[0m \t 3.5668333327348387\n",
      "94     \t [0.83406038 0.49926751 0.96336306]. \t  2.5839795928023706 \t 3.5668333327348387\n",
      "95     \t [0.85906739 0.60411887 0.59820621]. \t  0.8522079827951599 \t 3.5668333327348387\n",
      "96     \t [0.75833171 0.253738   0.0292391 ]. \t  0.09612694912065668 \t 3.5668333327348387\n",
      "97     \t [0.51508278 0.2294295  0.66213859]. \t  0.806875982384233 \t 3.5668333327348387\n",
      "98     \t [0.17960787 0.36340497 0.29699497]. \t  0.5075096027937913 \t 3.5668333327348387\n",
      "99     \t [0.59243786 0.96893281 0.2754484 ]. \t  0.13153647472057536 \t 3.5668333327348387\n",
      "100    \t [0.08917121 0.15241197 0.42408264]. \t  0.39739031089502425 \t 3.5668333327348387\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\n",
    "\n",
    "np.random.seed(run_num_5)\n",
    "surrogate_loser_5 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_5 = GPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_5.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.89286015 0.33197981 0.82122912]. \t  2.3879127825536575 \t 2.5106636917702634\n",
      "init   \t [0.04169663 0.10765668 0.59505206]. \t  0.2321934630074273 \t 2.5106636917702634\n",
      "init   \t [0.52981736 0.41880743 0.33540785]. \t  0.37989557867087576 \t 2.5106636917702634\n",
      "init   \t [0.62251943 0.43814143 0.73588211]. \t  2.5106636917702634 \t 2.5106636917702634\n",
      "init   \t [0.51803641 0.5788586  0.6453551 ]. \t  1.7289203948034264 \t 2.5106636917702634\n",
      "1      \t [0.46426722 0.31959039 0.90909418]. \t  2.0488314609076865 \t 2.5106636917702634\n",
      "2      \t [0.70898738 0.93302993 0.97435582]. \t  0.6738626249496847 \t 2.5106636917702634\n",
      "3      \t [0.75678295 0.02003835 0.75290539]. \t  0.287644796163147 \t 2.5106636917702634\n",
      "4      \t [0.97147912 0.88821771 0.207725  ]. \t  0.00947812438476387 \t 2.5106636917702634\n",
      "5      \t [0.99710485 0.42387924 0.61273974]. \t  0.8481431923526283 \t 2.5106636917702634\n",
      "6      \t [0.82762613 0.51322514 0.9423355 ]. \t  \u001b[92m2.9660960227381628\u001b[0m \t 2.9660960227381628\n",
      "7      \t [0.52154146 0.94289715 0.0141488 ]. \t  0.00041627150510049974 \t 2.9660960227381628\n",
      "8      \t [0.00676464 0.36240414 0.07004012]. \t  0.11516576142645876 \t 2.9660960227381628\n",
      "9      \t [0.95559854 0.87998361 0.87392421]. \t  1.3243935639269881 \t 2.9660960227381628\n",
      "10     \t [0.01988521 0.01189723 0.06101082]. \t  0.17333141682770897 \t 2.9660960227381628\n",
      "11     \t [0.76472106 0.79860774 0.67357588]. \t  1.1931746091910338 \t 2.9660960227381628\n",
      "12     \t [0.86589409 0.02143342 0.12579018]. \t  0.23857241047298577 \t 2.9660960227381628\n",
      "13     \t [0.06768473 0.98835432 0.43637529]. \t  1.7201092789518635 \t 2.9660960227381628\n",
      "14     \t [0.00960474 0.73501483 0.97670187]. \t  1.8789565432346857 \t 2.9660960227381628\n",
      "15     \t [0.14660221 0.55525094 0.39612766]. \t  0.6027790366540361 \t 2.9660960227381628\n",
      "16     \t [0.29280178 0.381848   0.94314101]. \t  2.2475614624833087 \t 2.9660960227381628\n",
      "17     \t [0.02884926 0.92107045 0.10038463]. \t  0.006359984812937052 \t 2.9660960227381628\n",
      "18     \t [0.68552777 0.90217347 0.08984633]. \t  0.0022844537103321114 \t 2.9660960227381628\n",
      "19     \t [0.24778727 0.51703487 0.30986352]. \t  0.3162293495446352 \t 2.9660960227381628\n",
      "20     \t [0.0332157  0.13370625 0.36932453]. \t  0.528056615697629 \t 2.9660960227381628\n",
      "21     \t [0.72570127 0.27053989 0.41939798]. \t  0.30570172226294623 \t 2.9660960227381628\n",
      "22     \t [0.22899771 0.88855773 0.66144849]. \t  2.3726789291100885 \t 2.9660960227381628\n",
      "23     \t [0.75660248 0.31819385 0.65511557]. \t  1.066387731221408 \t 2.9660960227381628\n",
      "24     \t [0.43367485 0.         0.        ]. \t  0.10096766099588854 \t 2.9660960227381628\n",
      "25     \t [1.         0.50488478 0.        ]. \t  0.007891266389360713 \t 2.9660960227381628\n",
      "26     \t [0.28925137 0.22910633 0.78929155]. \t  1.4476945024604517 \t 2.9660960227381628\n",
      "27     \t [0.04296544 0.92313739 0.7371167 ]. \t  1.6223698093850776 \t 2.9660960227381628\n",
      "28     \t [0.04117115 0.25557529 0.08207371]. \t  0.21370963201810345 \t 2.9660960227381628\n",
      "29     \t [0.52607309 0.28318212 0.15243849]. \t  0.4746576052019127 \t 2.9660960227381628\n",
      "30     \t [0.63939221 0.87737546 0.55554324]. \t  1.3686936883414307 \t 2.9660960227381628\n",
      "31     \t [0.4346958  0.60889655 0.8201429 ]. \t  \u001b[92m3.631042399892129\u001b[0m \t 3.631042399892129\n",
      "32     \t [0.13441687 0.86376956 0.57986455]. \t  3.0641504484097006 \t 3.631042399892129\n",
      "33     \t [0.55121056 0.62659953 0.55014174]. \t  1.195689113378892 \t 3.631042399892129\n",
      "34     \t [0.84182303 0.18550484 0.5241892 ]. \t  0.18986218486785345 \t 3.631042399892129\n",
      "35     \t [0.         0.68008636 0.78019863]. \t  3.0151899942303695 \t 3.631042399892129\n",
      "36     \t [0.90344081 0.17303917 0.15773467]. \t  0.2868981272674761 \t 3.631042399892129\n",
      "37     \t [0.35182468 0.62347921 0.93828355]. \t  3.0572181407413375 \t 3.631042399892129\n",
      "38     \t [0.04759745 0.15637077 0.32808732]. \t  0.6514615880268911 \t 3.631042399892129\n",
      "39     \t [0.19036093 0.6111129  0.7109371 ]. \t  2.675535818618081 \t 3.631042399892129\n",
      "40     \t [0.1862487  0.35049309 0.99584252]. \t  1.3618145428628112 \t 3.631042399892129\n",
      "41     \t [0.26109554 0.84873653 0.17389629]. \t  0.039416154328806276 \t 3.631042399892129\n",
      "42     \t [0.2411947  0.71089079 0.94830097]. \t  2.43940638666352 \t 3.631042399892129\n",
      "43     \t [0.95149478 0.05018079 0.53974879]. \t  0.098529041682867 \t 3.631042399892129\n",
      "44     \t [0.3233854  0.63321874 0.09207473]. \t  0.029931103458442163 \t 3.631042399892129\n",
      "45     \t [0.424223   0.12519416 0.67754617]. \t  0.4882972596028541 \t 3.631042399892129\n",
      "46     \t [0.1008011  0.69100806 0.11987073]. \t  0.02297185267312819 \t 3.631042399892129\n",
      "47     \t [0.85188265 0.04011227 0.89388837]. \t  0.2931216008636774 \t 3.631042399892129\n",
      "48     \t [0.01411906 0.26307292 0.27083666]. \t  0.5601798116996003 \t 3.631042399892129\n",
      "49     \t [0.07417431 0.28464511 0.69847017]. \t  1.327660116354567 \t 3.631042399892129\n",
      "50     \t [0.26391802 0.55233291 0.32548463]. \t  0.3396901722182805 \t 3.631042399892129\n",
      "51     \t [0.22836016 0.89651158 0.89707304]. \t  1.2740290385689355 \t 3.631042399892129\n",
      "52     \t [0.79818438 0.16817142 0.70514572]. \t  0.7416214286597129 \t 3.631042399892129\n",
      "53     \t [0.83527072 0.19572695 0.83014434]. \t  1.1944209938070887 \t 3.631042399892129\n",
      "54     \t [0.59300454 0.3111237  0.40598328]. \t  0.3818134617640583 \t 3.631042399892129\n",
      "55     \t [0.86854957 0.05055449 0.32404131]. \t  0.411411898931723 \t 3.631042399892129\n",
      "56     \t [0.37957024 0.35889641 0.92977225]. \t  2.2145067119344692 \t 3.631042399892129\n",
      "57     \t [0.13634234 0.05691787 0.98232037]. \t  0.19448113363052155 \t 3.631042399892129\n",
      "58     \t [0.5075492  0.04414528 0.82694882]. \t  0.3748726186448098 \t 3.631042399892129\n",
      "59     \t [0.05523261 0.01074321 0.57580439]. \t  0.11260749788850297 \t 3.631042399892129\n",
      "60     \t [0.18406096 0.38254387 0.22959094]. \t  0.43853690353229396 \t 3.631042399892129\n",
      "61     \t [0.68966182 0.15913004 0.20567779]. \t  0.6440137859907251 \t 3.631042399892129\n",
      "62     \t [0.65944577 0.19945516 0.65270416]. \t  0.6343995370203407 \t 3.631042399892129\n",
      "63     \t [0.21789836 0.46273006 0.66346585]. \t  1.8309817451061736 \t 3.631042399892129\n",
      "64     \t [0.07858971 0.62389709 0.13486611]. \t  0.04325978992777502 \t 3.631042399892129\n",
      "65     \t [0.56068032 0.24475387 0.08301478]. \t  0.2746559213717259 \t 3.631042399892129\n",
      "66     \t [0.61004954 0.97795743 0.5380338 ]. \t  1.2796657235618687 \t 3.631042399892129\n",
      "67     \t [0.66767275 0.75152206 0.79946861]. \t  2.3942920894158 \t 3.631042399892129\n",
      "68     \t [0.66563017 0.97212126 0.85647134]. \t  0.7520519051826682 \t 3.631042399892129\n",
      "69     \t [0.36162685 0.03053591 0.18498491]. \t  0.7571886080164373 \t 3.631042399892129\n",
      "70     \t [0.23469791 0.19626658 0.46905979]. \t  0.33214498871143167 \t 3.631042399892129\n",
      "71     \t [0.59802231 0.44790879 0.9045159 ]. \t  3.1786990245245974 \t 3.631042399892129\n",
      "72     \t [0.05790537 0.31858793 0.16691747]. \t  0.36982693021208274 \t 3.631042399892129\n",
      "73     \t [0.48766107 0.82537961 0.10713698]. \t  0.007622744217565988 \t 3.631042399892129\n",
      "74     \t [0.28141591 0.1240646  0.78396034]. \t  0.7251109616876069 \t 3.631042399892129\n",
      "75     \t [0.80008984 0.66469512 0.7276977 ]. \t  2.1404400065437885 \t 3.631042399892129\n",
      "76     \t [0.83401196 0.42203083 0.26259397]. \t  0.2125427448756169 \t 3.631042399892129\n",
      "77     \t [0.10405245 0.38935579 0.28404873]. \t  0.4153620553364183 \t 3.631042399892129\n",
      "78     \t [0.19158302 0.59681087 0.89878592]. \t  3.597480156692398 \t 3.631042399892129\n",
      "79     \t [0.26700239 0.05492885 0.29627968]. \t  0.9101782778992026 \t 3.631042399892129\n",
      "80     \t [0.27094016 0.33545742 0.61111408]. \t  0.8566533054218431 \t 3.631042399892129\n",
      "81     \t [0.33510451 0.26649245 0.92074688]. \t  1.4902139984299103 \t 3.631042399892129\n",
      "82     \t [0.41379935 0.4010954  0.50652043]. \t  0.4806340370735916 \t 3.631042399892129\n",
      "83     \t [0.06417411 0.59519414 0.71752662]. \t  2.730494212921118 \t 3.631042399892129\n",
      "84     \t [0.3349528  0.03801653 0.69238951]. \t  0.27286909381495894 \t 3.631042399892129\n",
      "85     \t [0.86768521 0.78698273 0.50741913]. \t  0.5257951628418611 \t 3.631042399892129\n",
      "86     \t [0.27408507 0.37107772 0.1495041 ]. \t  0.33823696322386365 \t 3.631042399892129\n",
      "87     \t [0.81845974 0.30297021 0.8933031 ]. \t  1.964727173041564 \t 3.631042399892129\n",
      "88     \t [0.69823095 0.49754794 0.62276322]. \t  1.1805592927479978 \t 3.631042399892129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.11019052 0.78583376 0.0661553 ]. \t  0.004931920308375605 \t 3.631042399892129\n",
      "90     \t [0.52215181 0.38692792 0.85436559]. \t  2.9769880833994264 \t 3.631042399892129\n",
      "91     \t [0.82915108 0.43047211 0.72997315]. \t  2.3337490349988457 \t 3.631042399892129\n",
      "92     \t [0.49310147 0.63063374 0.28767256]. \t  0.19396851355912742 \t 3.631042399892129\n",
      "93     \t [0.04932664 0.6633557  0.68319437]. \t  2.5067863857295816 \t 3.631042399892129\n",
      "94     \t [0.57858158 0.81528447 0.46693551]. \t  1.2134303056389917 \t 3.631042399892129\n",
      "95     \t [0.78572918 0.17154958 0.0568002 ]. \t  0.1525474308893347 \t 3.631042399892129\n",
      "96     \t [0.11506585 0.08818824 0.01786167]. \t  0.1264166391740304 \t 3.631042399892129\n",
      "97     \t [0.37050387 0.23864815 0.37059348]. \t  0.6477204306487983 \t 3.631042399892129\n",
      "98     \t [0.41873938 0.60890389 0.89831832]. \t  3.57023334698979 \t 3.631042399892129\n",
      "99     \t [0.21683791 0.49017501 0.76770751]. \t  3.185509555298485 \t 3.631042399892129\n",
      "100    \t [0.785572   0.05093827 0.50973963]. \t  0.1367271173738578 \t 3.631042399892129\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\n",
    "\n",
    "np.random.seed(run_num_6)\n",
    "surrogate_loser_6 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_6 = GPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_6.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.15266373 0.30235661 0.06203641]. \t  0.1742288077468587 \t 1.6237282255098657\n",
      "init   \t [0.45986034 0.83525338 0.92699705]. \t  1.6237282255098657 \t 1.6237282255098657\n",
      "init   \t [0.72698898 0.76849622 0.26920507]. \t  0.08405715787489784 \t 1.6237282255098657\n",
      "init   \t [0.64402929 0.09337326 0.07968589]. \t  0.2756481743251193 \t 1.6237282255098657\n",
      "init   \t [0.58961375 0.34334054 0.98887615]. \t  1.4020548914621052 \t 1.6237282255098657\n",
      "1      \t [0.0095305  0.06847696 0.97789675]. \t  0.22422993094430546 \t 1.6237282255098657\n",
      "2      \t [0.00962764 0.96956788 0.65396657]. \t  \u001b[92m2.132425580713128\u001b[0m \t 2.132425580713128\n",
      "3      \t [0.97409469 0.03143273 0.64953327]. \t  0.1879512798009964 \t 2.132425580713128\n",
      "4      \t [0.22595933 0.95348785 0.12294433]. \t  0.010520248665944764 \t 2.132425580713128\n",
      "5      \t [0.97122775 0.93199907 0.87171651]. \t  0.9308657054768225 \t 2.132425580713128\n",
      "6      \t [0.3714494  0.05992124 0.47930973]. \t  0.2780879042129226 \t 2.132425580713128\n",
      "7      \t [0.08978758 0.58671321 0.7206821 ]. \t  \u001b[92m2.7673013179295816\u001b[0m \t 2.7673013179295816\n",
      "8      \t [0.03685289 0.49116928 0.9040006 ]. \t  \u001b[92m3.4062663378427063\u001b[0m \t 3.4062663378427063\n",
      "9      \t [9.85660232e-01 7.88246377e-01 8.84621003e-04]. \t  0.00044769353083181577 \t 3.4062663378427063\n",
      "10     \t [0.9465703  0.61593804 0.8230332 ]. \t  \u001b[92m3.409651114729523\u001b[0m \t 3.409651114729523\n",
      "11     \t [0.         0.72985365 1.        ]. \t  1.590649948255382 \t 3.409651114729523\n",
      "12     \t [0.98382715 0.27993618 0.29751896]. \t  0.24196930797294067 \t 3.409651114729523\n",
      "13     \t [0.00580799 0.3712785  0.82623368]. \t  2.794577363798754 \t 3.409651114729523\n",
      "14     \t [0.32910241 0.02426588 0.07541194]. \t  0.3025913601388581 \t 3.409651114729523\n",
      "15     \t [0.48586458 0.50925965 0.78303676]. \t  3.362885414060049 \t 3.409651114729523\n",
      "16     \t [0.02599962 0.80037721 0.76595196]. \t  2.2200012557864595 \t 3.409651114729523\n",
      "17     \t [0.20125009 0.47131934 0.88759595]. \t  \u001b[92m3.4774994382775026\u001b[0m \t 3.4774994382775026\n",
      "18     \t [0.98211039 0.65919527 0.59261324]. \t  0.6482813411459536 \t 3.4774994382775026\n",
      "19     \t [0.98410255 0.31170284 0.82693474]. \t  2.184864208659012 \t 3.4774994382775026\n",
      "20     \t [0.4130355  0.00785469 0.12824955]. \t  0.4941035975615608 \t 3.4774994382775026\n",
      "21     \t [0.30533773 0.01277795 0.22343655]. \t  0.8366289500550946 \t 3.4774994382775026\n",
      "22     \t [0.78705306 0.66304608 0.9348571 ]. \t  2.8612161320195604 \t 3.4774994382775026\n",
      "23     \t [0.75381232 0.99392978 0.70725623]. \t  0.6068270173563677 \t 3.4774994382775026\n",
      "24     \t [0.02404341 0.0942877  0.62108141]. \t  0.25719593506293176 \t 3.4774994382775026\n",
      "25     \t [0.6520758  0.26029302 0.04619241]. \t  0.1477154540736463 \t 3.4774994382775026\n",
      "26     \t [0.43665953 0.03481133 0.11106203]. \t  0.4432502741230613 \t 3.4774994382775026\n",
      "27     \t [0.21798613 0.03979611 0.46539245]. \t  0.28887198045196705 \t 3.4774994382775026\n",
      "28     \t [0.98696718 0.46573214 0.96253201]. \t  2.4232975691625884 \t 3.4774994382775026\n",
      "29     \t [0.29933128 0.32524601 0.26997656]. \t  0.6507621925232626 \t 3.4774994382775026\n",
      "30     \t [0.04246869 0.2875836  0.97840552]. \t  1.1541184522251169 \t 3.4774994382775026\n",
      "31     \t [0.41417838 0.19311361 0.76261502]. \t  1.1087495702662022 \t 3.4774994382775026\n",
      "32     \t [0.87203038 0.42640354 0.3821151 ]. \t  0.16160955464686083 \t 3.4774994382775026\n",
      "33     \t [0.72418083 1.         0.        ]. \t  0.00011344635306565488 \t 3.4774994382775026\n",
      "34     \t [0.81241931 0.36064909 0.72616583]. \t  1.963127413588715 \t 3.4774994382775026\n",
      "35     \t [0.34502756 0.00681989 0.10873278]. \t  0.41586096294873914 \t 3.4774994382775026\n",
      "36     \t [0.43447726 0.04955604 0.96719786]. \t  0.20718758555534678 \t 3.4774994382775026\n",
      "37     \t [0.25432862 0.76856548 0.28473518]. \t  0.29737793386262007 \t 3.4774994382775026\n",
      "38     \t [0.04794507 0.26936512 0.54927872]. \t  0.3816210624341058 \t 3.4774994382775026\n",
      "39     \t [0.58522174 0.58869764 0.78062516]. \t  3.2728937092037884 \t 3.4774994382775026\n",
      "40     \t [0.93189822 0.24939339 0.75233946]. \t  1.4182079501843896 \t 3.4774994382775026\n",
      "41     \t [0.17802938 0.0294758  0.79506928]. \t  0.3313348891871022 \t 3.4774994382775026\n",
      "42     \t [0.97651534 0.4841796  0.21944172]. \t  0.08253771839622574 \t 3.4774994382775026\n",
      "43     \t [0.71300613 0.53677098 0.93514556]. \t  3.1533828886945177 \t 3.4774994382775026\n",
      "44     \t [0.62700145 0.8012262  0.97846886]. \t  1.4347536734805706 \t 3.4774994382775026\n",
      "45     \t [0.22934906 0.18684188 0.3254738 ]. \t  0.8182322861277761 \t 3.4774994382775026\n",
      "46     \t [0.87761517 0.03556698 0.76435623]. \t  0.3351948214917791 \t 3.4774994382775026\n",
      "47     \t [0.0643714  0.08522135 0.27871535]. \t  0.7473716413693425 \t 3.4774994382775026\n",
      "48     \t [0.46313356 0.47927302 0.88073863]. \t  \u001b[92m3.5704663420411387\u001b[0m \t 3.5704663420411387\n",
      "49     \t [0.23583495 0.52125632 0.26348166]. \t  0.25030329997336537 \t 3.5704663420411387\n",
      "50     \t [0.77135093 0.65990973 0.91322584]. \t  3.1234636405375196 \t 3.5704663420411387\n",
      "51     \t [0.59853585 0.3089528  0.64636139]. \t  0.9878666569123074 \t 3.5704663420411387\n",
      "52     \t [0.65619053 0.28700976 0.08111451]. \t  0.20672553862945914 \t 3.5704663420411387\n",
      "53     \t [0.11224669 0.42759766 0.89438893]. \t  3.1159234782761778 \t 3.5704663420411387\n",
      "54     \t [0.56877483 0.15510009 0.37961578]. \t  0.6073058194865852 \t 3.5704663420411387\n",
      "55     \t [0.88504033 0.87772647 0.67648178]. \t  0.7400437002684781 \t 3.5704663420411387\n",
      "56     \t [0.88778347 0.032233   0.87823258]. \t  0.2887325615180171 \t 3.5704663420411387\n",
      "57     \t [0.63252512 0.39155287 0.29382626]. \t  0.3917616767011882 \t 3.5704663420411387\n",
      "58     \t [0.78302483 0.84043631 0.11391371]. \t  0.0038097353465152486 \t 3.5704663420411387\n",
      "59     \t [0.30983316 0.54263777 0.86647888]. \t  \u001b[92m3.83722875791526\u001b[0m \t 3.83722875791526\n",
      "60     \t [0.93884062 0.6907784  0.40388567]. \t  0.15680004897093414 \t 3.83722875791526\n",
      "61     \t [0.45611335 0.03406024 0.61887374]. \t  0.1704663937816772 \t 3.83722875791526\n",
      "62     \t [0.39141075 0.6682249  0.32772495]. \t  0.37526470986621746 \t 3.83722875791526\n",
      "63     \t [0.93984625 0.87229769 0.12249176]. \t  0.0020613546464448135 \t 3.83722875791526\n",
      "64     \t [0.10799289 0.49298973 0.43224513]. \t  0.5767401050317421 \t 3.83722875791526\n",
      "65     \t [0.3998278  0.62664926 0.90508839]. \t  3.44288732608451 \t 3.83722875791526\n",
      "66     \t [0.83943545 0.77914392 0.08044774]. \t  0.0029026804888624066 \t 3.83722875791526\n",
      "67     \t [0.61241343 0.97717877 0.79372164]. \t  0.7635983690255415 \t 3.83722875791526\n",
      "68     \t [0.44986045 0.04096689 0.43530225]. \t  0.40659493777246697 \t 3.83722875791526\n",
      "69     \t [0.39279359 0.18140795 0.97664482]. \t  0.5999814463912994 \t 3.83722875791526\n",
      "70     \t [0.94246849 0.08998696 0.21367445]. \t  0.33946349176507806 \t 3.83722875791526\n",
      "71     \t [0.38151606 0.64937266 0.67849598]. \t  2.2736534531887607 \t 3.83722875791526\n",
      "72     \t [0.93543722 0.25526505 0.61043679]. \t  0.5372273868015285 \t 3.83722875791526\n",
      "73     \t [0.93182818 0.58102934 0.27465297]. \t  0.061145639167543266 \t 3.83722875791526\n",
      "74     \t [0.75148069 0.37839204 0.86510762]. \t  2.8356358916260005 \t 3.83722875791526\n",
      "75     \t [0.53842689 0.20209545 0.03517025]. \t  0.16946266418547523 \t 3.83722875791526\n",
      "76     \t [0.66666872 0.9879886  0.97856614]. \t  0.4295463085574229 \t 3.83722875791526\n",
      "77     \t [0.81938583 0.15484641 0.33275164]. \t  0.47376936237519857 \t 3.83722875791526\n",
      "78     \t [0.12984349 0.95836702 0.14229128]. \t  0.017391810552035626 \t 3.83722875791526\n",
      "79     \t [0.60561798 0.49346954 0.56466118]. \t  0.8014164770549999 \t 3.83722875791526\n",
      "80     \t [0.41979638 0.58009508 0.62413737]. \t  1.712321970347566 \t 3.83722875791526\n",
      "81     \t [0.44280206 0.63711349 0.86091196]. \t  3.611710810406915 \t 3.83722875791526\n",
      "82     \t [0.9833747  0.07992838 0.06819232]. \t  0.09673389388243601 \t 3.83722875791526\n",
      "83     \t [0.68086621 0.0946363  0.20945249]. \t  0.6721802110522616 \t 3.83722875791526\n",
      "84     \t [0.01984272 0.53509364 0.47590447]. \t  0.8991836318454244 \t 3.83722875791526\n",
      "85     \t [0.18697394 0.06951492 0.33744015]. \t  0.7658020018088214 \t 3.83722875791526\n",
      "86     \t [0.29234884 0.38878995 0.02698622]. \t  0.08307067463903102 \t 3.83722875791526\n",
      "87     \t [0.9501523  0.45799694 0.58188769]. \t  0.6322108433550255 \t 3.83722875791526\n",
      "88     \t [0.99377564 0.68733491 0.46147929]. \t  0.20195144177128935 \t 3.83722875791526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.12129095 0.65963091 0.57861225]. \t  2.2560765840740222 \t 3.83722875791526\n",
      "90     \t [0.61533137 0.16596426 0.11691732]. \t  0.41290596966549614 \t 3.83722875791526\n",
      "91     \t [0.1908268  0.52539051 0.95233098]. \t  2.8886375190571583 \t 3.83722875791526\n",
      "92     \t [0.71387159 0.68811182 0.42912745]. \t  0.4759020563078621 \t 3.83722875791526\n",
      "93     \t [0.25601229 0.57490552 0.53964237]. \t  1.4320519550088604 \t 3.83722875791526\n",
      "94     \t [0.66192095 0.37186563 0.85521659]. \t  2.8173963307276444 \t 3.83722875791526\n",
      "95     \t [0.3834012  0.00784596 0.41197929]. \t  0.47790389594330634 \t 3.83722875791526\n",
      "96     \t [0.28167736 0.09390389 0.82019296]. \t  0.5800442303293563 \t 3.83722875791526\n",
      "97     \t [0.53381469 0.51381329 0.15333844]. \t  0.1331021987449858 \t 3.83722875791526\n",
      "98     \t [0.43305606 0.25897579 0.04973795]. \t  0.19518771727017228 \t 3.83722875791526\n",
      "99     \t [0.06620756 0.07810312 0.39393038]. \t  0.4691539503144004 \t 3.83722875791526\n",
      "100    \t [0.51140729 0.95279908 0.47172793]. \t  1.417731105179221 \t 3.83722875791526\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\n",
    "\n",
    "np.random.seed(run_num_7)\n",
    "surrogate_loser_7 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_7 = GPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_7.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.10949987 0.04221501 0.59969258]. \t  0.15732379460832624 \t 0.8830091449513892\n",
      "init   \t [0.29987071 0.79661178 0.36619613]. \t  0.8830091449513892 \t 0.8830091449513892\n",
      "init   \t [0.88060834 0.29784163 0.32910326]. \t  0.2992873424905509 \t 0.8830091449513892\n",
      "init   \t [0.56871692 0.74392742 0.05385289]. \t  0.0051668955191694075 \t 0.8830091449513892\n",
      "init   \t [0.60207437 0.42931858 0.20547361]. \t  0.2908249205255991 \t 0.8830091449513892\n",
      "1      \t [0.82808055 0.95668265 0.90976448]. \t  0.7541633809323007 \t 0.8830091449513892\n",
      "2      \t [0.09250474 0.65862611 0.98455533]. \t  \u001b[92m2.16574947683527\u001b[0m \t 2.16574947683527\n",
      "3      \t [0.69332868 0.16959446 0.9783922 ]. \t  0.5362860668682835 \t 2.16574947683527\n",
      "4      \t [0.68307375 0.03550334 0.03657775]. \t  0.1409270525778632 \t 2.16574947683527\n",
      "5      \t [0.11068717 0.14200085 0.01146429]. \t  0.11419914532673142 \t 2.16574947683527\n",
      "6      \t [0.03242071 0.97346284 0.89384909]. \t  0.7560821232330165 \t 2.16574947683527\n",
      "7      \t [0.03589703 0.52950655 0.58126809]. \t  1.4370584070410368 \t 2.16574947683527\n",
      "8      \t [0.10377104 0.79581918 0.09490675]. \t  0.008284831979292443 \t 2.16574947683527\n",
      "9      \t [0.48739873 0.58974076 0.98082375]. \t  \u001b[92m2.4227838480742547\u001b[0m \t 2.4227838480742547\n",
      "10     \t [0.97332082 0.99909138 0.02867902]. \t  9.292333815586823e-05 \t 2.4227838480742547\n",
      "11     \t [0.9906633  0.6042244  0.77715191]. \t  \u001b[92m2.9598095568459186\u001b[0m \t 2.9598095568459186\n",
      "12     \t [0.77613797 0.53878338 0.64743613]. \t  1.4084814358616347 \t 2.9598095568459186\n",
      "13     \t [1.         0.57213136 1.        ]. \t  2.0083577428636286 \t 2.9598095568459186\n",
      "14     \t [0.24616264 0.07820646 0.98944629]. \t  0.2247034423793938 \t 2.9598095568459186\n",
      "15     \t [0.1914126  0.64581687 0.86492381]. \t  \u001b[92m3.5701181855273822\u001b[0m \t 3.5701181855273822\n",
      "16     \t [0.99580667 0.27194136 0.01705326]. \t  0.036965452639109435 \t 3.5701181855273822\n",
      "17     \t [0.31650606 0.54822602 0.74681844]. \t  3.028280798543296 \t 3.5701181855273822\n",
      "18     \t [0.30792028 0.85027456 0.92878414]. \t  1.495799410590166 \t 3.5701181855273822\n",
      "19     \t [0.72669299 0.91658801 0.82717177]. \t  1.0773200898112971 \t 3.5701181855273822\n",
      "20     \t [0.48588757 0.09949652 0.33749825]. \t  0.8276222574425918 \t 3.5701181855273822\n",
      "21     \t [0.98788057 0.77798366 0.7741983 ]. \t  1.7717396794624731 \t 3.5701181855273822\n",
      "22     \t [0.18037212 0.90226405 0.62511662]. \t  2.713771239385097 \t 3.5701181855273822\n",
      "23     \t [0.90632425 0.12502009 0.68561426]. \t  0.4949941621548021 \t 3.5701181855273822\n",
      "24     \t [0.55003502 0.01057428 0.80649535]. \t  0.2782023000581064 \t 3.5701181855273822\n",
      "25     \t [0.381375   0.68999117 0.39132503]. \t  0.8027532689164403 \t 3.5701181855273822\n",
      "26     \t [0.00312984 0.89922714 0.71033415]. \t  1.9149173508753687 \t 3.5701181855273822\n",
      "27     \t [0.19484077 0.78744943 0.43747096]. \t  1.8239120365786972 \t 3.5701181855273822\n",
      "28     \t [0.59115682 0.52130064 0.74278386]. \t  2.8282431935809615 \t 3.5701181855273822\n",
      "29     \t [0.77437245 0.53905503 0.38781696]. \t  0.1917749111377073 \t 3.5701181855273822\n",
      "30     \t [0.94176069 0.20788213 0.78219447]. \t  1.2289722585901697 \t 3.5701181855273822\n",
      "31     \t [0.76372796 0.88253182 0.97112984]. \t  0.9612361076011148 \t 3.5701181855273822\n",
      "32     \t [0.97102822 0.5982502  0.91255014]. \t  3.3300000947761403 \t 3.5701181855273822\n",
      "33     \t [0.50935375 0.65674599 0.06578369]. \t  0.01602800707847871 \t 3.5701181855273822\n",
      "34     \t [0.25736721 0.06043309 0.54387752]. \t  0.16930147927337816 \t 3.5701181855273822\n",
      "35     \t [0.28007331 0.56914378 0.822542  ]. \t  \u001b[92m3.7672150031822547\u001b[0m \t 3.7672150031822547\n",
      "36     \t [1.         0.69326112 0.        ]. \t  0.0013019479939693722 \t 3.7672150031822547\n",
      "37     \t [0.43612477 0.74315716 0.99982543]. \t  1.5449453903180206 \t 3.7672150031822547\n",
      "38     \t [0.06549346 0.38284032 0.59027247]. \t  0.8699407615245407 \t 3.7672150031822547\n",
      "39     \t [0.74109156 0.91761747 0.86118205]. \t  1.0853144889793076 \t 3.7672150031822547\n",
      "40     \t [0.73796804 0.9196598  0.81750362]. \t  1.037619834944521 \t 3.7672150031822547\n",
      "41     \t [0.87099602 0.3752066  0.34801774]. \t  0.21483581963381862 \t 3.7672150031822547\n",
      "42     \t [0.85677037 0.91573528 0.12124502]. \t  0.0024014498313956016 \t 3.7672150031822547\n",
      "43     \t [0.09669857 0.27306876 0.92663206]. \t  1.4897573498295789 \t 3.7672150031822547\n",
      "44     \t [0.13321294 0.88617355 0.83118368]. \t  1.5221620399944809 \t 3.7672150031822547\n",
      "45     \t [0.74090445 0.65481877 0.37955992]. \t  0.2561334240916374 \t 3.7672150031822547\n",
      "46     \t [0.         1.         0.30996995]. \t  0.4090723705146404 \t 3.7672150031822547\n",
      "47     \t [0.55373763 0.4016182  0.50618592]. \t  0.41152024536352294 \t 3.7672150031822547\n",
      "48     \t [0.88979299 0.5876564  0.16428358]. \t  0.0373805242846916 \t 3.7672150031822547\n",
      "49     \t [0.83734842 0.94579949 0.63069426]. \t  0.6291403642545439 \t 3.7672150031822547\n",
      "50     \t [0.83356731 0.99580414 0.24430581]. \t  0.029936131053122515 \t 3.7672150031822547\n",
      "51     \t [0.26806168 0.92072454 0.57991583]. \t  2.7504536542836218 \t 3.7672150031822547\n",
      "52     \t [0.94182564 0.0228358  0.47456489]. \t  0.1103708972030579 \t 3.7672150031822547\n",
      "53     \t [0.12816539 0.9651931  0.92527938]. \t  0.7167927946681384 \t 3.7672150031822547\n",
      "54     \t [0.39932832 0.94334256 0.46489544]. \t  1.7494874580703168 \t 3.7672150031822547\n",
      "55     \t [0.48724739 0.10690739 0.74174769]. \t  0.5810135538097464 \t 3.7672150031822547\n",
      "56     \t [0.92705413 0.12267879 0.87792272]. \t  0.6413473884501208 \t 3.7672150031822547\n",
      "57     \t [0.51834049 0.15609498 0.94885667]. \t  0.6066989659617829 \t 3.7672150031822547\n",
      "58     \t [0.080072   0.77547307 0.31861164]. \t  0.5209809020416881 \t 3.7672150031822547\n",
      "59     \t [0.54906896 0.90971067 0.16178112]. \t  0.01734433727723192 \t 3.7672150031822547\n",
      "60     \t [0.07648824 0.87617784 0.86502903]. \t  1.5338808447104273 \t 3.7672150031822547\n",
      "61     \t [0.65244562 0.21314933 0.18035824]. \t  0.5712228373905991 \t 3.7672150031822547\n",
      "62     \t [0.47071596 0.08439746 0.03367211]. \t  0.18652644999599177 \t 3.7672150031822547\n",
      "63     \t [0.08626338 0.32835822 0.30513615]. \t  0.50713427508067 \t 3.7672150031822547\n",
      "64     \t [0.07627262 0.60592078 0.41965131]. \t  0.9049507477790698 \t 3.7672150031822547\n",
      "65     \t [0.94888622 0.80625696 0.31376992]. \t  0.06400865401218608 \t 3.7672150031822547\n",
      "66     \t [0.49843812 0.60544155 0.73993733]. \t  2.788419670954484 \t 3.7672150031822547\n",
      "67     \t [0.77344706 0.99505961 0.61890148]. \t  0.6851708749626142 \t 3.7672150031822547\n",
      "68     \t [0.16201451 0.72884377 0.99655492]. \t  1.6594071824084362 \t 3.7672150031822547\n",
      "69     \t [0.6653514  0.13682213 0.54446613]. \t  0.20445606621572807 \t 3.7672150031822547\n",
      "70     \t [0.36867418 0.71232602 0.73035973]. \t  2.483995828835785 \t 3.7672150031822547\n",
      "71     \t [0.14683982 0.04030608 0.72601804]. \t  0.3195768296436863 \t 3.7672150031822547\n",
      "72     \t [0.27952306 0.33902367 0.34938724]. \t  0.5363309575637881 \t 3.7672150031822547\n",
      "73     \t [0.75972655 0.23410555 0.83296686]. \t  1.5149225869359015 \t 3.7672150031822547\n",
      "74     \t [0.59815987 0.48809715 0.61795398]. \t  1.2066492035041683 \t 3.7672150031822547\n",
      "75     \t [0.67189194 0.85942577 0.68460476]. \t  1.2313802906482274 \t 3.7672150031822547\n",
      "76     \t [0.60008011 0.92496505 0.52429065]. \t  1.4087284930885242 \t 3.7672150031822547\n",
      "77     \t [0.2444321  0.50485488 0.50788661]. \t  0.8987089976929616 \t 3.7672150031822547\n",
      "78     \t [0.81769418 0.13001474 0.88255499]. \t  0.6765649630943437 \t 3.7672150031822547\n",
      "79     \t [0.11151363 0.36185785 0.05050592]. \t  0.1099957916682531 \t 3.7672150031822547\n",
      "80     \t [0.15873662 0.25637374 0.46219008]. \t  0.3342996983425587 \t 3.7672150031822547\n",
      "81     \t [0.49180021 0.67289655 0.09640609]. \t  0.020477956936715228 \t 3.7672150031822547\n",
      "82     \t [0.98610069 0.4425689  0.12887743]. \t  0.062393181353264854 \t 3.7672150031822547\n",
      "83     \t [0.93180025 0.65465301 0.58391503]. \t  0.6572760948948935 \t 3.7672150031822547\n",
      "84     \t [0.58271614 0.30385449 0.08007295]. \t  0.21489779549585372 \t 3.7672150031822547\n",
      "85     \t [0.10362691 0.06223745 0.63789004]. \t  0.233616068741885 \t 3.7672150031822547\n",
      "86     \t [0.2574242  0.34725895 0.79140007]. \t  2.4829744875783115 \t 3.7672150031822547\n",
      "87     \t [0.51712563 0.9804653  0.41579239]. \t  0.9115376651984629 \t 3.7672150031822547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.77901981 0.30643979 0.83347927]. \t  2.1826575662159646 \t 3.7672150031822547\n",
      "89     \t [0.22800456 0.11021078 0.54544231]. \t  0.2056988119882285 \t 3.7672150031822547\n",
      "90     \t [0.16234545 0.15090905 0.73449207]. \t  0.7693729649432159 \t 3.7672150031822547\n",
      "91     \t [0.45222453 0.34502697 0.26254166]. \t  0.5921939442113907 \t 3.7672150031822547\n",
      "92     \t [0.54932523 0.36911071 0.26815621]. \t  0.4919621604955257 \t 3.7672150031822547\n",
      "93     \t [0.81733701 0.48742779 0.94661913]. \t  2.8131570579414014 \t 3.7672150031822547\n",
      "94     \t [0.21311319 0.7802126  0.09413853]. \t  0.009240500093104067 \t 3.7672150031822547\n",
      "95     \t [0.8763063  0.41730623 0.28379792]. \t  0.19368493148320307 \t 3.7672150031822547\n",
      "96     \t [0.33441288 0.55681958 0.67261602]. \t  2.1628781305061278 \t 3.7672150031822547\n",
      "97     \t [0.67963353 0.3424413  0.72058914]. \t  1.830922880788777 \t 3.7672150031822547\n",
      "98     \t [0.13579278 0.3600425  0.32872856]. \t  0.4692130012754629 \t 3.7672150031822547\n",
      "99     \t [0.30148919 0.01337579 0.65699551]. \t  0.18123347020220654 \t 3.7672150031822547\n",
      "100    \t [0.55912399 0.9183065  0.48880292]. \t  1.4218084320990023 \t 3.7672150031822547\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\n",
    "\n",
    "np.random.seed(run_num_8)\n",
    "surrogate_loser_8 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_8 = GPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_8.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.67227856 0.4880784  0.82549517]. \t  3.595021899183128 \t 3.595021899183128\n",
      "init   \t [0.03144639 0.80804996 0.56561742]. \t  2.9633561694281085 \t 3.595021899183128\n",
      "init   \t [0.2976225  0.04669572 0.9906274 ]. \t  0.16382388103073592 \t 3.595021899183128\n",
      "init   \t [0.00682573 0.76979303 0.7467671 ]. \t  2.382987807172393 \t 3.595021899183128\n",
      "init   \t [0.37743894 0.49414745 0.92894839]. \t  3.1588932929069533 \t 3.595021899183128\n",
      "1      \t [0.58444074 0.99050166 0.47658358]. \t  1.1115612161599435 \t 3.595021899183128\n",
      "2      \t [0.         0.55738032 0.        ]. \t  0.011313047963942802 \t 3.595021899183128\n",
      "3      \t [0.75987333 0.47216222 1.        ]. \t  1.8828686907446737 \t 3.595021899183128\n",
      "4      \t [0.47253623 0.43756399 0.55627173]. \t  0.7302848080952353 \t 3.595021899183128\n",
      "5      \t [0.59153848 0.65473503 0.79345695]. \t  3.1225321370050105 \t 3.595021899183128\n",
      "6      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.595021899183128\n",
      "7      \t [0.99980609 0.4161656  0.62696752]. \t  0.9757371065192627 \t 3.595021899183128\n",
      "8      \t [0.79508012 0.16582114 0.91622805]. \t  0.7773262025762627 \t 3.595021899183128\n",
      "9      \t [0.         0.56516844 0.63800499]. \t  1.9585390684307677 \t 3.595021899183128\n",
      "10     \t [0.98896185 0.76454532 0.58106913]. \t  0.49904318819013554 \t 3.595021899183128\n",
      "11     \t [0.36543417 0.95348505 0.09961531]. \t  0.0050196065781756526 \t 3.595021899183128\n",
      "12     \t [0.33046766 0.19829821 0.7391559 ]. \t  1.0594276320106952 \t 3.595021899183128\n",
      "13     \t [0.10000995 0.78361592 0.19528553]. \t  0.06553206290358828 \t 3.595021899183128\n",
      "14     \t [0.63796431 0.49553304 0.944787  ]. \t  2.9096373673656992 \t 3.595021899183128\n",
      "15     \t [0.06396978 0.46216013 0.09938513]. \t  0.09974740170445946 \t 3.595021899183128\n",
      "16     \t [0.28095921 0.95472615 0.6397489 ]. \t  2.218804440613135 \t 3.595021899183128\n",
      "17     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.595021899183128\n",
      "18     \t [0.28694241 0.39169052 0.85685861]. \t  3.021421380809735 \t 3.595021899183128\n",
      "19     \t [0.77352728 0.82355039 0.4683024 ]. \t  0.6454934775689937 \t 3.595021899183128\n",
      "20     \t [0.80824595 0.86678053 0.77477962]. \t  1.2344434935722344 \t 3.595021899183128\n",
      "21     \t [0.29766289 0.76762905 0.35821761]. \t  0.7708859721106268 \t 3.595021899183128\n",
      "22     \t [0.40563726 0.68505335 0.1754113 ]. \t  0.05228717190381487 \t 3.595021899183128\n",
      "23     \t [0.02067804 0.91099811 0.59912263]. \t  2.8424606783464554 \t 3.595021899183128\n",
      "24     \t [ 6.29688233e-01  4.15267304e-01 -8.67361738e-19]. \t  0.039291920191968 \t 3.595021899183128\n",
      "25     \t [0.06995427 0.01511866 0.72442506]. \t  0.25274913388809944 \t 3.595021899183128\n",
      "26     \t [0.67804479 0.77078918 0.56902073]. \t  1.2168931215562688 \t 3.595021899183128\n",
      "27     \t [0.86589987 0.92639659 0.87757439]. \t  0.9871183347890559 \t 3.595021899183128\n",
      "28     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 3.595021899183128\n",
      "29     \t [0.91335427 0.23583841 0.44450227]. \t  0.17714105470247507 \t 3.595021899183128\n",
      "30     \t [0.7506065  0.40634044 0.41486966]. \t  0.22632380203095379 \t 3.595021899183128\n",
      "31     \t [0.39977128 0.23813587 0.91268602]. \t  1.3095751931212423 \t 3.595021899183128\n",
      "32     \t [0.79450272 0.7126485  0.50934612]. \t  0.6327343940160386 \t 3.595021899183128\n",
      "33     \t [0.25772358 0.57968761 0.83700438]. \t  \u001b[92m3.814263786797786\u001b[0m \t 3.814263786797786\n",
      "34     \t [0.32687437 0.32426142 0.58229107]. \t  0.6370136299924598 \t 3.814263786797786\n",
      "35     \t [0.83192678 0.29758518 0.65755643]. \t  1.000957092930762 \t 3.814263786797786\n",
      "36     \t [0.2968788  0.59922979 0.79615695]. \t  3.5149009200745946 \t 3.814263786797786\n",
      "37     \t [0.00791438 0.7725744  0.76452674]. \t  2.388326089004248 \t 3.814263786797786\n",
      "38     \t [0.7831771  0.11393476 0.34031502]. \t  0.511090303764253 \t 3.814263786797786\n",
      "39     \t [0.58315801 0.99991139 0.65434991]. \t  1.0884058985143514 \t 3.814263786797786\n",
      "40     \t [0.53521012 0.82353753 0.11870219]. \t  0.008887745710349901 \t 3.814263786797786\n",
      "41     \t [0.43814079 0.78787176 0.17906828]. \t  0.037915227463000994 \t 3.814263786797786\n",
      "42     \t [0.3960321  0.29571919 0.3165956 ]. \t  0.690757086165492 \t 3.814263786797786\n",
      "43     \t [0.32673024 0.         0.        ]. \t  0.10170260750742209 \t 3.814263786797786\n",
      "44     \t [0.84324654 0.40795512 0.18468329]. \t  0.17907967355038906 \t 3.814263786797786\n",
      "45     \t [0.02685762 0.60280665 0.71467612]. \t  2.6889666815575426 \t 3.814263786797786\n",
      "46     \t [0.24561221 0.33233123 0.41387818]. \t  0.42211076022411576 \t 3.814263786797786\n",
      "47     \t [0.36447976 0.70156883 0.48132649]. \t  1.6321558603521293 \t 3.814263786797786\n",
      "48     \t [0.84842124 0.18218914 0.97560176]. \t  0.5961544894303186 \t 3.814263786797786\n",
      "49     \t [0.12984436 0.69852893 0.3573845 ]. \t  0.7123474682865216 \t 3.814263786797786\n",
      "50     \t [0.52534405 0.66792719 0.72398521]. \t  2.4018638450447756 \t 3.814263786797786\n",
      "51     \t [0.07786434 0.27164166 0.97193436]. \t  1.1132327915269886 \t 3.814263786797786\n",
      "52     \t [0.10962047 0.04483307 0.86191811]. \t  0.34796024304865303 \t 3.814263786797786\n",
      "53     \t [0.75000236 0.47560137 0.71576149]. \t  2.2966766459338115 \t 3.814263786797786\n",
      "54     \t [0.81693946 0.05488837 0.12012989]. \t  0.27511765755182194 \t 3.814263786797786\n",
      "55     \t [0.69690756 0.44841209 0.00256636]. \t  0.029511061790492753 \t 3.814263786797786\n",
      "56     \t [0.25528111 0.48415676 0.36037925]. \t  0.39866790747783953 \t 3.814263786797786\n",
      "57     \t [0.82551278 0.291437   0.69478736]. \t  1.2898866826883002 \t 3.814263786797786\n",
      "58     \t [0.81351655 0.42959636 0.4443309 ]. \t  0.19793697261499427 \t 3.814263786797786\n",
      "59     \t [0.35840926 0.15876879 0.42501352]. \t  0.48984090207098496 \t 3.814263786797786\n",
      "60     \t [0.42659616 0.26231807 0.48581641]. \t  0.3230057993212272 \t 3.814263786797786\n",
      "61     \t [0.27036096 0.91077795 0.38093927]. \t  1.1084812363376046 \t 3.814263786797786\n",
      "62     \t [0.70522617 0.92820702 0.95321304]. \t  0.7938744310416895 \t 3.814263786797786\n",
      "63     \t [0.91437015 0.15733016 0.32743429]. \t  0.36322474065860144 \t 3.814263786797786\n",
      "64     \t [0.04977767 0.70862733 0.4274287 ]. \t  1.4208589676070935 \t 3.814263786797786\n",
      "65     \t [0.5571242  0.20250162 0.69783511]. \t  0.8855766304236897 \t 3.814263786797786\n",
      "66     \t [0.87335527 0.39530656 0.35284998]. \t  0.19324353876702555 \t 3.814263786797786\n",
      "67     \t [0.50717256 0.63300668 0.98809787]. \t  2.2051804036179177 \t 3.814263786797786\n",
      "68     \t [0.74282132 0.09519347 0.86185807]. \t  0.5440572247124906 \t 3.814263786797786\n",
      "69     \t [0.88782106 0.036925   0.914746  ]. \t  0.2553094281926955 \t 3.814263786797786\n",
      "70     \t [0.98808529 0.39123085 0.14039753]. \t  0.09223007154309171 \t 3.814263786797786\n",
      "71     \t [0.33463186 0.69083589 0.55812411]. \t  2.1008406865618463 \t 3.814263786797786\n",
      "72     \t [0.94354885 0.93226586 0.79809   ]. \t  0.8239992082452622 \t 3.814263786797786\n",
      "73     \t [0.56098831 0.07435879 0.0274089 ]. \t  0.15640531297667126 \t 3.814263786797786\n",
      "74     \t [0.60219522 0.34545451 0.04807679]. \t  0.11923968083605012 \t 3.814263786797786\n",
      "75     \t [0.56539164 0.65339196 0.39425583]. \t  0.4990868816110166 \t 3.814263786797786\n",
      "76     \t [0.70344285 0.22611978 0.55108985]. \t  0.2906825411389715 \t 3.814263786797786\n",
      "77     \t [0.98769389 0.78995211 0.05810395]. \t  0.001089988756417544 \t 3.814263786797786\n",
      "78     \t [0.29520878 0.62503722 0.76480711]. \t  3.1389751822029917 \t 3.814263786797786\n",
      "79     \t [0.32313746 0.64663479 0.92416753]. \t  3.1444568439838494 \t 3.814263786797786\n",
      "80     \t [0.55755299 0.62021345 0.03969792]. \t  0.015402661237968218 \t 3.814263786797786\n",
      "81     \t [0.63083083 0.80771709 0.94964549]. \t  1.66189665408206 \t 3.814263786797786\n",
      "82     \t [0.67765879 0.80613949 0.87291363]. \t  2.0767113122745466 \t 3.814263786797786\n",
      "83     \t [0.24793462 0.03299496 0.24597322]. \t  0.8799301960103929 \t 3.814263786797786\n",
      "84     \t [0.26700692 0.69085293 0.69224926]. \t  2.4844544736083507 \t 3.814263786797786\n",
      "85     \t [0.45074871 0.7214667  0.66789062]. \t  2.0882138077978514 \t 3.814263786797786\n",
      "86     \t [0.61687711 0.09510488 0.91547047]. \t  0.44474067889320423 \t 3.814263786797786\n",
      "87     \t [0.23894909 0.69959286 0.52875898]. \t  2.2198741184287027 \t 3.814263786797786\n",
      "88     \t [0.0743263  0.81617286 0.52043236]. \t  2.8591383042700267 \t 3.814263786797786\n",
      "89     \t [0.70526481 0.82125252 0.89421046]. \t  1.8739824287549274 \t 3.814263786797786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.00271189 0.87698195 0.90354972]. \t  1.391326979456286 \t 3.814263786797786\n",
      "91     \t [0.45342712 0.29246917 0.08607081]. \t  0.26866794578786274 \t 3.814263786797786\n",
      "92     \t [0.27530935 0.29470349 0.96624981]. \t  1.3275575893186944 \t 3.814263786797786\n",
      "93     \t [0.53828561 0.01858217 0.9900779 ]. \t  0.12322679462156946 \t 3.814263786797786\n",
      "94     \t [0.83153634 0.0114916  0.86397406]. \t  0.24871690613153696 \t 3.814263786797786\n",
      "95     \t [0.25861071 0.05033044 0.506291  ]. \t  0.20544651675794345 \t 3.814263786797786\n",
      "96     \t [0.54742893 0.33386352 0.6356025 ]. \t  0.9920591266767318 \t 3.814263786797786\n",
      "97     \t [0.75344941 0.5909561  0.48116115]. \t  0.43885322229831075 \t 3.814263786797786\n",
      "98     \t [0.23922697 0.30041013 0.98878037]. \t  1.1458272347770426 \t 3.814263786797786\n",
      "99     \t [0.39628605 0.66448595 0.09270071]. \t  0.02246596304889642 \t 3.814263786797786\n",
      "100    \t [0.70606998 0.59208069 0.14527219]. \t  0.05066806780491258 \t 3.814263786797786\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\n",
    "\n",
    "np.random.seed(run_num_9)\n",
    "surrogate_loser_9 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_9 = GPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_9.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.65358959 0.11500694 0.95028286]. \t  0.42730315147591735 \t 1.1029187088185965\n",
      "init   \t [0.4821914  0.87247454 0.21233268]. \t  0.06161964400032635 \t 1.1029187088185965\n",
      "init   \t [0.04070962 0.39719446 0.2331322 ]. \t  0.33269334660262956 \t 1.1029187088185965\n",
      "init   \t [0.84174072 0.20708234 0.74246953]. \t  1.1029187088185965 \t 1.1029187088185965\n",
      "init   \t [0.39215413 0.18225652 0.74353941]. \t  0.9779763535009853 \t 1.1029187088185965\n",
      "1      \t [0.65784615 0.04582828 0.06191159]. \t  0.20874301192282538 \t 1.1029187088185965\n",
      "2      \t [0.13064464 0.91108377 0.95539833]. \t  0.902351403909183 \t 1.1029187088185965\n",
      "3      \t [0.96893897 0.97974629 0.99865138]. \t  0.381043852508212 \t 1.1029187088185965\n",
      "4      \t [0.98139485 0.81725946 0.12404836]. \t  0.0024384188819132563 \t 1.1029187088185965\n",
      "5      \t [0.10355689 0.02892137 0.41095543]. \t  0.4089719355888731 \t 1.1029187088185965\n",
      "6      \t [0.02218954 0.56584252 0.99967923]. \t  \u001b[92m2.067141380028605\u001b[0m \t 2.067141380028605\n",
      "7      \t [0.05955692 0.03232917 0.89086014]. \t  0.27726435960872364 \t 2.067141380028605\n",
      "8      \t [0.98484883 0.56561537 0.99985617]. \t  2.015018280106272 \t 2.067141380028605\n",
      "9      \t [0.63184882 0.63023147 0.86946882]. \t  \u001b[92m3.5909750349985345\u001b[0m \t 3.5909750349985345\n",
      "10     \t [0.57930165 0.65764517 0.96407083]. \t  2.508814976030725 \t 3.5909750349985345\n",
      "11     \t [0.8078532  0.7622684  0.63227738]. \t  1.0200554009147913 \t 3.5909750349985345\n",
      "12     \t [0.         0.61493441 0.71370723]. \t  2.6640858280248736 \t 3.5909750349985345\n",
      "13     \t [0.68277949 0.58597741 0.        ]. \t  0.009720792904080396 \t 3.5909750349985345\n",
      "14     \t [0.66430626 0.44457057 0.93318238]. \t  2.8124401992306645 \t 3.5909750349985345\n",
      "15     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.5909750349985345\n",
      "16     \t [0.34880716 0.62390797 0.74426788]. \t  2.9064433751584025 \t 3.5909750349985345\n",
      "17     \t [0.56383078 0.96512321 0.01989752]. \t  0.00038527602653866237 \t 3.5909750349985345\n",
      "18     \t [0.93467313 0.59513961 0.96751112]. \t  2.57238835070557 \t 3.5909750349985345\n",
      "19     \t [0.55401803 0.59081142 0.69702173]. \t  2.2293241627036107 \t 3.5909750349985345\n",
      "20     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.5909750349985345\n",
      "21     \t [0.49838547 0.91412823 0.84602665]. \t  1.1959894323347209 \t 3.5909750349985345\n",
      "22     \t [0.93481963 0.15852344 0.01178922]. \t  0.05304559981970046 \t 3.5909750349985345\n",
      "23     \t [0.88131572 0.44114157 0.73043951]. \t  2.359362749579099 \t 3.5909750349985345\n",
      "24     \t [0.56774596 0.16752077 0.11092578]. \t  0.415755972019419 \t 3.5909750349985345\n",
      "25     \t [0.82727226 0.01716246 0.19996375]. \t  0.4206393454321978 \t 3.5909750349985345\n",
      "26     \t [0.71908065 0.38259867 0.3836828 ]. \t  0.27627310137628897 \t 3.5909750349985345\n",
      "27     \t [0.4990589  0.30974256 0.24859155]. \t  0.6536270178128792 \t 3.5909750349985345\n",
      "28     \t [0.85541574 0.11938326 0.13548969]. \t  0.2919036809750154 \t 3.5909750349985345\n",
      "29     \t [0.02059083 0.05214196 0.52110823]. \t  0.1461744279058926 \t 3.5909750349985345\n",
      "30     \t [0.54390625 0.96535455 0.95093515]. \t  0.6194232780140717 \t 3.5909750349985345\n",
      "31     \t [0.36931778 0.2013132  0.99354813]. \t  0.5996045984881392 \t 3.5909750349985345\n",
      "32     \t [0.37977161 0.12368017 0.6437221 ]. \t  0.3839468807083688 \t 3.5909750349985345\n",
      "33     \t [0.8047245  0.59171294 0.41136894]. \t  0.22314224425083254 \t 3.5909750349985345\n",
      "34     \t [0.44638342 0.89418366 0.11933375]. \t  0.008414858529026797 \t 3.5909750349985345\n",
      "35     \t [0.52924027 0.07235039 0.7635873 ]. \t  0.46807615193261143 \t 3.5909750349985345\n",
      "36     \t [0.7411776  0.1226393  0.57007957]. \t  0.20544785956289124 \t 3.5909750349985345\n",
      "37     \t [0.40426692 0.43087405 0.72122275]. \t  2.358668318450057 \t 3.5909750349985345\n",
      "38     \t [0.34323487 0.61523632 0.05239501]. \t  0.0215357272331486 \t 3.5909750349985345\n",
      "39     \t [0.35604083 0.00039971 0.11023479]. \t  0.41622529501959665 \t 3.5909750349985345\n",
      "40     \t [0.54347741 0.80014797 0.14840083]. \t  0.017026410789868152 \t 3.5909750349985345\n",
      "41     \t [0.92786235 0.39842196 0.1261783 ]. \t  0.09778381676669251 \t 3.5909750349985345\n",
      "42     \t [0.35526549 0.85732693 0.77594706]. \t  1.7393881605971755 \t 3.5909750349985345\n",
      "43     \t [0.13928129 0.037642   0.31364399]. \t  0.7523955330830968 \t 3.5909750349985345\n",
      "44     \t [0.43943336 0.35058606 0.65226717]. \t  1.2254093193788633 \t 3.5909750349985345\n",
      "45     \t [0.41287946 0.11110351 0.80531922]. \t  0.668777513157417 \t 3.5909750349985345\n",
      "46     \t [0.75394388 0.67847445 0.46045262]. \t  0.5040847138299055 \t 3.5909750349985345\n",
      "47     \t [0.41261872 0.95628583 0.82782702]. \t  0.9611467135683864 \t 3.5909750349985345\n",
      "48     \t [0.23896224 0.39284001 0.62793845]. \t  1.1987990305882834 \t 3.5909750349985345\n",
      "49     \t [0.90063347 0.85098353 0.19437089]. \t  0.010944354013264199 \t 3.5909750349985345\n",
      "50     \t [0.3981577  0.45222437 0.68939628]. \t  2.042203289076502 \t 3.5909750349985345\n",
      "51     \t [0.27320175 0.32743207 0.54484384]. \t  0.4868679427897529 \t 3.5909750349985345\n",
      "52     \t [0.70426147 0.94977949 0.91975555]. \t  0.7859185485075025 \t 3.5909750349985345\n",
      "53     \t [0.29938903 0.33951197 0.10493827]. \t  0.2727704257910028 \t 3.5909750349985345\n",
      "54     \t [0.04227152 0.51632077 0.06614304]. \t  0.04442441130910269 \t 3.5909750349985345\n",
      "55     \t [0.99248764 0.16035644 0.36489517]. \t  0.233568848059692 \t 3.5909750349985345\n",
      "56     \t [0.7384411  0.96115012 0.40008681]. \t  0.4147517720316982 \t 3.5909750349985345\n",
      "57     \t [0.21183532 0.81474443 0.39789438]. \t  1.352137891327086 \t 3.5909750349985345\n",
      "58     \t [0.22519599 0.91835787 0.61079726]. \t  2.704469251438837 \t 3.5909750349985345\n",
      "59     \t [0.86614404 0.60244622 0.09429213]. \t  0.018831020154467 \t 3.5909750349985345\n",
      "60     \t [0.         1.         0.52421648]. \t  2.4116683076339074 \t 3.5909750349985345\n",
      "61     \t [0.27980633 0.46148506 0.1752227 ]. \t  0.2378344066527602 \t 3.5909750349985345\n",
      "62     \t [0.91427037 0.73030932 0.37220707]. \t  0.13958483090937676 \t 3.5909750349985345\n",
      "63     \t [0.3715256  0.97661857 0.83547791]. \t  0.842797440952053 \t 3.5909750349985345\n",
      "64     \t [0.94554547 0.95144071 0.33096856]. \t  0.07758239833381178 \t 3.5909750349985345\n",
      "65     \t [0.62945952 0.81213003 0.07417604]. \t  0.0033818119848788857 \t 3.5909750349985345\n",
      "66     \t [0.73893811 0.68093479 0.04192289]. \t  0.006243422255177505 \t 3.5909750349985345\n",
      "67     \t [0.14245061 0.4693838  0.2147183 ]. \t  0.2462921799924161 \t 3.5909750349985345\n",
      "68     \t [0.34945726 0.51161119 0.28253862]. \t  0.28359048954848454 \t 3.5909750349985345\n",
      "69     \t [0.65959258 0.15693117 0.49238012]. \t  0.23201619425484993 \t 3.5909750349985345\n",
      "70     \t [0.07867937 0.90892995 0.60080682]. \t  2.896572730126719 \t 3.5909750349985345\n",
      "71     \t [0.39040893 0.32429144 0.01740015]. \t  0.0998205204225004 \t 3.5909750349985345\n",
      "72     \t [0.61052577 0.39268847 0.46810742]. \t  0.31167649069899306 \t 3.5909750349985345\n",
      "73     \t [0.36260274 0.54841149 0.04248908]. \t  0.03446239000851175 \t 3.5909750349985345\n",
      "74     \t [0.00589812 0.27171843 0.98890298]. \t  0.9666106673936532 \t 3.5909750349985345\n",
      "75     \t [0.89290771 0.30965735 0.87323794]. \t  2.1147184202197984 \t 3.5909750349985345\n",
      "76     \t [0.53783974 0.37586609 0.11422795]. \t  0.23297846283288623 \t 3.5909750349985345\n",
      "77     \t [0.1100138  0.86478416 0.52766993]. \t  2.9856105882883006 \t 3.5909750349985345\n",
      "78     \t [0.84135491 0.32663725 0.05460281]. \t  0.08491332223000136 \t 3.5909750349985345\n",
      "79     \t [0.46396811 0.57353837 0.95369955]. \t  2.9132276860785464 \t 3.5909750349985345\n",
      "80     \t [0.75053175 0.07049347 0.35769157]. \t  0.496737531071639 \t 3.5909750349985345\n",
      "81     \t [0.03047332 0.0352073  0.14699465]. \t  0.42970513491749057 \t 3.5909750349985345\n",
      "82     \t [0.91763429 0.51515885 0.54189505]. \t  0.43106468558061406 \t 3.5909750349985345\n",
      "83     \t [0.67524817 0.56782453 0.72559978]. \t  2.5187639523066165 \t 3.5909750349985345\n",
      "84     \t [0.1837141  0.8699628  0.15373861]. \t  0.02583793048070997 \t 3.5909750349985345\n",
      "85     \t [0.96033402 0.15484275 0.55027791]. \t  0.17937950478392242 \t 3.5909750349985345\n",
      "86     \t [0.41268804 0.21677931 0.07668479]. \t  0.3026346220661753 \t 3.5909750349985345\n",
      "87     \t [0.96856976 0.08974637 0.01914393]. \t  0.053200915564779426 \t 3.5909750349985345\n",
      "88     \t [0.95030821 0.19386301 0.97179269]. \t  0.6620893120506729 \t 3.5909750349985345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.62693302 0.84101297 0.42004557]. \t  0.7795783747167534 \t 3.5909750349985345\n",
      "90     \t [0.71755206 0.26757393 0.39225026]. \t  0.3691089467773529 \t 3.5909750349985345\n",
      "91     \t [0.91502671 0.69756974 0.91725515]. \t  2.787194121186799 \t 3.5909750349985345\n",
      "92     \t [0.9270585  0.78468855 0.49436225]. \t  0.3833080971914929 \t 3.5909750349985345\n",
      "93     \t [0.06804019 0.76189743 0.21971389]. \t  0.10216064197127789 \t 3.5909750349985345\n",
      "94     \t [0.0962197  0.81525186 0.35072737]. \t  0.838479048398054 \t 3.5909750349985345\n",
      "95     \t [0.49508671 0.39037781 0.57475127]. \t  0.7079714096790843 \t 3.5909750349985345\n",
      "96     \t [0.84554329 0.19749171 0.53590731]. \t  0.21072762937007228 \t 3.5909750349985345\n",
      "97     \t [0.71358188 0.73749073 0.55743709]. \t  1.0333471264077945 \t 3.5909750349985345\n",
      "98     \t [0.42325437 0.52968222 0.00214746]. \t  0.021975714792266725 \t 3.5909750349985345\n",
      "99     \t [0.55324297 0.28908356 0.80797698]. \t  2.0124792267115676 \t 3.5909750349985345\n",
      "100    \t [0.54656951 0.65652098 0.75129329]. \t  2.7054350876838593 \t 3.5909750349985345\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\n",
    "\n",
    "np.random.seed(run_num_10)\n",
    "surrogate_loser_10 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_10 = GPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_10.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.98103566 0.82124785 0.64589361]. \t  0.6570240879762975 \t 0.687459437576373\n",
      "init   \t [0.42368801 0.20231098 0.49190677]. \t  0.29901296656621285 \t 0.687459437576373\n",
      "init   \t [0.13855833 0.45252104 0.11373463]. \t  0.1378633538075966 \t 0.687459437576373\n",
      "init   \t [0.00292449 0.52342617 0.18997116]. \t  0.12299763462541902 \t 0.687459437576373\n",
      "init   \t [0.14171979 0.33586637 0.58369219]. \t  0.687459437576373 \t 0.687459437576373\n",
      "1      \t [0.54383213 0.99819891 0.96605385]. \t  0.4340597074596305 \t 0.687459437576373\n",
      "2      \t [0.95983675 0.98162579 0.03051357]. \t  0.00011698065361110391 \t 0.687459437576373\n",
      "3      \t [0.99072322 0.00568382 0.85196183]. \t  0.23940397454961343 \t 0.687459437576373\n",
      "4      \t [0.96635878 0.05065822 0.14510777]. \t  0.2095493914518875 \t 0.687459437576373\n",
      "5      \t [0.11626065 0.11270374 0.98686942]. \t  0.3129882692341237 \t 0.687459437576373\n",
      "6      \t [0.0325319  0.0681414  0.34792992]. \t  0.574554419912479 \t 0.687459437576373\n",
      "7      \t [0.09903838 0.94911004 0.96556234]. \t  0.6473387535315734 \t 0.687459437576373\n",
      "8      \t [0.52665542 0.99837455 0.28378765]. \t  0.16856345575833884 \t 0.687459437576373\n",
      "9      \t [0.82239546 0.2976596  0.97693775]. \t  \u001b[92m1.223356686192842\u001b[0m \t 1.223356686192842\n",
      "10     \t [0.99622896 0.50984357 0.00987988]. \t  0.008998496185923171 \t 1.223356686192842\n",
      "11     \t [0.00147484 0.93964119 0.00205779]. \t  0.0003838484044950603 \t 1.223356686192842\n",
      "12     \t [0.53661097 0.44177042 0.00933445]. \t  0.0435128262406222 \t 1.223356686192842\n",
      "13     \t [0.58824586 0.92223861 0.52464089]. \t  \u001b[92m1.4628210506084425\u001b[0m \t 1.4628210506084425\n",
      "14     \t [0.07441325 0.30230799 0.04195948]. \t  0.11924842505494702 \t 1.4628210506084425\n",
      "15     \t [0.74469128 0.72600472 0.94859131]. \t  \u001b[92m2.2887077007421595\u001b[0m \t 2.2887077007421595\n",
      "16     \t [0.02080374 0.92826588 0.45911101]. \t  2.169348725091859 \t 2.2887077007421595\n",
      "17     \t [0.92520218 0.35168958 0.64241465]. \t  1.0181546173599993 \t 2.2887077007421595\n",
      "18     \t [0.83823964 0.78751959 0.86158897]. \t  2.2128807060592215 \t 2.2887077007421595\n",
      "19     \t [0.88340505 0.19776439 0.93043219]. \t  0.9015758371143319 \t 2.2887077007421595\n",
      "20     \t [0.73695676 0.88091896 0.60843112]. \t  1.0173206108842925 \t 2.2887077007421595\n",
      "21     \t [0.57403855 0.83920549 0.88971474]. \t  1.7493958325783987 \t 2.2887077007421595\n",
      "22     \t [1.         0.58989505 1.        ]. \t  1.9987300911710584 \t 2.2887077007421595\n",
      "23     \t [0.13354403 0.337556   0.94263112]. \t  1.8844777407532414 \t 2.2887077007421595\n",
      "24     \t [0.08209724 0.85599915 0.30246339]. \t  0.445951009497801 \t 2.2887077007421595\n",
      "25     \t [0.61480638 0.32456524 0.1872003 ]. \t  0.44842461410223 \t 2.2887077007421595\n",
      "26     \t [0.48455829 0.36987274 0.17834187]. \t  0.401964192772876 \t 2.2887077007421595\n",
      "27     \t [0.58695784 0.81532927 0.17236437]. \t  0.023264371249665364 \t 2.2887077007421595\n",
      "28     \t [0.46048979 0.81421302 0.16711461]. \t  0.027664804050156293 \t 2.2887077007421595\n",
      "29     \t [0.23701218 0.59591573 0.89894031]. \t  \u001b[92m3.6029773715032065\u001b[0m \t 3.6029773715032065\n",
      "30     \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.6029773715032065\n",
      "31     \t [0.         0.6853504  0.81053212]. \t  3.1623900291466653 \t 3.6029773715032065\n",
      "32     \t [0.43877667 0.3797842  0.02039755]. \t  0.07937499454939982 \t 3.6029773715032065\n",
      "33     \t [0.61121167 0.00107461 0.95751915]. \t  0.13534741531845035 \t 3.6029773715032065\n",
      "34     \t [0.00813916 0.40917623 0.83503383]. \t  3.138115703694798 \t 3.6029773715032065\n",
      "35     \t [0.49982731 0.90579256 0.2331232 ]. \t  0.08621967538113952 \t 3.6029773715032065\n",
      "36     \t [0.15800007 1.         0.66457775]. \t  1.9038778526831794 \t 3.6029773715032065\n",
      "37     \t [0.6752069  0.81796695 0.45441109]. \t  0.8417381747877882 \t 3.6029773715032065\n",
      "38     \t [0.86240987 0.78842564 0.51778972]. \t  0.5631960416924139 \t 3.6029773715032065\n",
      "39     \t [0.92713127 0.86718465 0.91675105]. \t  1.3413636838856968 \t 3.6029773715032065\n",
      "40     \t [0.35357211 0.51185782 0.02878209]. \t  0.03830683776229002 \t 3.6029773715032065\n",
      "41     \t [0.05762137 0.95472163 0.49114906]. \t  2.479043323684774 \t 3.6029773715032065\n",
      "42     \t [0.0088501  0.85339864 0.51987391]. \t  2.845347029086422 \t 3.6029773715032065\n",
      "43     \t [0.92527545 0.18887922 0.95619654]. \t  0.7185987031641645 \t 3.6029773715032065\n",
      "44     \t [0.52578548 0.1949655  0.37561227]. \t  0.6269693854008805 \t 3.6029773715032065\n",
      "45     \t [0.54450649 0.49039642 0.84509756]. \t  \u001b[92m3.6952070502329595\u001b[0m \t 3.6952070502329595\n",
      "46     \t [0.99263914 0.15298368 0.10210187]. \t  0.13549979390863812 \t 3.6952070502329595\n",
      "47     \t [0.00204512 0.53637534 0.90481753]. \t  3.5269212761465907 \t 3.6952070502329595\n",
      "48     \t [0.55971228 0.39288025 0.98728663]. \t  1.7225165152213575 \t 3.6952070502329595\n",
      "49     \t [0.49657515 0.58416467 0.779257  ]. \t  3.3128019824062505 \t 3.6952070502329595\n",
      "50     \t [0.47915436 0.73689439 0.09459543]. \t  0.01133009927251797 \t 3.6952070502329595\n",
      "51     \t [0.21412015 0.87387982 0.69532366]. \t  2.1471688886727778 \t 3.6952070502329595\n",
      "52     \t [0.12271917 0.37553842 0.4272115 ]. \t  0.3864043016317239 \t 3.6952070502329595\n",
      "53     \t [0.82080203 0.29618673 0.7727641 ]. \t  1.8990264295842576 \t 3.6952070502329595\n",
      "54     \t [0.81894502 0.14217946 0.85586045]. \t  0.7982175892835341 \t 3.6952070502329595\n",
      "55     \t [0.64443437 0.22217795 0.8371342 ]. \t  1.4200274340407004 \t 3.6952070502329595\n",
      "56     \t [0.02516017 0.58371562 0.30762436]. \t  0.28033234832751935 \t 3.6952070502329595\n",
      "57     \t [0.11362845 0.73784761 0.58499413]. \t  2.7329829476515304 \t 3.6952070502329595\n",
      "58     \t [0.30562469 0.08565643 0.2132352 ]. \t  0.8964390815744543 \t 3.6952070502329595\n",
      "59     \t [0.1711343  0.78616482 0.67357901]. \t  2.5332181225533814 \t 3.6952070502329595\n",
      "60     \t [0.78645719 0.57463958 0.04022363]. \t  0.01565100397479039 \t 3.6952070502329595\n",
      "61     \t [0.42029516 0.21429986 0.30920755]. \t  0.8618548387131872 \t 3.6952070502329595\n",
      "62     \t [0.97409418 0.46679145 0.23632156]. \t  0.09830335271047452 \t 3.6952070502329595\n",
      "63     \t [0.51847515 0.10331076 0.94773397]. \t  0.3949299809870835 \t 3.6952070502329595\n",
      "64     \t [0.51606021 0.93594717 0.69369794]. \t  1.3216254492453405 \t 3.6952070502329595\n",
      "65     \t [0.25034073 0.283311   0.84914915]. \t  1.9741592008394155 \t 3.6952070502329595\n",
      "66     \t [0.62661822 0.79305607 0.71171919]. \t  1.6353891844691524 \t 3.6952070502329595\n",
      "67     \t [0.99209761 0.52724509 0.59182   ]. \t  0.706813714232192 \t 3.6952070502329595\n",
      "68     \t [0.39815523 0.60099155 0.45032258]. \t  0.8846101320937436 \t 3.6952070502329595\n",
      "69     \t [0.42039382 0.75207267 0.10391814]. \t  0.012253459456886143 \t 3.6952070502329595\n",
      "70     \t [0.48486484 0.39706945 0.52374185]. \t  0.4903912089210618 \t 3.6952070502329595\n",
      "71     \t [0.65255185 0.37046885 0.60672359]. \t  0.8229914938001928 \t 3.6952070502329595\n",
      "72     \t [0.53269428 0.5021177  0.41613   ]. \t  0.38257475846236266 \t 3.6952070502329595\n",
      "73     \t [0.23292542 0.65429981 0.5694923 ]. \t  2.1114534977548267 \t 3.6952070502329595\n",
      "74     \t [0.06094383 0.75006948 0.91544775]. \t  2.4577926427867203 \t 3.6952070502329595\n",
      "75     \t [0.11839277 0.40045652 0.80436571]. \t  2.9845651362088654 \t 3.6952070502329595\n",
      "76     \t [0.47931034 0.84014484 0.67227953]. \t  1.8305430269273972 \t 3.6952070502329595\n",
      "77     \t [0.98152923 0.00976713 0.28631693]. \t  0.28612596862520334 \t 3.6952070502329595\n",
      "78     \t [0.31402171 0.04764952 0.37383835]. \t  0.6749958469159306 \t 3.6952070502329595\n",
      "79     \t [0.75106957 0.15814369 0.38654257]. \t  0.42218406246182705 \t 3.6952070502329595\n",
      "80     \t [0.27461031 0.94774274 0.29159062]. \t  0.328639689249174 \t 3.6952070502329595\n",
      "81     \t [0.88887881 0.39281112 0.92796296]. \t  2.4565050300574933 \t 3.6952070502329595\n",
      "82     \t [0.70673888 0.8113937  0.57526345]. \t  1.1486811700958668 \t 3.6952070502329595\n",
      "83     \t [0.76981789 0.64524744 0.58542349]. \t  0.9169360326409772 \t 3.6952070502329595\n",
      "84     \t [0.84248597 0.87114236 0.23971618]. \t  0.03217010731508287 \t 3.6952070502329595\n",
      "85     \t [0.60098498 0.42754807 0.83812588]. \t  3.3069210488382543 \t 3.6952070502329595\n",
      "86     \t [0.75331242 0.27518024 0.74191406]. \t  1.5632474996030026 \t 3.6952070502329595\n",
      "87     \t [0.98928277 0.83515542 0.82849782]. \t  1.6312442936979132 \t 3.6952070502329595\n",
      "88     \t [0.49240399 0.4472185  0.9468286 ]. \t  2.655421694864316 \t 3.6952070502329595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.23368604 0.82427426 0.64991027]. \t  2.5802624328376487 \t 3.6952070502329595\n",
      "90     \t [0.16053329 0.96913501 0.73523832]. \t  1.4046142063188611 \t 3.6952070502329595\n",
      "91     \t [0.29146938 0.27435274 0.85713041]. \t  1.8728266859564773 \t 3.6952070502329595\n",
      "92     \t [0.36158656 0.87476252 0.15758473]. \t  0.024083704335378493 \t 3.6952070502329595\n",
      "93     \t [0.15810814 0.76485029 0.44684918]. \t  1.8937374352144585 \t 3.6952070502329595\n",
      "94     \t [0.42975675 0.29917632 0.6583835 ]. \t  1.0741313974916602 \t 3.6952070502329595\n",
      "95     \t [0.47695731 0.57196415 0.97239096]. \t  2.587977947486678 \t 3.6952070502329595\n",
      "96     \t [0.14965119 0.83004101 0.03678273]. \t  0.0020265420497056937 \t 3.6952070502329595\n",
      "97     \t [0.70341769 0.04770042 0.57490573]. \t  0.13992091944485147 \t 3.6952070502329595\n",
      "98     \t [0.01379356 0.86797625 0.38832868]. \t  1.2760355953003144 \t 3.6952070502329595\n",
      "99     \t [0.85408766 0.561019   0.45399772]. \t  0.23760051016737732 \t 3.6952070502329595\n",
      "100    \t [0.21927488 0.77810318 0.82141228]. \t  2.469706919173057 \t 3.6952070502329595\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\n",
    "\n",
    "np.random.seed(run_num_11)\n",
    "surrogate_loser_11 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_11 = GPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_11.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.19151945 0.62210877 0.43772774]. \t  1.1006281843679786 \t 1.6482992955272024\n",
      "init   \t [0.78535858 0.77997581 0.27259261]. \t  0.07153771680480671 \t 1.6482992955272024\n",
      "init   \t [0.27646426 0.80187218 0.95813935]. \t  1.6482992955272024 \t 1.6482992955272024\n",
      "init   \t [0.87593263 0.35781727 0.50099513]. \t  0.2282556248207173 \t 1.6482992955272024\n",
      "init   \t [0.68346294 0.71270203 0.37025075]. \t  0.33032494760932407 \t 1.6482992955272024\n",
      "1      \t [0.00218943 0.04196524 0.14178738]. \t  0.3936517086962368 \t 1.6482992955272024\n",
      "2      \t [0.77928559 0.03180806 0.96261853]. \t  0.1774348183168348 \t 1.6482992955272024\n",
      "3      \t [0.03494356 0.11089546 0.89536938]. \t  0.5535687532914756 \t 1.6482992955272024\n",
      "4      \t [0.81035323 0.96410121 0.95771406]. \t  0.5855693863872368 \t 1.6482992955272024\n",
      "5      \t [0.02515426 0.98910186 0.07077422]. \t  0.002392320430631601 \t 1.6482992955272024\n",
      "6      \t [0.00761147 0.89633798 0.99755295]. \t  0.7296905183544732 \t 1.6482992955272024\n",
      "7      \t [0.57560745 0.079372   0.04232202]. \t  0.18998890510184638 \t 1.6482992955272024\n",
      "8      \t [0.14372017 0.45284623 0.00447107]. \t  0.03505772621982825 \t 1.6482992955272024\n",
      "9      \t [0.71425984 0.42838582 0.97182332]. \t  \u001b[92m2.142089118725871\u001b[0m \t 2.142089118725871\n",
      "10     \t [0.54971999 0.123454   0.34646439]. \t  0.7541630909040608 \t 2.142089118725871\n",
      "11     \t [0.28834187 0.66519844 0.82381613]. \t  \u001b[92m3.38993374702056\u001b[0m \t 3.38993374702056\n",
      "12     \t [0.29815323 0.44576715 0.84251532]. \t  \u001b[92m3.4662727508063926\u001b[0m \t 3.4662727508063926\n",
      "13     \t [0.02263787 0.72332989 0.77237958]. \t  2.7282178756611355 \t 3.4662727508063926\n",
      "14     \t [0.99156486 0.9086604  0.67066223]. \t  0.5046079730032078 \t 3.4662727508063926\n",
      "15     \t [0.49169148 0.43063243 0.71336222]. \t  2.2335820971778686 \t 3.4662727508063926\n",
      "16     \t [0.98395189 0.24594206 0.91930385]. \t  1.287223865695118 \t 3.4662727508063926\n",
      "17     \t [0.         0.54975567 1.        ]. \t  2.05146612034035 \t 3.4662727508063926\n",
      "18     \t [0.48930669 0.81426489 0.0195969 ]. \t  0.0015248017944159383 \t 3.4662727508063926\n",
      "19     \t [0.92356452 0.06609808 0.14107916]. \t  0.24007567894345905 \t 3.4662727508063926\n",
      "20     \t [0.01212006 0.74721957 0.66784193]. \t  2.5238887834047836 \t 3.4662727508063926\n",
      "21     \t [0.14912285 0.91183835 0.63297933]. \t  2.642458610255681 \t 3.4662727508063926\n",
      "22     \t [0.3824515  0.21765801 0.46043515]. \t  0.36322495039594555 \t 3.4662727508063926\n",
      "23     \t [0.85718337 0.57784222 0.63080965]. \t  1.1438848837877669 \t 3.4662727508063926\n",
      "24     \t [1.         0.73409037 1.        ]. \t  1.5276219985332578 \t 3.4662727508063926\n",
      "25     \t [0.37636552 0.0107913  0.25592252]. \t  0.889855102438802 \t 3.4662727508063926\n",
      "26     \t [0.77722177 0.51750901 0.95112688]. \t  2.8522184098251007 \t 3.4662727508063926\n",
      "27     \t [0.84430204 0.0931376  0.5407072 ]. \t  0.1409227125821996 \t 3.4662727508063926\n",
      "28     \t [0.36505371 0.86257435 0.39257391]. \t  1.124125831322702 \t 3.4662727508063926\n",
      "29     \t [0.07611994 0.17403627 0.16964161]. \t  0.5625197850100055 \t 3.4662727508063926\n",
      "30     \t [0.15040217 0.27487737 0.68886371]. \t  1.2068112176054355 \t 3.4662727508063926\n",
      "31     \t [0.18487098 0.79798731 0.69640597]. \t  2.3978639708547576 \t 3.4662727508063926\n",
      "32     \t [0.47005646 0.43008523 0.76644683]. \t  2.8747164606798186 \t 3.4662727508063926\n",
      "33     \t [0.4830335  0.3589971  0.01957983]. \t  0.08497849431200012 \t 3.4662727508063926\n",
      "34     \t [0.39065506 0.36531876 0.33968319]. \t  0.5088841364966702 \t 3.4662727508063926\n",
      "35     \t [0.16606988 0.85341586 0.08747172]. \t  0.005714791267828612 \t 3.4662727508063926\n",
      "36     \t [0.97116965 0.66728227 0.49654319]. \t  0.28674280245595724 \t 3.4662727508063926\n",
      "37     \t [0.49621793 0.89029341 0.85221119]. \t  1.3795236492204368 \t 3.4662727508063926\n",
      "38     \t [0.06580719 0.65240766 0.56850218]. \t  2.163646036187145 \t 3.4662727508063926\n",
      "39     \t [0.37502837 0.5797235  0.55210511]. \t  1.365261717231631 \t 3.4662727508063926\n",
      "40     \t [0.36536699 0.60093531 0.35029954]. \t  0.41726358763676213 \t 3.4662727508063926\n",
      "41     \t [0.99071082 0.51863531 0.06184161]. \t  0.017663949149639254 \t 3.4662727508063926\n",
      "42     \t [0.57705539 0.11731267 0.24619877]. \t  0.8668166475679191 \t 3.4662727508063926\n",
      "43     \t [0.09674508 0.79030655 0.88036037]. \t  2.2909610613092406 \t 3.4662727508063926\n",
      "44     \t [0.         1.         0.54103637]. \t  2.4711157335918816 \t 3.4662727508063926\n",
      "45     \t [0.82893282 0.38042007 0.29612236]. \t  0.26674348201046777 \t 3.4662727508063926\n",
      "46     \t [0.35657319 0.69697249 0.11065344]. \t  0.021505444175504602 \t 3.4662727508063926\n",
      "47     \t [0.14903979 0.14004251 0.06309594]. \t  0.24628545011331074 \t 3.4662727508063926\n",
      "48     \t [0.33649642 0.79002893 0.11813537]. \t  0.01339568728156521 \t 3.4662727508063926\n",
      "49     \t [0.19911323 0.00237939 0.87097552]. \t  0.22425533538853948 \t 3.4662727508063926\n",
      "50     \t [0.63719903 0.11168066 0.68479935]. \t  0.4601969693558089 \t 3.4662727508063926\n",
      "51     \t [0.38792124 0.61712546 0.28586968]. \t  0.2226509244019392 \t 3.4662727508063926\n",
      "52     \t [0.25904267 0.93828389 0.5850829 ]. \t  2.6921736979674034 \t 3.4662727508063926\n",
      "53     \t [0.4504184  0.03605091 0.84411583]. \t  0.3381776110807855 \t 3.4662727508063926\n",
      "54     \t [0.65381146 0.55497032 0.11298225]. \t  0.05763780126846553 \t 3.4662727508063926\n",
      "55     \t [0.25048847 0.35816706 0.75953867]. \t  2.3465209235283924 \t 3.4662727508063926\n",
      "56     \t [0.60248017 0.3231838  0.54524734]. \t  0.40827501689705414 \t 3.4662727508063926\n",
      "57     \t [0.80933285 0.26153161 0.72892783]. \t  1.3799198877907282 \t 3.4662727508063926\n",
      "58     \t [0.06935917 0.04562295 0.11885838]. \t  0.37488370638369756 \t 3.4662727508063926\n",
      "59     \t [0.19125182 0.06925342 0.12400518]. \t  0.4802546651737729 \t 3.4662727508063926\n",
      "60     \t [0.39667605 0.31587947 0.77118847]. \t  2.09764552636701 \t 3.4662727508063926\n",
      "61     \t [0.75257917 0.76666177 0.77475694]. \t  2.0249713671027507 \t 3.4662727508063926\n",
      "62     \t [0.15119604 0.56926613 0.79512519]. \t  \u001b[92m3.5645605148548265\u001b[0m \t 3.5645605148548265\n",
      "63     \t [0.95058887 0.60792326 0.4672321 ]. \t  0.21497354455016082 \t 3.5645605148548265\n",
      "64     \t [0.77521251 0.31322511 0.07287485]. \t  0.13344020497519432 \t 3.5645605148548265\n",
      "65     \t [0.95386747 0.78640043 0.70238134]. \t  1.1284191028386925 \t 3.5645605148548265\n",
      "66     \t [0.6303606  0.91787344 0.54542479]. \t  1.3455171557446375 \t 3.5645605148548265\n",
      "67     \t [0.31648226 0.21977504 0.63165128]. \t  0.6129235792428898 \t 3.5645605148548265\n",
      "68     \t [0.21123292 0.96019313 0.16994722]. \t  0.032336079936821825 \t 3.5645605148548265\n",
      "69     \t [0.7204923  0.85641747 0.12714917]. \t  0.005664526266435488 \t 3.5645605148548265\n",
      "70     \t [0.16132399 0.18463979 0.39212205]. \t  0.5455106698921409 \t 3.5645605148548265\n",
      "71     \t [0.43817054 0.11696402 0.3778372 ]. \t  0.6896644481940144 \t 3.5645605148548265\n",
      "72     \t [0.97000739 0.8651114  0.1323205 ]. \t  0.0022642447975693637 \t 3.5645605148548265\n",
      "73     \t [0.88402471 0.55017171 0.74039155]. \t  2.627584398234073 \t 3.5645605148548265\n",
      "74     \t [0.20725764 0.44661577 0.43293425]. \t  0.47973243351049316 \t 3.5645605148548265\n",
      "75     \t [0.70277869 0.98631596 0.48471729]. \t  0.7993628237820469 \t 3.5645605148548265\n",
      "76     \t [0.38963959 0.50754173 0.62896835]. \t  1.5548694758764974 \t 3.5645605148548265\n",
      "77     \t [0.81937767 0.82390328 0.37279227]. \t  0.24367774267758802 \t 3.5645605148548265\n",
      "78     \t [0.96814049 0.77015168 0.31479389]. \t  0.05749856420429448 \t 3.5645605148548265\n",
      "79     \t [0.88648415 0.64118345 0.12676937]. \t  0.017038199366140965 \t 3.5645605148548265\n",
      "80     \t [0.32415789 0.63931755 0.71370113]. \t  2.6053987677052417 \t 3.5645605148548265\n",
      "81     \t [0.45938261 0.75250997 0.30932276]. \t  0.3116825248599445 \t 3.5645605148548265\n",
      "82     \t [0.98790758 0.95210545 0.779161  ]. \t  0.6501774865339243 \t 3.5645605148548265\n",
      "83     \t [0.33721187 0.97906121 0.32228894]. \t  0.45450899396014893 \t 3.5645605148548265\n",
      "84     \t [0.07194143 0.57218387 0.00097635]. \t  0.011634653775367711 \t 3.5645605148548265\n",
      "85     \t [0.74376934 0.45618499 0.90031264]. \t  3.243537437346223 \t 3.5645605148548265\n",
      "86     \t [0.60832238 0.55700038 0.81874419]. \t  \u001b[92m3.683568939368378\u001b[0m \t 3.683568939368378\n",
      "87     \t [0.42324854 0.44603074 0.25738932]. \t  0.36038375658699134 \t 3.683568939368378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.4927525  0.5385138  0.27851717]. \t  0.2253183040613936 \t 3.683568939368378\n",
      "89     \t [0.04815468 0.70016081 0.19228493]. \t  0.06345418758305794 \t 3.683568939368378\n",
      "90     \t [0.96808327 0.58718331 0.88105544]. \t  3.592656151513638 \t 3.683568939368378\n",
      "91     \t [0.88857253 0.13831501 0.99785855]. \t  0.3461011837569411 \t 3.683568939368378\n",
      "92     \t [0.45240892 0.93089545 0.88793639]. \t  1.0218699268839455 \t 3.683568939368378\n",
      "93     \t [0.40131836 0.529441   0.64123593]. \t  1.7253094510661566 \t 3.683568939368378\n",
      "94     \t [0.3152538  0.63830075 0.01828794]. \t  0.010460757853160165 \t 3.683568939368378\n",
      "95     \t [0.29793335 0.26841084 0.64559196]. \t  0.8646917429051725 \t 3.683568939368378\n",
      "96     \t [0.05922251 0.19985131 0.18276069]. \t  0.5656147149471785 \t 3.683568939368378\n",
      "97     \t [4.03393898e-01 9.87857342e-01 9.78709217e-04]. \t  0.0002656821721122154 \t 3.683568939368378\n",
      "98     \t [0.43401217 0.70958608 0.81826318]. \t  2.9929994017946773 \t 3.683568939368378\n",
      "99     \t [0.52938386 0.10117051 0.74540064]. \t  0.5622130609904947 \t 3.683568939368378\n",
      "100    \t [0.32680064 0.74037044 0.49418899]. \t  2.0226489388748736 \t 3.683568939368378\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\n",
    "\n",
    "np.random.seed(run_num_12)\n",
    "surrogate_loser_12 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_12 = GPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_12.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.66064431 0.41360065 0.66810256]. \t  1.5457680899590245 \t 2.6697919207500047\n",
      "init   \t [0.22994342 0.80767834 0.63681846]. \t  2.6697919207500047 \t 2.6697919207500047\n",
      "init   \t [0.17219385 0.26038587 0.91531999]. \t  1.4685599870430508 \t 2.6697919207500047\n",
      "init   \t [0.46281551 0.12019095 0.88088551]. \t  0.639297028201682 \t 2.6697919207500047\n",
      "init   \t [0.22621895 0.81144033 0.44587892]. \t  1.960293029067079 \t 2.6697919207500047\n",
      "1      \t [0.41009259 1.         1.        ]. \t  0.33361389892979315 \t 2.6697919207500047\n",
      "2      \t [0.91418486 0.13498533 0.09072509]. \t  0.16031997849670143 \t 2.6697919207500047\n",
      "3      \t [0.05541546 0.94065901 0.80220177]. \t  1.1967277289563476 \t 2.6697919207500047\n",
      "4      \t [0.04415727 0.04815351 0.41655995]. \t  0.3638074619101327 \t 2.6697919207500047\n",
      "5      \t [0.99455258 0.92788983 0.2811148 ]. \t  0.029823578890430132 \t 2.6697919207500047\n",
      "6      \t [0.64946068 0.97212608 0.53549274]. \t  1.1445885705215288 \t 2.6697919207500047\n",
      "7      \t [0.00280639 0.56643874 0.56974004]. \t  1.5638719753560206 \t 2.6697919207500047\n",
      "8      \t [3.74128618e-01 2.90520797e-04 6.79335015e-03]. \t  0.1139263390418666 \t 2.6697919207500047\n",
      "9      \t [0.3540029  0.43577344 0.60362303]. \t  1.1125244414958395 \t 2.6697919207500047\n",
      "10     \t [1.         0.45118349 1.        ]. \t  1.7605076920363132 \t 2.6697919207500047\n",
      "11     \t [0.06668372 0.01516675 0.0183777 ]. \t  0.10682425407575023 \t 2.6697919207500047\n",
      "12     \t [0.5196858  0.69149659 0.95483845]. \t  2.4718493595269835 \t 2.6697919207500047\n",
      "13     \t [0.99985351 0.03121148 0.88559058]. \t  0.27446242085498995 \t 2.6697919207500047\n",
      "14     \t [0.99325398 0.62676208 0.55356945]. \t  0.4473890126884814 \t 2.6697919207500047\n",
      "15     \t [0.96394442 0.91735249 0.01956559]. \t  0.00015162727779548937 \t 2.6697919207500047\n",
      "16     \t [0.73330929 0.10079917 0.1790754 ]. \t  0.5302323258196112 \t 2.6697919207500047\n",
      "17     \t [0.80849755 0.15516763 0.55071728]. \t  0.20286319974479652 \t 2.6697919207500047\n",
      "18     \t [0.58308557 0.87109283 0.5936674 ]. \t  1.6129452256882726 \t 2.6697919207500047\n",
      "19     \t [0.19794147 0.6599606  0.46264745]. \t  1.5064933507193867 \t 2.6697919207500047\n",
      "20     \t [0.37910729 0.34556537 0.24838855]. \t  0.5956795291066099 \t 2.6697919207500047\n",
      "21     \t [0.15676912 0.2266729  0.71458064]. \t  1.1069290346962903 \t 2.6697919207500047\n",
      "22     \t [0.52038061 0.95614372 0.33226602]. \t  0.3830866615916619 \t 2.6697919207500047\n",
      "23     \t [0.92986348 0.40325218 0.60196825]. \t  0.7490893375906799 \t 2.6697919207500047\n",
      "24     \t [0.53542066 0.22558072 0.30585522]. \t  0.7871123420942476 \t 2.6697919207500047\n",
      "25     \t [0.5823161  0.00148667 0.37270086]. \t  0.5485870456558988 \t 2.6697919207500047\n",
      "26     \t [0.59357954 0.56878291 0.68481091]. \t  2.0452040645303198 \t 2.6697919207500047\n",
      "27     \t [0.12076705 0.86206906 0.00611273]. \t  0.0007762036397133444 \t 2.6697919207500047\n",
      "28     \t [0.9337669  0.48487116 0.36928721]. \t  0.11147461455551527 \t 2.6697919207500047\n",
      "29     \t [0.57331052 0.99917895 0.69537268]. \t  0.9411566287997121 \t 2.6697919207500047\n",
      "30     \t [0.34088191 0.45891124 0.17408725]. \t  0.24477884525232038 \t 2.6697919207500047\n",
      "31     \t [0.45346533 0.24195019 0.21707166]. \t  0.777584452254225 \t 2.6697919207500047\n",
      "32     \t [0.7336959  0.19992914 0.83601768]. \t  1.2323835870354274 \t 2.6697919207500047\n",
      "33     \t [0.62578916 0.9972927  0.83172719]. \t  0.6417088167630212 \t 2.6697919207500047\n",
      "34     \t [0.51438648 0.13364806 0.62148412]. \t  0.3430355524950182 \t 2.6697919207500047\n",
      "35     \t [0.06293522 0.83845774 0.66680146]. \t  2.518751420246468 \t 2.6697919207500047\n",
      "36     \t [0.82803501 0.75130533 0.12609098]. \t  0.0074436256194782115 \t 2.6697919207500047\n",
      "37     \t [0.9733965  0.04824388 0.53741531]. \t  0.09379129043720691 \t 2.6697919207500047\n",
      "38     \t [0.48614181 0.08087374 0.51203787]. \t  0.21111004333001018 \t 2.6697919207500047\n",
      "39     \t [0.1045581  0.03693532 0.28305847]. \t  0.7553175789879043 \t 2.6697919207500047\n",
      "40     \t [0.63740215 0.69850905 0.29943415]. \t  0.1626293940305324 \t 2.6697919207500047\n",
      "41     \t [0.01492616 0.79914268 0.07117118]. \t  0.004550701126035047 \t 2.6697919207500047\n",
      "42     \t [0.49810016 0.89864423 0.91889104]. \t  1.1531476462011079 \t 2.6697919207500047\n",
      "43     \t [0.71927048 0.24035136 0.84679427]. \t  1.5604251042519215 \t 2.6697919207500047\n",
      "44     \t [0.51772007 0.16459242 0.54156564]. \t  0.24703368892906002 \t 2.6697919207500047\n",
      "45     \t [0.91307953 0.54224691 0.01255567]. \t  0.009644528294269164 \t 2.6697919207500047\n",
      "46     \t [0.32018525 0.96995705 0.07045838]. \t  0.002320141225838857 \t 2.6697919207500047\n",
      "47     \t [0.63775282 0.84122823 0.98907381]. \t  1.0848821100342865 \t 2.6697919207500047\n",
      "48     \t [0.41687774 0.47665256 0.8930494 ]. \t  \u001b[92m3.4735349593341613\u001b[0m \t 3.4735349593341613\n",
      "49     \t [0.87544484 0.55099794 0.57339059]. \t  0.6560061477708956 \t 3.4735349593341613\n",
      "50     \t [0.82259705 0.18715503 0.24239246]. \t  0.5043136364337744 \t 3.4735349593341613\n",
      "51     \t [0.75320002 0.16866405 0.87634165]. \t  0.9279718806791477 \t 3.4735349593341613\n",
      "52     \t [0.14283124 0.96982648 0.46066381]. \t  2.0942187826157297 \t 3.4735349593341613\n",
      "53     \t [0.07023171 0.70890922 0.27658173]. \t  0.2471697766124015 \t 3.4735349593341613\n",
      "54     \t [0.92469231 0.95738827 0.17634737]. \t  0.005448549580207616 \t 3.4735349593341613\n",
      "55     \t [0.48102739 0.82261573 0.75166446]. \t  1.822862214107297 \t 3.4735349593341613\n",
      "56     \t [0.67393751 0.46596782 0.65886108]. \t  1.552198018826415 \t 3.4735349593341613\n",
      "57     \t [0.02708937 0.30626316 0.66384568]. \t  1.1499309828643371 \t 3.4735349593341613\n",
      "58     \t [0.83932096 0.82726693 0.50711291]. \t  0.6015910919907921 \t 3.4735349593341613\n",
      "59     \t [0.27701155 0.64444347 0.89608859]. \t  3.433467982743001 \t 3.4735349593341613\n",
      "60     \t [0.85806739 0.42003711 0.1988972 ]. \t  0.170858558567319 \t 3.4735349593341613\n",
      "61     \t [0.21806645 0.88497201 0.6999076 ]. \t  2.063426247291155 \t 3.4735349593341613\n",
      "62     \t [0.92301807 0.88585592 0.01980938]. \t  0.00024849928122262397 \t 3.4735349593341613\n",
      "63     \t [0.43194257 0.06871556 0.38614572]. \t  0.6366288501403745 \t 3.4735349593341613\n",
      "64     \t [0.31122698 0.46711811 0.02088006]. \t  0.04710443979578095 \t 3.4735349593341613\n",
      "65     \t [0.53570239 0.57571895 0.01116363]. \t  0.015773597038891984 \t 3.4735349593341613\n",
      "66     \t [0.41249547 0.65940938 0.78237046]. \t  3.1007216815605227 \t 3.4735349593341613\n",
      "67     \t [0.77935401 0.26930086 0.86935596]. \t  1.7638500802766433 \t 3.4735349593341613\n",
      "68     \t [0.43283647 0.76036953 0.41310426]. \t  1.0749063091057893 \t 3.4735349593341613\n",
      "69     \t [0.22385249 0.80812974 0.91181421]. \t  1.972010837229594 \t 3.4735349593341613\n",
      "70     \t [0.60743554 0.68562361 0.20006046]. \t  0.05206336040242722 \t 3.4735349593341613\n",
      "71     \t [0.25804814 0.13996467 0.81905318]. \t  0.8299592570505872 \t 3.4735349593341613\n",
      "72     \t [0.5821718  0.35246179 0.89182358]. \t  2.4801548321341045 \t 3.4735349593341613\n",
      "73     \t [0.23357874 0.73727731 0.3789872 ]. \t  0.9613318277059854 \t 3.4735349593341613\n",
      "74     \t [0.88888774 0.41874628 0.84717674]. \t  3.1732249560976977 \t 3.4735349593341613\n",
      "75     \t [0.02769668 0.13806226 0.81265602]. \t  0.8084523199876115 \t 3.4735349593341613\n",
      "76     \t [0.63695442 0.19345711 0.67316354]. \t  0.7158065553199365 \t 3.4735349593341613\n",
      "77     \t [0.50884202 0.31174631 0.03642599]. \t  0.13043191317951816 \t 3.4735349593341613\n",
      "78     \t [0.5557356  0.45374772 0.24007706]. \t  0.29810480269679684 \t 3.4735349593341613\n",
      "79     \t [0.10194547 0.48337965 0.67262343]. \t  2.0130384067844784 \t 3.4735349593341613\n",
      "80     \t [0.99357982 0.258955   0.11226195]. \t  0.123299984534561 \t 3.4735349593341613\n",
      "81     \t [0.71862106 0.57869395 0.48791097]. \t  0.4877635725523503 \t 3.4735349593341613\n",
      "82     \t [0.92160388 0.16911816 0.78896864]. \t  0.9773645613769912 \t 3.4735349593341613\n",
      "83     \t [0.90765207 0.08229488 0.34841664]. \t  0.3410480358603376 \t 3.4735349593341613\n",
      "84     \t [0.6177773  0.45181939 0.55973338]. \t  0.6756980288286273 \t 3.4735349593341613\n",
      "85     \t [0.7953169  0.48600213 0.60033648]. \t  0.8837266789465817 \t 3.4735349593341613\n",
      "86     \t [0.29537222 0.35230617 0.91184422]. \t  2.3294800034017893 \t 3.4735349593341613\n",
      "87     \t [0.63723314 0.7948855  0.96558784]. \t  1.6083594950866844 \t 3.4735349593341613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.17952302 0.7183282  0.35107356]. \t  0.6922487666296709 \t 3.4735349593341613\n",
      "89     \t [0.18214649 0.81090828 0.03565757]. \t  0.002337368518537894 \t 3.4735349593341613\n",
      "90     \t [0.91265406 0.94025652 0.6566742 ]. \t  0.5102062827977603 \t 3.4735349593341613\n",
      "91     \t [0.99370289 0.94780616 0.6772205 ]. \t  0.4206480841228158 \t 3.4735349593341613\n",
      "92     \t [0.71659102 0.29215869 0.45843115]. \t  0.25157630039769635 \t 3.4735349593341613\n",
      "93     \t [0.65917194 0.14424592 0.1377917 ]. \t  0.46612716240634694 \t 3.4735349593341613\n",
      "94     \t [0.98883086 0.81042525 0.54541417]. \t  0.3860550087247636 \t 3.4735349593341613\n",
      "95     \t [0.28215278 0.36647051 0.59496722]. \t  0.8409719875188555 \t 3.4735349593341613\n",
      "96     \t [0.41819026 0.01110224 0.42672782]. \t  0.42020265486539904 \t 3.4735349593341613\n",
      "97     \t [0.77153192 0.85421568 0.09593543]. \t  0.002562590911587586 \t 3.4735349593341613\n",
      "98     \t [0.13704579 0.51048051 0.96517546]. \t  2.626575319576149 \t 3.4735349593341613\n",
      "99     \t [0.10408355 0.96486528 0.16520414]. \t  0.029557288512020097 \t 3.4735349593341613\n",
      "100    \t [0.30368637 0.3434045  0.0622976 ]. \t  0.16772013086145363 \t 3.4735349593341613\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\n",
    "\n",
    "np.random.seed(run_num_13)\n",
    "surrogate_loser_13 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_13 = GPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_13.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.64755105 0.50714969 0.52834138]. \t  0.5963212059988954 \t 2.610000357863649\n",
      "init   \t [0.8962852  0.69999119 0.7142971 ]. \t  1.7197848290620104 \t 2.610000357863649\n",
      "init   \t [0.71733838 0.22281946 0.17515452]. \t  0.48166052848103497 \t 2.610000357863649\n",
      "init   \t [0.45684149 0.92873843 0.00988589]. \t  0.0004588015757462679 \t 2.610000357863649\n",
      "init   \t [0.08992219 0.85020027 0.48562106]. \t  2.610000357863649 \t 2.610000357863649\n",
      "1      \t [0. 1. 1.]. \t  0.330219860606422 \t 2.610000357863649\n",
      "2      \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.610000357863649\n",
      "3      \t [0.05593593 0.92782293 0.12748656]. \t  0.012672955321826789 \t 2.610000357863649\n",
      "4      \t [1.         1.         0.48695631]. \t  0.21258181210219015 \t 2.610000357863649\n",
      "5      \t [0.80829604 0.99997905 0.88385512]. \t  0.5623859876062987 \t 2.610000357863649\n",
      "6      \t [0.02146419 0.63896915 0.69822208]. \t  2.557184178249518 \t 2.610000357863649\n",
      "7      \t [0.98377295 0.45156257 0.12592043]. \t  0.05786822271687149 \t 2.610000357863649\n",
      "8      \t [0.98535496 0.31687436 0.75694447]. \t  1.921552441041293 \t 2.610000357863649\n",
      "9      \t [0.96994182 0.59808976 0.95922333]. \t  \u001b[92m2.694038523821318\u001b[0m \t 2.694038523821318\n",
      "10     \t [0.08960719 0.07019454 0.86353831]. \t  0.4361042465421674 \t 2.694038523821318\n",
      "11     \t [0.82661661 0.02338278 0.01499173]. \t  0.07237182666742638 \t 2.694038523821318\n",
      "12     \t [0.09208514 0.07037796 0.44908954]. \t  0.30589771667488297 \t 2.694038523821318\n",
      "13     \t [0.31388854 0.56171835 0.65470991]. \t  2.0249055367371938 \t 2.694038523821318\n",
      "14     \t [0.11218565 0.44702431 0.03432975]. \t  0.05434212210126889 \t 2.694038523821318\n",
      "15     \t [0.80939243 0.79010088 0.08327394]. \t  0.002997202454549464 \t 2.694038523821318\n",
      "16     \t [0.10052363 0.02367888 0.33536347]. \t  0.6437424979875838 \t 2.694038523821318\n",
      "17     \t [0.33319617 0.02386789 0.01858048]. \t  0.14278722241783126 \t 2.694038523821318\n",
      "18     \t [0.19115446 0.65697421 0.90020293]. \t  \u001b[92m3.322905191093616\u001b[0m \t 3.322905191093616\n",
      "19     \t [0.19675717 0.23104748 0.6933584 ]. \t  1.009417664252229 \t 3.322905191093616\n",
      "20     \t [0.93674272 0.00715582 0.42178829]. \t  0.16933941525743085 \t 3.322905191093616\n",
      "21     \t [0.36214764 0.49508642 0.97420567]. \t  2.436937429241984 \t 3.322905191093616\n",
      "22     \t [0.33814324 0.06018895 0.88562386]. \t  0.37360227617833996 \t 3.322905191093616\n",
      "23     \t [0.51299824 0.53435554 0.05110948]. \t  0.04079586528950364 \t 3.322905191093616\n",
      "24     \t [0.72598471 0.55989489 0.83116069]. \t  \u001b[92m3.7139472168367194\u001b[0m \t 3.7139472168367194\n",
      "25     \t [0.04591849 1.         0.68603563]. \t  1.6819784261276431 \t 3.7139472168367194\n",
      "26     \t [0.26032784 0.27375084 0.81176359]. \t  1.8818104124373705 \t 3.7139472168367194\n",
      "27     \t [0.73141003 0.25188286 0.79355714]. \t  1.6288419673048349 \t 3.7139472168367194\n",
      "28     \t [0.79772135 0.5752789  0.97944833]. \t  2.4178653602124673 \t 3.7139472168367194\n",
      "29     \t [0.40488625 0.56336259 0.72591955]. \t  2.728428649987446 \t 3.7139472168367194\n",
      "30     \t [0.80901536 0.21371557 0.00255886]. \t  0.06220844726521293 \t 3.7139472168367194\n",
      "31     \t [0.77487825 0.12440358 0.49843522]. \t  0.17868446358946294 \t 3.7139472168367194\n",
      "32     \t [0.58296738 0.76497902 0.24063865]. \t  0.08343007839597481 \t 3.7139472168367194\n",
      "33     \t [0.00420557 0.45156882 0.50342476]. \t  0.659104502994991 \t 3.7139472168367194\n",
      "34     \t [0.18285003 0.37844441 0.00287031]. \t  0.05587853797714881 \t 3.7139472168367194\n",
      "35     \t [0.14642965 0.91598237 0.2559379 ]. \t  0.20304412268097038 \t 3.7139472168367194\n",
      "36     \t [0.45615361 0.73727042 0.84871959]. \t  2.827008634102651 \t 3.7139472168367194\n",
      "37     \t [0.50993243 0.79119257 0.84788879]. \t  2.286440058928742 \t 3.7139472168367194\n",
      "38     \t [0.93623618 0.29605179 0.33829222]. \t  0.2437372125777743 \t 3.7139472168367194\n",
      "39     \t [0.7034763  0.7681872  0.59752879]. \t  1.191916990360724 \t 3.7139472168367194\n",
      "40     \t [0.60051018 0.29926604 0.66535384]. \t  1.1093207085561936 \t 3.7139472168367194\n",
      "41     \t [0.65646725 0.49265972 0.9653974 ]. \t  2.5605517011544756 \t 3.7139472168367194\n",
      "42     \t [0.14906706 0.07891761 0.45078817]. \t  0.3299405071818477 \t 3.7139472168367194\n",
      "43     \t [0.06361983 0.49685449 0.5492252 ]. \t  1.0900625695977924 \t 3.7139472168367194\n",
      "44     \t [0.48465398 0.53084226 0.76783388]. \t  3.216582404179128 \t 3.7139472168367194\n",
      "45     \t [0.49675483 0.31947015 0.54955814]. \t  0.44711691558888944 \t 3.7139472168367194\n",
      "46     \t [0.57531671 0.77714039 0.79601549]. \t  2.222017309872367 \t 3.7139472168367194\n",
      "47     \t [0.26885881 0.07215766 0.98682835]. \t  0.21748198695025048 \t 3.7139472168367194\n",
      "48     \t [0.78473699 0.80022914 0.48037126]. \t  0.6473946693924508 \t 3.7139472168367194\n",
      "49     \t [0.91186375 0.95770045 0.68081817]. \t  0.49059545288520023 \t 3.7139472168367194\n",
      "50     \t [0.76346494 0.19713688 0.57715695]. \t  0.3122080478524831 \t 3.7139472168367194\n",
      "51     \t [0.19454059 0.18159557 0.72237632]. \t  0.8914520401243811 \t 3.7139472168367194\n",
      "52     \t [0.44777979 0.55156613 0.32905811]. \t  0.2989331694716544 \t 3.7139472168367194\n",
      "53     \t [0.10993639 0.85226217 0.04170515]. \t  0.0019104756240350366 \t 3.7139472168367194\n",
      "54     \t [0.26209026 0.4376069  0.36118454]. \t  0.4084725546606177 \t 3.7139472168367194\n",
      "55     \t [0.06882112 0.21835609 0.21992618]. \t  0.6453713415558252 \t 3.7139472168367194\n",
      "56     \t [0.40019455 0.99076213 0.36883095]. \t  0.7195529257060763 \t 3.7139472168367194\n",
      "57     \t [0.74937503 0.30475534 0.21494596]. \t  0.4204924501714821 \t 3.7139472168367194\n",
      "58     \t [0.8803298  0.04036959 0.23308407]. \t  0.4154404081189263 \t 3.7139472168367194\n",
      "59     \t [0.70824533 0.16272843 0.55621025]. \t  0.23298576797584025 \t 3.7139472168367194\n",
      "60     \t [0.88090148 0.98500405 0.81756814]. \t  0.6036992108633918 \t 3.7139472168367194\n",
      "61     \t [0.53512709 0.16829529 0.9421505 ]. \t  0.6952432576112471 \t 3.7139472168367194\n",
      "62     \t [0.64401662 0.07139507 0.48504836]. \t  0.21907733667470577 \t 3.7139472168367194\n",
      "63     \t [0.87962619 0.97940685 0.19199823]. \t  0.0089505544210135 \t 3.7139472168367194\n",
      "64     \t [0.06312619 0.70383427 0.70474941]. \t  2.57097876829957 \t 3.7139472168367194\n",
      "65     \t [0.04272913 0.29614461 0.82233246]. \t  2.0848611009640106 \t 3.7139472168367194\n",
      "66     \t [0.28528739 0.46336557 0.26731631]. \t  0.33819052205087036 \t 3.7139472168367194\n",
      "67     \t [0.62690373 0.00985957 0.62415573]. \t  0.14153873039834525 \t 3.7139472168367194\n",
      "68     \t [0.51894156 0.10204151 0.11673319]. \t  0.47243581539184043 \t 3.7139472168367194\n",
      "69     \t [0.82250668 0.92070626 0.57262634]. \t  0.703700240487046 \t 3.7139472168367194\n",
      "70     \t [0.80836442 0.11740861 0.54025285]. \t  0.1632805156707197 \t 3.7139472168367194\n",
      "71     \t [0.55257834 0.99265052 0.26703074]. \t  0.1208697033520126 \t 3.7139472168367194\n",
      "72     \t [0.82454869 0.54477795 0.6759933 ]. \t  1.7256223502834849 \t 3.7139472168367194\n",
      "73     \t [0.05346613 0.14611429 0.75967501]. \t  0.8007182747420258 \t 3.7139472168367194\n",
      "74     \t [0.23981051 0.67263057 0.13086409]. \t  0.03353502772872412 \t 3.7139472168367194\n",
      "75     \t [0.9019401  0.82825263 0.81074213]. \t  1.6489711770283817 \t 3.7139472168367194\n",
      "76     \t [0.96803703 0.49412009 0.97246297]. \t  2.3800799393998964 \t 3.7139472168367194\n",
      "77     \t [0.11502712 0.97940057 0.38983881]. \t  1.1868805497135508 \t 3.7139472168367194\n",
      "78     \t [0.58754603 0.62029323 0.99348871]. \t  2.1370125930544903 \t 3.7139472168367194\n",
      "79     \t [0.9654962  0.13244232 0.08087639]. \t  0.12090328344314215 \t 3.7139472168367194\n",
      "80     \t [0.88177703 0.43718365 0.67654305]. \t  1.623967464378314 \t 3.7139472168367194\n",
      "81     \t [0.01465277 0.63397872 0.83901536]. \t  3.591801365983141 \t 3.7139472168367194\n",
      "82     \t [0.         0.53510087 1.        ]. \t  2.036606672273514 \t 3.7139472168367194\n",
      "83     \t [0.88256879 0.24667527 0.44384711]. \t  0.19112385305007243 \t 3.7139472168367194\n",
      "84     \t [0.02025095 0.62794742 0.34387863]. \t  0.468892639341909 \t 3.7139472168367194\n",
      "85     \t [0.66323936 0.67504463 0.77717188]. \t  2.786061946768787 \t 3.7139472168367194\n",
      "86     \t [0.96232362 0.52546593 0.40224017]. \t  0.10604193758342897 \t 3.7139472168367194\n",
      "87     \t [0.43523261 0.5618435  0.1594736 ]. \t  0.10387864219465551 \t 3.7139472168367194\n",
      "88     \t [0.71480031 0.33219295 0.31557587]. \t  0.4210776703142361 \t 3.7139472168367194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.58636839 0.44979184 0.79863073]. \t  3.2622995894011355 \t 3.7139472168367194\n",
      "90     \t [0.12175792 0.18456227 0.18306323]. \t  0.6433284296425059 \t 3.7139472168367194\n",
      "91     \t [0.28318428 0.71248563 0.53696315]. \t  2.2598699965857656 \t 3.7139472168367194\n",
      "92     \t [0.1244726  0.52626388 0.13790217]. \t  0.09965785828484391 \t 3.7139472168367194\n",
      "93     \t [0.38390603 0.72920448 0.50681676]. \t  1.9052606070746767 \t 3.7139472168367194\n",
      "94     \t [0.66904527 0.57598451 0.26215034]. \t  0.1301749742887242 \t 3.7139472168367194\n",
      "95     \t [0.96727165 0.74151729 0.17567326]. \t  0.00909902576108185 \t 3.7139472168367194\n",
      "96     \t [0.41527166 0.64813462 0.16672562]. \t  0.058604953951955356 \t 3.7139472168367194\n",
      "97     \t [0.38754664 0.11656231 0.04666429]. \t  0.23190351032409634 \t 3.7139472168367194\n",
      "98     \t [0.38700234 0.62982399 0.08687214]. \t  0.028972305826335615 \t 3.7139472168367194\n",
      "99     \t [0.20064203 0.83355521 0.68312582]. \t  2.3765749506506375 \t 3.7139472168367194\n",
      "100    \t [0.13333789 0.45299801 0.76109183]. \t  2.9534189917965623 \t 3.7139472168367194\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\n",
    "\n",
    "np.random.seed(run_num_14)\n",
    "surrogate_loser_14 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_14 = GPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_14.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.57788186 0.25550133 0.25252687]. \t  0.7217338409961529 \t 1.540625560354162\n",
      "init   \t [0.70990435 0.44755236 0.22694296]. \t  0.23199690080875204 \t 1.540625560354162\n",
      "init   \t [0.40170957 0.88225774 0.43721347]. \t  1.540625560354162 \t 1.540625560354162\n",
      "init   \t [0.87842518 0.78052775 0.53421429]. \t  0.5653855252190516 \t 1.540625560354162\n",
      "init   \t [0.01173301 0.26575648 0.3311941 ]. \t  0.5019682819961726 \t 1.540625560354162\n",
      "1      \t [0.08128701 1.         1.        ]. \t  0.3322425706424324 \t 1.540625560354162\n",
      "2      \t [0.61363169 0.18622958 0.87189166]. \t  1.0703600264488446 \t 1.540625560354162\n",
      "3      \t [0.09227183 0.81916727 0.00548775]. \t  0.001076579463298919 \t 1.540625560354162\n",
      "4      \t [0.0825884  0.10762681 0.97600228]. \t  0.32817121618245837 \t 1.540625560354162\n",
      "5      \t [0.99573902 0.01517269 0.64475385]. \t  0.15727336529876818 \t 1.540625560354162\n",
      "6      \t [0.6820242  0.83219259 0.97548164]. \t  1.2535582322531822 \t 1.540625560354162\n",
      "7      \t [0.48764269 0.93337811 0.0397321 ]. \t  0.0009185125389504879 \t 1.540625560354162\n",
      "8      \t [0.00105504 0.08339973 0.03624692]. \t  0.13281849367369486 \t 1.540625560354162\n",
      "9      \t [0.931463   0.05328064 0.13719876]. \t  0.22361958586423464 \t 1.540625560354162\n",
      "10     \t [0.04241685 0.57427846 0.86643881]. \t  \u001b[92m3.794734222483946\u001b[0m \t 3.794734222483946\n",
      "11     \t [0.         0.56479452 1.        ]. \t  2.0578130359176217 \t 3.794734222483946\n",
      "12     \t [0.14721214 0.6361499  0.74123246]. \t  2.921782726048466 \t 3.794734222483946\n",
      "13     \t [0.         1.         0.60780303]. \t  2.326909418848248 \t 3.794734222483946\n",
      "14     \t [0.70530993 1.         0.76314685]. \t  0.6027370549656079 \t 3.794734222483946\n",
      "15     \t [0.         0.27212117 0.75115939]. \t  1.6038604953515234 \t 3.794734222483946\n",
      "16     \t [1. 0. 1.]. \t  0.08848201872702738 \t 3.794734222483946\n",
      "17     \t [0.         0.71546873 0.69006733]. \t  2.4940029134996697 \t 3.794734222483946\n",
      "18     \t [0.29143127 0.39138629 1.        ]. \t  1.5400258342261837 \t 3.794734222483946\n",
      "19     \t [1.         0.62118487 1.        ]. \t  1.9522178911307033 \t 3.794734222483946\n",
      "20     \t [0.77580129 0.95488433 0.83877889]. \t  0.8189802946083149 \t 3.794734222483946\n",
      "21     \t [0.1745419  0.58861564 0.46678452]. \t  1.1425460938241916 \t 3.794734222483946\n",
      "22     \t [0.04590261 0.99449647 0.92188582]. \t  0.5763601772225795 \t 3.794734222483946\n",
      "23     \t [0.61034429 0.01704907 0.91874193]. \t  0.20844592155048333 \t 3.794734222483946\n",
      "24     \t [0.        1.        0.1115677]. \t  0.006945422843502955 \t 3.794734222483946\n",
      "25     \t [0.0003486  0.34853228 0.        ]. \t  0.045654879939926514 \t 3.794734222483946\n",
      "26     \t [0.15158623 0.95060302 0.64613729]. \t  2.356182250610943 \t 3.794734222483946\n",
      "27     \t [0.93902116 0.98300982 0.40163048]. \t  0.16799325633713869 \t 3.794734222483946\n",
      "28     \t [0.81029591 0.24678794 0.31937025]. \t  0.4381419149067207 \t 3.794734222483946\n",
      "29     \t [0.07219438 0.54592438 0.46719698]. \t  0.92582274020682 \t 3.794734222483946\n",
      "30     \t [0.25031903 0.57176203 0.29226148]. \t  0.26396392768388777 \t 3.794734222483946\n",
      "31     \t [0.96747193 0.73696735 0.56512663]. \t  0.49336417137675836 \t 3.794734222483946\n",
      "32     \t [0.30092112 0.04207641 0.49286411]. \t  0.23182925780625033 \t 3.794734222483946\n",
      "33     \t [0.49284216 0.24798455 0.17518899]. \t  0.6241596018331336 \t 3.794734222483946\n",
      "34     \t [0.94991439 0.25344513 0.47231953]. \t  0.15411571556249676 \t 3.794734222483946\n",
      "35     \t [0.42395109 0.46333711 0.69317732]. \t  2.1160544025142434 \t 3.794734222483946\n",
      "36     \t [0.57566654 0.86638367 0.42873776]. \t  0.9775902812427747 \t 3.794734222483946\n",
      "37     \t [0.62567376 0.48586688 0.12032655]. \t  0.11113690706796829 \t 3.794734222483946\n",
      "38     \t [0.42621411 0.29267786 0.06988823]. \t  0.2259749665115015 \t 3.794734222483946\n",
      "39     \t [0.76185184 0.23768587 0.64088868]. \t  0.6866343913709799 \t 3.794734222483946\n",
      "40     \t [0.65280619 0.29583628 0.3662211 ]. \t  0.4466117291379827 \t 3.794734222483946\n",
      "41     \t [0.95742796 0.23987328 0.12207954]. \t  0.16160564315557036 \t 3.794734222483946\n",
      "42     \t [0.9721892  0.6532657  0.66550756]. \t  1.2784958775059452 \t 3.794734222483946\n",
      "43     \t [0.25515437 0.53313499 0.73361559]. \t  2.8663227839682435 \t 3.794734222483946\n",
      "44     \t [0.06563885 0.5164127  0.68482816]. \t  2.251330443417286 \t 3.794734222483946\n",
      "45     \t [0.16197517 0.07401815 0.40257298]. \t  0.506158595836589 \t 3.794734222483946\n",
      "46     \t [0.94932689 0.7082576  0.15828948]. \t  0.010194875724603343 \t 3.794734222483946\n",
      "47     \t [0.03612574 0.20492158 0.96675868]. \t  0.7573473904144807 \t 3.794734222483946\n",
      "48     \t [0.31478473 0.75239736 0.59539435]. \t  2.503108446494391 \t 3.794734222483946\n",
      "49     \t [0.12807809 0.71043487 0.78441767]. \t  2.8944366251963025 \t 3.794734222483946\n",
      "50     \t [0.78169698 0.9679582  0.12423329]. \t  0.0029518948623640582 \t 3.794734222483946\n",
      "51     \t [0.17049127 0.30955908 0.5032451 ]. \t  0.36574896563945947 \t 3.794734222483946\n",
      "52     \t [0.62694206 0.23855795 0.03932852]. \t  0.14858552811675604 \t 3.794734222483946\n",
      "53     \t [0.02949578 0.52997778 0.92497741]. \t  3.2843093921713233 \t 3.794734222483946\n",
      "54     \t [0.39571091 0.76513454 0.11735628]. \t  0.014336754993934943 \t 3.794734222483946\n",
      "55     \t [0.49912256 0.57359354 0.07987976]. \t  0.0420897873141118 \t 3.794734222483946\n",
      "56     \t [0.71298415 0.07796515 0.02175991]. \t  0.1131417516293181 \t 3.794734222483946\n",
      "57     \t [0.08532404 0.07999008 0.5542384 ]. \t  0.16634200233107438 \t 3.794734222483946\n",
      "58     \t [0.31537954 0.50614944 0.14901243]. \t  0.1482404676065216 \t 3.794734222483946\n",
      "59     \t [0.95595308 0.70300921 0.2873902 ]. \t  0.04232035400132968 \t 3.794734222483946\n",
      "60     \t [0.13282356 0.51499461 0.7746907 ]. \t  3.323744793940068 \t 3.794734222483946\n",
      "61     \t [0.94502108 0.39990792 0.46043258]. \t  0.1539566760694928 \t 3.794734222483946\n",
      "62     \t [0.8196895  0.72686006 0.93511731]. \t  2.416662369877793 \t 3.794734222483946\n",
      "63     \t [0.21142111 0.13065041 0.63887592]. \t  0.3862661188276634 \t 3.794734222483946\n",
      "64     \t [0.89506211 0.53577709 0.97892019]. \t  2.3861619625231607 \t 3.794734222483946\n",
      "65     \t [0.07179407 0.307412   0.54957737]. \t  0.45644969200223207 \t 3.794734222483946\n",
      "66     \t [0.74102955 0.10310999 0.55334669]. \t  0.17062597575798433 \t 3.794734222483946\n",
      "67     \t [0.32117172 0.15156373 0.36337591]. \t  0.7518827261601073 \t 3.794734222483946\n",
      "68     \t [0.23987767 0.03561525 0.82833568]. \t  0.3449567060631165 \t 3.794734222483946\n",
      "69     \t [0.56859761 0.55077975 0.36159905]. \t  0.29355795375216914 \t 3.794734222483946\n",
      "70     \t [0.68819035 0.64189538 0.06515681]. \t  0.01423296030403576 \t 3.794734222483946\n",
      "71     \t [0.64977247 0.99241348 0.0401207 ]. \t  0.0004630520725081235 \t 3.794734222483946\n",
      "72     \t [0.35024029 0.27870533 0.4139076 ]. \t  0.4638658250097487 \t 3.794734222483946\n",
      "73     \t [0.70710257 0.39199623 0.73546271]. \t  2.262804163662324 \t 3.794734222483946\n",
      "74     \t [0.17094858 0.30939972 0.91923819]. \t  1.8661890406393606 \t 3.794734222483946\n",
      "75     \t [0.12129095 0.05188576 0.52608347]. \t  0.1609168422448141 \t 3.794734222483946\n",
      "76     \t [0.37857704 0.4008916  0.57095718]. \t  0.7635747625375403 \t 3.794734222483946\n",
      "77     \t [0.76748649 0.65576792 0.81978304]. \t  3.246026808706147 \t 3.794734222483946\n",
      "78     \t [0.78474209 0.09469402 0.27343527]. \t  0.5919355161280437 \t 3.794734222483946\n",
      "79     \t [0.3432652  0.99233688 0.29913662]. \t  0.311745229322761 \t 3.794734222483946\n",
      "80     \t [0.42594991 0.64093961 0.79628817]. \t  3.311886121166303 \t 3.794734222483946\n",
      "81     \t [0.81464089 0.20410306 0.19092096]. \t  0.42888437233317794 \t 3.794734222483946\n",
      "82     \t [0.01785545 0.43979856 0.33346611]. \t  0.31975513224542707 \t 3.794734222483946\n",
      "83     \t [0.31647395 0.46802472 0.27590367]. \t  0.3387216842663361 \t 3.794734222483946\n",
      "84     \t [0.41326538 0.23479662 0.33965651]. \t  0.7515899184306771 \t 3.794734222483946\n",
      "85     \t [0.32277329 0.62037369 0.19391519]. \t  0.0949095772444279 \t 3.794734222483946\n",
      "86     \t [0.24974013 0.69021691 0.23669921]. \t  0.1322942127617831 \t 3.794734222483946\n",
      "87     \t [0.27770224 0.88136147 0.45737355]. \t  2.084065645762579 \t 3.794734222483946\n",
      "88     \t [0.68300943 0.804197   0.27631666]. \t  0.11087677703003533 \t 3.794734222483946\n",
      "89     \t [0.63672142 0.53245896 0.5901552 ]. \t  1.017995686820874 \t 3.794734222483946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.84436452 0.90046832 0.78427641]. \t  1.02422684648505 \t 3.794734222483946\n",
      "91     \t [0.09235068 0.59453282 0.5299634 ]. \t  1.5829305238168287 \t 3.794734222483946\n",
      "92     \t [0.3594319  0.77854922 0.53836509]. \t  2.3715610925954347 \t 3.794734222483946\n",
      "93     \t [0.84754239 0.32899248 0.26725522]. \t  0.3237456416348651 \t 3.794734222483946\n",
      "94     \t [0.16133936 0.95229344 0.03212184]. \t  0.0009293574810958403 \t 3.794734222483946\n",
      "95     \t [0.16932797 0.89291599 0.43779014]. \t  1.9690371859422988 \t 3.794734222483946\n",
      "96     \t [0.45842086 0.96216734 0.79621177]. \t  0.9516993946869766 \t 3.794734222483946\n",
      "97     \t [0.88281347 0.56274476 0.02425408]. \t  0.01059400172578875 \t 3.794734222483946\n",
      "98     \t [0.50015117 0.62194574 0.45523951]. \t  0.8241009240885171 \t 3.794734222483946\n",
      "99     \t [0.24256493 0.3609237  0.17845439]. \t  0.4178813319039115 \t 3.794734222483946\n",
      "100    \t [0.8987482  0.08554098 0.91272598]. \t  0.40642636439750535 \t 3.794734222483946\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\n",
    "\n",
    "np.random.seed(run_num_15)\n",
    "surrogate_loser_15 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_15 = GPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_15.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.86394833 0.316366   0.67215078]. \t  1.1947633822631252 \t 3.8084053754826726\n",
      "init   \t [0.50791167 0.78166043 0.28368168]. \t  0.20052765717741963 \t 3.8084053754826726\n",
      "init   \t [0.23370878 0.56379969 0.87502436]. \t  3.8084053754826726 \t 3.8084053754826726\n",
      "init   \t [0.71894292 0.18213174 0.24380041]. \t  0.6532826728302027 \t 3.8084053754826726\n",
      "init   \t [0.08605673 0.53424539 0.83969965]. \t  3.8072565344663847 \t 3.8084053754826726\n",
      "1      \t [0.08056179 0.45877099 1.        ]. \t  1.8425209175454709 \t 3.8084053754826726\n",
      "2      \t [0.         1.         0.75308252]. \t  1.0928770389243105 \t 3.8084053754826726\n",
      "3      \t [0.         0.22301295 0.34500758]. \t  0.5095972391781063 \t 3.8084053754826726\n",
      "4      \t [0.24662864 0.55095703 0.66400731]. \t  2.124428548649976 \t 3.8084053754826726\n",
      "5      \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.8084053754826726\n",
      "6      \t [1.         0.61092457 0.        ]. \t  0.0031086994070658506 \t 3.8084053754826726\n",
      "7      \t [0.         0.78457803 1.        ]. \t  1.2979609366217637 \t 3.8084053754826726\n",
      "8      \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.8084053754826726\n",
      "9      \t [0.79982814 1.         0.        ]. \t  8.742179832770534e-05 \t 3.8084053754826726\n",
      "10     \t [0.69045809 0.68193354 1.        ]. \t  1.8176811195343365 \t 3.8084053754826726\n",
      "11     \t [0.58953027 1.         0.77429489]. \t  0.6953772508985578 \t 3.8084053754826726\n",
      "12     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.8084053754826726\n",
      "13     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.8084053754826726\n",
      "14     \t [0.25533903 0.76562693 0.84282053]. \t  2.6001998657892154 \t 3.8084053754826726\n",
      "15     \t [0.4369566  0.31390697 0.94432149]. \t  1.6861912722697086 \t 3.8084053754826726\n",
      "16     \t [0.97582172 0.68367138 0.60172171]. \t  0.6855234655190374 \t 3.8084053754826726\n",
      "17     \t [0.53799443 0.42946953 0.00548847]. \t  0.04425139695826853 \t 3.8084053754826726\n",
      "18     \t [0.56661062 0.34269926 0.26922376]. \t  0.5430076964051797 \t 3.8084053754826726\n",
      "19     \t [0.90848465 0.94727411 0.66475276]. \t  0.5069273890831931 \t 3.8084053754826726\n",
      "20     \t [0.83448508 0.12077406 0.25541979]. \t  0.519844097608949 \t 3.8084053754826726\n",
      "21     \t [0.04023156 0.45894557 0.70851272]. \t  2.3323726899042696 \t 3.8084053754826726\n",
      "22     \t [1.         0.45063554 1.        ]. \t  1.7583124130343637 \t 3.8084053754826726\n",
      "23     \t [0.86840893 0.50347298 0.18496287]. \t  0.08894346754563025 \t 3.8084053754826726\n",
      "24     \t [0.15507555 0.57267199 0.87068821]. \t  \u001b[92m3.8090213016418613\u001b[0m \t 3.8090213016418613\n",
      "25     \t [0.21207837 0.9958074  0.01335284]. \t  0.00043942655710574075 \t 3.8090213016418613\n",
      "26     \t [0.05885562 0.6198107  0.89982702]. \t  3.489981524852336 \t 3.8090213016418613\n",
      "27     \t [1.39120962e-01 2.82077871e-04 4.22672869e-01]. \t  0.36632513317281906 \t 3.8090213016418613\n",
      "28     \t [0.71969094 0.12141137 0.88384231]. \t  0.633205118123529 \t 3.8090213016418613\n",
      "29     \t [0.3437446  0.42971995 0.46588041]. \t  0.4770678941020929 \t 3.8090213016418613\n",
      "30     \t [0.43569532 0.69456793 0.64220519]. \t  2.048398826348916 \t 3.8090213016418613\n",
      "31     \t [0.38982814 0.55892186 0.89282072]. \t  3.702871815898514 \t 3.8090213016418613\n",
      "32     \t [0.99656962 0.06250794 0.78544588]. \t  0.43045263052217053 \t 3.8090213016418613\n",
      "33     \t [0.88781016 0.04638648 0.8933457 ]. \t  0.310998536788925 \t 3.8090213016418613\n",
      "34     \t [0.46625412 0.34168985 0.1594325 ]. \t  0.41493325966955924 \t 3.8090213016418613\n",
      "35     \t [0.75458476 0.91661048 0.98789737]. \t  0.6831435742486909 \t 3.8090213016418613\n",
      "36     \t [0.78144854 0.69357355 0.53307138]. \t  0.7174412868429118 \t 3.8090213016418613\n",
      "37     \t [0.49477579 0.98071266 0.09779905]. \t  0.003492061104007494 \t 3.8090213016418613\n",
      "38     \t [0.38225454 0.54813721 0.85226666]. \t  \u001b[92m3.8564430066401205\u001b[0m \t 3.8564430066401205\n",
      "39     \t [0.5272702  0.65121755 0.97036025]. \t  2.440683465823292 \t 3.8564430066401205\n",
      "40     \t [0.05773691 0.81600649 0.83255251]. \t  2.1203882471503688 \t 3.8564430066401205\n",
      "41     \t [0.56054202 0.5367984  0.09062523]. \t  0.06109184484329125 \t 3.8564430066401205\n",
      "42     \t [0.44468499 0.00937981 0.74491765]. \t  0.25889728035744847 \t 3.8564430066401205\n",
      "43     \t [0.64608835 0.5730285  0.80072763]. \t  3.496451410603565 \t 3.8564430066401205\n",
      "44     \t [0.87162801 0.47921632 0.9186832 ]. \t  3.1441940720976573 \t 3.8564430066401205\n",
      "45     \t [0.96714671 0.31877611 0.8109514 ]. \t  2.224195426707524 \t 3.8564430066401205\n",
      "46     \t [0.06196911 0.9379467  0.18851526]. \t  0.05190659427104055 \t 3.8564430066401205\n",
      "47     \t [0.25289096 0.16187821 0.28078821]. \t  0.9383184957295628 \t 3.8564430066401205\n",
      "48     \t [0.43728799 0.33399021 0.54974035]. \t  0.48720358065663055 \t 3.8564430066401205\n",
      "49     \t [0.31540751 0.4925806  0.18533984]. \t  0.20812756425445086 \t 3.8564430066401205\n",
      "50     \t [0.55388742 0.80271077 0.8283371 ]. \t  2.1264732229035648 \t 3.8564430066401205\n",
      "51     \t [0.01141799 0.7958181  0.04024629]. \t  0.0024257263715949007 \t 3.8564430066401205\n",
      "52     \t [0.38568653 0.43133829 0.84687915]. \t  3.36473683347935 \t 3.8564430066401205\n",
      "53     \t [0.96522448 0.72693724 0.54356502]. \t  0.43203529307071536 \t 3.8564430066401205\n",
      "54     \t [0.78805564 0.61380732 0.41349227]. \t  0.256670904458279 \t 3.8564430066401205\n",
      "55     \t [0.97318955 0.43407487 0.41427682]. \t  0.11515215923246044 \t 3.8564430066401205\n",
      "56     \t [0.90603606 0.25244893 0.40295125]. \t  0.21999200267847413 \t 3.8564430066401205\n",
      "57     \t [0.51003172 0.40408284 0.06036259]. \t  0.11447049881241596 \t 3.8564430066401205\n",
      "58     \t [0.04398814 0.28878408 0.93202463]. \t  1.5716951794842502 \t 3.8564430066401205\n",
      "59     \t [0.97372688 0.12096872 0.72139876]. \t  0.5768109144263086 \t 3.8564430066401205\n",
      "60     \t [0.93617363 0.31811842 0.34290007]. \t  0.22210349730127474 \t 3.8564430066401205\n",
      "61     \t [0.0918845  0.35222564 0.4215209 ]. \t  0.3685063985974734 \t 3.8564430066401205\n",
      "62     \t [0.58821847 0.87099    0.19609719]. \t  0.0343397149464509 \t 3.8564430066401205\n",
      "63     \t [0.49773967 0.30088814 0.81176042]. \t  2.135118119974003 \t 3.8564430066401205\n",
      "64     \t [0.13141619 0.40399005 0.92938214]. \t  2.5762376586942803 \t 3.8564430066401205\n",
      "65     \t [0.05166908 0.89712758 0.83222533]. \t  1.4256201907466846 \t 3.8564430066401205\n",
      "66     \t [0.6074091  0.5651644  0.76113511]. \t  3.062024712880875 \t 3.8564430066401205\n",
      "67     \t [0.3279515  0.0692452  0.67134912]. \t  0.31247617797407107 \t 3.8564430066401205\n",
      "68     \t [0.0961328  0.21997547 0.097758  ]. \t  0.30382130390439555 \t 3.8564430066401205\n",
      "69     \t [0.00527737 0.84067748 0.99465264]. \t  1.0374991182043767 \t 3.8564430066401205\n",
      "70     \t [0.73828086 0.51416956 0.16225059]. \t  0.10097721825159194 \t 3.8564430066401205\n",
      "71     \t [0.23621156 0.77273158 0.72314536]. \t  2.3689595603050706 \t 3.8564430066401205\n",
      "72     \t [0.69179496 0.9626543  0.04608057]. \t  0.00055817181923092 \t 3.8564430066401205\n",
      "73     \t [0.01135413 0.13766301 0.42708938]. \t  0.3369781391191165 \t 3.8564430066401205\n",
      "74     \t [0.01821837 0.06125134 0.50305178]. \t  0.16691697532744534 \t 3.8564430066401205\n",
      "75     \t [0.75887025 0.98992728 0.04830322]. \t  0.00041008097929826304 \t 3.8564430066401205\n",
      "76     \t [0.45034395 0.31184179 0.49118223]. \t  0.34019795084950083 \t 3.8564430066401205\n",
      "77     \t [0.34516894 0.87245412 0.01261572]. \t  0.0008504359352652114 \t 3.8564430066401205\n",
      "78     \t [0.5402775  0.11084036 0.71951798]. \t  0.5495022323459491 \t 3.8564430066401205\n",
      "79     \t [0.85245056 0.8370809  0.2820212 ]. \t  0.06353125045560243 \t 3.8564430066401205\n",
      "80     \t [0.65723325 0.81839565 0.23693994]. \t  0.0627265516022317 \t 3.8564430066401205\n",
      "81     \t [0.19064436 0.41679247 0.29594489]. \t  0.4111794760569056 \t 3.8564430066401205\n",
      "82     \t [0.10264431 0.13774606 0.43768167]. \t  0.36310734409873674 \t 3.8564430066401205\n",
      "83     \t [0.44394081 0.80240406 0.74670846]. \t  1.9883099573903038 \t 3.8564430066401205\n",
      "84     \t [0.1070112  0.18095404 0.22045493]. \t  0.7325303941307997 \t 3.8564430066401205\n",
      "85     \t [0.98222071 0.56216458 0.37437893]. \t  0.08341834321777766 \t 3.8564430066401205\n",
      "86     \t [0.13569583 0.80181312 0.26061919]. \t  0.2203440587860694 \t 3.8564430066401205\n",
      "87     \t [0.63531704 0.15550047 0.43784079]. \t  0.3572604518527468 \t 3.8564430066401205\n",
      "88     \t [0.7149028  0.61242382 0.36241602]. \t  0.21763972793491507 \t 3.8564430066401205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.72451742 0.87865968 0.01700706]. \t  0.00048062661902852907 \t 3.8564430066401205\n",
      "90     \t [0.06980833 0.06735126 0.19189161]. \t  0.6291019355032829 \t 3.8564430066401205\n",
      "91     \t [0.02629512 0.25758702 0.77117475]. \t  1.5962871548899078 \t 3.8564430066401205\n",
      "92     \t [0.057924   0.12644384 0.43121119]. \t  0.35490505058061267 \t 3.8564430066401205\n",
      "93     \t [0.23209014 0.82288377 0.68282029]. \t  2.3689475747338604 \t 3.8564430066401205\n",
      "94     \t [0.35882936 0.0239903  0.06063885]. \t  0.25459726411542455 \t 3.8564430066401205\n",
      "95     \t [0.73959719 0.3018457  0.40047305]. \t  0.3086314264790407 \t 3.8564430066401205\n",
      "96     \t [0.60526256 0.23723161 0.17647999]. \t  0.5717865799947605 \t 3.8564430066401205\n",
      "97     \t [0.58961367 0.45489445 0.43543263]. \t  0.33085397053812077 \t 3.8564430066401205\n",
      "98     \t [0.51236923 0.53830879 0.23496533]. \t  0.18246633710710705 \t 3.8564430066401205\n",
      "99     \t [0.97546352 0.36504299 0.38991969]. \t  0.13791651637606442 \t 3.8564430066401205\n",
      "100    \t [0.73376058 0.30712204 0.79077983]. \t  2.0952712170015175 \t 3.8564430066401205\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\n",
    "\n",
    "np.random.seed(run_num_16)\n",
    "surrogate_loser_16 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_16 = GPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_16.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.65182403 0.73051962 0.19336736]. \t  0.03562358530655929 \t 3.1179188940604616\n",
      "init   \t [0.51967724 0.67638327 0.80444487]. \t  3.1179188940604616 \t 3.1179188940604616\n",
      "init   \t [0.53351286 0.90803418 0.44348113]. \t  1.1984486017622078 \t 3.1179188940604616\n",
      "init   \t [0.25597221 0.27866502 0.04332186]. \t  0.1645724004847893 \t 3.1179188940604616\n",
      "init   \t [0.46897499 0.9416138  0.60040624]. \t  1.9192522196289552 \t 3.1179188940604616\n",
      "1      \t [0.29618333 0.33186883 1.        ]. \t  1.2124713708531745 \t 3.1179188940604616\n",
      "2      \t [1.         0.81744379 1.        ]. \t  1.0845992897598118 \t 3.1179188940604616\n",
      "3      \t [0.79727087 0.0538872  0.56023603]. \t  0.12825765959665458 \t 3.1179188940604616\n",
      "4      \t [0.04397697 0.85806207 0.53127678]. \t  2.965962353565724 \t 3.1179188940604616\n",
      "5      \t [0.05386169 0.67093251 0.88330276]. \t  \u001b[92m3.319999030720213\u001b[0m \t 3.319999030720213\n",
      "6      \t [0.0463506  0.87431407 0.95426815]. \t  1.1397777956785817 \t 3.319999030720213\n",
      "7      \t [0.         0.4527632  0.59860873]. \t  1.1775579862167573 \t 3.319999030720213\n",
      "8      \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.319999030720213\n",
      "9      \t [0.22300354 0.69323876 0.70266475]. \t  2.554835191736643 \t 3.319999030720213\n",
      "10     \t [0.         0.87574253 0.        ]. \t  0.0005299851578312664 \t 3.319999030720213\n",
      "11     \t [0.85890902 0.5169442  0.83052597]. \t  \u001b[92m3.6373255607522275\u001b[0m \t 3.6373255607522275\n",
      "12     \t [0.99170155 0.28892913 0.96914478]. \t  1.2177982335921864 \t 3.6373255607522275\n",
      "13     \t [0.83176945 0.61943008 0.72386123]. \t  2.2470532647540855 \t 3.6373255607522275\n",
      "14     \t [0.7773786  0.5273069  0.96831959]. \t  2.584725706260578 \t 3.6373255607522275\n",
      "15     \t [ 3.57055135e-01  1.00000000e+00 -1.38777878e-17]. \t  0.00025629310098966324 \t 3.6373255607522275\n",
      "16     \t [0.6186153  0.29607359 0.        ]. \t  0.07056951859788228 \t 3.6373255607522275\n",
      "17     \t [1.         0.26109414 0.67430872]. \t  0.9655659596693915 \t 3.6373255607522275\n",
      "18     \t [0.00520203 0.07833183 0.04023799]. \t  0.1410723662314045 \t 3.6373255607522275\n",
      "19     \t [0.         0.28104542 1.        ]. \t  0.9228789175236554 \t 3.6373255607522275\n",
      "20     \t [0.66708377 0.40556294 0.81743887]. \t  3.0664547158610658 \t 3.6373255607522275\n",
      "21     \t [0.94977414 0.19142298 0.76712315]. \t  1.0761414609657656 \t 3.6373255607522275\n",
      "22     \t [0.17428315 0.26781481 0.19964769]. \t  0.6215373191924979 \t 3.6373255607522275\n",
      "23     \t [0.43316865 0.33781837 0.16281783]. \t  0.43839356627360937 \t 3.6373255607522275\n",
      "24     \t [0.68987068 0.86716863 0.78450816]. \t  1.363504216858666 \t 3.6373255607522275\n",
      "25     \t [0.93893893 0.51145611 0.8570888 ]. \t  \u001b[92m3.648694769757588\u001b[0m \t 3.648694769757588\n",
      "26     \t [0.92303707 0.8869093  0.64767486]. \t  0.597178916481855 \t 3.648694769757588\n",
      "27     \t [0.61524298 0.8274235  0.25854383]. \t  0.10347423077592531 \t 3.648694769757588\n",
      "28     \t [0.49057397 0.00677537 0.68830688]. \t  0.20338784302438667 \t 3.648694769757588\n",
      "29     \t [0.47400839 0.40920829 0.45088014]. \t  0.37781778834869073 \t 3.648694769757588\n",
      "30     \t [0.55549822 0.02602027 0.4454878 ]. \t  0.33003912092135945 \t 3.648694769757588\n",
      "31     \t [0.11848612 0.07103457 0.02780259]. \t  0.14514648175093361 \t 3.648694769757588\n",
      "32     \t [0.12261411 0.9737731  0.40050435]. \t  1.3294543986203815 \t 3.648694769757588\n",
      "33     \t [0.47389126 0.18744075 0.81971356]. \t  1.1555089935923426 \t 3.648694769757588\n",
      "34     \t [0.16645579 0.969506   0.91332713]. \t  0.7307621502627861 \t 3.648694769757588\n",
      "35     \t [0.7200592  0.87547006 0.07956411]. \t  0.001882277042286545 \t 3.648694769757588\n",
      "36     \t [0.67785326 0.70861535 0.25442474]. \t  0.080663966526304 \t 3.648694769757588\n",
      "37     \t [0.3067936  0.89540176 0.06161733]. \t  0.002453658715214689 \t 3.648694769757588\n",
      "38     \t [0.1434548  0.08706172 0.50526542]. \t  0.20799888594699673 \t 3.648694769757588\n",
      "39     \t [0.05493339 0.34112824 0.90162177]. \t  2.284827208633714 \t 3.648694769757588\n",
      "40     \t [0.96431293 0.25428421 0.04062655]. \t  0.06121262862655542 \t 3.648694769757588\n",
      "41     \t [0.40092944 0.57225613 0.92167914]. \t  3.396291615289509 \t 3.648694769757588\n",
      "42     \t [0.73138278 0.52690713 0.45886613]. \t  0.3226847570174443 \t 3.648694769757588\n",
      "43     \t [0.74865906 0.59823871 0.87480389]. \t  \u001b[92m3.672251381247662\u001b[0m \t 3.672251381247662\n",
      "44     \t [0.33218932 0.55692604 0.34266385]. \t  0.37144478515930707 \t 3.672251381247662\n",
      "45     \t [0.83725307 0.79959111 0.70013841]. \t  1.1919591658793827 \t 3.672251381247662\n",
      "46     \t [0.         1.         0.69013001]. \t  1.6070938226630656 \t 3.672251381247662\n",
      "47     \t [0.66828    0.71894607 0.06229252]. \t  0.006423128990915802 \t 3.672251381247662\n",
      "48     \t [0.48565841 0.5055407  0.52301778]. \t  0.7532456792463784 \t 3.672251381247662\n",
      "49     \t [0.19068887 0.54653876 0.22469722]. \t  0.17468431327761985 \t 3.672251381247662\n",
      "50     \t [0.2511088 0.3079115 0.6974723]. \t  1.4612392662713283 \t 3.672251381247662\n",
      "51     \t [0.82399351 0.15394836 0.64722767]. \t  0.4584541049637956 \t 3.672251381247662\n",
      "52     \t [0.35348394 0.54054037 0.79326966]. \t  3.5423436991380184 \t 3.672251381247662\n",
      "53     \t [0.54033536 0.55708343 0.80453791]. \t  3.5940782171099377 \t 3.672251381247662\n",
      "54     \t [0.97518443 0.16518228 0.1573454 ]. \t  0.22569657611171431 \t 3.672251381247662\n",
      "55     \t [0.12084503 0.23291589 0.02030243]. \t  0.11658770253805333 \t 3.672251381247662\n",
      "56     \t [0.13352918 0.94357418 0.30028195]. \t  0.4097630621694289 \t 3.672251381247662\n",
      "57     \t [0.16425101 0.29241238 0.19842371]. \t  0.5646226947998655 \t 3.672251381247662\n",
      "58     \t [0.02994214 0.90579619 0.36261184]. \t  0.9648630155089746 \t 3.672251381247662\n",
      "59     \t [0.39197389 0.50386445 0.47102974]. \t  0.6412848508887127 \t 3.672251381247662\n",
      "60     \t [0.6745912  0.3495636  0.27472958]. \t  0.44668356148041866 \t 3.672251381247662\n",
      "61     \t [0.76576681 0.43251344 0.11100496]. \t  0.111032154670387 \t 3.672251381247662\n",
      "62     \t [0.19529861 0.25854139 0.58264935]. \t  0.4881278080966765 \t 3.672251381247662\n",
      "63     \t [0.01115601 0.37581528 0.5813964 ]. \t  0.779550659614829 \t 3.672251381247662\n",
      "64     \t [0.6247172  0.31633328 0.10535845]. \t  0.251616622079824 \t 3.672251381247662\n",
      "65     \t [0.24468336 0.35194759 0.03991848]. \t  0.11662447727743262 \t 3.672251381247662\n",
      "66     \t [0.71440553 0.77407405 0.36272561]. \t  0.3093169795364798 \t 3.672251381247662\n",
      "67     \t [0.58547314 0.77591115 0.66615084]. \t  1.668138336827412 \t 3.672251381247662\n",
      "68     \t [0.25904834 0.35509101 0.92005275]. \t  2.2764830385778954 \t 3.672251381247662\n",
      "69     \t [0.38886039 0.25135511 0.94066634]. \t  1.2307586237532115 \t 3.672251381247662\n",
      "70     \t [0.7748074  0.52998796 0.05385526]. \t  0.028382679565202167 \t 3.672251381247662\n",
      "71     \t [0.51800861 0.54551968 0.26487654]. \t  0.19936679977656882 \t 3.672251381247662\n",
      "72     \t [0.99412436 0.61009789 0.90025713]. \t  3.3969057743859796 \t 3.672251381247662\n",
      "73     \t [0.49427874 0.06078479 0.09831404]. \t  0.39241495499378787 \t 3.672251381247662\n",
      "74     \t [0.2380556  0.22872624 0.76367594]. \t  1.3619292087267638 \t 3.672251381247662\n",
      "75     \t [0.13132677 0.60403356 0.5170896 ]. \t  1.5744761173600037 \t 3.672251381247662\n",
      "76     \t [0.23072881 0.85476936 0.97600168]. \t  1.1203921741915033 \t 3.672251381247662\n",
      "77     \t [0.81168948 0.57008726 0.64103874]. \t  1.298527861838243 \t 3.672251381247662\n",
      "78     \t [0.16755414 0.78062145 0.89354321]. \t  2.3339433408261065 \t 3.672251381247662\n",
      "79     \t [0.2554513  0.19549951 0.23507527]. \t  0.878276890884476 \t 3.672251381247662\n",
      "80     \t [0.49204468 0.87623059 0.93071709]. \t  1.267310045025293 \t 3.672251381247662\n",
      "81     \t [0.34639487 0.50220626 0.23128726]. \t  0.24563542914317918 \t 3.672251381247662\n",
      "82     \t [0.25052621 0.17010318 0.71783121]. \t  0.8151619535206089 \t 3.672251381247662\n",
      "83     \t [0.96408758 0.63174507 0.05861642]. \t  0.0067284576879286274 \t 3.672251381247662\n",
      "84     \t [0.89258929 0.04884562 0.85635767]. \t  0.36203911780863607 \t 3.672251381247662\n",
      "85     \t [0.60846194 0.72990991 0.70982028]. \t  1.9273562235638337 \t 3.672251381247662\n",
      "86     \t [0.78764978 0.06042556 0.8302592 ]. \t  0.4267724380537717 \t 3.672251381247662\n",
      "87     \t [0.16752079 0.2572927  0.51443825]. \t  0.31906035279084605 \t 3.672251381247662\n",
      "88     \t [0.89954628 0.74870152 0.06193817]. \t  0.0025124416832664736 \t 3.672251381247662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.7557668  0.08660438 0.81475771]. \t  0.542425360440532 \t 3.672251381247662\n",
      "90     \t [0.33748504 0.79789433 0.85234081]. \t  2.2721529590135434 \t 3.672251381247662\n",
      "91     \t [0.08224507 0.52206253 0.92520623]. \t  3.274999790654435 \t 3.672251381247662\n",
      "92     \t [0.17671046 0.56556359 0.67002609]. \t  2.2476536673077527 \t 3.672251381247662\n",
      "93     \t [0.18882945 0.49185784 0.18836753]. \t  0.1969260192761571 \t 3.672251381247662\n",
      "94     \t [0.66936525 0.53565213 0.74427858]. \t  2.8125319218943465 \t 3.672251381247662\n",
      "95     \t [0.92825386 0.75906919 0.23961872]. \t  0.024146434416881628 \t 3.672251381247662\n",
      "96     \t [0.20240843 0.223458   0.35299039]. \t  0.6751471570789034 \t 3.672251381247662\n",
      "97     \t [0.40367685 0.02504577 0.86102811]. \t  0.29168278131574493 \t 3.672251381247662\n",
      "98     \t [0.95993186 0.62888357 0.6627181 ]. \t  1.3205584756640187 \t 3.672251381247662\n",
      "99     \t [0.55281618 0.37893901 0.09183871]. \t  0.18088350815242976 \t 3.672251381247662\n",
      "100    \t [0.12728804 0.15490575 0.95500312]. \t  0.5730027320889911 \t 3.672251381247662\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\n",
    "\n",
    "np.random.seed(run_num_17)\n",
    "surrogate_loser_17 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_17 = GPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_17.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.8734294  0.96854066 0.86919454]. \t  0.722531189286755 \t 1.1210522139432408\n",
      "init   \t [0.53085569 0.23272833 0.0113988 ]. \t  0.1133646794700679 \t 1.1210522139432408\n",
      "init   \t [0.43046882 0.40235136 0.52267467]. \t  0.5232646985053151 \t 1.1210522139432408\n",
      "init   \t [0.4783918  0.55535647 0.54338602]. \t  1.0390748854353227 \t 1.1210522139432408\n",
      "init   \t [0.76089558 0.71237457 0.6196821 ]. \t  1.1210522139432408 \t 1.1210522139432408\n",
      "1      \t [0.28982083 0.95201409 0.18404427]. \t  0.04220734680078784 \t 1.1210522139432408\n",
      "2      \t [0.00482474 0.64977529 0.94137026]. \t  \u001b[92m2.8634726533487465\u001b[0m \t 2.8634726533487465\n",
      "3      \t [0.41366768 0.97572252 0.94506975]. \t  0.5957593168891541 \t 2.8634726533487465\n",
      "4      \t [0.91119252 0.82772995 0.04090539]. \t  0.0007242881617636472 \t 2.8634726533487465\n",
      "5      \t [0.00124238 0.82714185 0.61084561]. \t  2.841218998812702 \t 2.8634726533487465\n",
      "6      \t [0.00806648 0.63086884 0.22273634]. \t  0.10481434203252675 \t 2.8634726533487465\n",
      "7      \t [0.96201929 0.19512248 0.17695609]. \t  0.2563913325924201 \t 2.8634726533487465\n",
      "8      \t [0.96186762 0.13180742 0.88285608]. \t  0.6761048728778654 \t 2.8634726533487465\n",
      "9      \t [0.08949126 0.0011825  0.8944142 ]. \t  0.19984817886284917 \t 2.8634726533487465\n",
      "10     \t [0.99940633 0.63248472 0.88650376]. \t  \u001b[92m3.387313380954288\u001b[0m \t 3.387313380954288\n",
      "11     \t [0.82662258 0.3037438  0.98623006]. \t  1.1708834329555582 \t 3.387313380954288\n",
      "12     \t [0.25588814 0.88061097 0.09237205]. \t  0.005740506407401016 \t 3.387313380954288\n",
      "13     \t [0.25792464 0.57693525 0.93129787]. \t  3.259598672522413 \t 3.387313380954288\n",
      "14     \t [0.01308225 0.54489594 0.90087337]. \t  \u001b[92m3.5798885554583455\u001b[0m \t 3.5798885554583455\n",
      "15     \t [0.92269907 0.77484678 0.19716845]. \t  0.012624191177223057 \t 3.5798885554583455\n",
      "16     \t [0.05792257 0.64083285 0.66966906]. \t  2.4211052253648244 \t 3.5798885554583455\n",
      "17     \t [0.        0.2994974 1.       ]. \t  1.0196129140814847 \t 3.5798885554583455\n",
      "18     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.5798885554583455\n",
      "19     \t [0.74762959 0.78914022 0.9857386 ]. \t  1.4239384966954582 \t 3.5798885554583455\n",
      "20     \t [0.00241175 0.97553421 0.78813045]. \t  1.0319974245624777 \t 3.5798885554583455\n",
      "21     \t [0.82568555 0.50352912 0.        ]. \t  0.014089229598992755 \t 3.5798885554583455\n",
      "22     \t [0.5745334  0.39027396 0.25373722]. \t  0.42541178255113576 \t 3.5798885554583455\n",
      "23     \t [0.69830022 0.0325326  0.49661531]. \t  0.16612375230900428 \t 3.5798885554583455\n",
      "24     \t [0.91663665 0.30295198 0.68005739]. \t  1.2015675769353986 \t 3.5798885554583455\n",
      "25     \t [0.44093437 0.04103895 0.73854636]. \t  0.33804505675103763 \t 3.5798885554583455\n",
      "26     \t [0.66340188 0.65801611 0.51603536]. \t  0.8655767782185495 \t 3.5798885554583455\n",
      "27     \t [0.15782908 0.54829002 0.67059706]. \t  2.2068851609836475 \t 3.5798885554583455\n",
      "28     \t [0.12998841 0.09323376 0.57671251]. \t  0.19631576362123299 \t 3.5798885554583455\n",
      "29     \t [0.13552027 0.89559607 0.21119359]. \t  0.0882385543282937 \t 3.5798885554583455\n",
      "30     \t [0.67508432 0.6080857  0.43994567]. \t  0.44681163801031826 \t 3.5798885554583455\n",
      "31     \t [0.36937662 0.06674274 0.68650969]. \t  0.3347794499589437 \t 3.5798885554583455\n",
      "32     \t [0.83329247 0.96765954 0.6498969 ]. \t  0.5853295207957866 \t 3.5798885554583455\n",
      "33     \t [0.90057306 0.04836091 0.75216672]. \t  0.3647904827951455 \t 3.5798885554583455\n",
      "34     \t [0.2230882 0.5989508 0.0977612]. \t  0.04140865508964812 \t 3.5798885554583455\n",
      "35     \t [0.47192915 0.47188413 0.51625333]. \t  0.6393275343949238 \t 3.5798885554583455\n",
      "36     \t [0.6562836  0.15725733 0.53242462]. \t  0.21607031060941992 \t 3.5798885554583455\n",
      "37     \t [0.59843347 0.59788546 0.13763625]. \t  0.054765100999425394 \t 3.5798885554583455\n",
      "38     \t [0.65581236 0.64433593 0.95466435]. \t  2.7030270738326463 \t 3.5798885554583455\n",
      "39     \t [0.76631698 0.37096161 0.31690851]. \t  0.3172903185290855 \t 3.5798885554583455\n",
      "40     \t [0.84720801 0.95276625 0.45241843]. \t  0.40668663748846934 \t 3.5798885554583455\n",
      "41     \t [0.4542793  0.27686867 0.87438832]. \t  1.8432876495954358 \t 3.5798885554583455\n",
      "42     \t [0.19937222 0.9446533  0.85827678]. \t  1.027241170393896 \t 3.5798885554583455\n",
      "43     \t [0.03788695 0.18369452 0.41445365]. \t  0.3868243184919618 \t 3.5798885554583455\n",
      "44     \t [0.53763434 0.54191688 0.09155918]. \t  0.060681180418439194 \t 3.5798885554583455\n",
      "45     \t [0.06481073 0.03109829 0.12946029]. \t  0.39805420104150985 \t 3.5798885554583455\n",
      "46     \t [0.8902259 0.5955858 0.0539357]. \t  0.011550329052696968 \t 3.5798885554583455\n",
      "47     \t [0.23498364 0.58358687 0.44877488]. \t  0.9705215381374465 \t 3.5798885554583455\n",
      "48     \t [0.14186784 0.0525015  0.09480535]. \t  0.3366051581818347 \t 3.5798885554583455\n",
      "49     \t [0.39394695 0.8794093  0.46626307]. \t  1.8739818935835015 \t 3.5798885554583455\n",
      "50     \t [0.98928275 0.19954874 0.27394902]. \t  0.2945825304755498 \t 3.5798885554583455\n",
      "51     \t [0.84397882 0.33832146 0.89353692]. \t  2.2931951790596177 \t 3.5798885554583455\n",
      "52     \t [0.55994313 0.65723271 0.65286312]. \t  1.7798879528313765 \t 3.5798885554583455\n",
      "53     \t [0.80052371 0.98971188 0.58128943]. \t  0.6561209309416411 \t 3.5798885554583455\n",
      "54     \t [0.3631394  0.22614052 0.92951751]. \t  1.120423704818359 \t 3.5798885554583455\n",
      "55     \t [0.01323057 0.10392014 0.56915771]. \t  0.18989175487929383 \t 3.5798885554583455\n",
      "56     \t [0.27856709 0.95133089 0.42404761]. \t  1.5548092352299898 \t 3.5798885554583455\n",
      "57     \t [0.51484935 0.37870079 0.17637177]. \t  0.3712327765620692 \t 3.5798885554583455\n",
      "58     \t [0.92378982 0.62698725 0.81919386]. \t  3.3375600803889807 \t 3.5798885554583455\n",
      "59     \t [0.9008106  0.89863301 0.50635468]. \t  0.4503584829037369 \t 3.5798885554583455\n",
      "60     \t [0.88708557 0.1325105  0.55939045]. \t  0.18261463847821882 \t 3.5798885554583455\n",
      "61     \t [0.04865049 0.09823542 0.4212928 ]. \t  0.37315811158047807 \t 3.5798885554583455\n",
      "62     \t [0.5390818  0.84958602 0.46101446]. \t  1.333636522568597 \t 3.5798885554583455\n",
      "63     \t [0.16612704 0.48916367 0.99398218]. \t  2.0611999549721984 \t 3.5798885554583455\n",
      "64     \t [0.10352517 0.56434995 0.45093582]. \t  0.9280919263469136 \t 3.5798885554583455\n",
      "65     \t [0.58532372 0.33593381 0.18464687]. \t  0.43973928736740575 \t 3.5798885554583455\n",
      "66     \t [0.42698077 0.79457181 0.04596618]. \t  0.0031936492213742503 \t 3.5798885554583455\n",
      "67     \t [0.85006149 0.81031404 0.60016   ]. \t  0.7714548886829327 \t 3.5798885554583455\n",
      "68     \t [0.73305282 0.20500763 0.01658051]. \t  0.09431982467160967 \t 3.5798885554583455\n",
      "69     \t [0.75377633 0.75683595 0.3869172 ]. \t  0.3356325003406052 \t 3.5798885554583455\n",
      "70     \t [0.87848359 0.5184638  0.78023268]. \t  3.190703770224456 \t 3.5798885554583455\n",
      "71     \t [0.77931996 0.70884583 0.55471014]. \t  0.8143693810560588 \t 3.5798885554583455\n",
      "72     \t [0.67753011 0.19888946 0.09034316]. \t  0.2746770093846463 \t 3.5798885554583455\n",
      "73     \t [0.76804294 0.3062466  0.78708901]. \t  2.065211349310254 \t 3.5798885554583455\n",
      "74     \t [0.42441418 0.124083   0.18432184]. \t  0.8056265166744665 \t 3.5798885554583455\n",
      "75     \t [0.46018519 0.96389053 0.21788154]. \t  0.0641936011807335 \t 3.5798885554583455\n",
      "76     \t [0.01807392 0.81368688 0.98082843]. \t  1.3266707709398322 \t 3.5798885554583455\n",
      "77     \t [0.93180825 0.88940732 0.61191999]. \t  0.5255793471524116 \t 3.5798885554583455\n",
      "78     \t [0.91670262 0.85636471 0.60385514]. \t  0.5857504506744929 \t 3.5798885554583455\n",
      "79     \t [0.08982395 0.22815226 0.07746712]. \t  0.23738274009980176 \t 3.5798885554583455\n",
      "80     \t [0.19465177 0.80094751 0.13652537]. \t  0.01975742475111471 \t 3.5798885554583455\n",
      "81     \t [0.80542036 0.93073344 0.58505498]. \t  0.7409700208735187 \t 3.5798885554583455\n",
      "82     \t [7.23789033e-01 5.75054111e-04 9.75390987e-01]. \t  0.11531495431160955 \t 3.5798885554583455\n",
      "83     \t [0.96954862 0.75827886 0.8010741 ]. \t  2.1707938772494266 \t 3.5798885554583455\n",
      "84     \t [0.08647243 0.96293921 0.32710117]. \t  0.5849356951338773 \t 3.5798885554583455\n",
      "85     \t [0.92456447 0.27602085 0.57067914]. \t  0.3769508605474541 \t 3.5798885554583455\n",
      "86     \t [0.64262746 0.50518779 0.79424085]. \t  3.4202876002328155 \t 3.5798885554583455\n",
      "87     \t [0.55959504 0.56036878 0.86034904]. \t  \u001b[92m3.824705320265667\u001b[0m \t 3.824705320265667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.56335103 0.63177929 0.83477455]. \t  3.5628077247782612 \t 3.824705320265667\n",
      "89     \t [0.12885795 0.47757019 0.43556527]. \t  0.5490663111180263 \t 3.824705320265667\n",
      "90     \t [0.8308906  0.29135679 0.79257402]. \t  1.949566771585567 \t 3.824705320265667\n",
      "91     \t [0.67338267 0.79139192 0.42711586]. \t  0.6770942979099852 \t 3.824705320265667\n",
      "92     \t [0.61770194 0.45288384 0.78835474]. \t  3.1810884718557237 \t 3.824705320265667\n",
      "93     \t [0.20510133 0.30126271 0.36898501]. \t  0.527957285978602 \t 3.824705320265667\n",
      "94     \t [0.32041972 0.09926547 0.11026052]. \t  0.47236637369737755 \t 3.824705320265667\n",
      "95     \t [0.42093232 0.75086645 0.04728742]. \t  0.005030015612563284 \t 3.824705320265667\n",
      "96     \t [0.96662041 0.17014501 0.54127832]. \t  0.17694808258495845 \t 3.824705320265667\n",
      "97     \t [0.70670786 0.2444462  0.48689606]. \t  0.24031854225005608 \t 3.824705320265667\n",
      "98     \t [0.33735701 0.43188609 0.10161533]. \t  0.163115079042684 \t 3.824705320265667\n",
      "99     \t [0.92682191 0.50050062 0.5879895 ]. \t  0.7087530192057957 \t 3.824705320265667\n",
      "100    \t [0.97993502 0.40425479 0.05151405]. \t  0.03537802264890189 \t 3.824705320265667\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\n",
    "\n",
    "np.random.seed(run_num_18)\n",
    "surrogate_loser_18 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_18 = GPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_18.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.55944558 0.76149199 0.24453155]. \t  0.09433080659504385 \t 2.524990008735946\n",
      "init   \t [0.77168947 0.35447208 0.7966974 ]. \t  2.524990008735946 \t 2.524990008735946\n",
      "init   \t [0.34600002 0.44387444 0.30330359]. \t  0.3914194506273641 \t 2.524990008735946\n",
      "init   \t [0.44211392 0.57320936 0.06566402]. \t  0.036928944981920925 \t 2.524990008735946\n",
      "init   \t [0.02408889 0.82062263 0.36513765]. \t  0.9772047942100802 \t 2.524990008735946\n",
      "1      \t [ 1.00000000e+00 -1.73472348e-18  1.00000000e+00]. \t  0.08848201872702738 \t 2.524990008735946\n",
      "2      \t [0.68990349 0.776492   1.        ]. \t  1.3453739209325697 \t 2.524990008735946\n",
      "3      \t [0.03364059 0.33501849 0.98410297]. \t  1.3937191686454828 \t 2.524990008735946\n",
      "4      \t [0.97920914 0.18641683 0.18896551]. \t  0.2593776366232241 \t 2.524990008735946\n",
      "5      \t [0.94980803 0.46087189 0.73882356]. \t  2.5067790538732235 \t 2.524990008735946\n",
      "6      \t [0.05527369 0.97794093 0.91437772]. \t  0.6786579002918043 \t 2.524990008735946\n",
      "7      \t [0.20854414 0.01247687 0.97683317]. \t  0.13019357750163824 \t 2.524990008735946\n",
      "8      \t [0.11992636 0.02082448 0.08122865]. \t  0.26790366205121774 \t 2.524990008735946\n",
      "9      \t [0.97764934 0.11376053 0.5752718 ]. \t  0.17853364585453999 \t 2.524990008735946\n",
      "10     \t [0.98239807 0.67839943 0.19235616]. \t  0.01575624859524108 \t 2.524990008735946\n",
      "11     \t [0.5420832  0.44256445 0.98376186]. \t  2.0477303896288763 \t 2.524990008735946\n",
      "12     \t [0.39416909 0.74193082 0.99616816]. \t  1.6009524125311398 \t 2.524990008735946\n",
      "13     \t [0.27237542 0.29248624 0.3262406 ]. \t  0.6658020777975745 \t 2.524990008735946\n",
      "14     \t [0.04351292 0.03821347 0.57005865]. \t  0.1315384795371757 \t 2.524990008735946\n",
      "15     \t [0.0193673  0.94170574 0.10132114]. \t  0.006200678197065262 \t 2.524990008735946\n",
      "16     \t [0.91803025 0.98243139 0.63030969]. \t  0.41606156342182465 \t 2.524990008735946\n",
      "17     \t [0.63913968 0.03275057 0.2196956 ]. \t  0.6990875164937015 \t 2.524990008735946\n",
      "18     \t [0.89959783 0.18640486 0.32532141]. \t  0.3721595728327397 \t 2.524990008735946\n",
      "19     \t [0.91429314 0.61394615 0.47079134]. \t  0.2553092982531287 \t 2.524990008735946\n",
      "20     \t [0.94077828 0.98246466 0.09758458]. \t  0.0007216140393015158 \t 2.524990008735946\n",
      "21     \t [0.20373518 0.10396556 0.26788834]. \t  0.9206405090054817 \t 2.524990008735946\n",
      "22     \t [0.51558025 0.23594348 0.914284  ]. \t  1.2812801110476435 \t 2.524990008735946\n",
      "23     \t [0.22373008 0.98392242 0.62774342]. \t  2.272209256080007 \t 2.524990008735946\n",
      "24     \t [0.9365443  0.31151369 0.92102654]. \t  1.824166726454697 \t 2.524990008735946\n",
      "25     \t [0.19715124 0.00517215 0.06584252]. \t  0.23904680250278545 \t 2.524990008735946\n",
      "26     \t [0.80542875 0.42263181 0.01153591]. \t  0.031185090331500635 \t 2.524990008735946\n",
      "27     \t [0.00146875 0.83148045 0.56000057]. \t  \u001b[92m2.96169336072112\u001b[0m \t 2.96169336072112\n",
      "28     \t [0.         0.72338906 0.80323217]. \t  2.853187203022144 \t 2.96169336072112\n",
      "29     \t [0.11647277 0.45705134 0.48522976]. \t  0.6400691058291559 \t 2.96169336072112\n",
      "30     \t [0.59033076 0.06363243 0.49189957]. \t  0.21782042126473788 \t 2.96169336072112\n",
      "31     \t [0.68179021 0.95676262 0.86552895]. \t  0.834328952573477 \t 2.96169336072112\n",
      "32     \t [0.56369632 0.30227669 0.67556081]. \t  1.2152410437939047 \t 2.96169336072112\n",
      "33     \t [0.11717895 0.51838382 0.90175872]. \t  \u001b[92m3.5454474450820195\u001b[0m \t 3.5454474450820195\n",
      "34     \t [0.5481816  0.50370555 0.2956876 ]. \t  0.25703180611592874 \t 3.5454474450820195\n",
      "35     \t [0.36087767 0.61068344 0.74838167]. \t  2.973602527068601 \t 3.5454474450820195\n",
      "36     \t [0.         1.         0.55144538]. \t  2.48828899435288 \t 3.5454474450820195\n",
      "37     \t [0.14524698 0.73480978 0.78685137]. \t  2.739250492595711 \t 3.5454474450820195\n",
      "38     \t [0.69855006 0.19290808 0.00415494]. \t  0.08535178549929102 \t 3.5454474450820195\n",
      "39     \t [0.03721759 0.89689153 0.75204622]. \t  1.6713578827737994 \t 3.5454474450820195\n",
      "40     \t [0.11450535 0.38877479 0.23858203]. \t  0.39830622429073204 \t 3.5454474450820195\n",
      "41     \t [0.09816446 0.74341611 0.27680073]. \t  0.2658459977900028 \t 3.5454474450820195\n",
      "42     \t [0.00960704 0.26918501 0.17780091]. \t  0.4245888949045053 \t 3.5454474450820195\n",
      "43     \t [0.49653534 0.94903274 0.57404065]. \t  1.8443640336229983 \t 3.5454474450820195\n",
      "44     \t [0.00791927 0.3957235  0.08991408]. \t  0.12148097787753802 \t 3.5454474450820195\n",
      "45     \t [0.59474975 0.87943433 0.23196203]. \t  0.06743639217319873 \t 3.5454474450820195\n",
      "46     \t [0.65894428 0.76166503 0.23018257]. \t  0.05712577600016635 \t 3.5454474450820195\n",
      "47     \t [0.62753153 0.55511343 0.47924242]. \t  0.5373316096470497 \t 3.5454474450820195\n",
      "48     \t [0.45682491 0.64570631 0.4459672 ]. \t  0.9320541455221174 \t 3.5454474450820195\n",
      "49     \t [0.94807395 0.46458315 0.60378518]. \t  0.8156892453882129 \t 3.5454474450820195\n",
      "50     \t [0.27308196 0.64427215 0.40757537]. \t  0.9044354849926574 \t 3.5454474450820195\n",
      "51     \t [0.28314539 0.00285815 0.07177572]. \t  0.27274097784952667 \t 3.5454474450820195\n",
      "52     \t [0.99554601 0.57829    0.9322286 ]. \t  3.119155902893649 \t 3.5454474450820195\n",
      "53     \t [0.56115138 0.04485817 0.05606849]. \t  0.2227992161080657 \t 3.5454474450820195\n",
      "54     \t [0.98920116 0.0248215  0.36647867]. \t  0.2170044321424345 \t 3.5454474450820195\n",
      "55     \t [0.91249575 0.33840021 0.15488226]. \t  0.17297843146191016 \t 3.5454474450820195\n",
      "56     \t [0.09645332 0.95171827 0.70824437]. \t  1.7335812415162595 \t 3.5454474450820195\n",
      "57     \t [0.422907   0.7620658  0.41076796]. \t  1.0772340142030297 \t 3.5454474450820195\n",
      "58     \t [0.55878957 0.66713579 0.56502813]. \t  1.3779589664532825 \t 3.5454474450820195\n",
      "59     \t [0.37090652 0.18889771 0.88455317]. \t  1.0565871932050406 \t 3.5454474450820195\n",
      "60     \t [0.69739797 0.08915418 0.06916519]. \t  0.22108680367723801 \t 3.5454474450820195\n",
      "61     \t [0.85557198 0.57367251 0.68673724]. \t  1.819144295018671 \t 3.5454474450820195\n",
      "62     \t [0.23203378 0.41794496 0.03128611]. \t  0.0719632455882965 \t 3.5454474450820195\n",
      "63     \t [0.38558226 0.19148405 0.25973858]. \t  0.9454651432780736 \t 3.5454474450820195\n",
      "64     \t [0.26345614 0.45403693 0.83321725]. \t  3.503332877541431 \t 3.5454474450820195\n",
      "65     \t [0.06710588 0.57006299 0.27212872]. \t  0.20650622491275084 \t 3.5454474450820195\n",
      "66     \t [0.56925757 0.90150448 0.25170707]. \t  0.10227822235163753 \t 3.5454474450820195\n",
      "67     \t [0.07837724 0.81772399 0.98030338]. \t  1.3114332086406393 \t 3.5454474450820195\n",
      "68     \t [0.50016584 0.29638218 0.64331495]. \t  0.934582442883535 \t 3.5454474450820195\n",
      "69     \t [0.83381273 0.97179445 0.73062929]. \t  0.5780568126236292 \t 3.5454474450820195\n",
      "70     \t [0.78997844 0.45990799 0.47618796]. \t  0.2606657712959982 \t 3.5454474450820195\n",
      "71     \t [0.50329508 0.22209549 0.64275971]. \t  0.6699701094542871 \t 3.5454474450820195\n",
      "72     \t [0.83868331 0.14753063 0.5079043 ]. \t  0.16519592078183637 \t 3.5454474450820195\n",
      "73     \t [0.3306307  0.45461128 0.25566971]. \t  0.34814944560713534 \t 3.5454474450820195\n",
      "74     \t [0.11257769 0.26654768 0.54955919]. \t  0.386115116736546 \t 3.5454474450820195\n",
      "75     \t [0.65076837 0.18819651 0.9704999 ]. \t  0.6577995178961773 \t 3.5454474450820195\n",
      "76     \t [0.04413445 0.32111221 0.5440949 ]. \t  0.46131651131415014 \t 3.5454474450820195\n",
      "77     \t [0.26588489 0.88394182 0.37817198]. \t  1.0963580191182685 \t 3.5454474450820195\n",
      "78     \t [0.24453041 0.88008028 0.70832336]. \t  1.9916542845969574 \t 3.5454474450820195\n",
      "79     \t [3.53470836e-01 5.04884895e-01 2.06142090e-04]. \t  0.026173774956853777 \t 3.5454474450820195\n",
      "80     \t [0.24131355 0.88185973 0.48672481]. \t  2.497795720181425 \t 3.5454474450820195\n",
      "81     \t [0.24364365 0.35900578 0.90424936]. \t  2.4538321445628757 \t 3.5454474450820195\n",
      "82     \t [0.26084987 0.36019883 0.97777207]. \t  1.6486021935148831 \t 3.5454474450820195\n",
      "83     \t [0.34523056 0.03967511 0.23774049]. \t  0.916197015229224 \t 3.5454474450820195\n",
      "84     \t [0.58560567 0.22961408 0.50522468]. \t  0.26909669789205626 \t 3.5454474450820195\n",
      "85     \t [0.24556394 0.49812268 0.95143745]. \t  2.828978729166592 \t 3.5454474450820195\n",
      "86     \t [0.01895849 0.29888338 0.15063556]. \t  0.33152174386330074 \t 3.5454474450820195\n",
      "87     \t [0.35944912 0.36778263 0.62847366]. \t  1.088953658732462 \t 3.5454474450820195\n",
      "88     \t [0.07735909 0.2245395  0.63856711]. \t  0.6577166796780516 \t 3.5454474450820195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.06917974 0.06494529 0.49902502]. \t  0.1877271021091782 \t 3.5454474450820195\n",
      "90     \t [0.19703774 0.39527822 0.70485073]. \t  2.0156820397306303 \t 3.5454474450820195\n",
      "91     \t [0.28985676 0.99895607 0.97563001]. \t  0.40933801513369666 \t 3.5454474450820195\n",
      "92     \t [0.98347018 0.28556126 0.76232988]. \t  1.7263497444515101 \t 3.5454474450820195\n",
      "93     \t [0.94852366 0.85046182 0.41574563]. \t  0.20814695647165224 \t 3.5454474450820195\n",
      "94     \t [0.09395706 0.81511356 0.72220645]. \t  2.2505063092552415 \t 3.5454474450820195\n",
      "95     \t [0.72400274 0.58656412 0.47478491]. \t  0.4520181952185668 \t 3.5454474450820195\n",
      "96     \t [0.42547655 0.0600468  0.47533445]. \t  0.28650314809862054 \t 3.5454474450820195\n",
      "97     \t [0.2572601  0.94126286 0.53286899]. \t  2.6743523315883073 \t 3.5454474450820195\n",
      "98     \t [0.62637209 0.36806122 0.12779578]. \t  0.24384738077880563 \t 3.5454474450820195\n",
      "99     \t [0.61627522 0.99132052 0.20447026]. \t  0.030783093232522537 \t 3.5454474450820195\n",
      "100    \t [0.03781833 0.95433331 0.95485782]. \t  0.6629899424553869 \t 3.5454474450820195\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\n",
    "\n",
    "np.random.seed(run_num_19)\n",
    "surrogate_loser_19 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_19 = GPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_19.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.57051729 0.56452876 0.48844183]. \t  0.675391399411646 \t 0.675391399411646\n",
      "init   \t [0.33647775 0.37586818 0.53203587]. \t  0.5331349538596052 \t 0.675391399411646\n",
      "init   \t [0.06810629 0.58452906 0.23789776]. \t  0.14747335095307315 \t 0.675391399411646\n",
      "init   \t [0.16075658 0.15211915 0.12706922]. \t  0.48089725804912437 \t 0.675391399411646\n",
      "init   \t [0.32744117 0.69415387 0.35896647]. \t  0.6289408047543804 \t 0.675391399411646\n",
      "1      \t [0.54663172 0.95772822 0.99370791]. \t  0.4914614738653129 \t 0.675391399411646\n",
      "2      \t [0.91932094 0.0125958  0.17618825]. \t  0.2816999755052942 \t 0.675391399411646\n",
      "3      \t [0.837611   0.97862966 0.10314233]. \t  0.0013418779384681767 \t 0.675391399411646\n",
      "4      \t [0.83546183 0.15352723 0.88175242]. \t  \u001b[92m0.8118727969835325\u001b[0m \t 0.8118727969835325\n",
      "5      \t [0.04282238 0.98714938 0.99662789]. \t  0.3795921540393995 \t 0.8118727969835325\n",
      "6      \t [0.00657248 0.04994934 0.95603609]. \t  0.22346365116669764 \t 0.8118727969835325\n",
      "7      \t [0.0140605  0.99189634 0.09256457]. \t  0.0043141598608878945 \t 0.8118727969835325\n",
      "8      \t [0.97302872 0.76767204 0.83300021]. \t  \u001b[92m2.2831423836103286\u001b[0m \t 2.2831423836103286\n",
      "9      \t [0.75244603 0.97391009 0.75870158]. \t  0.6651080101915691 \t 2.2831423836103286\n",
      "10     \t [0.00407095 0.01602185 0.22787245]. \t  0.5782528840560536 \t 2.2831423836103286\n",
      "11     \t [0.99078418 0.22121079 0.03507944]. \t  0.05576618977127451 \t 2.2831423836103286\n",
      "12     \t [0.98562056 0.3532819  0.97238955]. \t  1.6129586582258908 \t 2.2831423836103286\n",
      "13     \t [0.72818255 0.04068818 0.51966219]. \t  0.1375604883842715 \t 2.2831423836103286\n",
      "14     \t [0.21561842 0.81296904 0.1185526 ]. \t  0.013073952399992744 \t 2.2831423836103286\n",
      "15     \t [0.11631352 0.02556041 0.60513699]. \t  0.14369463056464749 \t 2.2831423836103286\n",
      "16     \t [0.7664446 0.6360279 0.767858 ]. \t  \u001b[92m2.8246323331682794\u001b[0m \t 2.8246323331682794\n",
      "17     \t [0.48254864 0.61420849 0.90966464]. \t  \u001b[92m3.4400563356463536\u001b[0m \t 3.4400563356463536\n",
      "18     \t [0.02592365 0.78832521 0.71176665]. \t  2.3541425416158104 \t 3.4400563356463536\n",
      "19     \t [0.73772067 0.50014702 0.94045278]. \t  2.9743049960010755 \t 3.4400563356463536\n",
      "20     \t [0.26761052 0.8975585  0.76765919]. \t  1.559581278102397 \t 3.4400563356463536\n",
      "21     \t [0.86095053 0.97464702 0.91172435]. \t  0.6481411249424998 \t 3.4400563356463536\n",
      "22     \t [0.97581581 0.14663182 0.44064067]. \t  0.15356793092299648 \t 3.4400563356463536\n",
      "23     \t [0.02843244 0.5384091  0.99128982]. \t  2.2016692511646556 \t 3.4400563356463536\n",
      "24     \t [0.70238185 0.1683291  0.57459002]. \t  0.2731710807913837 \t 3.4400563356463536\n",
      "25     \t [0.86044025 0.53007868 0.38860755]. \t  0.1435598115427928 \t 3.4400563356463536\n",
      "26     \t [0.01537935 0.26817864 0.34648792]. \t  0.4768440140289208 \t 3.4400563356463536\n",
      "27     \t [0.16104933 0.76869031 0.71439035]. \t  2.4356809957041783 \t 3.4400563356463536\n",
      "28     \t [0.5990851  0.44034166 0.16274615]. \t  0.2182581964935925 \t 3.4400563356463536\n",
      "29     \t [0.46911668 0.41725862 0.78373556]. \t  2.9645303334571738 \t 3.4400563356463536\n",
      "30     \t [0.55919641 0.15706264 0.21489171]. \t  0.8132787973204357 \t 3.4400563356463536\n",
      "31     \t [0.4171144  0.28870291 0.69791646]. \t  1.3512214601488175 \t 3.4400563356463536\n",
      "32     \t [0.36057657 0.23489665 0.43238393]. \t  0.43665360851459445 \t 3.4400563356463536\n",
      "33     \t [0.23169721 0.43114394 0.52938959]. \t  0.7036496618670851 \t 3.4400563356463536\n",
      "34     \t [0.16399227 0.29837935 0.1023087 ]. \t  0.2806156700136531 \t 3.4400563356463536\n",
      "35     \t [0.95184173 0.6875272  0.16795367]. \t  0.01319718155864041 \t 3.4400563356463536\n",
      "36     \t [0.65257082 0.75698146 0.23990171]. \t  0.06810984282763881 \t 3.4400563356463536\n",
      "37     \t [0.39538804 0.9042213  0.87995339]. \t  1.2484154431092795 \t 3.4400563356463536\n",
      "38     \t [0.85638566 0.3105256  0.90613037]. \t  1.9447564882408375 \t 3.4400563356463536\n",
      "39     \t [0.30206208 0.26687395 0.3378858 ]. \t  0.6979162454139326 \t 3.4400563356463536\n",
      "40     \t [0.35788206 0.31794543 0.8975966 ]. \t  2.11756620489918 \t 3.4400563356463536\n",
      "41     \t [0.31241449 0.381452   0.83839052]. \t  2.943967331975257 \t 3.4400563356463536\n",
      "42     \t [0.16182161 0.3510125  0.21914025]. \t  0.481039053682242 \t 3.4400563356463536\n",
      "43     \t [0.36338206 0.66509103 0.51988038]. \t  1.6999027543704892 \t 3.4400563356463536\n",
      "44     \t [0.46310391 0.20153928 0.66258093]. \t  0.7030942743589284 \t 3.4400563356463536\n",
      "45     \t [0.13314688 0.36283131 0.34132913]. \t  0.4524887204207148 \t 3.4400563356463536\n",
      "46     \t [0.66223288 0.96022554 0.91042743]. \t  0.752947491537213 \t 3.4400563356463536\n",
      "47     \t [0.27687362 0.79075161 0.9371752 ]. \t  1.933725641921452 \t 3.4400563356463536\n",
      "48     \t [0.51236514 0.15357309 0.12380179]. \t  0.5001764571324175 \t 3.4400563356463536\n",
      "49     \t [0.70041028 0.53058182 0.02232165]. \t  0.02154604077917894 \t 3.4400563356463536\n",
      "50     \t [0.3855267  0.32194759 0.06225079]. \t  0.186039907957913 \t 3.4400563356463536\n",
      "51     \t [0.614879   0.01897242 0.51500566]. \t  0.15367501249511611 \t 3.4400563356463536\n",
      "52     \t [0.41167536 0.16385165 0.98067855]. \t  0.5078843597974345 \t 3.4400563356463536\n",
      "53     \t [0.57461273 0.4549721  0.31272816]. \t  0.31286063907099904 \t 3.4400563356463536\n",
      "54     \t [0.37499163 0.63554262 0.10271291]. \t  0.03315445338769701 \t 3.4400563356463536\n",
      "55     \t [0.10772508 0.86123022 0.85728881]. \t  1.684811366751597 \t 3.4400563356463536\n",
      "56     \t [0.1392827  0.79366321 0.04088121]. \t  0.0029033334872716663 \t 3.4400563356463536\n",
      "57     \t [0.29673248 0.72120036 0.82671988]. \t  2.97236620024631 \t 3.4400563356463536\n",
      "58     \t [0.79655151 0.05901964 0.00650744]. \t  0.07260977253929579 \t 3.4400563356463536\n",
      "59     \t [0.95046973 0.09796401 0.89088472]. \t  0.49761056322740577 \t 3.4400563356463536\n",
      "60     \t [0.55213051 0.37210301 0.83852039]. \t  2.8470984778881414 \t 3.4400563356463536\n",
      "61     \t [0.19277311 0.22411141 0.23644552]. \t  0.7916688656986456 \t 3.4400563356463536\n",
      "62     \t [0.02302266 0.55759926 0.85307513]. \t  \u001b[92m3.8182835594078135\u001b[0m \t 3.8182835594078135\n",
      "63     \t [0.23443089 0.12249257 0.53249562]. \t  0.2182628279975289 \t 3.8182835594078135\n",
      "64     \t [0.25033324 0.17223292 0.66275429]. \t  0.5976263057593222 \t 3.8182835594078135\n",
      "65     \t [0.88551001 0.19374512 0.28819094]. \t  0.4188322926123537 \t 3.8182835594078135\n",
      "66     \t [0.29894393 0.69254729 0.07318243]. \t  0.013441052540799628 \t 3.8182835594078135\n",
      "67     \t [0.69204271 0.2617181  0.17389616]. \t  0.4567228690017591 \t 3.8182835594078135\n",
      "68     \t [0.09887742 0.80146926 0.92508264]. \t  1.9310060526836044 \t 3.8182835594078135\n",
      "69     \t [0.26022755 0.94512016 0.60619099]. \t  2.55818934338015 \t 3.8182835594078135\n",
      "70     \t [0.0419944  0.272961   0.60340039]. \t  0.6123246864289482 \t 3.8182835594078135\n",
      "71     \t [0.20739069 0.06601235 0.08339898]. \t  0.3266638276141411 \t 3.8182835594078135\n",
      "72     \t [0.79919213 0.55481786 0.02960821]. \t  0.015561981839875073 \t 3.8182835594078135\n",
      "73     \t [0.53323659 0.07847673 0.42246853]. \t  0.4514836825569653 \t 3.8182835594078135\n",
      "74     \t [0.85153165 0.80186875 0.91415992]. \t  1.9319188912520728 \t 3.8182835594078135\n",
      "75     \t [0.18191887 0.98086372 0.22579752]. \t  0.10293195576328541 \t 3.8182835594078135\n",
      "76     \t [0.59223853 0.49019339 0.514645  ]. \t  0.5636584653373499 \t 3.8182835594078135\n",
      "77     \t [0.05313924 0.61417086 0.68369727]. \t  2.45400306920748 \t 3.8182835594078135\n",
      "78     \t [0.29137267 0.69414151 0.36532438]. \t  0.7023140456625757 \t 3.8182835594078135\n",
      "79     \t [0.56143615 0.11198211 0.34216714]. \t  0.7586939620952345 \t 3.8182835594078135\n",
      "80     \t [0.43159082 0.58183171 0.77795281]. \t  3.33220213936571 \t 3.8182835594078135\n",
      "81     \t [0.82268004 0.38351431 0.59432374]. \t  0.6940951772220839 \t 3.8182835594078135\n",
      "82     \t [0.71742585 0.28987715 0.84530822]. \t  2.0254191443497147 \t 3.8182835594078135\n",
      "83     \t [0.87402104 0.06437169 0.84856424]. \t  0.4253112016867315 \t 3.8182835594078135\n",
      "84     \t [0.67455954 0.21569822 0.55133985]. \t  0.28522871971445246 \t 3.8182835594078135\n",
      "85     \t [0.42274541 0.65986703 0.21753009]. \t  0.09514203604510278 \t 3.8182835594078135\n",
      "86     \t [0.57752401 0.80350725 0.18556134]. \t  0.0312731366325119 \t 3.8182835594078135\n",
      "87     \t [0.6027241  0.49761061 0.81673337]. \t  3.597965090574125 \t 3.8182835594078135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.87891146 0.43847854 0.82573944]. \t  3.2745639011521805 \t 3.8182835594078135\n",
      "89     \t [0.07926141 0.02468115 0.92539295]. \t  0.21503412497346253 \t 3.8182835594078135\n",
      "90     \t [0.34679652 0.50690133 0.93526117]. \t  3.116157716524637 \t 3.8182835594078135\n",
      "91     \t [0.16067943 0.02817582 0.75723975]. \t  0.3137567177172803 \t 3.8182835594078135\n",
      "92     \t [0.26933286 0.18897611 0.34563904]. \t  0.7759469617211912 \t 3.8182835594078135\n",
      "93     \t [0.99474059 0.75673791 0.07898431]. \t  0.002059736298411999 \t 3.8182835594078135\n",
      "94     \t [0.05616427 0.79463796 0.3497037 ]. \t  0.8003549758232651 \t 3.8182835594078135\n",
      "95     \t [0.69138323 0.75223357 0.59086131]. \t  1.2089808454992501 \t 3.8182835594078135\n",
      "96     \t [0.47827067 0.1364753  0.25670492]. \t  0.9585843576107357 \t 3.8182835594078135\n",
      "97     \t [0.64636594 0.08580935 0.32084508]. \t  0.7223969230433356 \t 3.8182835594078135\n",
      "98     \t [0.21669174 0.21331674 0.18901403]. \t  0.708106387725855 \t 3.8182835594078135\n",
      "99     \t [0.73369361 0.68157457 0.39638145]. \t  0.3304032399723793 \t 3.8182835594078135\n",
      "100    \t [0.3689577  0.85363475 0.49223597]. \t  2.201032730366967 \t 3.8182835594078135\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\n",
    "\n",
    "np.random.seed(run_num_20)\n",
    "surrogate_loser_20 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_20 = GPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_20.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.61217018 0.16906975 0.43605902]. \t  0.37345684462559386 \t 2.3951473341797507\n",
      "init   \t [0.76926247 0.2953253  0.14916296]. \t  0.296162062221701 \t 2.3951473341797507\n",
      "init   \t [0.02247832 0.42022449 0.23868214]. \t  0.2904113287153621 \t 2.3951473341797507\n",
      "init   \t [0.33765619 0.99071246 0.23772645]. \t  0.11013785080555143 \t 2.3951473341797507\n",
      "init   \t [0.08119266 0.66960024 0.62124292]. \t  2.3951473341797507 \t 2.3951473341797507\n",
      "1      \t [0. 1. 1.]. \t  0.3302198606064223 \t 2.3951473341797507\n",
      "2      \t [1.         0.77045309 1.        ]. \t  1.3380076852138827 \t 2.3951473341797507\n",
      "3      \t [0.         0.12567554 1.        ]. \t  0.30752247433642443 \t 2.3951473341797507\n",
      "4      \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.3951473341797507\n",
      "5      \t [1.         1.         0.29332192]. \t  0.030576290020190595 \t 2.3951473341797507\n",
      "6      \t [0.4607834  0.53914461 1.        ]. \t  2.07028907016415 \t 2.3951473341797507\n",
      "7      \t [1. 0. 0.]. \t  0.03095471703300515 \t 2.3951473341797507\n",
      "8      \t [1.         0.43584747 0.58475595]. \t  0.6230176300725132 \t 2.3951473341797507\n",
      "9      \t [0.58576122 1.         0.8230032 ]. \t  0.6501346075977998 \t 2.3951473341797507\n",
      "10     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 2.3951473341797507\n",
      "11     \t [1.        0.7739591 0.       ]. \t  0.0004986312124524025 \t 2.3951473341797507\n",
      "12     \t [0.00000000e+00 0.00000000e+00 5.55111512e-17]. \t  0.06797411659013236 \t 2.3951473341797507\n",
      "13     \t [0.         1.         0.50880223]. \t  2.324674166956091 \t 2.3951473341797507\n",
      "14     \t [0.44611014 0.         1.        ]. \t  0.09171709289791793 \t 2.3951473341797507\n",
      "15     \t [0.         0.         0.52432983]. \t  0.11277053241718939 \t 2.3951473341797507\n",
      "16     \t [0.45690964 0.         0.        ]. \t  0.09989808288318044 \t 2.3951473341797507\n",
      "17     \t [0.8389088  0.36438526 1.        ]. \t  1.3663897627673378 \t 2.3951473341797507\n",
      "18     \t [0.         0.58086042 1.        ]. \t  2.0545359320993457 \t 2.3951473341797507\n",
      "19     \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.3951473341797507\n",
      "20     \t [0.74486057 1.         0.        ]. \t  0.00010597604838570549 \t 2.3951473341797507\n",
      "21     \t [1.         0.         0.41733862]. \t  0.1383078395383426 \t 2.3951473341797507\n",
      "22     \t [0.73165339 0.72932208 0.57997792]. \t  1.0421672604815275 \t 2.3951473341797507\n",
      "23     \t [0.32116736 0.50060494 0.        ]. \t  0.026797490237047826 \t 2.3951473341797507\n",
      "24     \t [0.         0.37111574 0.75766081]. \t  2.391289935790577 \t 2.3951473341797507\n",
      "25     \t [0.79795589 0.         0.7618127 ]. \t  0.24196502944392184 \t 2.3951473341797507\n",
      "26     \t [1.         1.         0.71811423]. \t  0.34915848633761315 \t 2.3951473341797507\n",
      "27     \t [0.         0.64047871 0.        ]. \t  0.005196507328001904 \t 2.3951473341797507\n",
      "28     \t [0.24179709 0.34112479 0.86783114]. \t  \u001b[92m2.4997765156586547\u001b[0m \t 2.4997765156586547\n",
      "29     \t [ 1.00000000e+00  3.03531187e-01 -6.93889390e-18]. \t  0.025066153641037446 \t 2.4997765156586547\n",
      "30     \t [0.16040815 1.         0.74111148]. \t  1.2019422065490188 \t 2.4997765156586547\n",
      "31     \t [0.         0.79779675 0.82386397]. \t  2.268834005364349 \t 2.4997765156586547\n",
      "32     \t [0.22850776 0.         0.73396304]. \t  0.22913345088105522 \t 2.4997765156586547\n",
      "33     \t [0.         0.25294214 0.        ]. \t  0.0648000086321858 \t 2.4997765156586547\n",
      "34     \t [5.84338907e-01 7.51932204e-01 3.80042537e-04]. \t  0.0019521424940404529 \t 2.4997765156586547\n",
      "35     \t [0.42983827 0.56325709 0.7644342 ]. \t  \u001b[92m3.19872956666641\u001b[0m \t 3.19872956666641\n",
      "36     \t [1.         0.0139748  0.69646472]. \t  0.2164174606307444 \t 3.19872956666641\n",
      "37     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 3.19872956666641\n",
      "38     \t [0.76587174 0.         0.        ]. \t  0.06372822246150338 \t 3.19872956666641\n",
      "39     \t [1.         0.46501056 0.94615724]. \t  2.6644081890015943 \t 3.19872956666641\n",
      "40     \t [0.29783953 0.73373259 0.9990358 ]. \t  1.6057541248535587 \t 3.19872956666641\n",
      "41     \t [3.82768854e-01 9.11875069e-08 2.60739570e-01]. \t  0.8705709181630654 \t 3.19872956666641\n",
      "42     \t [0.77291522 0.59171499 0.87262888]. \t  \u001b[92m3.689297778473602\u001b[0m \t 3.689297778473602\n",
      "43     \t [0.71255141 0.60795617 0.87284398]. \t  3.6570501565871 \t 3.689297778473602\n",
      "44     \t [0.67224087 0.5319929  0.83468019]. \t  \u001b[92m3.7455487591597416\u001b[0m \t 3.7455487591597416\n",
      "45     \t [0.7119629  0.47289995 0.8004392 ]. \t  3.352504716589066 \t 3.7455487591597416\n",
      "46     \t [0.62479294 0.53749551 0.8543062 ]. \t  \u001b[92m3.8055914562859767\u001b[0m \t 3.8055914562859767\n",
      "47     \t [0.64477861 0.54638969 0.83762987]. \t  3.7744911520591815 \t 3.8055914562859767\n",
      "48     \t [0.77659045 0.53395575 0.89796235]. \t  3.570852436087814 \t 3.8055914562859767\n",
      "49     \t [0.77249862 0.52623613 0.83596187]. \t  3.7090603920590954 \t 3.8055914562859767\n",
      "50     \t [0.23685289 0.99407983 0.00143825]. \t  0.0003056201606716556 \t 3.8055914562859767\n",
      "51     \t [0.66304386 0.58085032 0.87767171]. \t  3.729953862406619 \t 3.8055914562859767\n",
      "52     \t [0.68415344 0.53081559 0.88344663]. \t  3.696680325002105 \t 3.8055914562859767\n",
      "53     \t [0.78544577 0.52611711 0.82565098]. \t  3.6552230790692826 \t 3.8055914562859767\n",
      "54     \t [0.58134017 0.51852564 0.88473367]. \t  3.6847066403012727 \t 3.8055914562859767\n",
      "55     \t [0.83397901 0.60624616 0.9054517 ]. \t  3.4269641565140256 \t 3.8055914562859767\n",
      "56     \t [0.66490387 0.55188512 0.81370755]. \t  3.628979439448183 \t 3.8055914562859767\n",
      "57     \t [0.68212318 0.51887205 0.84299739]. \t  3.7470741595404444 \t 3.8055914562859767\n",
      "58     \t [0.         0.         0.99999998]. \t  0.09028948514067234 \t 3.8055914562859767\n",
      "59     \t [0.69347949 0.54470352 0.84827183]. \t  3.7857533348717944 \t 3.8055914562859767\n",
      "60     \t [0.88818115 0.57691842 0.87719895]. \t  3.662210086264213 \t 3.8055914562859767\n",
      "61     \t [0.73597311 0.56345636 0.89913418]. \t  3.5905790126284205 \t 3.8055914562859767\n",
      "62     \t [0.18486576 0.5247528  0.80260636]. \t  3.6140431162896736 \t 3.8055914562859767\n",
      "63     \t [0.         1.         0.23120476]. \t  0.10700809756046169 \t 3.8055914562859767\n",
      "64     \t [0.81775218 0.61346593 0.85661647]. \t  3.614067868366048 \t 3.8055914562859767\n",
      "65     \t [0.84923213 0.52824334 0.88670474]. \t  3.621093800826143 \t 3.8055914562859767\n",
      "66     \t [0.79294717 0.63862284 0.89919701]. \t  3.356433056428785 \t 3.8055914562859767\n",
      "67     \t [0.48825941 0.54786943 0.84111149]. \t  \u001b[92m3.825718612689603\u001b[0m \t 3.825718612689603\n",
      "68     \t [0.50699119 0.56642632 0.82044433]. \t  3.718089006813016 \t 3.825718612689603\n",
      "69     \t [0.62900246 0.68458516 0.84617138]. \t  3.2234775329547647 \t 3.825718612689603\n",
      "70     \t [0.43290026 0.53594981 0.74565291]. \t  2.958967570641856 \t 3.825718612689603\n",
      "71     \t [0.87121634 0.51465973 0.8645512 ]. \t  3.675271968418247 \t 3.825718612689603\n",
      "72     \t [0.57211507 0.59941331 0.88418629]. \t  3.6759366690353437 \t 3.825718612689603\n",
      "73     \t [0.88564397 0.59538023 0.8350837 ]. \t  3.5925732968566892 \t 3.825718612689603\n",
      "74     \t [0.15452757 0.12762919 0.33011757]. \t  0.776646368321716 \t 3.825718612689603\n",
      "75     \t [0.61272774 0.59809361 0.85115438]. \t  3.7383989306004715 \t 3.825718612689603\n",
      "76     \t [0.7269977  0.57082385 0.88398796]. \t  3.695271412511484 \t 3.825718612689603\n",
      "77     \t [0.47435298 0.46178707 0.7846849 ]. \t  3.222785205805443 \t 3.825718612689603\n",
      "78     \t [0.67565162 0.62323581 0.80546915]. \t  3.3595103007960563 \t 3.825718612689603\n",
      "79     \t [0.66715774 0.52942095 0.86427002]. \t  3.7741685536453238 \t 3.825718612689603\n",
      "80     \t [0.83193751 0.62176461 0.89276836]. \t  3.468557609229556 \t 3.825718612689603\n",
      "81     \t [0.61179418 0.59977686 0.89677922]. \t  3.5820722994810605 \t 3.825718612689603\n",
      "82     \t [0.75156965 0.54831402 0.8387538 ]. \t  3.7424978927452943 \t 3.825718612689603\n",
      "83     \t [0.49917783 0.45547798 0.90041505]. \t  3.2790073437094103 \t 3.825718612689603\n",
      "84     \t [0.61634877 0.57963376 0.92418096]. \t  3.333289700407676 \t 3.825718612689603\n",
      "85     \t [0.84681221 0.56751278 0.80904111]. \t  3.4961617709728046 \t 3.825718612689603\n",
      "86     \t [0.26834051 0.4957616  0.80089746]. \t  3.529613139010972 \t 3.825718612689603\n",
      "87     \t [0.83826163 0.55417167 0.85918682]. \t  3.7413436192615395 \t 3.825718612689603\n",
      "88     \t [0.77365696 0.42221071 0.8487435 ]. \t  3.234025953758735 \t 3.825718612689603\n",
      "89     \t [0.59459032 0.56067293 0.81150713]. \t  3.6309688413585786 \t 3.825718612689603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.61686258 0.61564013 0.82760354]. \t  3.585722208299774 \t 3.825718612689603\n",
      "91     \t [0.70822321 0.5796581  0.85518904]. \t  3.759830622179515 \t 3.825718612689603\n",
      "92     \t [0.13551098 0.61477521 0.86099235]. \t  3.7217456012500065 \t 3.825718612689603\n",
      "93     \t [0.39429267 0.57616473 0.87345213]. \t  3.803514396302414 \t 3.825718612689603\n",
      "94     \t [0.4750795  0.51701678 0.7636971 ]. \t  3.1531874771717407 \t 3.825718612689603\n",
      "95     \t [0.78403425 0.56597061 0.87503343]. \t  3.7226708827540422 \t 3.825718612689603\n",
      "96     \t [0.24940303 0.48230424 0.79459449]. \t  3.4271358926072937 \t 3.825718612689603\n",
      "97     \t [0.5141268  0.57571038 0.84288954]. \t  3.8045113701477753 \t 3.825718612689603\n",
      "98     \t [0.35321781 0.54229618 0.81064778]. \t  3.691339919948006 \t 3.825718612689603\n",
      "99     \t [0.05766898 0.62078331 0.81547535]. \t  3.567731963392981 \t 3.825718612689603\n",
      "100    \t [0.33421404 0.4973874  0.83747485]. \t  3.7322947199585452 \t 3.825718612689603\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \n",
    "\n",
    "np.random.seed(run_num_1)\n",
    "surrogate_winner_1 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_1 = GPGO(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_1.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.85198549 0.0739036  0.89493176]. \t  0.3999083566189884 \t 2.6229838112516717\n",
      "init   \t [0.43649355 0.12767773 0.57585787]. \t  0.24461577848211966 \t 2.6229838112516717\n",
      "init   \t [0.84047092 0.43512055 0.69591056]. \t  1.8897715258798413 \t 2.6229838112516717\n",
      "init   \t [0.6846381  0.70064837 0.77969426]. \t  2.6229838112516717 \t 2.6229838112516717\n",
      "init   \t [0.64274937 0.96102617 0.10846489]. \t  0.003309399496320042 \t 2.6229838112516717\n",
      "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.6229838112516717\n",
      "2      \t [0. 1. 1.]. \t  0.33021986060642144 \t 2.6229838112516717\n",
      "3      \t [0.         0.73831519 0.        ]. \t  0.0018780752803637007 \t 2.6229838112516717\n",
      "4      \t [0.         0.26516758 1.        ]. \t  0.8425764879645684 \t 2.6229838112516717\n",
      "5      \t [1.00000000e+00 0.00000000e+00 5.55111512e-17]. \t  0.030954717033005178 \t 2.6229838112516717\n",
      "6      \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.6229838112516717\n",
      "7      \t [0.54398875 0.56794869 1.        ]. \t  2.0823196005562874 \t 2.6229838112516717\n",
      "8      \t [0.         0.62337666 0.56571941]. \t  1.9041668276756087 \t 2.6229838112516717\n",
      "9      \t [1.         1.         0.39650339]. \t  0.11234441813798851 \t 2.6229838112516717\n",
      "10     \t [0.         1.         0.39368213]. \t  1.1333015713041503 \t 2.6229838112516717\n",
      "11     \t [1.         0.54518722 0.        ]. \t  0.005683709922851933 \t 2.6229838112516717\n",
      "12     \t [0.48106311 0.26801886 0.        ]. \t  0.08988430860685956 \t 2.6229838112516717\n",
      "13     \t [0.43101948 1.         0.74347178]. \t  0.958853645374312 \t 2.6229838112516717\n",
      "14     \t [1.        0.5445101 1.       ]. \t  1.998675651806869 \t 2.6229838112516717\n",
      "15     \t [0.         0.         0.65383627]. \t  0.15263447909576583 \t 2.6229838112516717\n",
      "16     \t [0.32612111 0.         1.        ]. \t  0.09168923178761967 \t 2.6229838112516717\n",
      "17     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.6229838112516717\n",
      "18     \t [1.         0.         0.48332548]. \t  0.08058784391821697 \t 2.6229838112516717\n",
      "19     \t [0.56812858 0.         0.        ]. \t  0.09076841214949347 \t 2.6229838112516717\n",
      "20     \t [0.18988412 1.         0.        ]. \t  0.0002891994888766454 \t 2.6229838112516717\n",
      "21     \t [0.33022258 0.69448559 0.4123991 ]. \t  1.0577928366700202 \t 2.6229838112516717\n",
      "22     \t [0.         0.75500484 0.83263002]. \t  \u001b[92m2.66534444557245\u001b[0m \t 2.66534444557245\n",
      "23     \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.66534444557245\n",
      "24     \t [1.         0.7150233  0.76730025]. \t  2.1754989153059894 \t 2.66534444557245\n",
      "25     \t [0.25199588 0.46510896 0.81257939]. \t  \u001b[92m3.479036170222344\u001b[0m \t 3.479036170222344\n",
      "26     \t [0.         1.         0.72590716]. \t  1.290117715782064 \t 3.479036170222344\n",
      "27     \t [0.2493988  0.63240971 0.83487779]. \t  \u001b[92m3.6263708824734415\u001b[0m \t 3.6263708824734415\n",
      "28     \t [0.         0.29503822 0.        ]. \t  0.0567817782749933 \t 3.6263708824734415\n",
      "29     \t [0.20548373 0.         0.        ]. \t  0.09437473777196216 \t 3.6263708824734415\n",
      "30     \t [1.         0.71685253 0.28236247]. \t  0.031991737884302145 \t 3.6263708824734415\n",
      "31     \t [4.18138765e-09 1.75398634e-09 3.25946604e-01]. \t  0.5235582733732168 \t 3.6263708824734415\n",
      "32     \t [0.1615436  0.55884325 1.        ]. \t  2.076796571475586 \t 3.6263708824734415\n",
      "33     \t [1.         0.26903896 0.85250942]. \t  1.7676422805357568 \t 3.6263708824734415\n",
      "34     \t [1.         0.18994825 0.1783539 ]. \t  0.22645243100254386 \t 3.6263708824734415\n",
      "35     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.6263708824734415\n",
      "36     \t [0. 0. 1.]. \t  0.0902894676548261 \t 3.6263708824734415\n",
      "37     \t [0.46636416 0.4315944  0.85723988]. \t  3.352067335225911 \t 3.6263708824734415\n",
      "38     \t [0.3324369  0.5196092  0.83709134]. \t  \u001b[92m3.798087466300836\u001b[0m \t 3.798087466300836\n",
      "39     \t [0.2702632  0.71446906 0.80669775]. \t  2.9583291128249547 \t 3.798087466300836\n",
      "40     \t [0.35630882 0.52745842 0.81471074]. \t  3.704386200286483 \t 3.798087466300836\n",
      "41     \t [0.34021854 0.55717348 0.80723566]. \t  3.6681465948543153 \t 3.798087466300836\n",
      "42     \t [1.        1.        0.7591884]. \t  0.411659038327713 \t 3.798087466300836\n",
      "43     \t [0.29394906 0.52712883 0.83367513]. \t  \u001b[92m3.803605723478701\u001b[0m \t 3.803605723478701\n",
      "44     \t [0.         0.5246048  0.84712254]. \t  3.7765706965325316 \t 3.803605723478701\n",
      "45     \t [0.60917302 1.         0.99999999]. \t  0.33002375899178565 \t 3.803605723478701\n",
      "46     \t [0.4376685  0.54496809 0.82120717]. \t  3.748834467624529 \t 3.803605723478701\n",
      "47     \t [0.13688191 0.45982219 0.80396273]. \t  3.3835014862615553 \t 3.803605723478701\n",
      "48     \t [0.01802209 0.48482708 0.84018974]. \t  3.641137032351714 \t 3.803605723478701\n",
      "49     \t [0.45312115 0.57666318 0.78139099]. \t  3.3700840895992945 \t 3.803605723478701\n",
      "50     \t [0.17641839 0.58032979 0.84847896]. \t  \u001b[92m3.8299426701200505\u001b[0m \t 3.8299426701200505\n",
      "51     \t [6.62897293e-01 5.02275455e-09 2.06179147e-01]. \t  0.60155928017799 \t 3.8299426701200505\n",
      "52     \t [0.27086107 0.58003252 0.81542543]. \t  3.7094113558392126 \t 3.8299426701200505\n",
      "53     \t [0.06892792 0.48866099 0.15889276]. \t  0.14101391876655195 \t 3.8299426701200505\n",
      "54     \t [0.25947305 0.54210736 0.81180877]. \t  3.706665726464189 \t 3.8299426701200505\n",
      "55     \t [0.29309269 0.50445278 0.82442886]. \t  3.7101997881864595 \t 3.8299426701200505\n",
      "56     \t [0.35604416 0.55268622 0.8225959 ]. \t  3.770951607011316 \t 3.8299426701200505\n",
      "57     \t [0.14516444 0.53118912 0.79171353]. \t  3.5248450143389145 \t 3.8299426701200505\n",
      "58     \t [0.12547343 0.58598869 0.77601423]. \t  3.3592035367884128 \t 3.8299426701200505\n",
      "59     \t [6.67325786e-01 6.94394423e-01 3.28006660e-07]. \t  0.003284198301060986 \t 3.8299426701200505\n",
      "60     \t [0.27661312 0.63517949 0.82127092]. \t  3.5535785620356437 \t 3.8299426701200505\n",
      "61     \t [0.4855787  0.53183925 0.83641269]. \t  3.79990812439774 \t 3.8299426701200505\n",
      "62     \t [0.24335372 0.60621575 0.82257363]. \t  3.684876164822847 \t 3.8299426701200505\n",
      "63     \t [0.25629721 0.57441184 0.82577019]. \t  3.7779278960340115 \t 3.8299426701200505\n",
      "64     \t [0.01435182 0.52547785 0.83829492]. \t  3.7687741672252457 \t 3.8299426701200505\n",
      "65     \t [0.87986058 0.56536435 0.90943976]. \t  3.4459637304134434 \t 3.8299426701200505\n",
      "66     \t [0.09918691 0.51251405 0.82103803]. \t  3.6962924579227248 \t 3.8299426701200505\n",
      "67     \t [0.48361665 0.55251965 0.80167543]. \t  3.5894138785970564 \t 3.8299426701200505\n",
      "68     \t [0.50248096 0.45176927 0.87077687]. \t  3.452565258187414 \t 3.8299426701200505\n",
      "69     \t [0.16320906 0.56551466 0.83080553]. \t  3.80387620738451 \t 3.8299426701200505\n",
      "70     \t [0.48225157 0.57479104 0.81533598]. \t  3.678399323976541 \t 3.8299426701200505\n",
      "71     \t [0.34524529 0.56222715 0.80580203]. \t  3.653136348505176 \t 3.8299426701200505\n",
      "72     \t [0.52235478 0.55336841 0.84285604]. \t  3.823073795344064 \t 3.8299426701200505\n",
      "73     \t [0.30557368 0.58689026 0.82214487]. \t  3.7335266186702007 \t 3.8299426701200505\n",
      "74     \t [5.52397692e-04 6.51207355e-01 9.17270952e-01]. \t  3.156943820984928 \t 3.8299426701200505\n",
      "75     \t [0.00395152 0.50109887 0.85495544]. \t  3.709891693126633 \t 3.8299426701200505\n",
      "76     \t [0.00644994 0.58259736 0.77864353]. \t  3.3564867701344787 \t 3.8299426701200505\n",
      "77     \t [0.22467374 0.4784821  0.78191732]. \t  3.2935424742408417 \t 3.8299426701200505\n",
      "78     \t [0.40294472 0.56708015 0.80480839]. \t  3.628727650637181 \t 3.8299426701200505\n",
      "79     \t [0.1634043  0.5930341  0.85038782]. \t  3.802050213939024 \t 3.8299426701200505\n",
      "80     \t [0.42489425 0.5116827  0.82885274]. \t  3.7413455664646778 \t 3.8299426701200505\n",
      "81     \t [0.45449055 0.58273397 0.81921567]. \t  3.697342084337479 \t 3.8299426701200505\n",
      "82     \t [0.32703156 0.60536996 0.81654932]. \t  3.6453131899497673 \t 3.8299426701200505\n",
      "83     \t [0.51440238 0.60125752 0.81327941]. \t  3.5852517119162064 \t 3.8299426701200505\n",
      "84     \t [0.02084952 0.55127803 0.87281829]. \t  3.7763908136399467 \t 3.8299426701200505\n",
      "85     \t [0.41530735 0.49717953 0.89941641]. \t  3.515607917296566 \t 3.8299426701200505\n",
      "86     \t [0.00271898 0.52013278 0.79671904]. \t  3.51507711699987 \t 3.8299426701200505\n",
      "87     \t [0.10443598 0.61709642 0.87175525]. \t  3.6805649870875166 \t 3.8299426701200505\n",
      "88     \t [2.55343423e-01 6.77527321e-01 3.58270368e-04]. \t  0.005091161738996259 \t 3.8299426701200505\n",
      "89     \t [0.33299929 0.61259981 0.80549341]. \t  3.5412527482888145 \t 3.8299426701200505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.77516159 0.52573268 0.85581973]. \t  3.7436054413783744 \t 3.8299426701200505\n",
      "91     \t [0.37910087 0.46289261 0.87713804]. \t  3.5020574738172026 \t 3.8299426701200505\n",
      "92     \t [0.47983376 0.5569422  0.84287575]. \t  \u001b[92m3.831182364806529\u001b[0m \t 3.831182364806529\n",
      "93     \t [0.41348224 0.48904863 0.81573269]. \t  3.6005968534243884 \t 3.831182364806529\n",
      "94     \t [0.73320515 0.64578794 0.90285158]. \t  3.307445103709022 \t 3.831182364806529\n",
      "95     \t [0.61679254 0.45459377 0.85515913]. \t  3.4937685109998466 \t 3.831182364806529\n",
      "96     \t [0.72721288 0.60204012 0.89428006]. \t  3.5648575282173596 \t 3.831182364806529\n",
      "97     \t [0.30397503 0.4859721  0.77150994]. \t  3.209459153279128 \t 3.831182364806529\n",
      "98     \t [0.52584907 0.5945736  0.76627285]. \t  3.1200146074895576 \t 3.831182364806529\n",
      "99     \t [0.2538122  0.49403225 0.9046972 ]. \t  3.4477101677204187 \t 3.831182364806529\n",
      "100    \t [0.70294556 0.49367566 0.88448412]. \t  3.5743992971004936 \t 3.831182364806529\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \n",
    "\n",
    "np.random.seed(run_num_2)\n",
    "surrogate_winner_2 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_2 = GPGO(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_2.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.75157561 0.10925128 0.48612128]. \t  0.19395982560567754 \t 0.5647137279144399\n",
      "init   \t [0.49983118 0.65711    0.23588471]. \t  0.10633623603471726 \t 0.5647137279144399\n",
      "init   \t [0.61279489 0.1196524  0.7122023 ]. \t  0.5647137279144399 \t 0.5647137279144399\n",
      "init   \t [0.37256054 0.52476827 0.26328708]. \t  0.24645583811739444 \t 0.5647137279144399\n",
      "init   \t [0.62163122 0.44909976 0.21906361]. \t  0.2633149120115605 \t 0.5647137279144399\n",
      "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 0.5647137279144399\n",
      "2      \t [0. 1. 1.]. \t  0.330219860606422 \t 0.5647137279144399\n",
      "3      \t [0. 0. 1.]. \t  0.0902894676548261 \t 0.5647137279144399\n",
      "4      \t [-5.55111512e-17  0.00000000e+00  1.38777878e-17]. \t  0.06797411659013229 \t 0.5647137279144399\n",
      "5      \t [ 1.00000000e+00 -5.55111512e-17  1.00000000e+00]. \t  0.08848201872702738 \t 0.5647137279144399\n",
      "6      \t [ 0.00000000e+00  1.00000000e+00 -1.38777878e-17]. \t  0.0002735367680454459 \t 0.5647137279144399\n",
      "7      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 0.5647137279144399\n",
      "8      \t [1. 0. 0.]. \t  0.03095471703300515 \t 0.5647137279144399\n",
      "9      \t [1.         0.50908545 1.        ]. \t  \u001b[92m1.943308692178596\u001b[0m \t 1.943308692178596\n",
      "10     \t [0.38298779 0.57374945 1.        ]. \t  \u001b[92m2.0874942737676263\u001b[0m \t 2.0874942737676263\n",
      "11     \t [1.         0.66013671 0.51310847]. \t  0.29907276086840756 \t 2.0874942737676263\n",
      "12     \t [0.        0.5009804 1.       ]. \t  1.9700925095477353 \t 2.0874942737676263\n",
      "13     \t [0.47288424 0.         0.        ]. \t  0.09898312623024198 \t 2.0874942737676263\n",
      "14     \t [-2.77555756e-17  1.00000000e+00  5.05137401e-01]. \t  \u001b[92m2.2997526188624007\u001b[0m \t 2.2997526188624007\n",
      "15     \t [0.         0.         0.49948112]. \t  0.13708781902966355 \t 2.2997526188624007\n",
      "16     \t [0.54178207 1.         0.6901878 ]. \t  1.0279983833040236 \t 2.2997526188624007\n",
      "17     \t [0.         0.66868252 0.65187441]. \t  \u001b[92m2.3857750196996608\u001b[0m \t 2.3857750196996608\n",
      "18     \t [1.         0.52010616 0.        ]. \t  0.0069977348451764525 \t 2.3857750196996608\n",
      "19     \t [0.         0.49583584 0.        ]. \t  0.018625320199533835 \t 2.3857750196996608\n",
      "20     \t [0.4917312 1.        0.       ]. \t  0.0002073719681819432 \t 2.3857750196996608\n",
      "21     \t [0.54750603 0.         1.        ]. \t  0.09153492757264481 \t 2.3857750196996608\n",
      "22     \t [0.74476611 0.69960506 1.        ]. \t  1.734766003598075 \t 2.3857750196996608\n",
      "23     \t [0.75070047 0.30221189 1.        ]. \t  1.0361700507895109 \t 2.3857750196996608\n",
      "24     \t [0.44442914 1.         1.        ]. \t  0.3331906697390829 \t 2.3857750196996608\n",
      "25     \t [1.         1.         0.35232089]. \t  0.06945498791865123 \t 2.3857750196996608\n",
      "26     \t [0.         0.30259674 0.75107982]. \t  1.8315638886953178 \t 2.3857750196996608\n",
      "27     \t [1.         0.         0.61595022]. \t  0.10976400900544335 \t 2.3857750196996608\n",
      "28     \t [0.         0.41569123 0.32935614]. \t  0.32315217142664326 \t 2.3857750196996608\n",
      "29     \t [0.1434852  1.         0.70543759]. \t  1.5080560845870639 \t 2.3857750196996608\n",
      "30     \t [2.48921900e-01 8.97417346e-10 6.50813882e-01]. \t  0.15518521430786425 \t 2.3857750196996608\n",
      "31     \t [0.29566636 1.         0.26518194]. \t  0.18656854339733933 \t 2.3857750196996608\n",
      "32     \t [0.72036069 0.19773132 0.        ]. \t  0.0758335253610062 \t 2.3857750196996608\n",
      "33     \t [1.         0.98854174 0.72832319]. \t  0.3945375528995849 \t 2.3857750196996608\n",
      "34     \t [0.15885135 0.16865118 0.0063532 ]. \t  0.11060234917818483 \t 2.3857750196996608\n",
      "35     \t [1.         0.24933658 0.26193929]. \t  0.2545404176011462 \t 2.3857750196996608\n",
      "36     \t [1.         0.29524198 0.77896021]. \t  1.8878222746101463 \t 2.3857750196996608\n",
      "37     \t [0.20772592 0.22203593 1.        ]. \t  0.6497764188054269 \t 2.3857750196996608\n",
      "38     \t [0.00894723 0.75315995 0.86597328]. \t  \u001b[92m2.673377799814471\u001b[0m \t 2.673377799814471\n",
      "39     \t [0.80156204 0.93253865 0.10794589]. \t  0.002068768961970836 \t 2.673377799814471\n",
      "40     \t [0.57096327 0.60912525 0.82015954]. \t  \u001b[92m3.584367575881185\u001b[0m \t 3.584367575881185\n",
      "41     \t [0.27908718 0.56395091 0.7985221 ]. \t  \u001b[92m3.599450782261955\u001b[0m \t 3.599450782261955\n",
      "42     \t [0.49220595 0.55155763 0.79329169]. \t  3.506637842077155 \t 3.599450782261955\n",
      "43     \t [0.80663465 0.54202135 0.81192032]. \t  3.560587176633142 \t 3.599450782261955\n",
      "44     \t [0.12645394 0.60375564 0.84656765]. \t  \u001b[92m3.76126701832608\u001b[0m \t 3.76126701832608\n",
      "45     \t [0.12140298 0.48527002 0.84207041]. \t  3.672051151600372 \t 3.76126701832608\n",
      "46     \t [0.76172286 0.52781806 0.77940916]. \t  3.2382847516113427 \t 3.76126701832608\n",
      "47     \t [0.25769037 0.60504474 0.84329457]. \t  \u001b[92m3.7640721911024353\u001b[0m \t 3.7640721911024353\n",
      "48     \t [0.20748119 0.54872155 0.83082116]. \t  \u001b[92m3.8125502003945257\u001b[0m \t 3.8125502003945257\n",
      "49     \t [0.31640978 0.60611193 0.81335911]. \t  3.623719437650128 \t 3.8125502003945257\n",
      "50     \t [0.25969364 0.52902961 0.84973823]. \t  \u001b[92m3.8369322853481114\u001b[0m \t 3.8369322853481114\n",
      "51     \t [0.11952825 0.59566026 0.83654179]. \t  3.7633478398783047 \t 3.8369322853481114\n",
      "52     \t [0.34965436 0.58045341 0.81464752]. \t  3.694640006408762 \t 3.8369322853481114\n",
      "53     \t [0.24357913 0.61567229 0.88748795]. \t  3.6275970211764723 \t 3.8369322853481114\n",
      "54     \t [0.16412609 0.61995822 0.83384743]. \t  3.6768152600251693 \t 3.8369322853481114\n",
      "55     \t [0.98640337 0.         0.21011945]. \t  0.2518642310110336 \t 3.8369322853481114\n",
      "56     \t [0.22127371 0.54594088 0.86521422]. \t  \u001b[92m3.8388967890278023\u001b[0m \t 3.8388967890278023\n",
      "57     \t [0.35683521 0.52543015 0.78546218]. \t  3.4488024363615706 \t 3.8388967890278023\n",
      "58     \t [2.06535027e-01 1.79275615e-09 2.15443787e-01]. \t  0.7433586536191393 \t 3.8388967890278023\n",
      "59     \t [0.28321444 0.50327676 0.86146421]. \t  3.758167675866681 \t 3.8388967890278023\n",
      "60     \t [0.20694316 0.51187558 0.85568091]. \t  3.789696458775204 \t 3.8388967890278023\n",
      "61     \t [0.18432635 0.49270522 0.86233689]. \t  3.7052134674838406 \t 3.8388967890278023\n",
      "62     \t [0.33676543 0.74626587 0.00994673]. \t  0.002903754495657265 \t 3.8388967890278023\n",
      "63     \t [0.2418182  0.5980806  0.88407997]. \t  3.706835780574466 \t 3.8388967890278023\n",
      "64     \t [0.22085321 0.62046185 0.82264409]. \t  3.63134988556917 \t 3.8388967890278023\n",
      "65     \t [0.27864228 0.58777322 0.84344024]. \t  3.8137286436938918 \t 3.8388967890278023\n",
      "66     \t [0.23751961 0.62020964 0.8495869 ]. \t  3.7132369731620565 \t 3.8388967890278023\n",
      "67     \t [0.15027676 0.54231413 0.82774046]. \t  3.789375034589102 \t 3.8388967890278023\n",
      "68     \t [0.34688346 0.4673508  0.86771571]. \t  3.570572230431621 \t 3.8388967890278023\n",
      "69     \t [0.74360522 0.54744438 0.84187109]. \t  3.7553497713450326 \t 3.8388967890278023\n",
      "70     \t [0.16230684 0.46997923 0.79398781]. \t  3.361963129490276 \t 3.8388967890278023\n",
      "71     \t [0.33367485 0.51168354 0.87073489]. \t  3.76005761916549 \t 3.8388967890278023\n",
      "72     \t [0.53063382 0.6016113  0.81888066]. \t  3.618356221806928 \t 3.8388967890278023\n",
      "73     \t [0.20874849 0.54831565 0.90126979]. \t  3.618532895871834 \t 3.8388967890278023\n",
      "74     \t [0.37643315 0.52408273 0.80169608]. \t  3.597281848019133 \t 3.8388967890278023\n",
      "75     \t [0.7466293  0.58954941 0.84542662]. \t  3.70700376835697 \t 3.8388967890278023\n",
      "76     \t [0.14687047 0.60003895 0.89106773]. \t  3.6441827255603814 \t 3.8388967890278023\n",
      "77     \t [0.23863015 0.59204473 0.86968537]. \t  3.7874536955201155 \t 3.8388967890278023\n",
      "78     \t [0.16027789 0.55694087 0.83317501]. \t  3.8165136607904513 \t 3.8388967890278023\n",
      "79     \t [0.35471647 0.5292129  0.85859023]. \t  3.833828142051225 \t 3.8388967890278023\n",
      "80     \t [0.12125842 0.59471263 0.86922824]. \t  3.766861277872451 \t 3.8388967890278023\n",
      "81     \t [0.37133005 0.50762501 0.87410252]. \t  3.7322761984047457 \t 3.8388967890278023\n",
      "82     \t [0.52337678 0.572788   0.9018212 ]. \t  3.6059766250623935 \t 3.8388967890278023\n",
      "83     \t [0.2897642  0.56974133 0.82350503]. \t  3.7712887105513078 \t 3.8388967890278023\n",
      "84     \t [0.15010992 0.45065388 0.83599841]. \t  3.4756929481425063 \t 3.8388967890278023\n",
      "85     \t [0.36232062 0.58059625 0.82916128]. \t  3.775737043490181 \t 3.8388967890278023\n",
      "86     \t [0.26054006 0.62774591 0.85186853]. \t  3.6802388525548717 \t 3.8388967890278023\n",
      "87     \t [0.42325074 0.51275842 0.8607311 ]. \t  3.787307090354167 \t 3.8388967890278023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.04526332 0.53483161 0.79514675]. \t  3.537975140431649 \t 3.8388967890278023\n",
      "89     \t [0.12867569 0.6140194  0.89088936]. \t  3.596919807667831 \t 3.8388967890278023\n",
      "90     \t [0.22236928 0.55235116 0.84878743]. \t  \u001b[92m3.856926445186091\u001b[0m \t 3.856926445186091\n",
      "91     \t [0.46643426 0.53799065 0.88523928]. \t  3.7372460997764447 \t 3.856926445186091\n",
      "92     \t [0.14531337 0.57343699 0.83270512]. \t  3.8002008640776603 \t 3.856926445186091\n",
      "93     \t [0.62128776 0.58337726 0.81558953]. \t  3.614300625629847 \t 3.856926445186091\n",
      "94     \t [0.00671729 0.74485106 0.16687828]. \t  0.03673916291455573 \t 3.856926445186091\n",
      "95     \t [0.1387827  0.57553845 0.90153865]. \t  3.602443486968207 \t 3.856926445186091\n",
      "96     \t [0.66063217 0.72225481 0.87240142]. \t  2.896230099256127 \t 3.856926445186091\n",
      "97     \t [0.82531345 0.5246003  0.84448207]. \t  3.7121915547755497 \t 3.856926445186091\n",
      "98     \t [0.17207351 0.51920494 0.85439046]. \t  3.806469552742535 \t 3.856926445186091\n",
      "99     \t [0.30441259 0.60621268 0.8849226 ]. \t  3.6794947083311698 \t 3.856926445186091\n",
      "100    \t [0.16536277 0.55283292 0.83993551]. \t  3.8372094254768907 \t 3.856926445186091\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \n",
    "\n",
    "np.random.seed(run_num_3)\n",
    "surrogate_winner_3 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_3 = GPGO(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_3.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.83936105 0.82129497 0.64220716]. \t  0.8796727111154451 \t 1.9592421489197056\n",
      "init   \t [0.66722262 0.03913991 0.06567239]. \t  0.21283931850025994 \t 1.9592421489197056\n",
      "init   \t [0.27650064 0.3163605  0.19359241]. \t  0.5590312289203825 \t 1.9592421489197056\n",
      "init   \t [0.19709288 0.84920429 0.84235809]. \t  1.818649590001362 \t 1.9592421489197056\n",
      "init   \t [0.34736473 0.7397423  0.49340914]. \t  1.9592421489197056 \t 1.9592421489197056\n",
      "1      \t [1.11022302e-16 1.00000000e+00 0.00000000e+00]. \t  0.0002735367680454459 \t 1.9592421489197056\n",
      "2      \t [0.61652872 0.         1.        ]. \t  0.09130370165792916 \t 1.9592421489197056\n",
      "3      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 1.9592421489197056\n",
      "4      \t [0. 0. 1.]. \t  0.0902894676548261 \t 1.9592421489197056\n",
      "5      \t [1.         0.         0.51422211]. \t  0.06905455658222702 \t 1.9592421489197056\n",
      "6      \t [1.         0.41630416 0.        ]. \t  0.014495204380819037 \t 1.9592421489197056\n",
      "7      \t [0.         1.         0.58764257]. \t  \u001b[92m2.4309654172078434\u001b[0m \t 2.4309654172078434\n",
      "8      \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.4309654172078434\n",
      "9      \t [1.        0.4044566 1.       ]. \t  1.54985854477701 \t 2.4309654172078434\n",
      "10     \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.4309654172078434\n",
      "11     \t [0.         0.55899962 0.64640352]. \t  1.9971243619434127 \t 2.4309654172078434\n",
      "12     \t [0.         0.         0.50476811]. \t  0.1308574156174005 \t 2.4309654172078434\n",
      "13     \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.4309654172078434\n",
      "14     \t [0.52931442 1.         0.        ]. \t  0.00019196343439091207 \t 2.4309654172078434\n",
      "15     \t [1. 0. 0.]. \t  0.03095471703300515 \t 2.4309654172078434\n",
      "16     \t [0. 1. 1.]. \t  0.330219860606422 \t 2.4309654172078434\n",
      "17     \t [0.58633113 1.         1.        ]. \t  0.3305633655291983 \t 2.4309654172078434\n",
      "18     \t [0.22815397 0.4112521  1.        ]. \t  1.6400817854068401 \t 2.4309654172078434\n",
      "19     \t [0.53029877 0.33575014 0.719003  ]. \t  1.801307613132999 \t 2.4309654172078434\n",
      "20     \t [0.36277701 1.         0.50870099]. \t  1.987442497021829 \t 2.4309654172078434\n",
      "21     \t [0.         0.55511953 0.        ]. \t  0.011536396355452632 \t 2.4309654172078434\n",
      "22     \t [1.         1.         0.38279628]. \t  0.09795913241902991 \t 2.4309654172078434\n",
      "23     \t [0.30726056 0.         0.68878015]. \t  0.19170812743878587 \t 2.4309654172078434\n",
      "24     \t [1.         0.43918622 0.60192197]. \t  0.7638870729760447 \t 2.4309654172078434\n",
      "25     \t [0.         0.63371506 1.        ]. \t  1.9721172469190553 \t 2.4309654172078434\n",
      "26     \t [0.39351312 0.56056936 0.        ]. \t  0.016448119555764012 \t 2.4309654172078434\n",
      "27     \t [0.69456797 0.63707218 1.        ]. \t  1.970886518931968 \t 2.4309654172078434\n",
      "28     \t [0.         0.88222135 0.30743118]. \t  0.4640263878909293 \t 2.4309654172078434\n",
      "29     \t [0.76339392 0.50381808 0.26864169]. \t  0.15901558389270973 \t 2.4309654172078434\n",
      "30     \t [1.         0.75511406 0.18409281]. \t  0.00811611586661663 \t 2.4309654172078434\n",
      "31     \t [0.47721522 0.         0.27997428]. \t  0.8380452263344924 \t 2.4309654172078434\n",
      "32     \t [0.79723602 0.         0.72289877]. \t  0.21840471218277943 \t 2.4309654172078434\n",
      "33     \t [0.         0.27631397 0.85108572]. \t  1.8759773835252407 \t 2.4309654172078434\n",
      "34     \t [0.27317176 0.         0.        ]. \t  0.09947399238726982 \t 2.4309654172078434\n",
      "35     \t [1.         0.73728782 1.        ]. \t  1.511504988163408 \t 2.4309654172078434\n",
      "36     \t [2.36606789e-01 9.95567194e-01 1.19950561e-08]. \t  0.0002904510498032823 \t 2.4309654172078434\n",
      "37     \t [0.00258074 0.27851284 0.        ]. \t  0.06039946157319676 \t 2.4309654172078434\n",
      "38     \t [0.99999998 0.19749976 0.23194479]. \t  0.27349350008692797 \t 2.4309654172078434\n",
      "39     \t [0.46009496 0.64709425 0.84035492]. \t  \u001b[92m3.5280466600998173\u001b[0m \t 3.5280466600998173\n",
      "40     \t [0.56994653 0.99999998 0.30127656]. \t  0.19711704936119015 \t 3.5280466600998173\n",
      "41     \t [1.         0.99854801 0.7498331 ]. \t  0.40061993741759844 \t 3.5280466600998173\n",
      "42     \t [0.30968525 0.54511893 0.77105846]. \t  3.318424916518532 \t 3.5280466600998173\n",
      "43     \t [0.48492651 0.62933395 0.77642977]. \t  3.144442413836361 \t 3.5280466600998173\n",
      "44     \t [2.37074393e-08 3.46254485e-01 3.26812755e-01]. \t  0.3936308613222475 \t 3.5280466600998173\n",
      "45     \t [3.52314121e-09 3.78631164e-01 1.00000000e+00]. \t  1.4509333676150704 \t 3.5280466600998173\n",
      "46     \t [6.99516318e-07 8.57494625e-01 7.95607322e-01]. \t  1.7772306244193494 \t 3.5280466600998173\n",
      "47     \t [0.9999995  0.14211006 0.83273535]. \t  0.8101606319848034 \t 3.5280466600998173\n",
      "48     \t [0.41796843 0.60654679 0.835454  ]. \t  \u001b[92m3.719847289057847\u001b[0m \t 3.719847289057847\n",
      "49     \t [0.33728125 0.61574954 0.96789632]. \t  2.607401190981623 \t 3.719847289057847\n",
      "50     \t [0.86393938 0.73105715 0.        ]. \t  0.0013383533557278671 \t 3.719847289057847\n",
      "51     \t [0.25020587 0.60018165 0.79297077]. \t  3.491190148261172 \t 3.719847289057847\n",
      "52     \t [0.22159922 0.56766273 0.74955434]. \t  3.0871801177913363 \t 3.719847289057847\n",
      "53     \t [9.61493918e-02 2.95346943e-09 1.77597334e-01]. \t  0.5480304059606113 \t 3.719847289057847\n",
      "54     \t [0.45447176 0.59570362 0.80339621]. \t  3.544122858972931 \t 3.719847289057847\n",
      "55     \t [0.47433576 0.52126925 0.79040874]. \t  3.465183073923222 \t 3.719847289057847\n",
      "56     \t [0.4277989  0.58549987 0.89556964]. \t  3.6533982435573678 \t 3.719847289057847\n",
      "57     \t [0.36791987 0.58306459 0.89876105]. \t  3.6340830387913625 \t 3.719847289057847\n",
      "58     \t [0.43775782 0.56235632 0.84066936]. \t  \u001b[92m3.830470981307728\u001b[0m \t 3.830470981307728\n",
      "59     \t [0.36026375 0.64513168 0.80302778]. \t  3.3689567292672704 \t 3.830470981307728\n",
      "60     \t [0.83729587 0.51642097 0.8686788 ]. \t  3.6824209931534146 \t 3.830470981307728\n",
      "61     \t [0.48633954 0.62352833 0.80665653]. \t  3.4560573577736946 \t 3.830470981307728\n",
      "62     \t [0.99629373 0.51326531 0.85306274]. \t  3.6280885511992906 \t 3.830470981307728\n",
      "63     \t [0.45610404 0.56140166 0.85473997]. \t  \u001b[92m3.8468107150729303\u001b[0m \t 3.8468107150729303\n",
      "64     \t [0.76222523 0.51235815 0.88446674]. \t  3.6251675128135075 \t 3.8468107150729303\n",
      "65     \t [0.87996159 0.54363418 0.92059188]. \t  3.3125383518725746 \t 3.8468107150729303\n",
      "66     \t [0.35955485 0.51797221 0.82401753]. \t  3.741218642778799 \t 3.8468107150729303\n",
      "67     \t [0.45618312 0.50450913 0.85197971]. \t  3.765776905597936 \t 3.8468107150729303\n",
      "68     \t [0.82214427 0.60836672 0.839404  ]. \t  3.591841696999635 \t 3.8468107150729303\n",
      "69     \t [0.1707546  0.9888216  0.22009002]. \t  0.09064964664679084 \t 3.8468107150729303\n",
      "70     \t [0.49550672 0.4808993  0.82965343]. \t  3.6238019484476287 \t 3.8468107150729303\n",
      "71     \t [0.6113382  0.53629204 0.88034999]. \t  3.739868633129333 \t 3.8468107150729303\n",
      "72     \t [0.69330808 0.62994978 0.84989704]. \t  3.573512388651653 \t 3.8468107150729303\n",
      "73     \t [0.6998718  0.54877599 0.8847734 ]. \t  3.704794399352922 \t 3.8468107150729303\n",
      "74     \t [0.49262761 0.59675626 0.83378174]. \t  3.7284219764572653 \t 3.8468107150729303\n",
      "75     \t [0.50689722 0.56486792 0.8822588 ]. \t  3.7602967740609596 \t 3.8468107150729303\n",
      "76     \t [0.45203417 0.53567756 0.8016389 ]. \t  3.5944493182566726 \t 3.8468107150729303\n",
      "77     \t [0.65915095 0.99981249 0.72640952]. \t  0.696957924178941 \t 3.8468107150729303\n",
      "78     \t [0.50451455 0.50641455 0.80840938]. \t  3.5892203588589298 \t 3.8468107150729303\n",
      "79     \t [0.40492172 0.60526548 0.79235256]. \t  3.4327262873751647 \t 3.8468107150729303\n",
      "80     \t [0.54544629 0.49027786 0.93236976]. \t  3.084989589259835 \t 3.8468107150729303\n",
      "81     \t [0.41189179 0.62351494 0.82882287]. \t  3.622973406461148 \t 3.8468107150729303\n",
      "82     \t [0.99470994 0.58203175 0.8600325 ]. \t  3.63980395161737 \t 3.8468107150729303\n",
      "83     \t [0.23978321 0.58549462 0.83966553]. \t  3.810007924726116 \t 3.8468107150729303\n",
      "84     \t [0.72757761 0.56533101 0.84228593]. \t  3.7527504284876114 \t 3.8468107150729303\n",
      "85     \t [0.84154407 0.54189181 0.8569562 ]. \t  3.738282861961302 \t 3.8468107150729303\n",
      "86     \t [0.61485533 0.59733862 0.78630001]. \t  3.297435206940757 \t 3.8468107150729303\n",
      "87     \t [0.46198785 0.55071717 0.83650748]. \t  3.8174136224564457 \t 3.8468107150729303\n",
      "88     \t [0.47147661 0.5594117  0.93361274]. \t  3.2314163476347626 \t 3.8468107150729303\n",
      "89     \t [0.3355472  0.58795246 0.79163087]. \t  3.4930329889376157 \t 3.8468107150729303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.53044111 0.6332262  0.76532754]. \t  2.9762866545007407 \t 3.8468107150729303\n",
      "91     \t [0.49112966 0.52910212 0.78376584]. \t  3.399139966543606 \t 3.8468107150729303\n",
      "92     \t [0.62228298 0.56596858 0.84842287]. \t  3.8005613905024203 \t 3.8468107150729303\n",
      "93     \t [0.57528114 0.59620818 0.79440127]. \t  3.407557838729203 \t 3.8468107150729303\n",
      "94     \t [0.35697241 0.49298102 0.81787039]. \t  3.6338924058143816 \t 3.8468107150729303\n",
      "95     \t [0.36220971 0.5266912  0.82820184]. \t  3.777955357581823 \t 3.8468107150729303\n",
      "96     \t [0.31736911 0.61633392 0.7469853 ]. \t  2.970280844796209 \t 3.8468107150729303\n",
      "97     \t [0.7031446  0.47190759 0.88263208]. \t  3.480019288040118 \t 3.8468107150729303\n",
      "98     \t [0.94883499 0.4750466  0.86131676]. \t  3.499763014282539 \t 3.8468107150729303\n",
      "99     \t [0.88964958 0.5066659  0.82398867]. \t  3.568400502570655 \t 3.8468107150729303\n",
      "100    \t [0.54294408 0.55322096 0.85591417]. \t  3.8328901042147554 \t 3.8468107150729303\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \n",
    "\n",
    "np.random.seed(run_num_4)\n",
    "surrogate_winner_4 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_4 = GPGO(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_4.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.55735327 0.54176063 0.56042378]. \t  0.9599610140567721 \t 0.9810564697651996\n",
      "init   \t [0.38204809 0.11909418 0.84599912]. \t  0.6913364116577386 \t 0.9810564697651996\n",
      "init   \t [0.73974161 0.93235756 0.85932801]. \t  0.9810564697651996 \t 0.9810564697651996\n",
      "init   \t [0.70810463 0.68096875 0.4086023 ]. \t  0.4017443293787304 \t 0.9810564697651996\n",
      "init   \t [0.77458462 0.9844762  0.96548486]. \t  0.4769494155906823 \t 0.9810564697651996\n",
      "1      \t [0.         1.         0.68584007]. \t  \u001b[92m1.6479460828233696\u001b[0m \t 1.6479460828233696\n",
      "2      \t [1. 0. 1.]. \t  0.08848201872702738 \t 1.6479460828233696\n",
      "3      \t [-1.11022302e-16  1.00000000e+00  0.00000000e+00]. \t  0.0002735367680454459 \t 1.6479460828233696\n",
      "4      \t [0. 0. 0.]. \t  0.06797411659013229 \t 1.6479460828233696\n",
      "5      \t [ 1.00000000e+00 -5.55111512e-17  0.00000000e+00]. \t  0.030954717033005136 \t 1.6479460828233696\n",
      "6      \t [0.         0.52454244 1.        ]. \t  \u001b[92m2.020697438564142\u001b[0m \t 2.020697438564142\n",
      "7      \t [0.20841836 1.         1.        ]. \t  0.33403680579911815 \t 2.020697438564142\n",
      "8      \t [0.         0.         0.64963372]. \t  0.14861148200529825 \t 2.020697438564142\n",
      "9      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.020697438564142\n",
      "10     \t [1.         0.54037322 1.        ]. \t  1.9946575952785257 \t 2.020697438564142\n",
      "11     \t [1.         0.32961932 0.57909101]. \t  0.47782242573668937 \t 2.020697438564142\n",
      "12     \t [0.         0.49888021 0.        ]. \t  0.01820207636671624 \t 2.020697438564142\n",
      "13     \t [ 4.92461272e-01 -1.38777878e-17  0.00000000e+00]. \t  0.09766911286137267 \t 2.020697438564142\n",
      "14     \t [1.         1.         0.58903846]. \t  0.2733487469111252 \t 2.020697438564142\n",
      "15     \t [0. 0. 1.]. \t  0.0902894676548261 \t 2.020697438564142\n",
      "16     \t [0.481169 1.       0.      ]. \t  0.00021162051249202628 \t 2.020697438564142\n",
      "17     \t [1.         0.46494643 0.        ]. \t  0.01058283342561674 \t 2.020697438564142\n",
      "18     \t [0.6729158  0.39280721 1.        ]. \t  1.5360879831115646 \t 2.020697438564142\n",
      "19     \t [0.25569974 1.         0.42821882]. \t  1.4840752095521397 \t 2.020697438564142\n",
      "20     \t [0.         0.66925143 0.56548373]. \t  \u001b[92m2.207936153360304\u001b[0m \t 2.207936153360304\n",
      "21     \t [0.75034998 0.         0.46776802]. \t  0.18076731599212578 \t 2.207936153360304\n",
      "22     \t [0.32331264 0.64993059 1.        ]. \t  1.9537019661166204 \t 2.207936153360304\n",
      "23     \t [0.         0.24505313 0.29962874]. \t  0.5555327393105244 \t 2.207936153360304\n",
      "24     \t [0.23871247 0.         0.28273644]. \t  0.8231733943517724 \t 2.207936153360304\n",
      "25     \t [0.         0.83193934 1.        ]. \t  1.0381441593093672 \t 2.207936153360304\n",
      "26     \t [0.32160514 0.68748694 0.        ]. \t  0.004677028831553973 \t 2.207936153360304\n",
      "27     \t [ 6.00086139e-01 -1.38777878e-17  1.00000000e+00]. \t  0.09136662628334798 \t 2.207936153360304\n",
      "28     \t [0.         0.98847697 0.28801893]. \t  0.3004959773040432 \t 2.207936153360304\n",
      "29     \t [0.77005147 0.2351786  0.13054579]. \t  0.30627488928769586 \t 2.207936153360304\n",
      "30     \t [1.         0.91954888 0.96942425]. \t  0.7411077044376327 \t 2.207936153360304\n",
      "31     \t [0.23077589 0.19581181 0.        ]. \t  0.10405881364580392 \t 2.207936153360304\n",
      "32     \t [1.         0.         0.36020905]. \t  0.2047374236703341 \t 2.207936153360304\n",
      "33     \t [0.1020683  0.68854816 0.79925067]. \t  \u001b[92m3.1136398931549136\u001b[0m \t 3.1136398931549136\n",
      "34     \t [1.         0.71185848 0.27363442]. \t  0.02899435609233946 \t 3.1136398931549136\n",
      "35     \t [1.49476988e-07 2.66646109e-01 7.49403423e-01]. \t  1.5533844726595292 \t 3.1136398931549136\n",
      "36     \t [0.20407677 0.23629849 1.        ]. \t  0.7135951349133537 \t 3.1136398931549136\n",
      "37     \t [0.87325072 0.73559938 0.        ]. \t  0.001232863481963994 \t 3.1136398931549136\n",
      "38     \t [0.59452545 1.         0.46093695]. \t  0.9717706821079536 \t 3.1136398931549136\n",
      "39     \t [0.28202888 0.88584049 0.73663047]. \t  1.749160389305855 \t 3.1136398931549136\n",
      "40     \t [0.9727447  0.         0.71099481]. \t  0.20468179246372015 \t 3.1136398931549136\n",
      "41     \t [0.87724973 0.68913184 1.        ]. \t  1.7600300609661519 \t 3.1136398931549136\n",
      "42     \t [0.18041584 0.53652146 0.34097447]. \t  0.3764829331111712 \t 3.1136398931549136\n",
      "43     \t [1.         0.66883669 0.8007774 ]. \t  2.8760971683758187 \t 3.1136398931549136\n",
      "44     \t [0.90068294 0.49876836 0.81978228]. \t  \u001b[92m3.5178489333410976\u001b[0m \t 3.5178489333410976\n",
      "45     \t [1.07736605e-06 7.22751655e-01 7.67558415e-01]. \t  2.698573417771266 \t 3.5178489333410976\n",
      "46     \t [0.83618556 0.9999996  0.11401353]. \t  0.0016481663027888313 \t 3.5178489333410976\n",
      "47     \t [0.9258054  0.55358425 0.84935037]. \t  \u001b[92m3.6989669213777905\u001b[0m \t 3.6989669213777905\n",
      "48     \t [0.78520168 0.54243235 0.79617473]. \t  3.421212958437888 \t 3.6989669213777905\n",
      "49     \t [1.         0.23663259 0.90243899]. \t  1.3046436458914785 \t 3.6989669213777905\n",
      "50     \t [0.81208458 0.56113075 0.84754504]. \t  \u001b[92m3.7384873915107293\u001b[0m \t 3.7384873915107293\n",
      "51     \t [0.84160546 0.61467759 0.81414748]. \t  3.3958365935772226 \t 3.7384873915107293\n",
      "52     \t [0.85669153 0.51388586 0.8892052 ]. \t  3.5678152592177894 \t 3.7384873915107293\n",
      "53     \t [0.93862099 0.53992952 0.80158376]. \t  3.4084350920980158 \t 3.7384873915107293\n",
      "54     \t [0.85799326 0.52776621 0.8260859 ]. \t  3.6309046992384704 \t 3.7384873915107293\n",
      "55     \t [0.95744977 0.52029413 0.85902506]. \t  3.6603568922962544 \t 3.7384873915107293\n",
      "56     \t [0. 1. 1.]. \t  0.330219860606422 \t 3.7384873915107293\n",
      "57     \t [0.88386965 0.49318671 0.84999414]. \t  3.6119297807492283 \t 3.7384873915107293\n",
      "58     \t [0.11366929 0.52554043 0.81366731]. \t  3.684973960226533 \t 3.7384873915107293\n",
      "59     \t [0.5815223  0.42771224 0.0088274 ]. \t  0.044839284223968746 \t 3.7384873915107293\n",
      "60     \t [0.86509121 0.53370521 0.85199752]. \t  3.7202749414307243 \t 3.7384873915107293\n",
      "61     \t [0.90299264 0.49656509 0.8713093 ]. \t  3.5864339732393367 \t 3.7384873915107293\n",
      "62     \t [0.8249072  0.51898016 0.83829651]. \t  3.6855745075752595 \t 3.7384873915107293\n",
      "63     \t [0.09093183 0.50957234 0.81022793]. \t  3.6214526371798117 \t 3.7384873915107293\n",
      "64     \t [0.09126617 0.53342664 0.86282401]. \t  \u001b[92m3.808338595829715\u001b[0m \t 3.808338595829715\n",
      "65     \t [0.04043466 0.51066878 0.84203392]. \t  3.748158641733193 \t 3.808338595829715\n",
      "66     \t [0.18927893 0.47263693 0.86433264]. \t  3.601353845993546 \t 3.808338595829715\n",
      "67     \t [0.04054331 0.58565011 0.89685432]. \t  3.608452590661665 \t 3.808338595829715\n",
      "68     \t [0.09929666 0.5672047  0.85253974]. \t  \u001b[92m3.8346740012319582\u001b[0m \t 3.8346740012319582\n",
      "69     \t [0.09531095 0.65148137 0.7859527 ]. \t  3.235770272178539 \t 3.8346740012319582\n",
      "70     \t [0.02003477 0.50325545 0.8399673 ]. \t  3.7147527328430594 \t 3.8346740012319582\n",
      "71     \t [0.07992132 0.56051333 0.77100134]. \t  3.318818927794818 \t 3.8346740012319582\n",
      "72     \t [0.65186084 0.5511969  0.85129255]. \t  3.8033933918562193 \t 3.8346740012319582\n",
      "73     \t [0.24029658 0.52957006 0.84034017]. \t  3.82486375147945 \t 3.8346740012319582\n",
      "74     \t [0.35867464 0.60216651 0.89857085]. \t  3.592520229769976 \t 3.8346740012319582\n",
      "75     \t [0.64609803 0.55084932 0.8402078 ]. \t  3.7828047693150326 \t 3.8346740012319582\n",
      "76     \t [0.88217606 0.52407882 0.81945433]. \t  3.5749680700316047 \t 3.8346740012319582\n",
      "77     \t [0.62233531 0.51700358 0.80870666]. \t  3.5812728627278627 \t 3.8346740012319582\n",
      "78     \t [0.31346075 0.43505334 0.87060836]. \t  3.3417093786704966 \t 3.8346740012319582\n",
      "79     \t [0.20032605 0.58190216 0.82961714]. \t  3.7812300583222687 \t 3.8346740012319582\n",
      "80     \t [0.38553756 0.58379017 0.90188919]. \t  3.6038980689696194 \t 3.8346740012319582\n",
      "81     \t [0.69943223 0.56800705 0.86485474]. \t  3.7752444729144776 \t 3.8346740012319582\n",
      "82     \t [0.5260944  0.5475656  0.88737583]. \t  3.725117083375216 \t 3.8346740012319582\n",
      "83     \t [0.72915484 0.51568366 0.87850258]. \t  3.6773375543117726 \t 3.8346740012319582\n",
      "84     \t [0.79382591 0.55657937 0.84139225]. \t  3.7328741141277177 \t 3.8346740012319582\n",
      "85     \t [0.04922856 0.55986432 0.83288443]. \t  3.7900802899355748 \t 3.8346740012319582\n",
      "86     \t [0.62299058 0.62852724 0.82275593]. \t  3.4924184577655435 \t 3.8346740012319582\n",
      "87     \t [0.63286263 0.52230176 0.85725152]. \t  3.779025673796119 \t 3.8346740012319582\n",
      "88     \t [0.25779046 0.47917257 0.87196063]. \t  3.6161392631926645 \t 3.8346740012319582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.01145241 0.64330292 0.80261839]. \t  3.3690139922537727 \t 3.8346740012319582\n",
      "90     \t [0.38620394 0.5719537  0.86495206]. \t  \u001b[92m3.8349004600216596\u001b[0m \t 3.8349004600216596\n",
      "91     \t [0.82228074 0.53537673 0.87257206]. \t  3.7114650387128876 \t 3.8349004600216596\n",
      "92     \t [0.29407828 0.52533554 0.9395918 ]. \t  3.1010637466764313 \t 3.8349004600216596\n",
      "93     \t [0.73955287 0.57672236 0.8312487 ]. \t  3.684553607776505 \t 3.8349004600216596\n",
      "94     \t [0.51106611 0.49257438 0.82948567]. \t  3.6687884120526353 \t 3.8349004600216596\n",
      "95     \t [0.72370151 0.5154064  0.88829475]. \t  3.62007234293138 \t 3.8349004600216596\n",
      "96     \t [0.59207485 0.55841117 0.87090679]. \t  3.796236548224915 \t 3.8349004600216596\n",
      "97     \t [0.81215072 0.58144017 0.8645283 ]. \t  3.7158660817206686 \t 3.8349004600216596\n",
      "98     \t [0.00307973 0.51657471 0.82973861]. \t  3.7185670463452465 \t 3.8349004600216596\n",
      "99     \t [0.63682568 0.55966293 0.88271345]. \t  3.734200787463858 \t 3.8349004600216596\n",
      "100    \t [0.18965348 0.5303133  0.8726354 ]. \t  3.789761948325971 \t 3.8349004600216596\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \n",
    "\n",
    "np.random.seed(run_num_5)\n",
    "surrogate_winner_5 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_5 = GPGO(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_5.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.89286015 0.33197981 0.82122912]. \t  2.3879127825536575 \t 2.5106636917702634\n",
      "init   \t [0.04169663 0.10765668 0.59505206]. \t  0.2321934630074273 \t 2.5106636917702634\n",
      "init   \t [0.52981736 0.41880743 0.33540785]. \t  0.37989557867087576 \t 2.5106636917702634\n",
      "init   \t [0.62251943 0.43814143 0.73588211]. \t  2.5106636917702634 \t 2.5106636917702634\n",
      "init   \t [0.51803641 0.5788586  0.6453551 ]. \t  1.7289203948034264 \t 2.5106636917702634\n",
      "1      \t [0.53526222 0.         1.        ]. \t  0.09156689767117489 \t 2.5106636917702634\n",
      "2      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.5106636917702634\n",
      "3      \t [0. 1. 1.]. \t  0.330219860606422 \t 2.5106636917702634\n",
      "4      \t [1. 0. 0.]. \t  0.03095471703300515 \t 2.5106636917702634\n",
      "5      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.5106636917702634\n",
      "6      \t [0.         0.41352053 1.        ]. \t  1.6309281197773737 \t 2.5106636917702634\n",
      "7      \t [1.         0.53811382 0.46028108]. \t  0.1512475286864764 \t 2.5106636917702634\n",
      "8      \t [0.63290052 0.54906981 1.        ]. \t  2.0675877100156455 \t 2.5106636917702634\n",
      "9      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 2.5106636917702634\n",
      "10     \t [ 0.00000000e+00  0.00000000e+00 -1.11022302e-16]. \t  0.06797411659013218 \t 2.5106636917702634\n",
      "11     \t [1.         0.         0.56827446]. \t  0.07680446062491905 \t 2.5106636917702634\n",
      "12     \t [0.         0.61239785 0.56527306]. \t  1.829809024528139 \t 2.5106636917702634\n",
      "13     \t [0.         0.47573541 0.        ]. \t  0.021581046083015854 \t 2.5106636917702634\n",
      "14     \t [0.49872128 0.         0.        ]. \t  0.09720545477346638 \t 2.5106636917702634\n",
      "15     \t [1.         1.         0.50331198]. \t  0.2276958563748813 \t 2.5106636917702634\n",
      "16     \t [0.        1.        0.5457683]. \t  2.480810462151056 \t 2.5106636917702634\n",
      "17     \t [0.55529205 1.         0.        ]. \t  0.00018115280329300133 \t 2.5106636917702634\n",
      "18     \t [0.58727863 1.         0.87220555]. \t  0.6104406487918543 \t 2.5106636917702634\n",
      "19     \t [0. 0. 1.]. \t  0.0902894676548261 \t 2.5106636917702634\n",
      "20     \t [0.80638777 0.49247031 0.        ]. \t  0.016140070818830347 \t 2.5106636917702634\n",
      "21     \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.5106636917702634\n",
      "22     \t [0.31155971 0.33863566 0.94093434]. \t  1.9227746534572236 \t 2.5106636917702634\n",
      "23     \t [0.24140555 0.2559184  0.        ]. \t  0.0920743089567683 \t 2.5106636917702634\n",
      "24     \t [0.1989588  1.         0.26921102]. \t  0.21660351259959557 \t 2.5106636917702634\n",
      "25     \t [1.         0.73521048 0.98205981]. \t  1.7548241673330025 \t 2.5106636917702634\n",
      "26     \t [0.52439114 0.         0.59287423]. \t  0.11702314165554924 \t 2.5106636917702634\n",
      "27     \t [0.73483777 1.         0.28990816]. \t  0.09661032322123789 \t 2.5106636917702634\n",
      "28     \t [0.12248724 0.84472005 0.80045452]. \t  1.8988387953037438 \t 2.5106636917702634\n",
      "29     \t [1.         0.40901505 1.        ]. \t  1.5721881560030642 \t 2.5106636917702634\n",
      "30     \t [0.27427138 0.70998233 0.04844488]. \t  0.007843358450229402 \t 2.5106636917702634\n",
      "31     \t [0.         0.29134156 0.82380961]. \t  2.033452748905888 \t 2.5106636917702634\n",
      "32     \t [1.0000000e+00 8.1781550e-01 3.0124479e-08]. \t  0.0002876741674255845 \t 2.5106636917702634\n",
      "33     \t [0.32016363 0.95441531 0.99531379]. \t  0.5011825344996661 \t 2.5106636917702634\n",
      "34     \t [0.85784916 0.         0.19947218]. \t  0.37079993105639286 \t 2.5106636917702634\n",
      "35     \t [0.19643177 0.         0.16904437]. \t  0.5970700531234358 \t 2.5106636917702634\n",
      "36     \t [1.         0.13950548 0.09045447]. \t  0.11787402118161006 \t 2.5106636917702634\n",
      "37     \t [0.69925502 0.2612789  1.        ]. \t  0.8280406145436641 \t 2.5106636917702634\n",
      "38     \t [0.         0.24448667 0.19708602]. \t  0.48860935250335685 \t 2.5106636917702634\n",
      "39     \t [2.64374042e-08 9.18831724e-01 2.33259543e-01]. \t  0.12880320019333394 \t 2.5106636917702634\n",
      "40     \t [0.7738608  0.75235837 1.        ]. \t  1.4680521982432555 \t 2.5106636917702634\n",
      "41     \t [0.17223004 0.53282818 0.79125832]. \t  \u001b[92m3.5258017222138296\u001b[0m \t 3.5258017222138296\n",
      "42     \t [1.53767186e-07 6.91123012e-01 8.60122074e-01]. \t  3.2256078548920413 \t 3.5258017222138296\n",
      "43     \t [0.14765282 0.53296983 0.86998264]. \t  \u001b[92m3.798350471914235\u001b[0m \t 3.798350471914235\n",
      "44     \t [0.3620489  0.56404387 0.89933494]. \t  3.6477496463672376 \t 3.798350471914235\n",
      "45     \t [0.19145764 0.54749616 0.87017835]. \t  \u001b[92m3.821465381028421\u001b[0m \t 3.821465381028421\n",
      "46     \t [0.11584936 0.52061621 0.89680646]. \t  3.597236381864438 \t 3.821465381028421\n",
      "47     \t [0.15336782 0.54256201 0.86222968]. \t  \u001b[92m3.8339094207023483\u001b[0m \t 3.8339094207023483\n",
      "48     \t [0.07424156 0.53886559 0.81090963]. \t  3.6751368684850085 \t 3.8339094207023483\n",
      "49     \t [0.17159689 0.54341717 0.85151722]. \t  \u001b[92m3.847790918157131\u001b[0m \t 3.847790918157131\n",
      "50     \t [0.         0.         0.33390236]. \t  0.5082823660302883 \t 3.847790918157131\n",
      "51     \t [0.11705125 0.60644709 0.80980848]. \t  3.598173725789355 \t 3.847790918157131\n",
      "52     \t [0.0469073  0.57065684 0.87297561]. \t  3.7785238339133818 \t 3.847790918157131\n",
      "53     \t [0.15318273 0.63809932 0.94055891]. \t  2.9544506340806844 \t 3.847790918157131\n",
      "54     \t [0.16184387 0.58901879 0.88645656]. \t  3.7052065499523277 \t 3.847790918157131\n",
      "55     \t [0.33814282 0.53888113 0.87698888]. \t  3.7927451677081425 \t 3.847790918157131\n",
      "56     \t [0.06028896 0.50725533 0.7995812 ]. \t  3.5274702545999874 \t 3.847790918157131\n",
      "57     \t [0.07202099 0.56991988 0.78711419]. \t  3.4755743668919497 \t 3.847790918157131\n",
      "58     \t [0.31507888 0.54815323 0.81845873]. \t  3.7504107844497367 \t 3.847790918157131\n",
      "59     \t [0.23941217 0.60985372 0.92239041]. \t  3.31325468605987 \t 3.847790918157131\n",
      "60     \t [0.73090592 0.         0.85965113]. \t  0.22622928733499936 \t 3.847790918157131\n",
      "61     \t [0.25291047 0.50134377 0.87877027]. \t  3.68314006396718 \t 3.847790918157131\n",
      "62     \t [0.97867971 0.53592481 0.84564993]. \t  3.663710529817563 \t 3.847790918157131\n",
      "63     \t [0.12963703 0.55076909 0.85642362]. \t  3.8432660221043555 \t 3.847790918157131\n",
      "64     \t [0.05845565 0.58597231 0.81668321]. \t  3.681698357972299 \t 3.847790918157131\n",
      "65     \t [0.77374429 0.5678907  0.80133869]. \t  3.454951825395743 \t 3.847790918157131\n",
      "66     \t [0.20032656 0.51385866 0.90228229]. \t  3.540166562986497 \t 3.847790918157131\n",
      "67     \t [0.40010784 0.53122678 0.84825016]. \t  3.8360777647040067 \t 3.847790918157131\n",
      "68     \t [0.33329258 0.54664526 0.83139229]. \t  3.8145770739700966 \t 3.847790918157131\n",
      "69     \t [0.83101304 0.58747812 0.8334238 ]. \t  3.63226844690401 \t 3.847790918157131\n",
      "70     \t [0.68099706 0.59186827 0.85195425]. \t  3.736557827353513 \t 3.847790918157131\n",
      "71     \t [0.50216668 0.55018466 0.84890974]. \t  3.8378529121759537 \t 3.847790918157131\n",
      "72     \t [0.24954343 0.53897139 0.86664284]. \t  3.830473198949535 \t 3.847790918157131\n",
      "73     \t [0.28666939 0.48010564 0.8620845 ]. \t  3.65582150501466 \t 3.847790918157131\n",
      "74     \t [0.18871211 0.56982644 0.82247719]. \t  3.764083858012199 \t 3.847790918157131\n",
      "75     \t [0.90133866 0.80831698 0.78918792]. \t  1.6918050464881729 \t 3.847790918157131\n",
      "76     \t [0.38994577 0.53083879 0.86894058]. \t  3.8116274492381086 \t 3.847790918157131\n",
      "77     \t [0.5277165  0.65386089 0.88897478]. \t  3.4045380771848577 \t 3.847790918157131\n",
      "78     \t [0.19158302 0.59681087 0.89878592]. \t  3.597480156692398 \t 3.847790918157131\n",
      "79     \t [0.25753667 0.58244486 0.82938498]. \t  3.7810964644350458 \t 3.847790918157131\n",
      "80     \t [0.28262418 0.52692446 0.81849768]. \t  3.7322641428906804 \t 3.847790918157131\n",
      "81     \t [0.04029342 0.56346044 0.85726162]. \t  3.819585476375908 \t 3.847790918157131\n",
      "82     \t [0.77321297 0.56835633 0.86641472]. \t  3.7478068933842987 \t 3.847790918157131\n",
      "83     \t [0.73923531 0.51802981 0.92636515]. \t  3.2309253851231867 \t 3.847790918157131\n",
      "84     \t [0.72274968 0.60418878 0.84785165]. \t  3.674357254380366 \t 3.847790918157131\n",
      "85     \t [0.50307209 0.49125194 0.86759801]. \t  3.683046516533702 \t 3.847790918157131\n",
      "86     \t [0.06135312 0.57958481 0.82363923]. \t  3.733048858519077 \t 3.847790918157131\n",
      "87     \t [0.07161124 0.57118637 0.88215563]. \t  3.7401338172573286 \t 3.847790918157131\n",
      "88     \t [0.1511783  0.56633615 0.85894067]. \t  3.841747081893721 \t 3.847790918157131\n",
      "89     \t [0.36171231 0.53321685 0.84676057]. \t  3.8405973042314603 \t 3.847790918157131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.24957294 0.5080365  0.78131799]. \t  3.383918972588327 \t 3.847790918157131\n",
      "91     \t [0.71937626 0.54406077 0.92860849]. \t  3.2542654532939315 \t 3.847790918157131\n",
      "92     \t [0.35107005 0.50660794 0.78377531]. \t  3.3959523556745026 \t 3.847790918157131\n",
      "93     \t [0.4892291  0.56872635 0.83990144]. \t  3.812130597036334 \t 3.847790918157131\n",
      "94     \t [0.65954701 0.48516216 0.84579156]. \t  3.6481421946514487 \t 3.847790918157131\n",
      "95     \t [0.00883835 0.6013543  0.83189179]. \t  3.7006348348705727 \t 3.847790918157131\n",
      "96     \t [0.60226808 0.56527437 0.90002825]. \t  3.6142165519154896 \t 3.847790918157131\n",
      "97     \t [0.28897352 0.46675652 0.88104347]. \t  3.5019043779108125 \t 3.847790918157131\n",
      "98     \t [0.04275802 0.55625307 0.83663759]. \t  3.801224806991899 \t 3.847790918157131\n",
      "99     \t [0.76586006 0.55485107 0.85431554]. \t  3.7679018648531404 \t 3.847790918157131\n",
      "100    \t [0.3778065  0.51867278 0.8502434 ]. \t  3.8140234881610064 \t 3.847790918157131\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \n",
    "\n",
    "np.random.seed(run_num_6)\n",
    "surrogate_winner_6 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_6 = GPGO(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_6.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.15266373 0.30235661 0.06203641]. \t  0.1742288077468587 \t 1.6237282255098657\n",
      "init   \t [0.45986034 0.83525338 0.92699705]. \t  1.6237282255098657 \t 1.6237282255098657\n",
      "init   \t [0.72698898 0.76849622 0.26920507]. \t  0.08405715787489784 \t 1.6237282255098657\n",
      "init   \t [0.64402929 0.09337326 0.07968589]. \t  0.2756481743251193 \t 1.6237282255098657\n",
      "init   \t [0.58961375 0.34334054 0.98887615]. \t  1.4020548914621052 \t 1.6237282255098657\n",
      "1      \t [0. 0. 1.]. \t  0.0902894676548261 \t 1.6237282255098657\n",
      "2      \t [1. 1. 1.]. \t  0.31688362070415665 \t 1.6237282255098657\n",
      "3      \t [0.00000000e+00 1.00000000e+00 1.11022302e-16]. \t  0.0002735367680454468 \t 1.6237282255098657\n",
      "4      \t [-5.55111512e-17  1.00000000e+00  1.00000000e+00]. \t  0.330219860606422 \t 1.6237282255098657\n",
      "5      \t [1.         0.         0.72249648]. \t  0.21360949541744043 \t 1.6237282255098657\n",
      "6      \t [ 1.00000000e+00  1.00000000e+00 -1.11022302e-16]. \t  3.7727185179443814e-05 \t 1.6237282255098657\n",
      "7      \t [1.         0.27250743 0.        ]. \t  0.027871812301633225 \t 1.6237282255098657\n",
      "8      \t [0. 0. 0.]. \t  0.06797411659013229 \t 1.6237282255098657\n",
      "9      \t [0.         0.49622388 0.61811824]. \t  1.5139247246634346 \t 1.6237282255098657\n",
      "10     \t [0.29455581 0.         0.58484451]. \t  0.11777284994813261 \t 1.6237282255098657\n",
      "11     \t [0.12649853 1.         0.52130737]. \t  \u001b[92m2.4821007485599598\u001b[0m \t 2.4821007485599598\n",
      "12     \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.4821007485599598\n",
      "13     \t [1.         1.         0.52616608]. \t  0.24540055339219055 \t 2.4821007485599598\n",
      "14     \t [1.         0.51024329 1.        ]. \t  1.9458498121113932 \t 2.4821007485599598\n",
      "15     \t [1. 0. 0.]. \t  0.03095471703300515 \t 2.4821007485599598\n",
      "16     \t [0.44750691 1.         0.        ]. \t  0.00022481340943736122 \t 2.4821007485599598\n",
      "17     \t [0.10204634 0.53150125 1.        ]. \t  2.045673916245638 \t 2.4821007485599598\n",
      "18     \t [0.         0.         0.35941578]. \t  0.45080887548230886 \t 2.4821007485599598\n",
      "19     \t [1.         0.27761501 0.37297852]. \t  0.17715326080565633 \t 2.4821007485599598\n",
      "20     \t [0.55236173 1.         0.60241824]. \t  1.3812100841366663 \t 2.4821007485599598\n",
      "21     \t [0.37257485 0.52306732 0.66580092]. \t  1.9815039261997893 \t 2.4821007485599598\n",
      "22     \t [0.31739524 0.         0.        ]. \t  0.1014361568870034 \t 2.4821007485599598\n",
      "23     \t [0.         0.76152019 0.29360873]. \t  0.3411346879146718 \t 2.4821007485599598\n",
      "24     \t [0.         1.         0.67688557]. \t  1.7340696151515562 \t 2.4821007485599598\n",
      "25     \t [1.         0.68883174 0.74616456]. \t  2.0951118182550847 \t 2.4821007485599598\n",
      "26     \t [0.35996417 0.         1.        ]. \t  0.09172383506852846 \t 2.4821007485599598\n",
      "27     \t [0.26305266 1.         0.86911537]. \t  0.6713503231194738 \t 2.4821007485599598\n",
      "28     \t [0.         1.         0.28662487]. \t  0.28571859887707074 \t 2.4821007485599598\n",
      "29     \t [0.         0.71058219 0.9991886 ]. \t  1.6967709701076799 \t 2.4821007485599598\n",
      "30     \t [1.         0.         0.29079047]. \t  0.2598139868445362 \t 2.4821007485599598\n",
      "31     \t [0.38081868 0.70162084 0.        ]. \t  0.004017634706753112 \t 2.4821007485599598\n",
      "32     \t [0.87045833 0.66313113 1.        ]. \t  1.862029416830977 \t 2.4821007485599598\n",
      "33     \t [0.         0.36402267 0.        ]. \t  0.042364641265079606 \t 2.4821007485599598\n",
      "34     \t [1.         0.73046725 0.25296745]. \t  0.021816669527359077 \t 2.4821007485599598\n",
      "35     \t [1.75583452e-08 2.10445891e-02 7.38515024e-01]. \t  0.277421166124433 \t 2.4821007485599598\n",
      "36     \t [0.82566522 0.00219752 0.61197825]. \t  0.11582695424406726 \t 2.4821007485599598\n",
      "37     \t [0.2141689  0.80614971 0.65308446]. \t  \u001b[92m2.5964397409138105\u001b[0m \t 2.5964397409138105\n",
      "38     \t [0.         0.36695955 1.        ]. \t  1.3879199783749288 \t 2.5964397409138105\n",
      "39     \t [0.99999998 0.77820145 1.        ]. \t  1.2963659812183654 \t 2.5964397409138105\n",
      "40     \t [0.66757644 1.         1.        ]. \t  0.3285153580921568 \t 2.5964397409138105\n",
      "41     \t [9.50716775e-01 7.17047565e-01 7.94856833e-10]. \t  0.0011872263910326936 \t 2.5964397409138105\n",
      "42     \t [0.23645493 1.         0.13731269]. \t  0.013308175676397088 \t 2.5964397409138105\n",
      "43     \t [0.63999818 0.39922953 0.        ]. \t  0.04241803489653596 \t 2.5964397409138105\n",
      "44     \t [0.35923009 0.56158516 0.3560522 ]. \t  0.40268904014585255 \t 2.5964397409138105\n",
      "45     \t [0.79934489 0.73712071 0.68926693]. \t  1.4306313272695865 \t 2.5964397409138105\n",
      "46     \t [7.49737135e-01 1.16981451e-07 6.22750621e-08]. \t  0.06617334393592707 \t 2.5964397409138105\n",
      "47     \t [0.7902636  1.         0.16300456]. \t  0.006539825230532083 \t 2.5964397409138105\n",
      "48     \t [0.87567666 0.26495784 0.88752446]. \t  1.636136474385197 \t 2.5964397409138105\n",
      "49     \t [7.06763330e-01 2.52951544e-07 1.00000000e+00]. \t  0.09087194015628605 \t 2.5964397409138105\n",
      "50     \t [0.22028085 0.15471098 0.69344921]. \t  0.6512762903333255 \t 2.5964397409138105\n",
      "51     \t [4.08581162e-07 7.49346056e-01 7.93189957e-08]. \t  0.0016729022297932172 \t 2.5964397409138105\n",
      "52     \t [1.         0.50026318 0.82931526]. \t  \u001b[92m3.531050096678842\u001b[0m \t 3.531050096678842\n",
      "53     \t [0.         0.75143864 0.76212039]. \t  2.5086617229970622 \t 3.531050096678842\n",
      "54     \t [0.98256985 0.4875633  0.83619073]. \t  3.5233265066367796 \t 3.531050096678842\n",
      "55     \t [0.13136249 0.64733307 0.80588447]. \t  3.4019362796448487 \t 3.531050096678842\n",
      "56     \t [0.28854607 0.56457628 0.80121012]. \t  \u001b[92m3.6211391294663913\u001b[0m \t 3.6211391294663913\n",
      "57     \t [0.25511949 0.5339568  0.91524758]. \t  3.4526899417095476 \t 3.6211391294663913\n",
      "58     \t [0.38696314 0.58504357 0.84403361]. \t  \u001b[92m3.8129380437893245\u001b[0m \t 3.8129380437893245\n",
      "59     \t [0.30983316 0.54263777 0.86647888]. \t  \u001b[92m3.83722875791526\u001b[0m \t 3.83722875791526\n",
      "60     \t [0.27358538 0.60648733 0.82194975]. \t  3.679291792400992 \t 3.83722875791526\n",
      "61     \t [0.75245449 0.53520935 0.90714353]. \t  3.493155986638841 \t 3.83722875791526\n",
      "62     \t [0.60491442 0.5829491  0.86498069]. \t  3.778503004055379 \t 3.83722875791526\n",
      "63     \t [0.33211168 0.42871656 0.8174478 ]. \t  3.2798010743818917 \t 3.83722875791526\n",
      "64     \t [0.52723276 0.54418996 0.88793047]. \t  3.718497073835522 \t 3.83722875791526\n",
      "65     \t [0.61651109 0.53078658 0.88652691]. \t  3.693418145664878 \t 3.83722875791526\n",
      "66     \t [0.10685633 0.4201238  0.84077208]. \t  3.254626847815607 \t 3.83722875791526\n",
      "67     \t [0.17555335 0.62157021 0.8381856 ]. \t  3.683950582762198 \t 3.83722875791526\n",
      "68     \t [0.17641929 0.4846253  0.81569628]. \t  3.583938329419495 \t 3.83722875791526\n",
      "69     \t [0.43048141 0.59224318 0.90225983]. \t  3.5812824004322286 \t 3.83722875791526\n",
      "70     \t [0.23860644 0.66366059 0.87201011]. \t  3.4425464492855973 \t 3.83722875791526\n",
      "71     \t [0.2410097  0.54972405 0.86637305]. \t  \u001b[92m3.8397860495791347\u001b[0m \t 3.8397860495791347\n",
      "72     \t [0.57250866 0.48978445 0.89326673]. \t  3.5210867781983075 \t 3.8397860495791347\n",
      "73     \t [0.53861597 0.44641865 0.85545538]. \t  3.453175761462283 \t 3.8397860495791347\n",
      "74     \t [0.40986174 0.49504099 0.8598248 ]. \t  3.727721661366961 \t 3.8397860495791347\n",
      "75     \t [0.46005619 0.63688966 0.82287113]. \t  3.511395234551427 \t 3.8397860495791347\n",
      "76     \t [0.71140475 0.47609772 0.8409448 ]. \t  3.5850207350519465 \t 3.8397860495791347\n",
      "77     \t [0.52350357 0.49301625 0.84890562]. \t  3.7128242872194104 \t 3.8397860495791347\n",
      "78     \t [0.50979905 0.51869445 0.89015176]. \t  3.6589329744007264 \t 3.8397860495791347\n",
      "79     \t [0.20404984 0.56419343 0.89716421]. \t  3.6594639939013054 \t 3.8397860495791347\n",
      "80     \t [0.45000358 0.47408458 0.88754588]. \t  3.4996528914979375 \t 3.8397860495791347\n",
      "81     \t [0.44280485 0.63711458 0.86091334]. \t  3.6117032110521023 \t 3.8397860495791347\n",
      "82     \t [0.67727952 0.54831169 0.90166368]. \t  3.5810933739950856 \t 3.8397860495791347\n",
      "83     \t [0.23855087 0.54897999 0.90258909]. \t  3.609302021752424 \t 3.8397860495791347\n",
      "84     \t [0.22343934 0.49960566 0.82614201]. \t  3.6994351844813487 \t 3.8397860495791347\n",
      "85     \t [0.20601977 0.66581318 0.83807222]. \t  3.4382841043578454 \t 3.8397860495791347\n",
      "86     \t [0.41685038 0.44945009 0.83287734]. \t  3.4715623130968685 \t 3.8397860495791347\n",
      "87     \t [0.51265442 0.51873363 0.88156489]. \t  3.7155059248687343 \t 3.8397860495791347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.3552446  0.59906634 0.85134439]. \t  3.789204011969339 \t 3.8397860495791347\n",
      "89     \t [0.3600519  0.58110092 0.79378462]. \t  3.5206209014183907 \t 3.8397860495791347\n",
      "90     \t [0.47394487 0.48791573 0.81130644]. \t  3.5580051430415844 \t 3.8397860495791347\n",
      "91     \t [0.31999881 0.64126366 0.84543475]. \t  3.599968030079599 \t 3.8397860495791347\n",
      "92     \t [0.66885226 0.57050478 0.88276053]. \t  3.7187650958225165 \t 3.8397860495791347\n",
      "93     \t [0.24190681 0.47435571 0.94809099]. \t  2.781438865751517 \t 3.8397860495791347\n",
      "94     \t [0.45396944 0.48853578 0.85343547]. \t  3.7034157776544046 \t 3.8397860495791347\n",
      "95     \t [0.59559431 0.60536316 0.87923851]. \t  3.675957651663423 \t 3.8397860495791347\n",
      "96     \t [0.38164947 0.6227622  0.88162782]. \t  3.630615361188895 \t 3.8397860495791347\n",
      "97     \t [0.29531631 0.59319113 0.81554321]. \t  3.679480612979479 \t 3.8397860495791347\n",
      "98     \t [0.79655235 0.52445673 0.79255321]. \t  3.3723694054037994 \t 3.8397860495791347\n",
      "99     \t [0.3727917  0.56240331 0.82791641]. \t  3.792527699224668 \t 3.8397860495791347\n",
      "100    \t [0.42800467 0.55021692 0.85076079]. \t  \u001b[92m3.851574397924254\u001b[0m \t 3.851574397924254\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \n",
    "\n",
    "np.random.seed(run_num_7)\n",
    "surrogate_winner_7 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_7 = GPGO(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_7.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.10949987 0.04221501 0.59969258]. \t  0.15732379460832624 \t 0.8830091449513892\n",
      "init   \t [0.29987071 0.79661178 0.36619613]. \t  0.8830091449513892 \t 0.8830091449513892\n",
      "init   \t [0.88060834 0.29784163 0.32910326]. \t  0.2992873424905509 \t 0.8830091449513892\n",
      "init   \t [0.56871692 0.74392742 0.05385289]. \t  0.0051668955191694075 \t 0.8830091449513892\n",
      "init   \t [0.60207437 0.42931858 0.20547361]. \t  0.2908249205255991 \t 0.8830091449513892\n",
      "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 0.8830091449513892\n",
      "2      \t [0. 1. 1.]. \t  0.330219860606422 \t 0.8830091449513892\n",
      "3      \t [1. 0. 1.]. \t  0.08848201872702738 \t 0.8830091449513892\n",
      "4      \t [0. 0. 0.]. \t  0.06797411659013229 \t 0.8830091449513892\n",
      "5      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 0.8830091449513892\n",
      "6      \t [1. 0. 0.]. \t  0.03095471703300515 \t 0.8830091449513892\n",
      "7      \t [0.         0.40281298 1.        ]. \t  \u001b[92m1.577398703894718\u001b[0m \t 1.577398703894718\n",
      "8      \t [0.52680219 0.56127784 1.        ]. \t  \u001b[92m2.0826770053794093\u001b[0m \t 2.0826770053794093\n",
      "9      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.0826770053794093\n",
      "10     \t [0.32979423 0.         1.        ]. \t  0.09169400337928031 \t 2.0826770053794093\n",
      "11     \t [0.         0.50802663 0.        ]. \t  0.01696939683850839 \t 2.0826770053794093\n",
      "12     \t [1.         0.51314254 1.        ]. \t  1.9520026769983667 \t 2.0826770053794093\n",
      "13     \t [1.         0.50262776 0.        ]. \t  0.008030005474337933 \t 2.0826770053794093\n",
      "14     \t [1.         1.         0.53149803]. \t  0.2489115989124939 \t 2.0826770053794093\n",
      "15     \t [0. 0. 1.]. \t  0.0902894676548261 \t 2.0826770053794093\n",
      "16     \t [0.         1.         0.55708965]. \t  \u001b[92m2.4912206709265994\u001b[0m \t 2.4912206709265994\n",
      "17     \t [0.         0.69604004 0.67209009]. \t  2.4712979131567887 \t 2.4912206709265994\n",
      "18     \t [0.47929175 0.         0.        ]. \t  0.09857607023556501 \t 2.4912206709265994\n",
      "19     \t [0.55443207 1.         0.73358723]. \t  0.829507234449324 \t 2.4912206709265994\n",
      "20     \t [0.69498014 0.34707796 0.795454  ]. \t  2.4708047441107617 \t 2.4912206709265994\n",
      "21     \t [0.76035402 0.27018269 1.        ]. \t  0.8688582142719262 \t 2.4912206709265994\n",
      "22     \t [0.26678727 0.47078174 0.8003062 ]. \t  \u001b[92m3.423735682593871\u001b[0m \t 3.423735682593871\n",
      "23     \t [1.         0.         0.60845477]. \t  0.10330258694937736 \t 3.423735682593871\n",
      "24     \t [1.         0.59253967 0.72361006]. \t  2.2214118299440084 \t 3.423735682593871\n",
      "25     \t [0.40307726 1.         0.        ]. \t  0.00024114076412258388 \t 3.423735682593871\n",
      "26     \t [0.17455816 0.79946148 0.79317435]. \t  2.2585849726189187 \t 3.423735682593871\n",
      "27     \t [1.         0.3115342  0.79678143]. \t  2.10388906410551 \t 3.423735682593871\n",
      "28     \t [0.         0.30073338 0.72463872]. \t  1.6220430932964924 \t 3.423735682593871\n",
      "29     \t [0.72464368 1.         0.23936456]. \t  0.04178593786716591 \t 3.423735682593871\n",
      "30     \t [0.23379935 0.21291121 0.        ]. \t  0.10124728150421389 \t 3.423735682593871\n",
      "31     \t [0.49646665 1.         1.        ]. \t  0.3323826622009877 \t 3.423735682593871\n",
      "32     \t [0.         0.41892605 0.3327433 ]. \t  0.32175559421768696 \t 3.423735682593871\n",
      "33     \t [0.748927   0.36551374 0.        ]. \t  0.04100048098672926 \t 3.423735682593871\n",
      "34     \t [0.4103277  0.28832443 0.61644961]. \t  0.7306145525423839 \t 3.423735682593871\n",
      "35     \t [0.84207229 0.65260967 0.88712168]. \t  3.3344997175501234 \t 3.423735682593871\n",
      "36     \t [0.78497491 0.55548559 0.77874283]. \t  3.209896925098927 \t 3.423735682593871\n",
      "37     \t [0.05774072 1.         0.19700631]. \t  0.05482786446179775 \t 3.423735682593871\n",
      "38     \t [0.20173677 0.2762691  0.92686143]. \t  1.522256076218987 \t 3.423735682593871\n",
      "39     \t [0.71572551 0.         0.78198857]. \t  0.24901129754705387 \t 3.423735682593871\n",
      "40     \t [0.99551402 0.86359976 0.22201311]. \t  0.01136723444315357 \t 3.423735682593871\n",
      "41     \t [8.43593747e-07 3.38923410e-08 2.53181371e-01]. \t  0.5764245421487246 \t 3.423735682593871\n",
      "42     \t [0.4981026  0.60456329 0.78105321]. \t  3.2782026060589518 \t 3.423735682593871\n",
      "43     \t [1.45617845e-08 9.99999982e-01 7.35300841e-01]. \t  1.2169974841964306 \t 3.423735682593871\n",
      "44     \t [0.17700311 0.60665401 0.66362562]. \t  2.309770949400161 \t 3.423735682593871\n",
      "45     \t [6.69641968e-01 1.98306124e-07 2.04047945e-01]. \t  0.589655145504733 \t 3.423735682593871\n",
      "46     \t [0.71922348 0.72719444 0.67679585]. \t  1.5111013207661925 \t 3.423735682593871\n",
      "47     \t [0.45564106 0.57293319 0.83650464]. \t  \u001b[92m3.8025464071923736\u001b[0m \t 3.8025464071923736\n",
      "48     \t [0.54489838 0.59692741 0.86589476]. \t  3.7561086019849825 \t 3.8025464071923736\n",
      "49     \t [0.60586943 0.51849646 0.86115219]. \t  3.772623060705572 \t 3.8025464071923736\n",
      "50     \t [0.72866486 0.53382209 0.91777023]. \t  3.377980622193749 \t 3.8025464071923736\n",
      "51     \t [0.40286212 0.57941246 0.81202679]. \t  3.6672549219615 \t 3.8025464071923736\n",
      "52     \t [0.51928478 0.55947661 0.84022249]. \t  \u001b[92m3.8145555387885737\u001b[0m \t 3.8145555387885737\n",
      "53     \t [0.9932032  0.02987466 0.21659039]. \t  0.26653667486441907 \t 3.8145555387885737\n",
      "54     \t [0.57186717 0.57785783 0.82427591]. \t  3.7037939103422906 \t 3.8145555387885737\n",
      "55     \t [0.12974052 0.73669712 0.        ]. \t  0.0023658272823484723 \t 3.8145555387885737\n",
      "56     \t [0.51866603 0.54593488 0.83646783]. \t  3.8045473121396887 \t 3.8145555387885737\n",
      "57     \t [0.55480589 0.49092082 0.88741311]. \t  3.5729652953829993 \t 3.8145555387885737\n",
      "58     \t [0.41350698 0.58417863 0.82564036]. \t  3.7419348296975317 \t 3.8145555387885737\n",
      "59     \t [2.28586295e-01 2.25981760e-04 2.24491394e-01]. \t  0.7785529913622037 \t 3.8145555387885737\n",
      "60     \t [0.21829871 0.58297942 0.85185433]. \t  \u001b[92m3.83122810254845\u001b[0m \t 3.83122810254845\n",
      "61     \t [0.3502787  0.66268268 0.90009295]. \t  3.2930630271094605 \t 3.83122810254845\n",
      "62     \t [0.87278937 0.44551315 0.89059078]. \t  3.2168597926125453 \t 3.83122810254845\n",
      "63     \t [0.70163136 0.53335439 0.87212908]. \t  3.749145143195598 \t 3.83122810254845\n",
      "64     \t [0.67271374 0.52706591 0.8801307 ]. \t  3.7110615764052812 \t 3.83122810254845\n",
      "65     \t [0.39276016 0.58752677 0.86321599]. \t  3.810722524192706 \t 3.83122810254845\n",
      "66     \t [0.50253181 0.6158468  0.74553622]. \t  2.8269110760606164 \t 3.83122810254845\n",
      "67     \t [0.48432796 0.55004989 0.79642198]. \t  3.5406313284643423 \t 3.83122810254845\n",
      "68     \t [0.10752413 0.55976668 0.90634732]. \t  3.556637187954875 \t 3.83122810254845\n",
      "69     \t [0.49426608 0.54625266 0.76798283]. \t  3.2201153815803822 \t 3.83122810254845\n",
      "70     \t [0.29913789 0.55688478 0.88784953]. \t  3.740407344061839 \t 3.83122810254845\n",
      "71     \t [0.68576242 0.67436051 0.88911054]. \t  3.2284370884314835 \t 3.83122810254845\n",
      "72     \t [0.29206334 0.52581095 0.83811972]. \t  3.8145029161418647 \t 3.83122810254845\n",
      "73     \t [0.65374409 0.48195269 0.86115928]. \t  3.6290187462189936 \t 3.83122810254845\n",
      "74     \t [0.33566488 0.55382198 0.92684707]. \t  3.32955436056712 \t 3.83122810254845\n",
      "75     \t [0.37212223 0.55754695 0.87933418]. \t  3.7917278305402933 \t 3.83122810254845\n",
      "76     \t [0.23709052 0.59848574 0.87599191]. \t  3.746745174512352 \t 3.83122810254845\n",
      "77     \t [0.74005198 0.53998498 0.88628689]. \t  3.6769685031755497 \t 3.83122810254845\n",
      "78     \t [0.2883582  0.52756233 0.89764901]. \t  3.627776760090991 \t 3.83122810254845\n",
      "79     \t [0.35311828 0.55108818 0.84034606]. \t  \u001b[92m3.8434418063933053\u001b[0m \t 3.8434418063933053\n",
      "80     \t [0.48449423 0.50569442 0.87314253]. \t  3.721354955441691 \t 3.8434418063933053\n",
      "81     \t [0.82312171 0.48629612 0.9142881 ]. \t  3.2431994307631773 \t 3.8434418063933053\n",
      "82     \t [0.40943426 0.49718776 0.86570501]. \t  3.7228340813492276 \t 3.8434418063933053\n",
      "83     \t [0.87306617 0.48154108 0.83261162]. \t  3.5320706967237703 \t 3.8434418063933053\n",
      "84     \t [0.04449301 0.61279943 0.80339196]. \t  3.5138867411207233 \t 3.8434418063933053\n",
      "85     \t [0.29101632 0.52363381 0.84969161]. \t  3.8273325580104 \t 3.8434418063933053\n",
      "86     \t [0.50079545 0.57211675 0.82560825]. \t  3.7431512387027466 \t 3.8434418063933053\n",
      "87     \t [0.39165263 0.5831688  0.84318725]. \t  3.814531929588499 \t 3.8434418063933053\n",
      "88     \t [0.29305678 0.55948981 0.83491252]. \t  3.829359519930548 \t 3.8434418063933053\n",
      "89     \t [0.33670334 0.53929127 0.83206581]. \t  3.8122884307432807 \t 3.8434418063933053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.21999986 0.62116361 0.83894622]. \t  3.690704846217956 \t 3.8434418063933053\n",
      "91     \t [0.41227442 0.57998277 0.90276412]. \t  3.600077969092219 \t 3.8434418063933053\n",
      "92     \t [0.55721493 0.58055509 0.85675384]. \t  3.8023953995610937 \t 3.8434418063933053\n",
      "93     \t [0.20705158 0.51730406 0.83333512]. \t  3.77795985060964 \t 3.8434418063933053\n",
      "94     \t [0.72649696 0.52685863 0.82643375]. \t  3.682162250096872 \t 3.8434418063933053\n",
      "95     \t [0.24505424 0.62794901 0.92736783]. \t  3.1864252583884465 \t 3.8434418063933053\n",
      "96     \t [0.33348557 0.55633974 0.82812494]. \t  3.8009115236441464 \t 3.8434418063933053\n",
      "97     \t [0.03641205 0.55854808 0.79936474]. \t  3.5811697456760556 \t 3.8434418063933053\n",
      "98     \t [0.3865261  0.57123201 0.89047821]. \t  3.7150155424359044 \t 3.8434418063933053\n",
      "99     \t [0.17548675 0.56077158 0.84874857]. \t  \u001b[92m3.8509526525165247\u001b[0m \t 3.8509526525165247\n",
      "100    \t [0.35474914 0.48793582 0.8946866 ]. \t  3.518024571783889 \t 3.8509526525165247\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \n",
    "\n",
    "np.random.seed(run_num_8)\n",
    "surrogate_winner_8 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_8 = GPGO(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_8.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.67227856 0.4880784  0.82549517]. \t  3.595021899183128 \t 3.595021899183128\n",
      "init   \t [0.03144639 0.80804996 0.56561742]. \t  2.9633561694281085 \t 3.595021899183128\n",
      "init   \t [0.2976225  0.04669572 0.9906274 ]. \t  0.16382388103073592 \t 3.595021899183128\n",
      "init   \t [0.00682573 0.76979303 0.7467671 ]. \t  2.382987807172393 \t 3.595021899183128\n",
      "init   \t [0.37743894 0.49414745 0.92894839]. \t  3.1588932929069533 \t 3.595021899183128\n",
      "1      \t [0.60810261 1.         0.        ]. \t  0.00015916632875064 \t 3.595021899183128\n",
      "2      \t [0.81207838 1.         1.        ]. \t  0.32406564987092407 \t 3.595021899183128\n",
      "3      \t [0.19982548 0.22129312 0.        ]. \t  0.09652382419658188 \t 3.595021899183128\n",
      "4      \t [1.         0.12367194 1.        ]. \t  0.2959595021879816 \t 3.595021899183128\n",
      "5      \t [0.45857959 0.58579366 0.58121802]. \t  1.3988187647550658 \t 3.595021899183128\n",
      "6      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.595021899183128\n",
      "7      \t [1.         0.58705711 1.        ]. \t  2.001097429186462 \t 3.595021899183128\n",
      "8      \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.595021899183128\n",
      "9      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 3.595021899183128\n",
      "10     \t [0.         1.         0.59884734]. \t  2.378804547383393 \t 3.595021899183128\n",
      "11     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.595021899183128\n",
      "12     \t [0.67806528 0.43739445 1.        ]. \t  1.7536273955857167 \t 3.595021899183128\n",
      "13     \t [1.         0.         0.60197599]. \t  0.09805119546784562 \t 3.595021899183128\n",
      "14     \t [1.         1.         0.66730708]. \t  0.3004359535412881 \t 3.595021899183128\n",
      "15     \t [0.         0.26702107 0.69599736]. \t  1.2022729062977222 \t 3.595021899183128\n",
      "16     \t [1.         0.46766932 0.        ]. \t  0.010383635767332205 \t 3.595021899183128\n",
      "17     \t [1.         0.41903064 0.79161669]. \t  2.8995473372635634 \t 3.595021899183128\n",
      "18     \t [0.         0.47344612 0.        ]. \t  0.02193527095368942 \t 3.595021899183128\n",
      "19     \t [0.48569703 0.         0.7514548 ]. \t  0.24061106091430115 \t 3.595021899183128\n",
      "20     \t [0.51157015 0.7599667  0.81315109]. \t  2.495249742725835 \t 3.595021899183128\n",
      "21     \t [0.35191288 0.43803808 0.80853431]. \t  3.2953907335373445 \t 3.595021899183128\n",
      "22     \t [0.64504416 0.         0.        ]. \t  0.08133858794269971 \t 3.595021899183128\n",
      "23     \t [0.         1.         0.34443675]. \t  0.6547164192605748 \t 3.595021899183128\n",
      "24     \t [1.         0.78924196 0.86831249]. \t  2.143597563779279 \t 3.595021899183128\n",
      "25     \t [0.         0.         0.82278896]. \t  0.24305585694067663 \t 3.595021899183128\n",
      "26     \t [0.61023129 0.57511365 0.        ]. \t  0.012128720134945935 \t 3.595021899183128\n",
      "27     \t [0.00000000e+00 3.46148582e-09 3.41810999e-01]. \t  0.49171930704481254 \t 3.595021899183128\n",
      "28     \t [0.1541383 1.        1.       ]. \t  0.33347602382516295 \t 3.595021899183128\n",
      "29     \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.595021899183128\n",
      "30     \t [0.81087644 0.22520353 0.        ]. \t  0.058043016240430956 \t 3.595021899183128\n",
      "31     \t [0. 0. 1.]. \t  0.0902894676548261 \t 3.595021899183128\n",
      "32     \t [2.00128872e-01 8.35319535e-01 2.07457769e-13]. \t  0.0009007970411281137 \t 3.595021899183128\n",
      "33     \t [0.28670284 1.         0.70034292]. \t  1.4400768613254515 \t 3.595021899183128\n",
      "34     \t [0.99999999 0.72068224 0.31689589]. \t  0.04848726473423949 \t 3.595021899183128\n",
      "35     \t [0.83515997 0.50278933 0.76357168]. \t  2.978858929114912 \t 3.595021899183128\n",
      "36     \t [8.04158534e-01 3.46944694e-18 2.54310375e-01]. \t  0.49155423483844024 \t 3.595021899183128\n",
      "37     \t [0.         0.55098419 0.51371846]. \t  1.1716704445033812 \t 3.595021899183128\n",
      "38     \t [0.27951617 0.         0.22155096]. \t  0.7996560056054839 \t 3.595021899183128\n",
      "39     \t [0.58887929 0.25782775 0.72137411]. \t  1.330716747865608 \t 3.595021899183128\n",
      "40     \t [0.18600197 0.61667933 0.77234769]. \t  3.2594079061326147 \t 3.595021899183128\n",
      "41     \t [0.8013149  0.91756802 0.1928872 ]. \t  0.014560874565761732 \t 3.595021899183128\n",
      "42     \t [0.01795892 0.33146497 0.99647587]. \t  1.2331661936137746 \t 3.595021899183128\n",
      "43     \t [0.86731969 0.         0.94769552]. \t  0.14174752401367152 \t 3.595021899183128\n",
      "44     \t [5.67249596e-08 1.00000000e+00 1.00000000e+00]. \t  0.3302198622501698 \t 3.595021899183128\n",
      "45     \t [0.83138833 0.74881404 0.02186354]. \t  0.0017026502578071868 \t 3.595021899183128\n",
      "46     \t [0.80411057 0.57710657 0.80404795]. \t  3.449422087397721 \t 3.595021899183128\n",
      "47     \t [0.98740581 0.0734196  0.26661125]. \t  0.3115438193984831 \t 3.595021899183128\n",
      "48     \t [0.93244003 0.46399201 0.89647976]. \t  3.266708323635118 \t 3.595021899183128\n",
      "49     \t [2.95229493e-08 7.90459338e-01 0.00000000e+00]. \t  0.0011005800852362322 \t 3.595021899183128\n",
      "50     \t [0.09630916 0.42444161 0.16331493]. \t  0.22885738315827267 \t 3.595021899183128\n",
      "51     \t [0.15603173 0.77997407 0.7701499 ]. \t  2.38030466218847 \t 3.595021899183128\n",
      "52     \t [1. 0. 1.]. \t  0.08848201872702738 \t 3.595021899183128\n",
      "53     \t [0.85006677 0.49353113 0.80607524]. \t  3.4229562451045696 \t 3.595021899183128\n",
      "54     \t [0.90804461 0.50067247 0.83705486]. \t  \u001b[92m3.602887612372074\u001b[0m \t 3.602887612372074\n",
      "55     \t [0.99512841 0.53160337 0.88575637]. \t  3.57410693852952 \t 3.602887612372074\n",
      "56     \t [0.86859572 0.61560732 0.83685212]. \t  3.5293642831300764 \t 3.602887612372074\n",
      "57     \t [0.87095552 0.55606865 0.88069051]. \t  \u001b[92m3.671598841611249\u001b[0m \t 3.671598841611249\n",
      "58     \t [0.83856489 0.54880585 0.86013067]. \t  \u001b[92m3.740842494150444\u001b[0m \t 3.740842494150444\n",
      "59     \t [0.71589021 0.45645852 0.84589166]. \t  3.483945774794558 \t 3.740842494150444\n",
      "60     \t [0.90090496 0.50441812 0.83703889]. \t  3.61676373140525 \t 3.740842494150444\n",
      "61     \t [0.39061094 0.49745454 0.82341429]. \t  3.6766933939319237 \t 3.740842494150444\n",
      "62     \t [0.19497865 0.39743978 0.84016515]. \t  3.079233315090682 \t 3.740842494150444\n",
      "63     \t [0.52973179 0.33840557 0.90856453]. \t  2.2256997719034795 \t 3.740842494150444\n",
      "64     \t [0.70725022 0.57121303 0.87027469]. \t  \u001b[92m3.757391981789638\u001b[0m \t 3.757391981789638\n",
      "65     \t [0.58124421 0.5797453  0.90268673]. \t  3.580175109998657 \t 3.757391981789638\n",
      "66     \t [0.2695232  0.63343593 0.83922821]. \t  3.6329264800862067 \t 3.757391981789638\n",
      "67     \t [0.33649286 0.52556969 0.8960267 ]. \t  3.639064702093258 \t 3.757391981789638\n",
      "68     \t [0.49265462 0.54289676 0.85190865]. \t  \u001b[92m3.838836456729617\u001b[0m \t 3.838836456729617\n",
      "69     \t [0.59369681 0.60027081 0.89897404]. \t  3.566634584584764 \t 3.838836456729617\n",
      "70     \t [0.71546133 0.6560008  0.85788179]. \t  3.419583093069522 \t 3.838836456729617\n",
      "71     \t [0.82240087 0.64670274 0.8592813 ]. \t  3.439625769398138 \t 3.838836456729617\n",
      "72     \t [0.51982837 0.49132132 0.88432192]. \t  3.600376688012345 \t 3.838836456729617\n",
      "73     \t [0.58572198 0.5625836  0.81101365]. \t  3.628205772822818 \t 3.838836456729617\n",
      "74     \t [0.30936578 0.52824833 0.87581893]. \t  3.7806163699640045 \t 3.838836456729617\n",
      "75     \t [0.39787997 0.54005518 0.75942001]. \t  3.151896705429179 \t 3.838836456729617\n",
      "76     \t [0.2789821  0.49847214 0.77995761]. \t  3.3424480381051658 \t 3.838836456729617\n",
      "77     \t [0.761864   0.60707749 0.81926702]. \t  3.5059262058324276 \t 3.838836456729617\n",
      "78     \t [0.40354863 0.59055204 0.88115388]. \t  3.7409452030302797 \t 3.838836456729617\n",
      "79     \t [0.37409206 0.59439558 0.82485044]. \t  3.7197502427248272 \t 3.838836456729617\n",
      "80     \t [0.55347369 0.56568787 0.8204301 ]. \t  3.704858272070446 \t 3.838836456729617\n",
      "81     \t [0.28492512 0.60087947 0.85594908]. \t  3.787817257684213 \t 3.838836456729617\n",
      "82     \t [0.72756456 0.61578316 0.86521054]. \t  3.6357957097530953 \t 3.838836456729617\n",
      "83     \t [0.83736613 0.59424936 0.87826192]. \t  3.640042003225873 \t 3.838836456729617\n",
      "84     \t [0.35361067 0.58270226 0.90982823]. \t  3.526368401282176 \t 3.838836456729617\n",
      "85     \t [0.26626138 0.59676413 0.80477505]. \t  3.596456532816174 \t 3.838836456729617\n",
      "86     \t [0.4386891  0.54777888 0.82256148]. \t  3.757262555758777 \t 3.838836456729617\n",
      "87     \t [0.89963684 0.49336545 0.80722905]. \t  3.4120726699131882 \t 3.838836456729617\n",
      "88     \t [0.31027025 0.53257136 0.87874984]. \t  3.7740387723598445 \t 3.838836456729617\n",
      "89     \t [0.31097836 0.48427554 0.9049251 ]. \t  3.403836012698526 \t 3.838836456729617\n",
      "90     \t [0.37092138 0.58426769 0.83262281]. \t  3.7819908722749616 \t 3.838836456729617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91     \t [0.17866488 0.69459811 0.83812507]. \t  3.226805142623942 \t 3.838836456729617\n",
      "92     \t [0.75606547 0.51797537 0.88539366]. \t  3.636895208638631 \t 3.838836456729617\n",
      "93     \t [0.40110135 0.57232548 0.88431382]. \t  3.754170892017967 \t 3.838836456729617\n",
      "94     \t [0.5180887  0.63460569 0.84094922]. \t  3.584888962683578 \t 3.838836456729617\n",
      "95     \t [0.53480306 0.62078735 0.87953675]. \t  3.627468695131249 \t 3.838836456729617\n",
      "96     \t [0.50782101 0.49603556 0.81465066]. \t  3.6025319397577764 \t 3.838836456729617\n",
      "97     \t [0.88576153 0.51839394 0.89410184]. \t  3.533425570594232 \t 3.838836456729617\n",
      "98     \t [0.50772457 0.53934594 0.88284166]. \t  3.7486082055621224 \t 3.838836456729617\n",
      "99     \t [0.33868623 0.58515162 0.84729604]. \t  3.8231828474717817 \t 3.838836456729617\n",
      "100    \t [0.57165835 0.65089202 0.89220416]. \t  3.394374575159 \t 3.838836456729617\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \n",
    "\n",
    "np.random.seed(run_num_9)\n",
    "surrogate_winner_9 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_9 = GPGO(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_9.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.65358959 0.11500694 0.95028286]. \t  0.42730315147591735 \t 1.1029187088185965\n",
      "init   \t [0.4821914  0.87247454 0.21233268]. \t  0.06161964400032635 \t 1.1029187088185965\n",
      "init   \t [0.04070962 0.39719446 0.2331322 ]. \t  0.33269334660262956 \t 1.1029187088185965\n",
      "init   \t [0.84174072 0.20708234 0.74246953]. \t  1.1029187088185965 \t 1.1029187088185965\n",
      "init   \t [0.39215413 0.18225652 0.74353941]. \t  0.9779763535009853 \t 1.1029187088185965\n",
      "1      \t [1. 0. 0.]. \t  0.03095471703300515 \t 1.1029187088185965\n",
      "2      \t [1. 1. 1.]. \t  0.316883620704157 \t 1.1029187088185965\n",
      "3      \t [5.55111512e-17 1.00000000e+00 1.00000000e+00]. \t  0.330219860606422 \t 1.1029187088185965\n",
      "4      \t [ 1.00000000e+00  1.00000000e+00 -5.55111512e-17]. \t  3.7727185179443895e-05 \t 1.1029187088185965\n",
      "5      \t [ 0.00000000e+00 -1.38777878e-17  1.00000000e+00]. \t  0.0902894676548261 \t 1.1029187088185965\n",
      "6      \t [0. 0. 0.]. \t  0.06797411659013229 \t 1.1029187088185965\n",
      "7      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 1.1029187088185965\n",
      "8      \t [0.         1.         0.51363374]. \t  \u001b[92m2.3551105723146892\u001b[0m \t 2.3551105723146892\n",
      "9      \t [1.         0.         0.61904786]. \t  0.11254445560045993 \t 2.3551105723146892\n",
      "10     \t [1.         1.         0.51265218]. \t  0.23544148954532035 \t 2.3551105723146892\n",
      "11     \t [-2.77555756e-17  5.12316151e-01  1.00000000e+00]. \t  1.996982288936772 \t 2.3551105723146892\n",
      "12     \t [1.         0.49100531 1.        ]. \t  1.8975859714767676 \t 2.3551105723146892\n",
      "13     \t [0.51685311 0.743424   1.        ]. \t  1.5381401267866215 \t 2.3551105723146892\n",
      "14     \t [1.         0.51897904 0.        ]. \t  0.007061389822418306 \t 2.3551105723146892\n",
      "15     \t [0.50676632 0.         0.        ]. \t  0.09657946518981401 \t 2.3551105723146892\n",
      "16     \t [0.         0.         0.48245749]. \t  0.16100460632985336 \t 2.3551105723146892\n",
      "17     \t [0.         0.70459472 0.69665166]. \t  \u001b[92m2.511381096384519\u001b[0m \t 2.511381096384519\n",
      "18     \t [0.37849992 1.         0.7274055 ]. \t  1.1147863730354262 \t 2.511381096384519\n",
      "19     \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.511381096384519\n",
      "20     \t [1.         0.37062353 0.35198302]. \t  0.13962161489610664 \t 2.511381096384519\n",
      "21     \t [1.         0.69658431 0.75641934]. \t  2.1743524168758466 \t 2.511381096384519\n",
      "22     \t [0.54199863 1.         0.        ]. \t  0.0001866941044612095 \t 2.511381096384519\n",
      "23     \t [0.         0.30679692 0.76474867]. \t  1.9537268498688085 \t 2.511381096384519\n",
      "24     \t [0.32921402 0.50752897 0.        ]. \t  0.025458459996733664 \t 2.511381096384519\n",
      "25     \t [0.74548493 0.75128845 0.74553805]. \t  1.8993822200941182 \t 2.511381096384519\n",
      "26     \t [0.         0.61295448 0.        ]. \t  0.006805329101824011 \t 2.511381096384519\n",
      "27     \t [0.7265968  0.         0.33113201]. \t  0.5262462508954752 \t 2.511381096384519\n",
      "28     \t [0.73942357 0.27962279 0.        ]. \t  0.05962028840497304 \t 2.511381096384519\n",
      "29     \t [0.29627893 0.42147866 1.        ]. \t  1.6932543687974033 \t 2.511381096384519\n",
      "30     \t [0.64273838 1.         1.        ]. \t  0.32917877150016944 \t 2.511381096384519\n",
      "31     \t [0.83113578 0.80302153 0.        ]. \t  0.0006168049918356678 \t 2.511381096384519\n",
      "32     \t [0.50663852 0.46305275 0.60680526]. \t  1.1248500391271192 \t 2.511381096384519\n",
      "33     \t [0.02640826 0.80966025 0.30509504]. \t  0.44135234972435866 \t 2.511381096384519\n",
      "34     \t [1.         0.77156468 0.36777615]. \t  0.09347102695567201 \t 2.511381096384519\n",
      "35     \t [0.38993603 0.04780947 0.24112786]. \t  0.932796665244939 \t 2.511381096384519\n",
      "36     \t [5.07173163e-09 9.75148005e-01 7.83848985e-01]. \t  1.0518537322401205 \t 2.511381096384519\n",
      "37     \t [0.18020314 0.18217551 0.        ]. \t  0.10098811190330284 \t 2.511381096384519\n",
      "38     \t [2.72912483e-01 8.50895502e-09 9.70444688e-01]. \t  0.12065762060853016 \t 2.511381096384519\n",
      "39     \t [0.15040622 0.5740977  0.82457538]. \t  \u001b[92m3.7651780690868892\u001b[0m \t 3.7651780690868892\n",
      "40     \t [0.12212256 0.5520552  0.75915809]. \t  3.194688947968644 \t 3.7651780690868892\n",
      "41     \t [0.15634948 0.77608184 0.98224448]. \t  1.5651760670484995 \t 3.7651780690868892\n",
      "42     \t [0.85393542 0.55621485 0.86784121]. \t  3.7234307432333535 \t 3.7651780690868892\n",
      "43     \t [0.82889944 0.5198734  0.87929141]. \t  3.6523990600797376 \t 3.7651780690868892\n",
      "44     \t [0.66276242 0.51532277 0.84926039]. \t  3.7542422992023967 \t 3.7651780690868892\n",
      "45     \t [0.7621837  0.9845328  0.16453059]. \t  0.00790633902899323 \t 3.7651780690868892\n",
      "46     \t [0.79579298 0.51901065 0.90868074]. \t  3.4307485224740106 \t 3.7651780690868892\n",
      "47     \t [1.         0.         0.26335399]. \t  0.26394925660311747 \t 3.7651780690868892\n",
      "48     \t [0.20084388 1.         0.21132704]. \t  0.07281689950422468 \t 3.7651780690868892\n",
      "49     \t [0.35751759 0.61345243 0.80829723]. \t  3.5534596281454824 \t 3.7651780690868892\n",
      "50     \t [0.98232798 0.37395574 0.79702407]. \t  2.6282373305671367 \t 3.7651780690868892\n",
      "51     \t [0.65591769 0.57132923 0.89769444]. \t  3.6184906453078884 \t 3.7651780690868892\n",
      "52     \t [0.56951969 0.59930017 0.84925982]. \t  3.744359587586567 \t 3.7651780690868892\n",
      "53     \t [0.70452998 0.60832336 0.90713007]. \t  3.4432303896810432 \t 3.7651780690868892\n",
      "54     \t [0.2588813  0.99999998 0.99999981]. \t  0.3342903522576151 \t 3.7651780690868892\n",
      "55     \t [0.51884018 0.57083199 0.85833946]. \t  \u001b[92m3.825950075211796\u001b[0m \t 3.825950075211796\n",
      "56     \t [0.25767106 0.00398853 0.72901199]. \t  0.2344956212705532 \t 3.825950075211796\n",
      "57     \t [0.0165505  0.52046709 0.82017624]. \t  3.687508960156136 \t 3.825950075211796\n",
      "58     \t [0.22846334 0.58820708 0.82333703]. \t  3.7399428189077066 \t 3.825950075211796\n",
      "59     \t [0.5593614  0.61021027 0.87489433]. \t  3.683853551342828 \t 3.825950075211796\n",
      "60     \t [0.02253709 0.46072794 0.89874421]. \t  3.2904526050347687 \t 3.825950075211796\n",
      "61     \t [0.46272517 0.53566929 0.86939574]. \t  3.811156917848459 \t 3.825950075211796\n",
      "62     \t [0.54584046 0.55102769 0.85185237]. \t  \u001b[92m3.8313343465261176\u001b[0m \t 3.8313343465261176\n",
      "63     \t [0.45929237 0.58821551 0.83125875]. \t  3.7498614078639596 \t 3.8313343465261176\n",
      "64     \t [0.7897559  0.55101315 0.83951689]. \t  3.730511032740199 \t 3.8313343465261176\n",
      "65     \t [0.34638043 0.56541976 0.85196636]. \t  \u001b[92m3.8557270512462374\u001b[0m \t 3.8557270512462374\n",
      "66     \t [0.26456337 0.41164979 0.83452644]. \t  3.199631139975319 \t 3.8557270512462374\n",
      "67     \t [0.52199881 0.56106566 0.83556974]. \t  3.797179347107459 \t 3.8557270512462374\n",
      "68     \t [0.7218618  0.54150807 0.8753091 ]. \t  3.7408297678985267 \t 3.8557270512462374\n",
      "69     \t [0.31550114 0.5718902  0.83286249]. \t  3.810185662781201 \t 3.8557270512462374\n",
      "70     \t [0.35171343 0.48156892 0.87819285]. \t  3.6015197319732932 \t 3.8557270512462374\n",
      "71     \t [0.36689723 0.46891745 0.80815362]. \t  3.4681345143155995 \t 3.8557270512462374\n",
      "72     \t [0.25649127 0.66677559 0.84244584]. \t  3.4399747417510342 \t 3.8557270512462374\n",
      "73     \t [0.32078452 0.5653035  0.91972557]. \t  3.4249850246036213 \t 3.8557270512462374\n",
      "74     \t [0.6657422  0.53602642 0.87915674]. \t  3.7327606277135947 \t 3.8557270512462374\n",
      "75     \t [0.51330755 0.54342114 0.83184118]. \t  3.7867568580510107 \t 3.8557270512462374\n",
      "76     \t [0.91636756 0.48853315 0.83997222]. \t  3.565745860048678 \t 3.8557270512462374\n",
      "77     \t [0.73619138 0.61631097 0.85374441]. \t  3.630111727402146 \t 3.8557270512462374\n",
      "78     \t [0.3723361  0.60712206 0.8870054 ]. \t  3.6620430536601574 \t 3.8557270512462374\n",
      "79     \t [0.2835769  0.55848818 0.85431636]. \t  \u001b[92m3.86082960135366\u001b[0m \t 3.86082960135366\n",
      "80     \t [0.59568275 0.57641037 0.8752092 ]. \t  3.7643049664093517 \t 3.86082960135366\n",
      "81     \t [0.08117898 0.5604155  0.87319674]. \t  3.7924279838519777 \t 3.86082960135366\n",
      "82     \t [0.15421961 0.58138463 0.86315443]. \t  3.8170253126461455 \t 3.86082960135366\n",
      "83     \t [0.37579014 0.56459625 0.89313639]. \t  3.6997044489529882 \t 3.86082960135366\n",
      "84     \t [0.66850444 0.5572298  0.84118816]. \t  3.776586574530085 \t 3.86082960135366\n",
      "85     \t [0.03127939 0.60986704 0.8462831 ]. \t  3.7158909199603967 \t 3.86082960135366\n",
      "86     \t [0.03327001 0.44343059 0.84275354]. \t  3.4110487490207393 \t 3.86082960135366\n",
      "87     \t [0.26918101 0.53514104 0.85578544]. \t  3.8460884066314325 \t 3.86082960135366\n",
      "88     \t [0.47297634 0.5366286  0.82105836]. \t  3.735687698398111 \t 3.86082960135366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.52443718 0.54372326 0.79887095]. \t  3.5512242427520704 \t 3.86082960135366\n",
      "90     \t [0.73039592 0.53077173 0.83070651]. \t  3.707337743834635 \t 3.86082960135366\n",
      "91     \t [0.43252743 0.68975417 0.820368  ]. \t  3.1604136670402947 \t 3.86082960135366\n",
      "92     \t [0.24783234 0.47236336 0.83995561]. \t  3.621890069380503 \t 3.86082960135366\n",
      "93     \t [0.52212218 0.55121483 0.84680292]. \t  3.8310199425327123 \t 3.86082960135366\n",
      "94     \t [0.09016755 0.56322698 0.8978165 ]. \t  3.6346934552031027 \t 3.86082960135366\n",
      "95     \t [0.2326055  0.52617439 0.8524771 ]. \t  3.8300230674427915 \t 3.86082960135366\n",
      "96     \t [0.64688986 0.56633895 0.81046338]. \t  3.595854693074133 \t 3.86082960135366\n",
      "97     \t [0.91728228 0.5405181  0.84522064]. \t  3.693393345263674 \t 3.86082960135366\n",
      "98     \t [0.14856571 0.52169883 0.8577611 ]. \t  3.8055116671948204 \t 3.86082960135366\n",
      "99     \t [0.24580154 0.4780867  0.83406884]. \t  3.638437935343501 \t 3.86082960135366\n",
      "100    \t [0.27021196 0.46904299 0.91493543]. \t  3.211712971143558 \t 3.86082960135366\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \n",
    "\n",
    "np.random.seed(run_num_10)\n",
    "surrogate_winner_10 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_10 = GPGO(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_10.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.98103566 0.82124785 0.64589361]. \t  0.6570240879762975 \t 0.687459437576373\n",
      "init   \t [0.42368801 0.20231098 0.49190677]. \t  0.29901296656621285 \t 0.687459437576373\n",
      "init   \t [0.13855833 0.45252104 0.11373463]. \t  0.1378633538075966 \t 0.687459437576373\n",
      "init   \t [0.00292449 0.52342617 0.18997116]. \t  0.12299763462541902 \t 0.687459437576373\n",
      "init   \t [0.14171979 0.33586637 0.58369219]. \t  0.687459437576373 \t 0.687459437576373\n",
      "1      \t [0. 1. 1.]. \t  0.330219860606422 \t 0.687459437576373\n",
      "2      \t [ 1.0000000e+00  1.0000000e+00 -6.9388939e-18]. \t  3.7727185179443916e-05 \t 0.687459437576373\n",
      "3      \t [1. 0. 1.]. \t  0.08848201872702738 \t 0.687459437576373\n",
      "4      \t [ 1.00000000e+00 -5.55111512e-17  0.00000000e+00]. \t  0.030954717033005136 \t 0.687459437576373\n",
      "5      \t [0. 0. 0.]. \t  0.06797411659013229 \t 0.687459437576373\n",
      "6      \t [0. 0. 1.]. \t  0.0902894676548261 \t 0.687459437576373\n",
      "7      \t [1. 1. 1.]. \t  0.31688362070415665 \t 0.687459437576373\n",
      "8      \t [0.39480325 1.         0.43751142]. \t  \u001b[92m1.3268709572102413\u001b[0m \t 1.3268709572102413\n",
      "9      \t [0.52465965 0.57032316 1.        ]. \t  \u001b[92m2.0834480974763046\u001b[0m \t 2.0834480974763046\n",
      "10     \t [0.00000000e+00 1.00000000e+00 5.55111512e-17]. \t  0.0002735367680454459 \t 2.0834480974763046\n",
      "11     \t [1.         0.49158735 1.        ]. \t  1.8992301995679732 \t 2.0834480974763046\n",
      "12     \t [1.         0.49476697 0.        ]. \t  0.008525809516837323 \t 2.0834480974763046\n",
      "13     \t [0.         0.50622139 1.        ]. \t  1.983098474422443 \t 2.0834480974763046\n",
      "14     \t [1.         0.         0.49340945]. \t  0.07557129562022392 \t 2.0834480974763046\n",
      "15     \t [0.56289446 1.         0.        ]. \t  0.00017797942503985966 \t 2.0834480974763046\n",
      "16     \t [0.         1.         0.53962277]. \t  \u001b[92m2.4676124241197717\u001b[0m \t 2.4676124241197717\n",
      "17     \t [0.5213074 0.        0.       ]. \t  0.0953642186191707 \t 2.4676124241197717\n",
      "18     \t [0.43629001 0.         1.        ]. \t  0.09172473732404694 \t 2.4676124241197717\n",
      "19     \t [0.19149704 0.75784513 0.70653299]. \t  2.462099141943704 \t 2.4676124241197717\n",
      "20     \t [0.77060721 0.33307566 0.80220273]. \t  2.3662265979023087 \t 2.4676124241197717\n",
      "21     \t [0.57526287 1.         0.82539028]. \t  0.6529978653821489 \t 2.4676124241197717\n",
      "22     \t [1.         0.3431785  0.39656342]. \t  0.13278157555462294 \t 2.4676124241197717\n",
      "23     \t [0.         0.         0.52739494]. \t  0.11063881715610019 \t 2.4676124241197717\n",
      "24     \t [0.68081188 0.6970043  0.2717646 ]. \t  0.10077685625959575 \t 2.4676124241197717\n",
      "25     \t [0.71760723 0.27269928 0.        ]. \t  0.06388629943808741 \t 2.4676124241197717\n",
      "26     \t [0.         0.77182515 0.80120611]. \t  2.462330318202847 \t 2.4676124241197717\n",
      "27     \t [0.754669   0.23359812 1.        ]. \t  0.6944076772600228 \t 2.4676124241197717\n",
      "28     \t [0.81808889 0.73778158 1.        ]. \t  1.5389357340318444 \t 2.4676124241197717\n",
      "29     \t [0.         0.56664772 0.        ]. \t  0.010431860484727032 \t 2.4676124241197717\n",
      "30     \t [1.         0.23592051 0.78993122]. \t  1.4507283926124344 \t 2.4676124241197717\n",
      "31     \t [0.32276658 1.         1.        ]. \t  0.3342548758738913 \t 2.4676124241197717\n",
      "32     \t [0.66637846 0.         0.7636007 ]. \t  0.24467055106997937 \t 2.4676124241197717\n",
      "33     \t [0.19210063 0.79613221 0.04894974]. \t  0.003450591279455914 \t 2.4676124241197717\n",
      "34     \t [0.91359366 1.         0.41511247]. \t  0.20565907825852742 \t 2.4676124241197717\n",
      "35     \t [0.14671221 0.6967034  1.        ]. \t  1.764641658611585 \t 2.4676124241197717\n",
      "36     \t [0.12108113 1.         0.72690985]. \t  1.3206418261570492 \t 2.4676124241197717\n",
      "37     \t [2.39020899e-01 3.96624267e-08 6.44379899e-01]. \t  0.14936545761790554 \t 2.4676124241197717\n",
      "38     \t [7.46185639e-01 1.24503363e-08 2.27274280e-01]. \t  0.5423043822800475 \t 2.4676124241197717\n",
      "39     \t [0.08854108 0.         0.21205789]. \t  0.6286610321996255 \t 2.4676124241197717\n",
      "40     \t [0.27551253 0.32102609 0.89559374]. \t  2.15796411150745 \t 2.4676124241197717\n",
      "41     \t [0.50785923 0.61309235 0.79912472]. \t  \u001b[92m3.425721260084875\u001b[0m \t 3.425721260084875\n",
      "42     \t [7.01933935e-08 9.99999916e-01 2.02934961e-01]. \t  0.06044568014444443 \t 3.425721260084875\n",
      "43     \t [0.66693874 0.55512816 0.66692368]. \t  1.7477434292083873 \t 3.425721260084875\n",
      "44     \t [0.01676805 0.18329228 0.05929724]. \t  0.1801792067259915 \t 3.425721260084875\n",
      "45     \t [1.         1.         0.21490028]. \t  0.007510363866922377 \t 3.425721260084875\n",
      "46     \t [0.99263914 0.15298368 0.10210187]. \t  0.13549979390863812 \t 3.425721260084875\n",
      "47     \t [0.45223898 0.53569262 0.01587154]. \t  0.025578564990171766 \t 3.425721260084875\n",
      "48     \t [0.0092094  0.25450405 0.85555155]. \t  1.664088240513188 \t 3.425721260084875\n",
      "49     \t [1.         1.         0.75409551]. \t  0.4031617033556136 \t 3.425721260084875\n",
      "50     \t [0.8571645  0.69410592 0.01214265]. \t  0.0025408944849540493 \t 3.425721260084875\n",
      "51     \t [0.45630963 0.76552469 0.89730723]. \t  2.4444189974193926 \t 3.425721260084875\n",
      "52     \t [0.96766175 0.54507712 0.76698188]. \t  2.9767821843922353 \t 3.425721260084875\n",
      "53     \t [0.48075362 0.48685681 0.84329237]. \t  \u001b[92m3.6882311281433515\u001b[0m \t 3.6882311281433515\n",
      "54     \t [0.41981431 0.53971636 0.86767526]. \t  \u001b[92m3.8260966078820147\u001b[0m \t 3.8260966078820147\n",
      "55     \t [0.40823875 0.54556269 0.75812108]. \t  3.133965095063636 \t 3.8260966078820147\n",
      "56     \t [0.50456992 0.57333048 0.80207693]. \t  3.5654261605853854 \t 3.8260966078820147\n",
      "57     \t [0.95138236 0.53841569 0.8481078 ]. \t  3.6828339466330835 \t 3.8260966078820147\n",
      "58     \t [0.61148847 0.54199118 0.83251843]. \t  3.762927919540767 \t 3.8260966078820147\n",
      "59     \t [0.76169442 0.54266803 0.87436004]. \t  3.7331734303393844 \t 3.8260966078820147\n",
      "60     \t [0.46604002 0.54679621 0.87616237]. \t  3.7959285666807006 \t 3.8260966078820147\n",
      "61     \t [0.45168602 0.44695124 0.89291568]. \t  3.290098642858884 \t 3.8260966078820147\n",
      "62     \t [0.52418857 0.54162605 0.86536465]. \t  3.8198633393933203 \t 3.8260966078820147\n",
      "63     \t [0.17943106 0.63742331 0.81877806]. \t  3.531914664775532 \t 3.8260966078820147\n",
      "64     \t [0.37983677 0.60591068 0.93774192]. \t  3.118714803284154 \t 3.8260966078820147\n",
      "65     \t [0.87755228 0.50984694 0.84819322]. \t  3.666385286401634 \t 3.8260966078820147\n",
      "66     \t [0.38311766 0.57716223 0.84585103]. \t  \u001b[92m3.832003952745011\u001b[0m \t 3.832003952745011\n",
      "67     \t [0.42543403 0.51665722 0.7933275 ]. \t  3.4988543654716215 \t 3.832003952745011\n",
      "68     \t [0.46530287 0.56618435 0.82141744]. \t  3.735628173123137 \t 3.832003952745011\n",
      "69     \t [0.44287795 0.56525754 0.81561841]. \t  3.704077081131519 \t 3.832003952745011\n",
      "70     \t [0.69027419 0.55898522 0.89190966]. \t  3.661836273231449 \t 3.832003952745011\n",
      "71     \t [0.16028985 0.57018088 0.79471086]. \t  3.561052023701031 \t 3.832003952745011\n",
      "72     \t [0.97818867 0.45233578 0.88390651]. \t  3.268710931535794 \t 3.832003952745011\n",
      "73     \t [0.03264455 0.52045942 0.82058886]. \t  3.6949291320564894 \t 3.832003952745011\n",
      "74     \t [0.29175317 0.5587257  0.88714593]. \t  3.745084287930277 \t 3.832003952745011\n",
      "75     \t [0.41443302 0.60581242 0.86379827]. \t  3.7542567579078483 \t 3.832003952745011\n",
      "76     \t [0.3815902  0.59862142 0.87130605]. \t  3.7629202365594123 \t 3.832003952745011\n",
      "77     \t [0.45640563 0.46463583 0.82603149]. \t  3.534449362333077 \t 3.832003952745011\n",
      "78     \t [0.37441595 0.50152922 0.89357223]. \t  3.585464395890391 \t 3.832003952745011\n",
      "79     \t [0.44004829 0.57021048 0.83858703]. \t  3.8162505296339466 \t 3.832003952745011\n",
      "80     \t [0.82171766 0.54829748 0.85575203]. \t  3.7483058201873005 \t 3.832003952745011\n",
      "81     \t [0.96110632 0.50696518 0.80713948]. \t  3.418348500937991 \t 3.832003952745011\n",
      "82     \t [0.3768818  0.58418398 0.82541667]. \t  3.747688861624937 \t 3.832003952745011\n",
      "83     \t [0.72992233 0.53146607 0.82321569]. \t  3.6673290669638297 \t 3.832003952745011\n",
      "84     \t [0.08201242 0.4989633  0.78819076]. \t  3.4086009625642144 \t 3.832003952745011\n",
      "85     \t [0.8082946  0.57809204 0.86395982]. \t  3.7244915005304446 \t 3.832003952745011\n",
      "86     \t [0.3508875  0.60968053 0.8876112 ]. \t  3.6503589841860644 \t 3.832003952745011\n",
      "87     \t [0.65751097 0.56409753 0.88461654]. \t  3.71644654925447 \t 3.832003952745011\n",
      "88     \t [0.75942035 0.52718378 0.85730657]. \t  3.7509613322068147 \t 3.832003952745011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.50940052 0.57562185 0.85740423]. \t  3.821596765216153 \t 3.832003952745011\n",
      "90     \t [3.36437397e-04 5.92900497e-01 8.76078865e-01]. \t  3.714162490553864 \t 3.832003952745011\n",
      "91     \t [0.50664781 0.49179029 0.86479967]. \t  3.693097341444991 \t 3.832003952745011\n",
      "92     \t [0.41503592 0.54752483 0.84909805]. \t  \u001b[92m3.8511165079641128\u001b[0m \t 3.8511165079641128\n",
      "93     \t [0.43280195 0.49019981 0.83056748]. \t  3.6758388635559784 \t 3.8511165079641128\n",
      "94     \t [0.47540019 0.50247134 0.84723974]. \t  3.7544355481009624 \t 3.8511165079641128\n",
      "95     \t [0.71968158 0.55350655 0.94186709]. \t  3.06841822267361 \t 3.8511165079641128\n",
      "96     \t [0.27678611 0.47274792 0.90379985]. \t  3.353286858293141 \t 3.8511165079641128\n",
      "97     \t [0.19024427 0.57824987 0.79120795]. \t  3.5220739260038987 \t 3.8511165079641128\n",
      "98     \t [0.39260134 0.5181469  0.78806525]. \t  3.4566962423159744 \t 3.8511165079641128\n",
      "99     \t [0.80699874 0.5839507  0.86000111]. \t  3.7158284679169844 \t 3.8511165079641128\n",
      "100    \t [0.19969712 0.61923166 0.87975831]. \t  3.6533042261629225 \t 3.8511165079641128\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \n",
    "\n",
    "np.random.seed(run_num_11)\n",
    "surrogate_winner_11 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_11 = GPGO(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_11.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.19151945 0.62210877 0.43772774]. \t  1.1006281843679786 \t 1.6482992955272024\n",
      "init   \t [0.78535858 0.77997581 0.27259261]. \t  0.07153771680480671 \t 1.6482992955272024\n",
      "init   \t [0.27646426 0.80187218 0.95813935]. \t  1.6482992955272024 \t 1.6482992955272024\n",
      "init   \t [0.87593263 0.35781727 0.50099513]. \t  0.2282556248207173 \t 1.6482992955272024\n",
      "init   \t [0.68346294 0.71270203 0.37025075]. \t  0.33032494760932407 \t 1.6482992955272024\n",
      "1      \t [0. 0. 1.]. \t  0.0902894676548261 \t 1.6482992955272024\n",
      "2      \t [1.11022302e-16 0.00000000e+00 0.00000000e+00]. \t  0.06797411659013229 \t 1.6482992955272024\n",
      "3      \t [1. 1. 1.]. \t  0.31688362070415665 \t 1.6482992955272024\n",
      "4      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 1.6482992955272024\n",
      "5      \t [1. 0. 0.]. \t  0.03095471703300515 \t 1.6482992955272024\n",
      "6      \t [0. 1. 1.]. \t  0.330219860606422 \t 1.6482992955272024\n",
      "7      \t [1. 0. 1.]. \t  0.08848201872702738 \t 1.6482992955272024\n",
      "8      \t [0.4384707  0.45261502 0.        ]. \t  0.03750025495958049 \t 1.6482992955272024\n",
      "9      \t [ 4.43184352e-01 -1.73472348e-18  5.79349399e-01]. \t  0.1174183473902934 \t 1.6482992955272024\n",
      "10     \t [0.         0.50627859 1.        ]. \t  \u001b[92m1.983235037750519\u001b[0m \t 1.983235037750519\n",
      "11     \t [0.59067642 0.35796287 1.        ]. \t  1.3534072586525794 \t 1.983235037750519\n",
      "12     \t [0.53952448 1.         0.        ]. \t  0.0001877236380244219 \t 1.983235037750519\n",
      "13     \t [1.         0.56659266 1.        ]. \t  \u001b[92m2.0088218552006616\u001b[0m \t 2.0088218552006616\n",
      "14     \t [0.         0.         0.49556136]. \t  0.14207559326497868 \t 2.0088218552006616\n",
      "15     \t [0.         1.         0.55392969]. \t  \u001b[92m2.4901325436980155\u001b[0m \t 2.4901325436980155\n",
      "16     \t [0.5019686 0.        0.       ]. \t  0.09695682350417714 \t 2.4901325436980155\n",
      "17     \t [1.         0.54928723 0.        ]. \t  0.005487259224273083 \t 2.4901325436980155\n",
      "18     \t [1.         1.         0.60650718]. \t  0.2776922845621523 \t 2.4901325436980155\n",
      "19     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.4901325436980155\n",
      "20     \t [0.         0.72429487 0.73172881]. \t  \u001b[92m2.5537731908889048\u001b[0m \t 2.5537731908889048\n",
      "21     \t [0.5666336  1.         0.78978669]. \t  0.6925736230723439 \t 2.5537731908889048\n",
      "22     \t [1.         0.         0.54754714]. \t  0.06969745567925145 \t 2.5537731908889048\n",
      "23     \t [0.17301598 0.3293287  0.80540601]. \t  2.377787559182141 \t 2.5537731908889048\n",
      "24     \t [0.         0.39063884 0.        ]. \t  0.03689153817205877 \t 2.5537731908889048\n",
      "25     \t [0.99875262 0.67295536 0.77976179]. \t  \u001b[92m2.6173466296149543\u001b[0m \t 2.6173466296149543\n",
      "26     \t [0.35192971 0.         1.        ]. \t  0.09171752158973331 \t 2.6173466296149543\n",
      "27     \t [0.79942872 0.78500108 0.98255064]. \t  1.4780079242995199 \t 2.6173466296149543\n",
      "28     \t [0.81050597 0.         0.84827616]. \t  0.23227229109190256 \t 2.6173466296149543\n",
      "29     \t [0.98116892 0.16287696 0.79488068]. \t  0.9379916287932597 \t 2.6173466296149543\n",
      "30     \t [0.         0.42802818 0.58599325]. \t  0.9892784035723241 \t 2.6173466296149543\n",
      "31     \t [0.19179606 0.98989384 0.27907891]. \t  0.2630826830308028 \t 2.6173466296149543\n",
      "32     \t [0.84114482 0.23711567 0.        ]. \t  0.05198593518426685 \t 2.6173466296149543\n",
      "33     \t [0.19105366 0.15941458 0.1791032 ]. \t  0.7076124655884598 \t 2.6173466296149543\n",
      "34     \t [0.17946772 0.97122789 0.6787671 ]. \t  1.9048614447716017 \t 2.6173466296149543\n",
      "35     \t [0.32332659 0.58284319 0.79091003]. \t  \u001b[92m3.4990136397485117\u001b[0m \t 3.4990136397485117\n",
      "36     \t [0.8436863  0.         0.14594067]. \t  0.28507871379004357 \t 3.4990136397485117\n",
      "37     \t [0.66161418 0.62043373 0.79270997]. \t  3.254037645185857 \t 3.4990136397485117\n",
      "38     \t [0.         0.76475804 0.04026367]. \t  0.0030496963959497584 \t 3.4990136397485117\n",
      "39     \t [0.22982233 0.59684836 0.85143207]. \t  \u001b[92m3.7985968448059717\u001b[0m \t 3.7985968448059717\n",
      "40     \t [0.28380502 0.50473842 0.89111108]. \t  3.615653187462004 \t 3.7985968448059717\n",
      "41     \t [0.16685984 0.5754676  0.86451393]. \t  \u001b[92m3.8252016979356935\u001b[0m \t 3.8252016979356935\n",
      "42     \t [0.19009097 0.58100188 0.85092726]. \t  \u001b[92m3.8319450843407212\u001b[0m \t 3.8319450843407212\n",
      "43     \t [0.26412513 0.60018927 0.85698908]. \t  3.7893788245470357 \t 3.8319450843407212\n",
      "44     \t [0.30391521 0.53281715 0.84571678]. \t  \u001b[92m3.8407909794957296\u001b[0m \t 3.8407909794957296\n",
      "45     \t [0.29558204 0.50885202 0.8470315 ]. \t  3.7870298667533944 \t 3.8407909794957296\n",
      "46     \t [0.26343182 0.50566855 0.90731377]. \t  3.4671853273459505 \t 3.8407909794957296\n",
      "47     \t [0.74636002 0.82707232 0.00898746]. \t  0.0006837259737639345 \t 3.8407909794957296\n",
      "48     \t [0.333898   0.5064229  0.86294645]. \t  3.766884808303199 \t 3.8407909794957296\n",
      "49     \t [0.15592131 0.5545084  0.88659279]. \t  3.734778807473873 \t 3.8407909794957296\n",
      "50     \t [0.289169   0.53513555 0.83825267]. \t  3.8295049326409334 \t 3.8407909794957296\n",
      "51     \t [0.28693793 0.57893842 0.87102885]. \t  3.811494042026766 \t 3.8407909794957296\n",
      "52     \t [0.33963113 0.59094793 0.80643817]. \t  3.6134813826100673 \t 3.8407909794957296\n",
      "53     \t [0.23668164 0.56401481 0.84416171]. \t  \u001b[92m3.849536500776267\u001b[0m \t 3.849536500776267\n",
      "54     \t [0.12956568 0.62255068 0.87836948]. \t  3.6359298400405966 \t 3.849536500776267\n",
      "55     \t [0.23661014 0.52490254 0.81156587]. \t  3.684208625499016 \t 3.849536500776267\n",
      "56     \t [0.23285957 0.63654884 0.85667027]. \t  3.6348032440617386 \t 3.849536500776267\n",
      "57     \t [0.80605409 0.56497584 0.89545719]. \t  3.5984633076012225 \t 3.849536500776267\n",
      "58     \t [0.99913009 0.3044506  0.27155595]. \t  0.21498436316408256 \t 3.849536500776267\n",
      "59     \t [0.51688836 0.56900757 0.83937028]. \t  3.8037326865716565 \t 3.849536500776267\n",
      "60     \t [0.38072636 0.62409627 0.8861067 ]. \t  3.6002967398163808 \t 3.849536500776267\n",
      "61     \t [0.32065486 0.59586172 0.83741202]. \t  3.7755992713714672 \t 3.849536500776267\n",
      "62     \t [0.15119604 0.56926613 0.79512519]. \t  3.5645605148548265 \t 3.849536500776267\n",
      "63     \t [0.67525325 0.54281563 0.88667249]. \t  3.695260712050545 \t 3.849536500776267\n",
      "64     \t [0.35601953 0.60182617 0.83915746]. \t  3.75871121505775 \t 3.849536500776267\n",
      "65     \t [0.27283442 0.57237229 0.86144053]. \t  3.8442345038125856 \t 3.849536500776267\n",
      "66     \t [0.26103967 0.56434181 0.91202007]. \t  3.51502008404899 \t 3.849536500776267\n",
      "67     \t [0.42963923 0.54629188 0.83674534]. \t  3.822897176723592 \t 3.849536500776267\n",
      "68     \t [1.81077886e-01 7.29907392e-07 0.00000000e+00]. \t  0.09197882525337855 \t 3.849536500776267\n",
      "69     \t [0.71105103 0.59549029 0.91885192]. \t  3.3516490144414095 \t 3.849536500776267\n",
      "70     \t [0.3173907  0.6049759  0.79598172]. \t  3.4925804576197246 \t 3.849536500776267\n",
      "71     \t [0.13236804 0.60735909 0.88660395]. \t  3.6496344162352563 \t 3.849536500776267\n",
      "72     \t [0.29742224 0.58939561 0.87242298]. \t  3.78654524829388 \t 3.849536500776267\n",
      "73     \t [0.24674788 0.57368206 0.86915477]. \t  3.823279931462598 \t 3.849536500776267\n",
      "74     \t [0.37343638 0.5368685  0.86520033]. \t  3.832619829817369 \t 3.849536500776267\n",
      "75     \t [0.36005605 0.51304862 0.89467286]. \t  3.616699405192142 \t 3.849536500776267\n",
      "76     \t [0.75836091 0.52791318 0.84825561]. \t  3.7483225944188154 \t 3.849536500776267\n",
      "77     \t [0.79601336 0.53598043 0.84987784]. \t  3.7469806915840733 \t 3.849536500776267\n",
      "78     \t [0.38289288 0.53742998 0.81764235]. \t  3.731833190479916 \t 3.849536500776267\n",
      "79     \t [0.51945386 0.48536039 0.78092203]. \t  3.266350144629677 \t 3.849536500776267\n",
      "80     \t [0.28539533 0.5976575  0.85838615]. \t  3.7961476404798056 \t 3.849536500776267\n",
      "81     \t [0.43750971 0.61952808 0.88054042]. \t  3.6443448703595704 \t 3.849536500776267\n",
      "82     \t [0.72382964 1.         0.28335996]. \t  0.09060953074459843 \t 3.849536500776267\n",
      "83     \t [0.         1.         0.76883629]. \t  0.9991386416181605 \t 3.849536500776267\n",
      "84     \t [0.31278764 0.60636253 0.77168828]. \t  3.256238558779419 \t 3.849536500776267\n",
      "85     \t [0.74376934 0.45618499 0.90031264]. \t  3.243537437346223 \t 3.849536500776267\n",
      "86     \t [0.6083201  0.55699829 0.81874112]. \t  3.6835498849604913 \t 3.849536500776267\n",
      "87     \t [0.16024986 0.58343514 0.84986564]. \t  3.8230231092403253 \t 3.849536500776267\n",
      "88     \t [0.66368625 0.60431967 0.82985734]. \t  3.626387316372348 \t 3.849536500776267\n",
      "89     \t [0.32432703 0.52778817 0.87755112]. \t  3.771366313912431 \t 3.849536500776267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.42652492 0.52862037 0.77247941]. \t  3.2912807532496595 \t 3.849536500776267\n",
      "91     \t [0.66546094 0.55096428 0.89342967]. \t  3.6555229757209715 \t 3.849536500776267\n",
      "92     \t [0.40862356 0.60480594 0.86182643]. \t  3.7613318811887737 \t 3.849536500776267\n",
      "93     \t [0.59580853 0.54706989 0.8954845 ]. \t  3.6520416857870073 \t 3.849536500776267\n",
      "94     \t [0.36016406 0.56044179 0.87076428]. \t  3.828787703864702 \t 3.849536500776267\n",
      "95     \t [0.4525258  0.61035657 0.92169845]. \t  3.319879560769567 \t 3.849536500776267\n",
      "96     \t [0.12726301 0.52402271 0.88224719]. \t  3.716830777144037 \t 3.849536500776267\n",
      "97     \t [0.79397038 0.46725429 0.86642632]. \t  3.501720129624022 \t 3.849536500776267\n",
      "98     \t [0.33723774 0.58684078 0.84895181]. \t  3.8214814789930904 \t 3.849536500776267\n",
      "99     \t [0.34608303 0.4819577  0.83666806]. \t  3.6652180373579744 \t 3.849536500776267\n",
      "100    \t [0.39414903 0.57429471 0.81947017]. \t  3.728661995275446 \t 3.849536500776267\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\n",
    "\n",
    "np.random.seed(run_num_12)\n",
    "surrogate_winner_12 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_12 = GPGO(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_12.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.66064431 0.41360065 0.66810256]. \t  1.5457680899590245 \t 2.6697919207500047\n",
      "init   \t [0.22994342 0.80767834 0.63681846]. \t  2.6697919207500047 \t 2.6697919207500047\n",
      "init   \t [0.17219385 0.26038587 0.91531999]. \t  1.4685599870430508 \t 2.6697919207500047\n",
      "init   \t [0.46281551 0.12019095 0.88088551]. \t  0.639297028201682 \t 2.6697919207500047\n",
      "init   \t [0.22621895 0.81144033 0.44587892]. \t  1.960293029067079 \t 2.6697919207500047\n",
      "1      \t [0.82177594 1.         1.        ]. \t  0.3237329237262565 \t 2.6697919207500047\n",
      "2      \t [0. 1. 1.]. \t  0.330219860606422 \t 2.6697919207500047\n",
      "3      \t [ 0.00000000e+00  0.00000000e+00 -5.55111512e-17]. \t  0.06797411659013224 \t 2.6697919207500047\n",
      "4      \t [ 1.00000000e+00 -2.77555756e-17  0.00000000e+00]. \t  0.030954717033005136 \t 2.6697919207500047\n",
      "5      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.6697919207500047\n",
      "6      \t [0.         0.         0.56696934]. \t  0.0992456281951739 \t 2.6697919207500047\n",
      "7      \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.6697919207500047\n",
      "8      \t [1.         1.         0.53140469]. \t  0.24885218096832187 \t 2.6697919207500047\n",
      "9      \t [0.         0.58021761 0.57670781]. \t  1.6805472388414289 \t 2.6697919207500047\n",
      "10     \t [ 0.00000000e+00  1.00000000e+00 -5.55111512e-17]. \t  0.0002735367680454458 \t 2.6697919207500047\n",
      "11     \t [1.         0.53969777 1.        ]. \t  1.9939387807396773 \t 2.6697919207500047\n",
      "12     \t [0.45297101 0.64457701 1.        ]. \t  1.9691326300634686 \t 2.6697919207500047\n",
      "13     \t [1.         0.50545974 0.        ]. \t  0.007856181708642287 \t 2.6697919207500047\n",
      "14     \t [0.4912565 0.        0.       ]. \t  0.09775596222389746 \t 2.6697919207500047\n",
      "15     \t [0.         1.         0.53263956]. \t  2.446248871598776 \t 2.6697919207500047\n",
      "16     \t [0. 0. 1.]. \t  0.0902894676548261 \t 2.6697919207500047\n",
      "17     \t [0.54660259 1.         0.        ]. \t  0.00018477658399308598 \t 2.6697919207500047\n",
      "18     \t [1.         0.         0.50278263]. \t  0.07198566803764267 \t 2.6697919207500047\n",
      "19     \t [0.37755062 1.         0.70138351]. \t  1.292007427026875 \t 2.6697919207500047\n",
      "20     \t [0.         0.60330516 1.        ]. \t  2.0326790554768053 \t 2.6697919207500047\n",
      "21     \t [0.26804189 0.47799752 0.        ]. \t  0.03095132355059764 \t 2.6697919207500047\n",
      "22     \t [1.         0.71332473 0.77604374]. \t  2.28572795166576 \t 2.6697919207500047\n",
      "23     \t [1.         0.30429734 0.76543411]. \t  1.88273492581859 \t 2.6697919207500047\n",
      "24     \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.6697919207500047\n",
      "25     \t [1.         0.48311392 0.36747522]. \t  0.08745535829551311 \t 2.6697919207500047\n",
      "26     \t [0.24688114 0.         0.28554365]. \t  0.82594490218374 \t 2.6697919207500047\n",
      "27     \t [0.         0.69835936 0.        ]. \t  0.00286334803709694 \t 2.6697919207500047\n",
      "28     \t [0.71967931 0.72881402 0.        ]. \t  0.0019973091339168278 \t 2.6697919207500047\n",
      "29     \t [0.79977411 0.2266652  0.        ]. \t  0.059565265671074535 \t 2.6697919207500047\n",
      "30     \t [0.77180262 0.29849138 1.        ]. \t  1.0147079664410572 \t 2.6697919207500047\n",
      "31     \t [0.69914823 1.         0.39672099]. \t  0.42613873737333025 \t 2.6697919207500047\n",
      "32     \t [0.72048063 0.         0.65828527]. \t  0.1586311831845354 \t 2.6697919207500047\n",
      "33     \t [3.60471899e-04 8.61152124e-01 7.59471770e-01]. \t  1.8311734199287573 \t 2.6697919207500047\n",
      "34     \t [0.         0.24267485 0.15510488]. \t  0.3895876566340064 \t 2.6697919207500047\n",
      "35     \t [3.33192397e-01 1.94245774e-08 1.00000000e+00]. \t  0.09169821703633074 \t 2.6697919207500047\n",
      "36     \t [9.71126532e-08 9.93090179e-01 2.45742047e-01]. \t  0.14337403830238196 \t 2.6697919207500047\n",
      "37     \t [0.26235137 0.20536595 0.51071253]. \t  0.28629148815101985 \t 2.6697919207500047\n",
      "38     \t [0.67612931 0.79050854 0.72039157]. \t  1.5963733226429648 \t 2.6697919207500047\n",
      "39     \t [0.         0.         0.24696749]. \t  0.5727176633558632 \t 2.6697919207500047\n",
      "40     \t [0.65441934 0.01135112 0.29634419]. \t  0.6831018313973569 \t 2.6697919207500047\n",
      "41     \t [0.         0.25520993 1.        ]. \t  0.7938495765198115 \t 2.6697919207500047\n",
      "42     \t [0.25070799 0.99182161 1.        ]. \t  0.3579410016087661 \t 2.6697919207500047\n",
      "43     \t [0.20583886 1.         0.        ]. \t  0.0002878455254032459 \t 2.6697919207500047\n",
      "44     \t [0.22865287 0.57052593 0.72599714]. \t  \u001b[92m2.814422781301484\u001b[0m \t 2.814422781301484\n",
      "45     \t [0.35400526 0.36957064 0.99999999]. \t  1.4231687183254245 \t 2.814422781301484\n",
      "46     \t [1.         0.89367758 0.2053154 ]. \t  0.007744785885529623 \t 2.814422781301484\n",
      "47     \t [0.89427906 0.65291487 0.92513121]. \t  \u001b[92m3.001948096229356\u001b[0m \t 3.001948096229356\n",
      "48     \t [0.52043532 0.53703668 0.20058446]. \t  0.15350140472578003 \t 3.001948096229356\n",
      "49     \t [0.83020229 0.54032427 0.81583954]. \t  \u001b[92m3.5817836808692127\u001b[0m \t 3.5817836808692127\n",
      "50     \t [1.         0.1885968  0.17785088]. \t  0.22628396556739666 \t 3.5817836808692127\n",
      "51     \t [0.00665899 0.4585453  0.86007684]. \t  3.4899099727518115 \t 3.5817836808692127\n",
      "52     \t [0.85545926 0.50111045 0.91359362]. \t  3.302211533170022 \t 3.5817836808692127\n",
      "53     \t [0.00353274 0.5490535  0.84900383]. \t  \u001b[92m3.809578189645349\u001b[0m \t 3.809578189645349\n",
      "54     \t [0.94379563 0.52880764 0.84094636]. \t  3.6601875303439946 \t 3.809578189645349\n",
      "55     \t [0.67759502 0.58290963 0.84620842]. \t  3.750467013258447 \t 3.809578189645349\n",
      "56     \t [0.11486888 0.58061736 0.88393872]. \t  3.7278945411623896 \t 3.809578189645349\n",
      "57     \t [0.18364287 0.63777852 0.79766307]. \t  3.3960727174508696 \t 3.809578189645349\n",
      "58     \t [0.24980177 0.57092624 0.80597727]. \t  3.6563297201804232 \t 3.809578189645349\n",
      "59     \t [0.1315993  0.97356383 0.79449721]. \t  1.0368587347130929 \t 3.809578189645349\n",
      "60     \t [0.47351221 0.5501445  0.83357589]. \t  3.804300829664129 \t 3.809578189645349\n",
      "61     \t [0.01035617 0.50591679 0.84796524]. \t  3.7302032258326205 \t 3.809578189645349\n",
      "62     \t [0.48256842 0.5616719  0.85367466]. \t  \u001b[92m3.8421244455185763\u001b[0m \t 3.8421244455185763\n",
      "63     \t [0.55203281 0.50446827 0.8642495 ]. \t  3.736353356301684 \t 3.8421244455185763\n",
      "64     \t [0.75726854 0.54330057 0.8982174 ]. \t  3.5864410275999807 \t 3.8421244455185763\n",
      "65     \t [0.82673529 0.52087031 0.78203683]. \t  3.237091708022243 \t 3.8421244455185763\n",
      "66     \t [0.37743352 0.53675925 0.84009327]. \t  3.8319104811912172 \t 3.8421244455185763\n",
      "67     \t [0.74413859 0.50011464 0.86989513]. \t  3.6589177774387114 \t 3.8421244455185763\n",
      "68     \t [0.39491499 0.56397357 0.84322694]. \t  3.841675347530251 \t 3.8421244455185763\n",
      "69     \t [0.57100481 0.54355358 0.88237556]. \t  3.7452300730552173 \t 3.8421244455185763\n",
      "70     \t [0.73999113 0.51658583 0.89231812]. \t  3.5895994430253797 \t 3.8421244455185763\n",
      "71     \t [0.53083421 0.56961007 0.85009631]. \t  3.8225969778860365 \t 3.8421244455185763\n",
      "72     \t [0.60949432 0.57545537 0.83868024]. \t  3.765749236890471 \t 3.8421244455185763\n",
      "73     \t [0.27221764 0.53618763 0.83202772]. \t  3.8108517988240846 \t 3.8421244455185763\n",
      "74     \t [0.58212183 0.58224386 0.88011211]. \t  3.7362875572513397 \t 3.8421244455185763\n",
      "75     \t [0.74581085 0.50856939 0.90342896]. \t  3.4672261811070184 \t 3.8421244455185763\n",
      "76     \t [0.45611373 0.59907634 0.79962507]. \t  3.500251720386479 \t 3.8421244455185763\n",
      "77     \t [0.38669209 0.55845271 0.7963555 ]. \t  3.56479220373276 \t 3.8421244455185763\n",
      "78     \t [0.45184372 0.62694879 0.80758093]. \t  3.461359518189825 \t 3.8421244455185763\n",
      "79     \t [0.45000256 0.56475846 0.79074666]. \t  3.4864726630139966 \t 3.8421244455185763\n",
      "80     \t [0.68939258 0.52375207 0.88165166]. \t  3.691592425297216 \t 3.8421244455185763\n",
      "81     \t [0.77188742 0.58862194 0.89803571]. \t  3.557402551457265 \t 3.8421244455185763\n",
      "82     \t [0.62950017 0.54667585 0.86157548]. \t  3.80675748814033 \t 3.8421244455185763\n",
      "83     \t [0.45094296 0.55922117 0.79823659]. \t  3.564280359421383 \t 3.8421244455185763\n",
      "84     \t [0.80984916 0.55964952 0.79882561]. \t  3.425194588510967 \t 3.8421244455185763\n",
      "85     \t [0.47728323 0.54986398 0.86548131]. \t  3.83238149104036 \t 3.8421244455185763\n",
      "86     \t [0.28671175 0.52451826 0.84196658]. \t  3.8204689061891073 \t 3.8421244455185763\n",
      "87     \t [0.45458558 0.55732375 0.86320295]. \t  3.840820160074265 \t 3.8421244455185763\n",
      "88     \t [0.54716721 0.53897076 0.8655719 ]. \t  3.8124807367108136 \t 3.8421244455185763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.50711214 0.60160627 0.87229572]. \t  3.7334036263483377 \t 3.8421244455185763\n",
      "90     \t [0.80233167 0.48814811 0.90335908]. \t  3.3723543906826103 \t 3.8421244455185763\n",
      "91     \t [0.53765772 0.55259465 0.85440826]. \t  3.8341023124015146 \t 3.8421244455185763\n",
      "92     \t [0.03254855 0.48844159 0.8090117 ]. \t  3.5274355741907066 \t 3.8421244455185763\n",
      "93     \t [0.61153397 0.54534242 0.85233782]. \t  3.8141240646128285 \t 3.8421244455185763\n",
      "94     \t [0.91989239 0.50565545 0.85647007]. \t  3.6404883697349604 \t 3.8421244455185763\n",
      "95     \t [0.48904417 0.57651288 0.85369525]. \t  3.8242154868625153 \t 3.8421244455185763\n",
      "96     \t [0.64835443 0.59681207 0.84264735]. \t  3.712611342931939 \t 3.8421244455185763\n",
      "97     \t [0.49096163 0.53728696 0.8724899 ]. \t  3.7986841220737206 \t 3.8421244455185763\n",
      "98     \t [0.28166824 0.48534093 0.78825041]. \t  3.3811846022001966 \t 3.8421244455185763\n",
      "99     \t [0.79144612 0.56616938 0.89651198]. \t  3.594204634488944 \t 3.8421244455185763\n",
      "100    \t [0.3639437  0.49296533 0.84844836]. \t  3.7294242837998492 \t 3.8421244455185763\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \n",
    "\n",
    "np.random.seed(run_num_13)\n",
    "surrogate_winner_13 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_13 = GPGO(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_13.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.64755105 0.50714969 0.52834138]. \t  0.5963212059988954 \t 2.610000357863649\n",
      "init   \t [0.8962852  0.69999119 0.7142971 ]. \t  1.7197848290620104 \t 2.610000357863649\n",
      "init   \t [0.71733838 0.22281946 0.17515452]. \t  0.48166052848103497 \t 2.610000357863649\n",
      "init   \t [0.45684149 0.92873843 0.00988589]. \t  0.0004588015757462679 \t 2.610000357863649\n",
      "init   \t [0.08992219 0.85020027 0.48562106]. \t  2.610000357863649 \t 2.610000357863649\n",
      "1      \t [0. 1. 1.]. \t  0.33021986060642144 \t 2.610000357863649\n",
      "2      \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.610000357863649\n",
      "3      \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.610000357863649\n",
      "4      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 2.610000357863649\n",
      "5      \t [1.         1.         0.40684201]. \t  0.12368334200808734 \t 2.610000357863649\n",
      "6      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.610000357863649\n",
      "7      \t [1. 0. 0.]. \t  0.03095471703300515 \t 2.610000357863649\n",
      "8      \t [0. 0. 1.]. \t  0.0902894676548261 \t 2.610000357863649\n",
      "9      \t [1.         0.55327784 0.        ]. \t  0.005300895565241407 \t 2.610000357863649\n",
      "10     \t [0.         0.53627042 0.57098513]. \t  1.3943552162353634 \t 2.610000357863649\n",
      "11     \t [0.         1.         0.52512888]. \t  2.4158702039905773 \t 2.610000357863649\n",
      "12     \t [0.         0.50861658 0.        ]. \t  0.016891893338049604 \t 2.610000357863649\n",
      "13     \t [1.        0.5200215 1.       ]. \t  1.9653845916051291 \t 2.610000357863649\n",
      "14     \t [1.         0.         0.47512792]. \t  0.08551996113037623 \t 2.610000357863649\n",
      "15     \t [0.         0.         0.47356862]. \t  0.17577751465019548 \t 2.610000357863649\n",
      "16     \t [0.49004045 1.         1.        ]. \t  0.3324927842575573 \t 2.610000357863649\n",
      "17     \t [0.43196425 0.         0.        ]. \t  0.10103392233336983 \t 2.610000357863649\n",
      "18     \t [0.50437428 0.         1.        ]. \t  0.09163539407728738 \t 2.610000357863649\n",
      "19     \t [0.18331832 0.58395852 1.        ]. \t  2.074964270054745 \t 2.610000357863649\n",
      "20     \t [0.29437162 1.         0.57793417]. \t  2.3065034889262614 \t 2.610000357863649\n",
      "21     \t [1.         0.47116625 0.43317737]. \t  0.11230499777390536 \t 2.610000357863649\n",
      "22     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.610000357863649\n",
      "23     \t [0.         0.65429412 1.        ]. \t  1.912379610347552 \t 2.610000357863649\n",
      "24     \t [0.1108714  0.82789609 0.77794482]. \t  2.0473297349236956 \t 2.610000357863649\n",
      "25     \t [0.71589674 0.67424333 1.        ]. \t  1.8451891602733306 \t 2.610000357863649\n",
      "26     \t [0.20861885 0.23236992 0.20889659]. \t  0.7329724789622607 \t 2.610000357863649\n",
      "27     \t [0.86566091 0.25036791 0.84381529]. \t  1.6356201794469862 \t 2.610000357863649\n",
      "28     \t [0.58885022 0.         0.38944427]. \t  0.4844411041049416 \t 2.610000357863649\n",
      "29     \t [0.67766204 1.         0.25235937]. \t  0.06271295677035094 \t 2.610000357863649\n",
      "30     \t [0.81139265 1.         0.74717015]. \t  0.511679131952684 \t 2.610000357863649\n",
      "31     \t [0.22601809 0.         0.76263586]. \t  0.24466807470846913 \t 2.610000357863649\n",
      "32     \t [0.         0.26017857 0.95374917]. \t  1.1775797440512028 \t 2.610000357863649\n",
      "33     \t [0.16105995 1.         0.18261198]. \t  0.04009723784976729 \t 2.610000357863649\n",
      "34     \t [1.         0.24617073 0.0191177 ]. \t  0.04037593966052635 \t 2.610000357863649\n",
      "35     \t [1.10051568e-09 7.54737283e-01 2.45160855e-01]. \t  0.1532947796147139 \t 2.610000357863649\n",
      "36     \t [8.09846520e-01 8.53515359e-10 7.67349041e-01]. \t  0.2437832025179657 \t 2.610000357863649\n",
      "37     \t [6.04419016e-01 4.73221303e-01 1.21947639e-10]. \t  0.027936977415983784 \t 2.610000357863649\n",
      "38     \t [0.79661019 0.27300065 1.        ]. \t  0.8805835906206323 \t 2.610000357863649\n",
      "39     \t [0.00000000e+00 1.91853479e-01 1.46693729e-08]. \t  0.07370140476525021 \t 2.610000357863649\n",
      "40     \t [0.76096304 0.         0.01449524]. \t  0.08083568269491996 \t 2.610000357863649\n",
      "41     \t [0.43273218 0.19697725 0.60515165]. \t  0.4337889505372622 \t 2.610000357863649\n",
      "42     \t [8.05866058e-01 8.25969769e-01 3.03873647e-11]. \t  0.0005009029572148748 \t 2.610000357863649\n",
      "43     \t [0.38434573 0.81572101 0.56777016]. \t  2.450525415684812 \t 2.610000357863649\n",
      "44     \t [0.99999998 0.15628334 0.76911598]. \t  0.8584999781545167 \t 2.610000357863649\n",
      "45     \t [1.38883778e-01 7.34562774e-01 3.34060595e-08]. \t  0.002449077803838921 \t 2.610000357863649\n",
      "46     \t [0.51416173 0.52019198 0.88824063]. \t  \u001b[92m3.676196319975271\u001b[0m \t 3.676196319975271\n",
      "47     \t [0.47056728 0.54317585 0.84558246]. \t  \u001b[92m3.836335755831662\u001b[0m \t 3.836335755831662\n",
      "48     \t [0.51221868 0.53526386 0.93543771]. \t  3.1784097918137544 \t 3.836335755831662\n",
      "49     \t [0.45687647 0.54307374 0.84216839]. \t  3.8319169679487284 \t 3.836335755831662\n",
      "50     \t [0.42962487 0.63169473 0.77915914]. \t  3.194235750200931 \t 3.836335755831662\n",
      "51     \t [0.3085557  0.47213246 0.82093385]. \t  3.5611376729480275 \t 3.836335755831662\n",
      "52     \t [0.55939124 0.46246222 0.8822425 ]. \t  3.4554435689444656 \t 3.836335755831662\n",
      "53     \t [1.68772641e-01 3.60751916e-07 2.32303969e-01]. \t  0.7455181755559641 \t 3.836335755831662\n",
      "54     \t [0.38986295 0.52899791 0.81242769]. \t  3.6863635906832815 \t 3.836335755831662\n",
      "55     \t [0.50067365 0.55814314 0.89026591]. \t  3.7120369714138044 \t 3.836335755831662\n",
      "56     \t [0.41241706 0.51721956 0.89883796]. \t  3.5910130121763935 \t 3.836335755831662\n",
      "57     \t [0.45722529 0.61733784 0.82283267]. \t  3.606682167011595 \t 3.836335755831662\n",
      "58     \t [0.44660655 0.59127984 0.7972218 ]. \t  3.503562591746211 \t 3.836335755831662\n",
      "59     \t [0.97886466 0.51205042 0.81735401]. \t  3.499213335410236 \t 3.836335755831662\n",
      "60     \t [0.44848448 0.42639742 0.87725207]. \t  3.241696651461566 \t 3.836335755831662\n",
      "61     \t [0.37222373 0.56859072 0.87761859]. \t  3.7955228665101495 \t 3.836335755831662\n",
      "62     \t [0.30874421 0.59841267 0.80209753]. \t  3.5650235237437338 \t 3.836335755831662\n",
      "63     \t [0.99673217 0.59663525 0.85441138]. \t  3.5965665351648743 \t 3.836335755831662\n",
      "64     \t [0.95005222 0.50086531 0.8573808 ]. \t  3.6120366409065676 \t 3.836335755831662\n",
      "65     \t [0.44354707 0.53893244 0.83841449]. \t  3.8212533833598075 \t 3.836335755831662\n",
      "66     \t [0.47594435 0.57926257 0.80768115]. \t  3.6130716513586485 \t 3.836335755831662\n",
      "67     \t [0.24088002 0.6288889  0.78576667]. \t  3.3344435635876586 \t 3.836335755831662\n",
      "68     \t [0.42624631 0.6569913  0.84140088]. \t  3.4773705616152224 \t 3.836335755831662\n",
      "69     \t [1.         0.         0.23998535]. \t  0.25819875611072274 \t 3.836335755831662\n",
      "70     \t [0.11986042 0.52958551 0.76864242]. \t  3.28131454696987 \t 3.836335755831662\n",
      "71     \t [0.38388113 0.44650801 0.84355185]. \t  3.471667377986165 \t 3.836335755831662\n",
      "72     \t [0.38493687 0.54553008 0.82933352]. \t  3.800234806809863 \t 3.836335755831662\n",
      "73     \t [0.28694735 0.57115189 0.74554504]. \t  3.024369239667151 \t 3.836335755831662\n",
      "74     \t [0.97345089 0.6132736  0.84793901]. \t  3.532503653830809 \t 3.836335755831662\n",
      "75     \t [0.45506756 0.49871733 0.86237334]. \t  3.733421628631016 \t 3.836335755831662\n",
      "76     \t [0.28631144 0.60016256 0.80280097]. \t  3.56925730817328 \t 3.836335755831662\n",
      "77     \t [0.5514336  0.46903334 0.9237975 ]. \t  3.096239618703682 \t 3.836335755831662\n",
      "78     \t [0.25924131 0.58228283 0.69862044]. \t  2.516315722235435 \t 3.836335755831662\n",
      "79     \t [0.2479777  0.56996802 0.79525282]. \t  3.5677035511327047 \t 3.836335755831662\n",
      "80     \t [0.97528777 0.61101149 0.88199524]. \t  3.5132640977745324 \t 3.836335755831662\n",
      "81     \t [0.33007346 0.54705772 0.92983898]. \t  3.2825130716359934 \t 3.836335755831662\n",
      "82     \t [0.99807668 0.53227242 0.89848652]. \t  3.481955888934822 \t 3.836335755831662\n",
      "83     \t [0.02038024 0.63572492 0.80601882]. \t  3.431345705954302 \t 3.836335755831662\n",
      "84     \t [0.72264673 0.54185131 0.85937662]. \t  3.7792188593199314 \t 3.836335755831662\n",
      "85     \t [0.98273641 0.52756968 0.85090924]. \t  3.6607535971295606 \t 3.836335755831662\n",
      "86     \t [0.38608762 0.54343647 0.89390799]. \t  3.6863256155315387 \t 3.836335755831662\n",
      "87     \t [0.2491679  0.56829007 0.86205731]. \t  \u001b[92m3.8463719539915955\u001b[0m \t 3.8463719539915955\n",
      "88     \t [0.38241356 0.55081167 0.79808717]. \t  3.5841523228158785 \t 3.8463719539915955\n",
      "89     \t [0.99993123 0.8886335  0.1467336 ]. \t  0.002386487530232224 \t 3.8463719539915955\n",
      "90     \t [0.97454733 0.63602148 0.83970249]. \t  3.3890613180697233 \t 3.8463719539915955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91     \t [0.42561506 0.54511276 0.81686928]. \t  3.7238911163332777 \t 3.8463719539915955\n",
      "92     \t [0.58915779 0.61827511 0.85944557]. \t  3.67202858198469 \t 3.8463719539915955\n",
      "93     \t [0.46892992 0.62611875 0.82729685]. \t  3.5874853718221016 \t 3.8463719539915955\n",
      "94     \t [0.53823233 0.58285809 0.78559635]. \t  3.366631498563113 \t 3.8463719539915955\n",
      "95     \t [0.59996432 0.4988854  0.8278061 ]. \t  3.6640349961844114 \t 3.8463719539915955\n",
      "96     \t [0.4644948  0.5572509  0.73362992]. \t  2.789393503387713 \t 3.8463719539915955\n",
      "97     \t [0.32523804 0.52599822 0.84823202]. \t  3.831315180540739 \t 3.8463719539915955\n",
      "98     \t [0.54014559 0.55060061 0.76343144]. \t  3.140226723212505 \t 3.8463719539915955\n",
      "99     \t [0.4061525  0.61296701 0.89524398]. \t  3.5816071816864943 \t 3.8463719539915955\n",
      "100    \t [0.65785439 0.50215252 0.83164283]. \t  3.6752325666209935 \t 3.8463719539915955\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \n",
    "\n",
    "np.random.seed(run_num_14)\n",
    "surrogate_winner_14 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_14 = GPGO(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_14.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.57788186 0.25550133 0.25252687]. \t  0.7217338409961529 \t 1.540625560354162\n",
      "init   \t [0.70990435 0.44755236 0.22694296]. \t  0.23199690080875204 \t 1.540625560354162\n",
      "init   \t [0.40170957 0.88225774 0.43721347]. \t  1.540625560354162 \t 1.540625560354162\n",
      "init   \t [0.87842518 0.78052775 0.53421429]. \t  0.5653855252190516 \t 1.540625560354162\n",
      "init   \t [0.01173301 0.26575648 0.3311941 ]. \t  0.5019682819961726 \t 1.540625560354162\n",
      "1      \t [0. 1. 1.]. \t  0.330219860606422 \t 1.540625560354162\n",
      "2      \t [ 1.00000000e+00 -5.55111512e-17  1.00000000e+00]. \t  0.08848201872702738 \t 1.540625560354162\n",
      "3      \t [0.08782525 0.         1.        ]. \t  0.09085443024069767 \t 1.540625560354162\n",
      "4      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 1.540625560354162\n",
      "5      \t [0.73164376 1.         1.        ]. \t  0.3266627934829137 \t 1.540625560354162\n",
      "6      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 1.540625560354162\n",
      "7      \t [1. 0. 0.]. \t  0.03095471703300515 \t 1.540625560354162\n",
      "8      \t [0. 0. 0.]. \t  0.06797411659013229 \t 1.540625560354162\n",
      "9      \t [0.41945685 0.48221997 1.        ]. \t  \u001b[92m1.9440519661006996\u001b[0m \t 1.9440519661006996\n",
      "10     \t [1.         0.         0.50394854]. \t  0.07161372389391657 \t 1.9440519661006996\n",
      "11     \t [0.         0.49475966 1.        ]. \t  \u001b[92m1.9534044972627354\u001b[0m \t 1.9534044972627354\n",
      "12     \t [1.         0.56238792 1.        ]. \t  \u001b[92m2.00836654830732\u001b[0m \t 2.00836654830732\n",
      "13     \t [0.4997039 1.        0.       ]. \t  0.00020413742242540012 \t 2.00836654830732\n",
      "14     \t [0.         1.         0.52617242]. \t  \u001b[92m2.420541633693648\u001b[0m \t 2.420541633693648\n",
      "15     \t [0.         0.         0.54252537]. \t  0.10282929926279445 \t 2.420541633693648\n",
      "16     \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.420541633693648\n",
      "17     \t [0.         0.71135098 0.68773055]. \t  \u001b[92m2.4937114866116765\u001b[0m \t 2.4937114866116765\n",
      "18     \t [0.46809753 0.         0.        ]. \t  0.09927235358592236 \t 2.4937114866116765\n",
      "19     \t [0.60591136 0.         0.78023478]. \t  0.25000606785843205 \t 2.4937114866116765\n",
      "20     \t [1.         0.31909604 0.75797856]. \t  1.9415935081922027 \t 2.4937114866116765\n",
      "21     \t [0.25185185 1.         0.73420101]. \t  1.2057092855847555 \t 2.4937114866116765\n",
      "22     \t [0.21616395 0.50293985 0.        ]. \t  0.02472257337391613 \t 2.4937114866116765\n",
      "23     \t [0.79211961 0.30952714 1.        ]. \t  1.0721201890722336 \t 2.4937114866116765\n",
      "24     \t [0.2820488  0.37602976 0.69524565]. \t  1.8068624177652977 \t 2.4937114866116765\n",
      "25     \t [1.         0.41596796 0.        ]. \t  0.014524372432628503 \t 2.4937114866116765\n",
      "26     \t [1.         1.         0.34235122]. \t  0.06133688780340304 \t 2.4937114866116765\n",
      "27     \t [0.24523698 0.         0.29688493]. \t  0.8117087749592349 \t 2.4937114866116765\n",
      "28     \t [1.         0.46754632 0.37626912]. \t  0.09239852958233778 \t 2.4937114866116765\n",
      "29     \t [0.27127689 0.80541347 1.        ]. \t  1.198479313085428 \t 2.4937114866116765\n",
      "30     \t [0.         0.70735717 0.22934634]. \t  0.11139459086244215 \t 2.4937114866116765\n",
      "31     \t [0.64908114 1.         0.44485087]. \t  0.7444777207414871 \t 2.4937114866116765\n",
      "32     \t [0.64135821 0.61664465 0.83718853]. \t  \u001b[92m3.6190282843892976\u001b[0m \t 3.6190282843892976\n",
      "33     \t [0.27242788 1.         0.15252372]. \t  0.01868477896645441 \t 3.6190282843892976\n",
      "34     \t [0.53533047 0.63502992 0.80768819]. \t  3.3872325023552543 \t 3.6190282843892976\n",
      "35     \t [0.         0.34100593 0.        ]. \t  0.047208891666861 \t 3.6190282843892976\n",
      "36     \t [0.75608763 0.74099063 1.        ]. \t  1.5303405698756933 \t 3.6190282843892976\n",
      "37     \t [0.99999997 1.         0.72913581]. \t  0.3642708794210003 \t 3.6190282843892976\n",
      "38     \t [0.62607049 0.49579345 0.72475038]. \t  2.522759638058592 \t 3.6190282843892976\n",
      "39     \t [0.         0.26367336 0.85734204]. \t  1.7435494303304822 \t 3.6190282843892976\n",
      "40     \t [0.25326293 0.15529018 0.0010484 ]. \t  0.11287660988575554 \t 3.6190282843892976\n",
      "41     \t [1.         0.80385541 0.        ]. \t  0.00034307813928786485 \t 3.6190282843892976\n",
      "42     \t [0.55029061 0.         1.        ]. \t  0.09152727537244132 \t 3.6190282843892976\n",
      "43     \t [0.51003573 0.73915746 0.81072533]. \t  2.6683024669159745 \t 3.6190282843892976\n",
      "44     \t [0.99927757 0.62970196 0.84082578]. \t  3.416796142328732 \t 3.6190282843892976\n",
      "45     \t [0.79119677 0.         0.3062449 ]. \t  0.48827311512851534 \t 3.6190282843892976\n",
      "46     \t [1.94312546e-09 8.22225082e-11 1.00000000e+00]. \t  0.09028946800101079 \t 3.6190282843892976\n",
      "47     \t [6.63221060e-01 3.91267526e-01 1.08188789e-07]. \t  0.0426252193413024 \t 3.6190282843892976\n",
      "48     \t [0.         0.99999992 0.82496059]. \t  0.7742600599034122 \t 3.6190282843892976\n",
      "49     \t [1.         0.         0.18134682]. \t  0.21153404675894702 \t 3.6190282843892976\n",
      "50     \t [0.82601497 0.61188576 0.8763501 ]. \t  3.5920519476599377 \t 3.6190282843892976\n",
      "51     \t [0.77466552 0.51472641 0.82920059]. \t  \u001b[92m3.658761375513091\u001b[0m \t 3.658761375513091\n",
      "52     \t [0.95543608 0.54071453 0.78284344]. \t  3.1920711030406923 \t 3.658761375513091\n",
      "53     \t [0.74179611 0.45393872 0.84370023]. \t  3.459665470938972 \t 3.658761375513091\n",
      "54     \t [0.80988271 0.57396563 0.87442142]. \t  \u001b[92m3.7066777756857445\u001b[0m \t 3.7066777756857445\n",
      "55     \t [0.78831681 0.6133951  0.8261288 ]. \t  3.515218565020726 \t 3.7066777756857445\n",
      "56     \t [5.11298429e-07 0.00000000e+00 1.95732558e-01]. \t  0.4972115819464701 \t 3.7066777756857445\n",
      "57     \t [0.54134615 0.55111158 0.84553353]. \t  \u001b[92m3.8245307409040867\u001b[0m \t 3.8245307409040867\n",
      "58     \t [0.90232639 0.52957889 0.86519088]. \t  3.6923795748979615 \t 3.8245307409040867\n",
      "59     \t [0.79536176 0.6383282  0.8702079 ]. \t  3.492262817819598 \t 3.8245307409040867\n",
      "60     \t [0.14060498 0.61584489 0.79904161]. \t  3.492433963530025 \t 3.8245307409040867\n",
      "61     \t [0.83904552 0.59078034 0.88926598]. \t  3.5925638950617884 \t 3.8245307409040867\n",
      "62     \t [0.68446677 0.51279333 0.81229842]. \t  3.580839860297626 \t 3.8245307409040867\n",
      "63     \t [0.46289451 0.59651343 0.84664731]. \t  3.7749669571843256 \t 3.8245307409040867\n",
      "64     \t [0.2197874  0.56140462 0.82618839]. \t  3.7918306753846847 \t 3.8245307409040867\n",
      "65     \t [0.50210644 0.60926051 0.86352891]. \t  3.7272114888041994 \t 3.8245307409040867\n",
      "66     \t [0.06418233 0.50351305 0.73509287]. \t  2.816985152498666 \t 3.8245307409040867\n",
      "67     \t [0.4776729  0.63141189 0.83429465]. \t  3.590361715172583 \t 3.8245307409040867\n",
      "68     \t [0.55810899 0.5456041  0.89752859]. \t  3.640309105343187 \t 3.8245307409040867\n",
      "69     \t [0.32013167 0.59082633 0.85144526]. \t  3.8148940195154895 \t 3.8245307409040867\n",
      "70     \t [0.65347555 0.52821071 0.88467281]. \t  3.692109724830107 \t 3.8245307409040867\n",
      "71     \t [0.35617063 0.61723116 0.69508498]. \t  2.4266705176826346 \t 3.8245307409040867\n",
      "72     \t [0.29494328 0.61369571 0.87519277]. \t  3.700837810097693 \t 3.8245307409040867\n",
      "73     \t [0.3662879  0.6236053  0.84029481]. \t  3.6738729433577983 \t 3.8245307409040867\n",
      "74     \t [0.60767664 0.52111926 0.86551856]. \t  3.769805716921334 \t 3.8245307409040867\n",
      "75     \t [0.4871254  0.52984346 0.82683505]. \t  3.7571558209719513 \t 3.8245307409040867\n",
      "76     \t [0.43021636 0.57769662 0.8488693 ]. \t  \u001b[92m3.8288949400132744\u001b[0m \t 3.8288949400132744\n",
      "77     \t [0.27905166 0.48833496 0.9016899 ]. \t  3.4538593011718115 \t 3.8288949400132744\n",
      "78     \t [0.86376683 0.55275623 0.86425874]. \t  3.7267047572284535 \t 3.8288949400132744\n",
      "79     \t [0.23971755 0.57924837 0.8705885 ]. \t  3.810420017853351 \t 3.8288949400132744\n",
      "80     \t [0.36969825 0.62193064 0.88236617]. \t  3.631351212969145 \t 3.8288949400132744\n",
      "81     \t [0.70537116 0.55344942 0.88799167]. \t  3.6847666261187584 \t 3.8288949400132744\n",
      "82     \t [0.69390742 0.5783171  0.83294935]. \t  3.7072361538013103 \t 3.8288949400132744\n",
      "83     \t [0.86849217 0.5815052  0.86750985]. \t  3.6892206450215186 \t 3.8288949400132744\n",
      "84     \t [0.45554389 0.56453606 0.87103392]. \t  3.817638523035912 \t 3.8288949400132744\n",
      "85     \t [0.31987897 0.57677953 0.8572652 ]. \t  \u001b[92m3.8429444041621084\u001b[0m \t 3.8429444041621084\n",
      "86     \t [0.56965328 0.53517017 0.84099421]. \t  3.7989218302072594 \t 3.8429444041621084\n",
      "87     \t [0.69616679 0.52192755 0.84846969]. \t  3.7583165101890335 \t 3.8429444041621084\n",
      "88     \t [0.19333861 0.52943629 0.84441475]. \t  3.82755004937123 \t 3.8429444041621084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.34791611 0.58703952 0.82966702]. \t  3.766291818936691 \t 3.8429444041621084\n",
      "90     \t [0.14392481 0.55259473 0.8598797 ]. \t  3.842481915551602 \t 3.8429444041621084\n",
      "91     \t [0.99283541 0.49417818 0.87946025]. \t  3.5052187616548487 \t 3.8429444041621084\n",
      "92     \t [0.66882374 0.62941179 0.81375748]. \t  3.402796766106826 \t 3.8429444041621084\n",
      "93     \t [0.68473257 0.56041232 0.85863421]. \t  3.7925072704550424 \t 3.8429444041621084\n",
      "94     \t [0.15838102 0.53702434 0.90588317]. \t  3.5519377057061154 \t 3.8429444041621084\n",
      "95     \t [0.18900851 0.54422568 0.87385412]. \t  3.804083153529043 \t 3.8429444041621084\n",
      "96     \t [0.10810471 0.53022162 0.80970496]. \t  3.664568127208052 \t 3.8429444041621084\n",
      "97     \t [0.45523434 0.6097238  0.80957371]. \t  3.5478833715175773 \t 3.8429444041621084\n",
      "98     \t [0.72665915 0.48520817 0.85434783]. \t  3.6328765874134885 \t 3.8429444041621084\n",
      "99     \t [0.61987901 0.50286908 0.85537619]. \t  3.729787091818289 \t 3.8429444041621084\n",
      "100    \t [0.7484816  0.568986   0.88041113]. \t  3.708972976074263 \t 3.8429444041621084\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \n",
    "\n",
    "np.random.seed(run_num_15)\n",
    "surrogate_winner_15 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_15 = GPGO(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_15.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.86394833 0.316366   0.67215078]. \t  1.1947633822631252 \t 3.8084053754826726\n",
      "init   \t [0.50791167 0.78166043 0.28368168]. \t  0.20052765717741963 \t 3.8084053754826726\n",
      "init   \t [0.23370878 0.56379969 0.87502436]. \t  3.8084053754826726 \t 3.8084053754826726\n",
      "init   \t [0.71894292 0.18213174 0.24380041]. \t  0.6532826728302027 \t 3.8084053754826726\n",
      "init   \t [0.08605673 0.53424539 0.83969965]. \t  3.8072565344663847 \t 3.8084053754826726\n",
      "1      \t [0.07508674 0.11940113 1.        ]. \t  0.2929009299584284 \t 3.8084053754826726\n",
      "2      \t [-1.11022302e-16  1.00000000e+00  1.00000000e+00]. \t  0.330219860606422 \t 3.8084053754826726\n",
      "3      \t [0.       0.172818 0.      ]. \t  0.0755569207126871 \t 3.8084053754826726\n",
      "4      \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.8084053754826726\n",
      "5      \t [0.17207956 0.49088407 0.64144794]. \t  1.7223392147640402 \t 3.8084053754826726\n",
      "6      \t [1.         0.52505114 0.        ]. \t  0.0067231824764441206 \t 3.8084053754826726\n",
      "7      \t [0.         0.55747196 1.        ]. \t  2.055857217058621 \t 3.8084053754826726\n",
      "8      \t [1. 0. 0.]. \t  0.03095471703300515 \t 3.8084053754826726\n",
      "9      \t [ 1.00000000e+00  1.00000000e+00 -5.55111512e-17]. \t  3.7727185179443895e-05 \t 3.8084053754826726\n",
      "10     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.8084053754826726\n",
      "11     \t [1. 0. 1.]. \t  0.08848201872702738 \t 3.8084053754826726\n",
      "12     \t [0.5029662 0.        0.       ]. \t  0.09687933911771353 \t 3.8084053754826726\n",
      "13     \t [0.48873143 1.         0.78758203]. \t  0.7543031001642269 \t 3.8084053754826726\n",
      "14     \t [0.57435553 1.         0.        ]. \t  0.00017319502700174813 \t 3.8084053754826726\n",
      "15     \t [0.         0.89779768 0.68937042]. \t  2.0958737453691394 \t 3.8084053754826726\n",
      "16     \t [1.         0.         0.52562241]. \t  0.06773733662839025 \t 3.8084053754826726\n",
      "17     \t [1.         1.         0.52006948]. \t  0.2410946224898114 \t 3.8084053754826726\n",
      "18     \t [0.         0.61075089 0.        ]. \t  0.006950550328419048 \t 3.8084053754826726\n",
      "19     \t [0.         0.         0.66744546]. \t  0.16597181009198225 \t 3.8084053754826726\n",
      "20     \t [0.60490072 0.         0.93381916]. \t  0.15894137960712051 \t 3.8084053754826726\n",
      "21     \t [0.6408086  0.65816841 1.        ]. \t  1.9125890947862703 \t 3.8084053754826726\n",
      "22     \t [0.15927596 0.69219639 0.85354596]. \t  3.25663850950185 \t 3.8084053754826726\n",
      "23     \t [0.67692253 0.51248433 0.        ]. \t  0.01848964686376681 \t 3.8084053754826726\n",
      "24     \t [1.         0.44906139 1.        ]. \t  1.7519638894074143 \t 3.8084053754826726\n",
      "25     \t [0.33694455 0.40744361 0.95212875]. \t  2.3214573127789135 \t 3.8084053754826726\n",
      "26     \t [0.67473688 0.67490531 0.73292872]. \t  2.284545958499262 \t 3.8084053754826726\n",
      "27     \t [1.         0.63498863 0.44096448]. \t  0.14816669810108704 \t 3.8084053754826726\n",
      "28     \t [0.85967965 1.         0.22408363]. \t  0.017940104913254727 \t 3.8084053754826726\n",
      "29     \t [0.         0.28263023 0.82963248]. \t  1.9538311686249195 \t 3.8084053754826726\n",
      "30     \t [1.39752346e-09 0.00000000e+00 0.00000000e+00]. \t  0.06797411680039486 \t 3.8084053754826726\n",
      "31     \t [3.16335372e-01 1.93791501e-08 7.28747140e-01]. \t  0.22616402202037453 \t 3.8084053754826726\n",
      "32     \t [0.2674576  0.36714012 0.        ]. \t  0.06082800941257266 \t 3.8084053754826726\n",
      "33     \t [0.24194783 0.99202441 0.32014807]. \t  0.4747948390812326 \t 3.8084053754826726\n",
      "34     \t [0.         0.         0.91551096]. \t  0.17568728536432432 \t 3.8084053754826726\n",
      "35     \t [7.33592338e-01 3.94246029e-08 5.00320801e-01]. \t  0.136552526136651 \t 3.8084053754826726\n",
      "36     \t [1.         0.71069532 0.86671778]. \t  2.8824942305708277 \t 3.8084053754826726\n",
      "37     \t [1.03047021e-01 6.14824686e-09 2.17917690e-01]. \t  0.6557387860340389 \t 3.8084053754826726\n",
      "38     \t [0.         0.78763701 0.30299136]. \t  0.40960393788897603 \t 3.8084053754826726\n",
      "39     \t [4.54781265e-04 5.75026051e-01 8.25804966e-01]. \t  3.730587116959958 \t 3.8084053754826726\n",
      "40     \t [0.06185243 0.59715066 0.85415526]. \t  3.7705603778480974 \t 3.8084053754826726\n",
      "41     \t [0.01701724 0.56729604 0.84207567]. \t  3.8008516643178782 \t 3.8084053754826726\n",
      "42     \t [0.08531183 0.60031852 0.85120611]. \t  3.7669953054830256 \t 3.8084053754826726\n",
      "43     \t [0.10792407 0.64141394 0.86519221]. \t  3.5823843292482125 \t 3.8084053754826726\n",
      "44     \t [9.99999997e-01 1.94339909e-01 5.67425784e-09]. \t  0.03343517862349923 \t 3.8084053754826726\n",
      "45     \t [1.49589383e-04 6.01327133e-01 8.20444305e-01]. \t  3.644787607386588 \t 3.8084053754826726\n",
      "46     \t [0.088315   0.55897867 0.85682939]. \t  \u001b[92m3.834535832040667\u001b[0m \t 3.834535832040667\n",
      "47     \t [0.11908485 0.58958303 0.83209633]. \t  3.7639029838628995 \t 3.834535832040667\n",
      "48     \t [2.05990801e-01 8.97566231e-01 3.11146058e-07]. \t  0.0005286741814395931 \t 3.834535832040667\n",
      "49     \t [0.07642079 0.55468944 0.86310849]. \t  3.8225480245635226 \t 3.834535832040667\n",
      "50     \t [0.10044796 0.52212701 0.87756453]. \t  3.733032748772543 \t 3.834535832040667\n",
      "51     \t [0.86053648 0.97630966 0.79781242]. \t  0.6266784097985962 \t 3.834535832040667\n",
      "52     \t [0.04345529 0.65940503 0.84965698]. \t  3.4702595428248726 \t 3.834535832040667\n",
      "53     \t [0.3318536  0.97761753 0.99767169]. \t  0.410011090061879 \t 3.834535832040667\n",
      "54     \t [1.         0.73452116 0.99999998]. \t  1.525458798318307 \t 3.834535832040667\n",
      "55     \t [0.24784751 0.         1.        ]. \t  0.09152884606642028 \t 3.834535832040667\n",
      "56     \t [0.07073502 0.53615646 0.8771541 ]. \t  3.7560978694486398 \t 3.834535832040667\n",
      "57     \t [0.22919796 0.64189721 0.84653793]. \t  3.6017045317451863 \t 3.834535832040667\n",
      "58     \t [0.85271566 0.5938461  0.89869618]. \t  3.5131406796053333 \t 3.834535832040667\n",
      "59     \t [0.79821258 0.44717416 0.94556333]. \t  2.630572727176866 \t 3.834535832040667\n",
      "60     \t [1.00000000e+00 4.19399116e-07 8.21409700e-01]. \t  0.24005046671973185 \t 3.834535832040667\n",
      "61     \t [0.11280494 0.53382274 0.80833144]. \t  3.6605051881229835 \t 3.834535832040667\n",
      "62     \t [0.13718202 0.52044274 0.91206784]. \t  3.4452616045125963 \t 3.834535832040667\n",
      "63     \t [0.19396969 0.68182326 0.82359752]. \t  3.2823438381357812 \t 3.834535832040667\n",
      "64     \t [0.08396861 0.47948844 0.89881785]. \t  3.4104619798152576 \t 3.834535832040667\n",
      "65     \t [0.88337521 0.         0.13417471]. \t  0.23163143597825805 \t 3.834535832040667\n",
      "66     \t [0.04860075 0.61903474 0.81938855]. \t  3.5938977021928387 \t 3.834535832040667\n",
      "67     \t [0.         1.         0.81525257]. \t  0.8032847383754387 \t 3.834535832040667\n",
      "68     \t [0.0796643  0.536108   0.88064967]. \t  3.7395446042988794 \t 3.834535832040667\n",
      "69     \t [0.08688444 0.6271243  0.92520369]. \t  3.196413594926658 \t 3.834535832040667\n",
      "70     \t [0.2379653  0.56620641 0.84659914]. \t  \u001b[92m3.8514974443651546\u001b[0m \t 3.8514974443651546\n",
      "71     \t [0.44069882 0.47538723 0.79461575]. \t  3.3819593056487145 \t 3.8514974443651546\n",
      "72     \t [0.25025865 0.49817968 0.80930379]. \t  3.6007228933938826 \t 3.8514974443651546\n",
      "73     \t [0.96568491 0.57587396 0.82438204]. \t  3.5463247813830288 \t 3.8514974443651546\n",
      "74     \t [0.05736842 0.4879553  0.82102697]. \t  3.602568948532407 \t 3.8514974443651546\n",
      "75     \t [0.40832333 0.54854892 0.85297237]. \t  \u001b[92m3.8542209203846474\u001b[0m \t 3.8542209203846474\n",
      "76     \t [0.51989299 0.59820417 0.81655199]. \t  3.6173031921829955 \t 3.8542209203846474\n",
      "77     \t [0.88721719 0.58590453 0.81744956]. \t  3.5074431251266347 \t 3.8542209203846474\n",
      "78     \t [0.31570161 0.58666302 0.88680737]. \t  3.719656721684636 \t 3.8542209203846474\n",
      "79     \t [0.49254834 0.57721608 0.81827002]. \t  3.69147813877718 \t 3.8542209203846474\n",
      "80     \t [0.17877232 0.56354459 0.86455409]. \t  3.8376933643674103 \t 3.8542209203846474\n",
      "81     \t [0.50582512 0.56983854 0.90053025]. \t  3.622777062742224 \t 3.8542209203846474\n",
      "82     \t [0.45538443 0.54676863 0.85461912]. \t  3.8474944398734765 \t 3.8542209203846474\n",
      "83     \t [0.6683905  0.55267886 0.84459153]. \t  3.787407445243658 \t 3.8542209203846474\n",
      "84     \t [0.06028526 0.65060778 0.78171677]. \t  3.1984273993169476 \t 3.8542209203846474\n",
      "85     \t [0.43836018 0.65505649 0.86875082]. \t  3.493430563750225 \t 3.8542209203846474\n",
      "86     \t [0.73550657 0.57307334 0.88608439]. \t  3.6781523966723073 \t 3.8542209203846474\n",
      "87     \t [0.32014186 0.4927334  0.85450152]. \t  3.7278381942572056 \t 3.8542209203846474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     \t [0.13044132 0.58809795 0.87017856]. \t  3.781360571917484 \t 3.8542209203846474\n",
      "89     \t [0.07412994 0.60184889 0.84501703]. \t  3.7536695536451683 \t 3.8542209203846474\n",
      "90     \t [0.06092941 0.5655031  0.85550129]. \t  3.8255131095708985 \t 3.8542209203846474\n",
      "91     \t [0.31402878 0.46534755 0.84569053]. \t  3.59167572041799 \t 3.8542209203846474\n",
      "92     \t [0.40161686 0.51766142 0.84997913]. \t  3.8096405055789813 \t 3.8542209203846474\n",
      "93     \t [0.21066949 0.56809281 0.79998352]. \t  3.611303020689033 \t 3.8542209203846474\n",
      "94     \t [0.35389871 0.53837809 0.87350463]. \t  3.8076405716292836 \t 3.8542209203846474\n",
      "95     \t [0.28068152 0.58823198 0.79639273]. \t  3.5461195678693453 \t 3.8542209203846474\n",
      "96     \t [0.16126629 0.53280991 0.91131986]. \t  3.4857788657651545 \t 3.8542209203846474\n",
      "97     \t [0.25198225 0.5943798  0.79597747]. \t  3.53193393321682 \t 3.8542209203846474\n",
      "98     \t [0.60075337 0.60321411 0.81045252]. \t  3.521705392593244 \t 3.8542209203846474\n",
      "99     \t [0.01656921 0.65373082 0.83730304]. \t  3.4785760357885573 \t 3.8542209203846474\n",
      "100    \t [0.21353895 0.59635412 0.86288813]. \t  3.7909707771150956 \t 3.8542209203846474\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \n",
    "\n",
    "np.random.seed(run_num_16)\n",
    "surrogate_winner_16 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_16 = GPGO(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_16.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.65182403 0.73051962 0.19336736]. \t  0.03562358530655929 \t 3.1179188940604616\n",
      "init   \t [0.51967724 0.67638327 0.80444487]. \t  3.1179188940604616 \t 3.1179188940604616\n",
      "init   \t [0.53351286 0.90803418 0.44348113]. \t  1.1984486017622078 \t 3.1179188940604616\n",
      "init   \t [0.25597221 0.27866502 0.04332186]. \t  0.1645724004847893 \t 3.1179188940604616\n",
      "init   \t [0.46897499 0.9416138  0.60040624]. \t  1.9192522196289552 \t 3.1179188940604616\n",
      "1      \t [0.         0.16484254 1.        ]. \t  0.4237621082702968 \t 3.1179188940604616\n",
      "2      \t [1.         0.74353786 1.        ]. \t  1.4796276756869056 \t 3.1179188940604616\n",
      "3      \t [0.98940663 0.         1.        ]. \t  0.08859374492103808 \t 3.1179188940604616\n",
      "4      \t [0.         0.84716963 1.        ]. \t  0.9572229991077599 \t 3.1179188940604616\n",
      "5      \t [1.         0.         0.16015001]. \t  0.18708856153714423 \t 3.1179188940604616\n",
      "6      \t [0.44366897 0.         0.55328228]. \t  0.12451506127413901 \t 3.1179188940604616\n",
      "7      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 3.1179188940604616\n",
      "8      \t [0.51304984 0.70991379 1.        ]. \t  1.708223995438932 \t 3.1179188940604616\n",
      "9      \t [0.         0.59833204 0.53250889]. \t  1.5724443706719724 \t 3.1179188940604616\n",
      "10     \t [1.         0.43585001 0.6459518 ]. \t  1.2063917713873749 \t 3.1179188940604616\n",
      "11     \t [1.         1.         0.73780442]. \t  0.3771347000333301 \t 3.1179188940604616\n",
      "12     \t [0.66815603 0.         0.        ]. \t  0.07815740378144706 \t 3.1179188940604616\n",
      "13     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.1179188940604616\n",
      "14     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 3.1179188940604616\n",
      "15     \t [0.44628 0.      1.     ]. \t  0.09171694511809744 \t 3.1179188940604616\n",
      "16     \t [0.27503891 0.51946881 0.78737244]. \t  \u001b[92m3.468884676975459\u001b[0m \t 3.468884676975459\n",
      "17     \t [1.         0.31551378 0.        ]. \t  0.023936117617873915 \t 3.468884676975459\n",
      "18     \t [0.         1.         0.61918193]. \t  2.249392498988173 \t 3.468884676975459\n",
      "19     \t [0.48752241 1.         0.        ]. \t  0.00020907019855402942 \t 3.468884676975459\n",
      "20     \t [0.59363442 0.36317404 0.82279896]. \t  2.736728321446724 \t 3.468884676975459\n",
      "21     \t [0.         0.         0.64338141]. \t  0.14275631378870157 \t 3.468884676975459\n",
      "22     \t [0.         0.59412168 0.        ]. \t  0.008131147574298635 \t 3.468884676975459\n",
      "23     \t [1.         0.38420884 1.        ]. \t  1.447356591011883 \t 3.468884676975459\n",
      "24     \t [0.17001291 1.         1.        ]. \t  0.33367147619726606 \t 3.468884676975459\n",
      "25     \t [0.27365532 0.72040667 0.57080463]. \t  2.4426205375022345 \t 3.468884676975459\n",
      "26     \t [1.         0.         0.70294943]. \t  0.19673242146484432 \t 3.468884676975459\n",
      "27     \t [0.8144157 1.        1.       ]. \t  0.3239858360335569 \t 3.468884676975459\n",
      "28     \t [0.81402787 0.19215307 0.35098315]. \t  0.42733324921517807 \t 3.468884676975459\n",
      "29     \t [0.        0.1434556 0.2617272]. \t  0.6607563093922362 \t 3.468884676975459\n",
      "30     \t [0.45454098 0.53617282 0.68810265]. \t  2.198557857761883 \t 3.468884676975459\n",
      "31     \t [0.09096328 0.78848735 0.75843281]. \t  2.316680353680668 \t 3.468884676975459\n",
      "32     \t [7.86368336e-13 9.99946360e-01 3.29243474e-01]. \t  0.5368813785763863 \t 3.468884676975459\n",
      "33     \t [0.11685434 0.34137019 0.79741349]. \t  2.447738341837478 \t 3.468884676975459\n",
      "34     \t [0.23563855 0.         0.11430347]. \t  0.40967400937210574 \t 3.468884676975459\n",
      "35     \t [1.         0.59567197 0.15825161]. \t  0.022586962000716786 \t 3.468884676975459\n",
      "36     \t [0.26721695 0.41054021 0.99999973]. \t  1.6382774907822268 \t 3.468884676975459\n",
      "37     \t [0.81930922 0.60283932 0.887581  ]. \t  \u001b[92m3.575992523164776\u001b[0m \t 3.575992523164776\n",
      "38     \t [7.62230704e-01 1.65814466e-11 7.84538193e-01]. \t  0.2486570450202728 \t 3.575992523164776\n",
      "39     \t [1.         1.         0.23131484]. \t  0.01037297025856663 \t 3.575992523164776\n",
      "40     \t [3.14628794e-08 9.99999964e-01 8.65346596e-01]. \t  0.6766356762745755 \t 3.575992523164776\n",
      "41     \t [0.83081184 0.99851587 0.02280735]. \t  0.00014775910435782652 \t 3.575992523164776\n",
      "42     \t [0.79497382 0.79497382 0.79497384]. \t  1.9052242532324009 \t 3.575992523164776\n",
      "43     \t [0.09928868 0.         0.99999976]. \t  0.09091829479453699 \t 3.575992523164776\n",
      "44     \t [7.59832446e-01 5.72817837e-01 1.08271605e-05]. \t  0.009317247741546133 \t 3.575992523164776\n",
      "45     \t [0.76763664 0.52422773 0.9157722 ]. \t  3.3724509985785063 \t 3.575992523164776\n",
      "46     \t [0.99999999 1.         0.99999998]. \t  0.31688367388431793 \t 3.575992523164776\n",
      "47     \t [0.33891276 0.99999991 0.28068837]. \t  0.22965247459987356 \t 3.575992523164776\n",
      "48     \t [1.00000000e+00 1.11751301e-07 0.00000000e+00]. \t  0.0309547251276231 \t 3.575992523164776\n",
      "49     \t [0.00943835 0.63017828 0.89394183]. \t  3.479340170035955 \t 3.575992523164776\n",
      "50     \t [0.11196565 0.57207886 0.85416251]. \t  \u001b[92m3.832525811684745\u001b[0m \t 3.832525811684745\n",
      "51     \t [0.00000000e+00 2.48628071e-01 2.11496545e-08]. \t  0.06555197541360956 \t 3.832525811684745\n",
      "52     \t [2.38931636e-07 5.36091381e-01 9.99999737e-01]. \t  2.0378822247295205 \t 3.832525811684745\n",
      "53     \t [0.75169261 0.4103934  0.99532861]. \t  1.685652318825027 \t 3.832525811684745\n",
      "54     \t [0.35816367 0.58616054 0.85245891]. \t  3.823826733426621 \t 3.832525811684745\n",
      "55     \t [0.5303114  0.60924847 0.85482475]. \t  3.7247590419638543 \t 3.832525811684745\n",
      "56     \t [0.80687223 0.50929445 0.83292369]. \t  3.6511682502687544 \t 3.832525811684745\n",
      "57     \t [0.68280622 0.54208468 0.82677662]. \t  3.711415529470189 \t 3.832525811684745\n",
      "58     \t [0.22766076 0.60392399 0.82235184]. \t  3.691414729604961 \t 3.832525811684745\n",
      "59     \t [0.11293304 0.57314507 0.85437512]. \t  3.83141849877468 \t 3.832525811684745\n",
      "60     \t [0.16404367 0.81974924 0.03995033]. \t  0.0023599088778576936 \t 3.832525811684745\n",
      "61     \t [7.62205714e-01 2.41549353e-04 2.28840379e-01]. \t  0.5248157802920506 \t 3.832525811684745\n",
      "62     \t [0.84524226 0.57755745 0.84675021]. \t  3.7000405358800523 \t 3.832525811684745\n",
      "63     \t [0.92619571 0.49125989 0.86353706]. \t  3.5780131981626058 \t 3.832525811684745\n",
      "64     \t [0.37473371 0.66767014 0.87668444]. \t  3.3939129107915464 \t 3.832525811684745\n",
      "65     \t [0.1637473  0.61947206 0.80975277]. \t  3.5578897868967374 \t 3.832525811684745\n",
      "66     \t [0.0033798  0.61768491 0.81983961]. \t  3.585962056706626 \t 3.832525811684745\n",
      "67     \t [0.807605   0.51916932 0.80456447]. \t  3.481123669798251 \t 3.832525811684745\n",
      "68     \t [0.04620393 0.62054444 0.85232195]. \t  3.682444347433167 \t 3.832525811684745\n",
      "69     \t [0.09242895 0.58401918 0.81684372]. \t  3.695469783619871 \t 3.832525811684745\n",
      "70     \t [0.03043002 0.55365845 0.84809944]. \t  3.8189294254843715 \t 3.832525811684745\n",
      "71     \t [0.08694098 0.55728279 0.84279917]. \t  3.8276894368877628 \t 3.832525811684745\n",
      "72     \t [0.18611268 0.51218358 0.78794693]. \t  3.458731265848717 \t 3.832525811684745\n",
      "73     \t [0.33833847 0.47539656 0.87150038]. \t  3.6012942631832976 \t 3.832525811684745\n",
      "74     \t [0.8164848  0.49448314 0.81679688]. \t  3.5189316168496485 \t 3.832525811684745\n",
      "75     \t [0.50626713 0.57877589 0.83649483]. \t  3.780949829184512 \t 3.832525811684745\n",
      "76     \t [0.14317004 0.62598688 0.77885482]. \t  3.2887925632578012 \t 3.832525811684745\n",
      "77     \t [0.07170851 0.57333203 0.84139345]. \t  3.809500036006538 \t 3.832525811684745\n",
      "78     \t [0.31363731 0.60692359 0.80456387]. \t  3.5577914377074142 \t 3.832525811684745\n",
      "79     \t [0.07151121 0.63553636 0.85790768]. \t  3.6162959052515844 \t 3.832525811684745\n",
      "80     \t [0.99496218 0.79468566 0.05444872]. \t  0.0009496889936742377 \t 3.832525811684745\n",
      "81     \t [0.11445659 0.56450754 0.87695938]. \t  3.7818782666094624 \t 3.832525811684745\n",
      "82     \t [0.12809953 0.64319692 0.87886278]. \t  3.5308342440356726 \t 3.832525811684745\n",
      "83     \t [0.27535266 0.61161526 0.86852591]. \t  3.73109187617871 \t 3.832525811684745\n",
      "84     \t [0.1489341  0.5939359  0.82331715]. \t  3.7196410448364263 \t 3.832525811684745\n",
      "85     \t [0.09193734 0.61321285 0.84694692]. \t  3.7207741360644984 \t 3.832525811684745\n",
      "86     \t [0.66654787 0.55489635 0.87672866]. \t  3.7566116382623895 \t 3.832525811684745\n",
      "87     \t [0.43563422 0.57535857 0.83388054]. \t  3.7929148829603063 \t 3.832525811684745\n",
      "88     \t [0.07414726 0.60314139 0.89546282]. \t  3.5871874459624085 \t 3.832525811684745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.25595994 0.54700625 0.84137901]. \t  \u001b[92m3.8466845086838832\u001b[0m \t 3.8466845086838832\n",
      "90     \t [0.46895803 0.55730655 0.86723613]. \t  3.8299709955292145 \t 3.8466845086838832\n",
      "91     \t [0.62286553 0.51232416 0.83071445]. \t  3.7090981155778033 \t 3.8466845086838832\n",
      "92     \t [0.91334588 0.50867019 0.89564803]. \t  3.4827985955144687 \t 3.8466845086838832\n",
      "93     \t [0.14313954 0.55366964 0.80960785]. \t  3.686967005062959 \t 3.8466845086838832\n",
      "94     \t [0.07423357 0.64133616 0.82406195]. \t  3.5205030869577483 \t 3.8466845086838832\n",
      "95     \t [0.49798336 0.56154612 0.81747095]. \t  3.7058749243719067 \t 3.8466845086838832\n",
      "96     \t [0.02711159 0.59690034 0.78751511]. \t  3.4227280198392096 \t 3.8466845086838832\n",
      "97     \t [0.69679067 0.57558748 0.82492903]. \t  3.6671784397956975 \t 3.8466845086838832\n",
      "98     \t [0.08318335 0.5999498  0.85911038]. \t  3.7647973811711997 \t 3.8466845086838832\n",
      "99     \t [0.48974353 0.52558157 0.83322613]. \t  3.7783752111390907 \t 3.8466845086838832\n",
      "100    \t [0.6030834  0.56780367 0.82718772]. \t  3.727205092445762 \t 3.8466845086838832\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \n",
    "\n",
    "np.random.seed(run_num_17)\n",
    "surrogate_winner_17 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_17 = GPGO(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_17.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.8734294  0.96854066 0.86919454]. \t  0.722531189286755 \t 1.1210522139432408\n",
      "init   \t [0.53085569 0.23272833 0.0113988 ]. \t  0.1133646794700679 \t 1.1210522139432408\n",
      "init   \t [0.43046882 0.40235136 0.52267467]. \t  0.5232646985053151 \t 1.1210522139432408\n",
      "init   \t [0.4783918  0.55535647 0.54338602]. \t  1.0390748854353227 \t 1.1210522139432408\n",
      "init   \t [0.76089558 0.71237457 0.6196821 ]. \t  1.1210522139432408 \t 1.1210522139432408\n",
      "1      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 1.1210522139432408\n",
      "2      \t [0. 1. 1.]. \t  0.330219860606422 \t 1.1210522139432408\n",
      "3      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 1.1210522139432408\n",
      "4      \t [1. 0. 1.]. \t  0.08848201872702738 \t 1.1210522139432408\n",
      "5      \t [1. 0. 0.]. \t  0.03095471703300515 \t 1.1210522139432408\n",
      "6      \t [0. 0. 1.]. \t  0.0902894676548261 \t 1.1210522139432408\n",
      "7      \t [5.55111512e-17 0.00000000e+00 0.00000000e+00]. \t  0.06797411659013229 \t 1.1210522139432408\n",
      "8      \t [0.52366665 0.55162589 1.        ]. \t  \u001b[92m2.078513916577869\u001b[0m \t 2.078513916577869\n",
      "9      \t [0.         0.51329878 1.        ]. \t  1.9990938858071161 \t 2.078513916577869\n",
      "10     \t [1.        0.5733809 1.       ]. \t  2.0080858446803007 \t 2.078513916577869\n",
      "11     \t [0.         0.50815672 0.        ]. \t  0.016952286482904085 \t 2.078513916577869\n",
      "12     \t [0.50263244 1.         0.        ]. \t  0.0002029439457237927 \t 2.078513916577869\n",
      "13     \t [1.         0.49706409 0.        ]. \t  0.008378899515449557 \t 2.078513916577869\n",
      "14     \t [0.         1.         0.50576303]. \t  \u001b[92m2.304116417005264\u001b[0m \t 2.304116417005264\n",
      "15     \t [0.48816845 0.         1.        ]. \t  0.09166435771220957 \t 2.304116417005264\n",
      "16     \t [0.         0.63845935 0.6541368 ]. \t  \u001b[92m2.308542847764925\u001b[0m \t 2.308542847764925\n",
      "17     \t [1.         0.         0.48933349]. \t  0.07745654121560969 \t 2.308542847764925\n",
      "18     \t [1.         1.         0.42719819]. \t  0.14680915437802092 \t 2.308542847764925\n",
      "19     \t [0.30478202 1.         0.72956008]. \t  1.1902729673891508 \t 2.308542847764925\n",
      "20     \t [0.         0.         0.40644093]. \t  0.32807079157850955 \t 2.308542847764925\n",
      "21     \t [0.59343717 0.         0.21729852]. \t  0.6955187389328785 \t 2.308542847764925\n",
      "22     \t [0.         0.74646584 0.31227834]. \t  0.4353776267474793 \t 2.308542847764925\n",
      "23     \t [0.27809233 0.         0.69893562]. \t  0.20100647355736265 \t 2.308542847764925\n",
      "24     \t [0.80899231 0.27679701 1.        ]. \t  0.8989052652878347 \t 2.308542847764925\n",
      "25     \t [0.70621011 1.         0.29315839]. \t  0.11292702323835606 \t 2.308542847764925\n",
      "26     \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.308542847764925\n",
      "27     \t [0.69736335 0.69683557 0.        ]. \t  0.003016584734955552 \t 2.308542847764925\n",
      "28     \t [0.72455027 0.         0.73422285]. \t  0.22800326693412432 \t 2.308542847764925\n",
      "29     \t [0.         0.81510197 0.76129049]. \t  2.1138448092056974 \t 2.308542847764925\n",
      "30     \t [1.         0.36206591 0.79590068]. \t  \u001b[92m2.5224253313851017\u001b[0m \t 2.5224253313851017\n",
      "31     \t [0.64325748 1.         1.        ]. \t  0.3291652327459951 \t 2.5224253313851017\n",
      "32     \t [3.22888210e-01 0.00000000e+00 9.90850915e-13]. \t  0.10159929150288631 \t 2.5224253313851017\n",
      "33     \t [0.76235076 0.         0.        ]. \t  0.06426252899475202 \t 2.5224253313851017\n",
      "34     \t [0.         0.15877875 0.88563266]. \t  0.8351861601258257 \t 2.5224253313851017\n",
      "35     \t [1.         0.23827818 0.33182924]. \t  0.23365361285483277 \t 2.5224253313851017\n",
      "36     \t [0.21535281 0.67138732 0.92610547]. \t  \u001b[92m2.976195666433978\u001b[0m \t 2.976195666433978\n",
      "37     \t [0.20729914 0.99999998 0.23372885]. \t  0.11321387171575227 \t 2.976195666433978\n",
      "38     \t [0.00370631 0.2689793  0.29244571]. \t  0.5322375273650785 \t 2.976195666433978\n",
      "39     \t [0.18935423 0.28046061 0.99999999]. \t  0.9302306706593062 \t 2.976195666433978\n",
      "40     \t [0.35129437 0.7491325  1.        ]. \t  1.511664278961269 \t 2.976195666433978\n",
      "41     \t [0.99999992 0.66672919 0.7760804 ]. \t  2.6130455655849056 \t 2.976195666433978\n",
      "42     \t [1.         0.79668877 0.14478522]. \t  0.0035946972424834044 \t 2.976195666433978\n",
      "43     \t [0.3892045  0.56611629 0.79855714]. \t  \u001b[92m3.578544509343603\u001b[0m \t 3.578544509343603\n",
      "44     \t [0.45902852 0.51409905 0.86991543]. \t  \u001b[92m3.7634929252053553\u001b[0m \t 3.7634929252053553\n",
      "45     \t [0.15916751 0.52186354 0.8221668 ]. \t  3.735286570622132 \t 3.7634929252053553\n",
      "46     \t [0.35416178 0.54648136 0.84785968]. \t  \u001b[92m3.8550388754342437\u001b[0m \t 3.8550388754342437\n",
      "47     \t [0.71391684 0.54946888 0.88714648]. \t  3.6865347296514095 \t 3.8550388754342437\n",
      "48     \t [0.49469454 0.5474933  0.85335067]. \t  3.8414068589346515 \t 3.8550388754342437\n",
      "49     \t [0.77208429 0.52901909 0.82125144]. \t  3.636841701345817 \t 3.8550388754342437\n",
      "50     \t [0.23082112 0.51981109 0.85882598]. \t  3.810184033282763 \t 3.8550388754342437\n",
      "51     \t [0.56315064 0.52995123 0.84836773]. \t  3.8075377434301636 \t 3.8550388754342437\n",
      "52     \t [0.16782662 0.60363178 0.85410864]. \t  3.7723368097939387 \t 3.8550388754342437\n",
      "53     \t [0.37398462 0.44826558 0.85639717]. \t  3.4781382239617766 \t 3.8550388754342437\n",
      "54     \t [0.4203164  0.53693494 0.83331019]. \t  3.8058338903377753 \t 3.8550388754342437\n",
      "55     \t [0.2379928  0.58575777 0.86214264]. \t  3.819811947872349 \t 3.8550388754342437\n",
      "56     \t [0.28436178 0.53660734 0.83885419]. \t  3.8328285254927414 \t 3.8550388754342437\n",
      "57     \t [0.25949261 0.52742295 0.85295643]. \t  3.8341577796788573 \t 3.8550388754342437\n",
      "58     \t [0.30225995 0.54110443 0.85924339]. \t  3.850527088094071 \t 3.8550388754342437\n",
      "59     \t [0.49414726 0.57687811 0.83764265]. \t  3.791554422939629 \t 3.8550388754342437\n",
      "60     \t [0.43826718 0.52545086 0.87044294]. \t  3.792247341181973 \t 3.8550388754342437\n",
      "61     \t [0.03274552 0.49433205 0.82010498]. \t  3.615918920174174 \t 3.8550388754342437\n",
      "62     \t [0.12583939 0.53821094 0.85233906]. \t  3.8346421445733085 \t 3.8550388754342437\n",
      "63     \t [0.72160458 0.52892569 0.86315764]. \t  3.7596745948788 \t 3.8550388754342437\n",
      "64     \t [0.70881154 0.56442765 0.89117796]. \t  3.6602456894915916 \t 3.8550388754342437\n",
      "65     \t [0.60083425 0.54440845 0.86481586]. \t  3.807286832906732 \t 3.8550388754342437\n",
      "66     \t [0.4388733  0.50269102 0.82061755]. \t  3.6728888922147287 \t 3.8550388754342437\n",
      "67     \t [0.68064318 0.53083709 0.87019285]. \t  3.757598362474253 \t 3.8550388754342437\n",
      "68     \t [0.5116967  0.6023511  0.86043261]. \t  3.753205413127553 \t 3.8550388754342437\n",
      "69     \t [0.90696403 0.50357807 0.89089319]. \t  3.504736111864369 \t 3.8550388754342437\n",
      "70     \t [0.41973997 0.61250422 0.81805395]. \t  3.6082376633646365 \t 3.8550388754342437\n",
      "71     \t [0.02625219 0.55786579 0.84645774]. \t  3.815789616347213 \t 3.8550388754342437\n",
      "72     \t [0.24878524 0.56947782 0.86426663]. \t  3.840916042002484 \t 3.8550388754342437\n",
      "73     \t [0.6205178  0.46815578 0.85552278]. \t  3.573167619190373 \t 3.8550388754342437\n",
      "74     \t [0.09175818 0.5107623  0.81961765]. \t  3.6823338961985392 \t 3.8550388754342437\n",
      "75     \t [0.84731135 0.56909259 0.86038837]. \t  3.726225548474351 \t 3.8550388754342437\n",
      "76     \t [0.21774654 0.60955027 0.83948732]. \t  3.738703500869203 \t 3.8550388754342437\n",
      "77     \t [0.39069322 0.49608983 0.8277802 ]. \t  3.6918614241565475 \t 3.8550388754342437\n",
      "78     \t [0.26065029 0.59573368 0.8698221 ]. \t  3.778615657348264 \t 3.8550388754342437\n",
      "79     \t [0.41104443 0.52636092 0.84153467]. \t  3.8166588087274276 \t 3.8550388754342437\n",
      "80     \t [0.71274456 0.59195057 0.87774894]. \t  3.690753114881555 \t 3.8550388754342437\n",
      "81     \t [0.2814167  0.55188973 0.81134526]. \t  3.7058421079583477 \t 3.8550388754342437\n",
      "82     \t [0.62597116 0.55066637 0.85060037]. \t  3.810133704180711 \t 3.8550388754342437\n",
      "83     \t [0.24625677 0.58182177 0.85524616]. \t  3.8349771677194155 \t 3.8550388754342437\n",
      "84     \t [0.16883855 0.53646462 0.82004888]. \t  3.7482387849161896 \t 3.8550388754342437\n",
      "85     \t [0.16730157 0.50270775 0.83847782]. \t  3.743949726876562 \t 3.8550388754342437\n",
      "86     \t [0.41170721 0.53357721 0.8776474 ]. \t  3.7788738505433788 \t 3.8550388754342437\n",
      "87     \t [0.55959805 0.56037191 0.86035019]. \t  3.824701922716385 \t 3.8550388754342437\n",
      "88     \t [0.76471428 0.59732476 0.82755583]. \t  3.597197965441242 \t 3.8550388754342437\n",
      "89     \t [0.30152842 0.55281078 0.82864814]. \t  3.805872527480709 \t 3.8550388754342437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.71764436 0.50120913 0.89630177]. \t  3.513416336552514 \t 3.8550388754342437\n",
      "91     \t [0.03252844 0.50919257 0.83273742]. \t  3.718838050868568 \t 3.8550388754342437\n",
      "92     \t [0.62376515 0.55811165 0.82502523]. \t  3.7179660067775107 \t 3.8550388754342437\n",
      "93     \t [0.35394244 0.63044841 0.86791606]. \t  3.6471533883964837 \t 3.8550388754342437\n",
      "94     \t [0.52897344 0.54925235 0.8806865 ]. \t  3.765995509432212 \t 3.8550388754342437\n",
      "95     \t [0.8499885  0.49942563 0.89155455]. \t  3.505154897399043 \t 3.8550388754342437\n",
      "96     \t [0.20567677 0.5884143  0.03704573]. \t  0.020807476685159473 \t 3.8550388754342437\n",
      "97     \t [0.3027532  0.54716679 0.85440675]. \t  \u001b[92m3.8592543871813114\u001b[0m \t 3.8592543871813114\n",
      "98     \t [0.86373766 0.45817015 0.14811133]. \t  0.09849400399042767 \t 3.8592543871813114\n",
      "99     \t [0.72206819 0.52574194 0.86187993]. \t  3.755946347264982 \t 3.8592543871813114\n",
      "100    \t [0.30253764 0.55981534 0.84904758]. \t  3.858978021196116 \t 3.8592543871813114\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \n",
    "\n",
    "np.random.seed(run_num_18)\n",
    "surrogate_winner_18 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_18 = GPGO(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_18.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.55944558 0.76149199 0.24453155]. \t  0.09433080659504385 \t 2.524990008735946\n",
      "init   \t [0.77168947 0.35447208 0.7966974 ]. \t  2.524990008735946 \t 2.524990008735946\n",
      "init   \t [0.34600002 0.44387444 0.30330359]. \t  0.3914194506273641 \t 2.524990008735946\n",
      "init   \t [0.44211392 0.57320936 0.06566402]. \t  0.036928944981920925 \t 2.524990008735946\n",
      "init   \t [0.02408889 0.82062263 0.36513765]. \t  0.9772047942100802 \t 2.524990008735946\n",
      "1      \t [1. 0. 1.]. \t  0.08848201872702738 \t 2.524990008735946\n",
      "2      \t [1.        0.9752649 1.       ]. \t  0.389127747402877 \t 2.524990008735946\n",
      "3      \t [0.         0.68398789 1.        ]. \t  1.8027634244928294 \t 2.524990008735946\n",
      "4      \t [1. 0. 0.]. \t  0.03095471703300515 \t 2.524990008735946\n",
      "5      \t [0. 0. 1.]. \t  0.0902894676548261 \t 2.524990008735946\n",
      "6      \t [0.53023444 0.5294111  1.        ]. \t  2.0538188102116544 \t 2.524990008735946\n",
      "7      \t [-5.55111512e-17  0.00000000e+00  0.00000000e+00]. \t  0.06797411659013229 \t 2.524990008735946\n",
      "8      \t [1.         0.70397852 0.        ]. \t  0.001152399234569558 \t 2.524990008735946\n",
      "9      \t [0. 1. 1.]. \t  0.330219860606422 \t 2.524990008735946\n",
      "10     \t [1.         0.49724633 1.        ]. \t  1.9146294566701372 \t 2.524990008735946\n",
      "11     \t [0. 1. 0.]. \t  0.0002735367680454459 \t 2.524990008735946\n",
      "12     \t [1.         0.49247097 0.54863846]. \t  0.41569341206950716 \t 2.524990008735946\n",
      "13     \t [0.53289401 0.         0.        ]. \t  0.09432114263125188 \t 2.524990008735946\n",
      "14     \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 2.524990008735946\n",
      "15     \t [0.         0.         0.55807984]. \t  0.09931947695684107 \t 2.524990008735946\n",
      "16     \t [0.53400838 0.         1.        ]. \t  0.091570017317772 \t 2.524990008735946\n",
      "17     \t [0.87408375 0.         0.49445868]. \t  0.10554652652307897 \t 2.524990008735946\n",
      "18     \t [0.3288665 1.        0.6910936]. \t  1.4550204591372415 \t 2.524990008735946\n",
      "19     \t [0.         0.39798234 0.77526349]. \t  \u001b[92m2.7349966953244533\u001b[0m \t 2.7349966953244533\n",
      "20     \t [-2.77555756e-17  5.00908926e-01  0.00000000e+00]. \t  0.01792362237959027 \t 2.7349966953244533\n",
      "21     \t [0.2776391  0.63014849 0.75707213]. \t  \u001b[92m3.0542730093557697\u001b[0m \t 3.0542730093557697\n",
      "22     \t [1.         1.         0.59624763]. \t  0.2751963498353216 \t 3.0542730093557697\n",
      "23     \t [0.         1.         0.67415795]. \t  1.7604177448099074 \t 3.0542730093557697\n",
      "24     \t [0.74215548 0.73476865 0.82445969]. \t  2.653030863978111 \t 3.0542730093557697\n",
      "25     \t [ 4.88189579e-01  1.00000000e+00 -2.77555756e-17]. \t  0.00020880144944793768 \t 3.0542730093557697\n",
      "26     \t [0.80661689 0.27899472 0.        ]. \t  0.05075981071294053 \t 3.0542730093557697\n",
      "27     \t [0.26396089 0.21538103 0.81954031]. \t  1.37425910929619 \t 3.0542730093557697\n",
      "28     \t [0.         0.69174192 0.73352827]. \t  2.672432678592557 \t 3.0542730093557697\n",
      "29     \t [1.         0.         0.73522491]. \t  0.22296769483663675 \t 3.0542730093557697\n",
      "30     \t [0.22864122 1.         0.24467574]. \t  0.13729110755080054 \t 3.0542730093557697\n",
      "31     \t [1.         0.78899586 0.75894589]. \t  1.549655316235517 \t 3.0542730093557697\n",
      "32     \t [0.55218247 1.         1.        ]. \t  0.33131375532130947 \t 3.0542730093557697\n",
      "33     \t [0.52616859 0.         0.60946282]. \t  0.12387778443151362 \t 3.0542730093557697\n",
      "34     \t [4.44514682e-08 2.64930963e-01 2.28180937e-01]. \t  0.5131653348332047 \t 3.0542730093557697\n",
      "35     \t [0.5286488  0.51491586 0.73525889]. \t  2.7498157388494846 \t 3.0542730093557697\n",
      "36     \t [0.38038751 0.15924228 0.        ]. \t  0.11512852582732144 \t 3.0542730093557697\n",
      "37     \t [0.25893594 0.77169907 0.98891725]. \t  1.5188370068664123 \t 3.0542730093557697\n",
      "38     \t [0.15807996 0.82454137 0.        ]. \t  0.0009721473725182795 \t 3.0542730093557697\n",
      "39     \t [2.29416407e-01 1.51666304e-08 2.97425805e-01]. \t  0.8008685748578235 \t 3.0542730093557697\n",
      "40     \t [4.86970473e-08 3.43715905e-01 1.00000000e+00]. \t  1.260501437500549 \t 3.0542730093557697\n",
      "41     \t [1.         0.25633155 0.22615055]. \t  0.23728223286914965 \t 3.0542730093557697\n",
      "42     \t [0.99999999 0.96245678 0.22620641]. \t  0.010290005636219887 \t 3.0542730093557697\n",
      "43     \t [1.         0.18893949 0.97525716]. \t  0.6181327865111523 \t 3.0542730093557697\n",
      "44     \t [0.85605105 0.69894507 0.99999998]. \t  1.7210996549335327 \t 3.0542730093557697\n",
      "45     \t [0.71615573 1.         0.43720627]. \t  0.561248137380814 \t 3.0542730093557697\n",
      "46     \t [7.62622576e-01 7.29269775e-08 8.80230941e-01]. \t  0.2098113212138173 \t 3.0542730093557697\n",
      "47     \t [0.07811404 0.60042653 0.83077336]. \t  \u001b[92m3.720633139211072\u001b[0m \t 3.720633139211072\n",
      "48     \t [0.08675335 0.47912391 0.806259  ]. \t  3.4842618338817797 \t 3.720633139211072\n",
      "49     \t [0.0825789  0.57700205 0.89339527]. \t  3.659963527846937 \t 3.720633139211072\n",
      "50     \t [0.18644215 0.52110689 0.79521514]. \t  3.5450398997796584 \t 3.720633139211072\n",
      "51     \t [0.90658168 0.80702444 0.03125805]. \t  0.0007902038485462962 \t 3.720633139211072\n",
      "52     \t [0.15881316 0.5498972  0.89829679]. \t  3.6405050609135476 \t 3.720633139211072\n",
      "53     \t [0.17668749 0.48760178 0.84184563]. \t  3.6916506482565152 \t 3.720633139211072\n",
      "54     \t [0.12268228 0.47091473 0.76180543]. \t  3.039918613504023 \t 3.720633139211072\n",
      "55     \t [0.33125427 0.53246641 0.88153146]. \t  \u001b[92m3.7585880839645496\u001b[0m \t 3.7585880839645496\n",
      "56     \t [0.11953117 0.43209217 0.88789621]. \t  3.2006981109556336 \t 3.7585880839645496\n",
      "57     \t [0.06469001 0.55675172 0.8567813 ]. \t  \u001b[92m3.8288560692114673\u001b[0m \t 3.8288560692114673\n",
      "58     \t [0.1564116  0.55701035 0.85611115]. \t  \u001b[92m3.849099744519981\u001b[0m \t 3.849099744519981\n",
      "59     \t [0.11691045 0.59687293 0.80664387]. \t  3.603738901172531 \t 3.849099744519981\n",
      "60     \t [0.1695363  0.56023909 0.91245538]. \t  3.5005602132574625 \t 3.849099744519981\n",
      "61     \t [0.14159906 0.57156278 0.86077006]. \t  3.832890375441463 \t 3.849099744519981\n",
      "62     \t [0.03259838 0.52476698 0.8033054 ]. \t  3.5881806085520553 \t 3.849099744519981\n",
      "63     \t [0.30714679 0.5778924  0.88562967]. \t  3.7419061915422085 \t 3.849099744519981\n",
      "64     \t [0.26345828 0.45403852 0.83321774]. \t  3.5033440836759757 \t 3.849099744519981\n",
      "65     \t [0.04437257 0.64063289 0.85349306]. \t  3.583585899311366 \t 3.849099744519981\n",
      "66     \t [0.16889282 0.53371454 0.83922726]. \t  3.821543429737374 \t 3.849099744519981\n",
      "67     \t [0.34067869 0.52503487 0.86286287]. \t  3.818707209648414 \t 3.849099744519981\n",
      "68     \t [0.26599928 0.48821558 0.84169181]. \t  3.7025713211576052 \t 3.849099744519981\n",
      "69     \t [0.24572139 0.5989164  0.81010177]. \t  3.631111501717042 \t 3.849099744519981\n",
      "70     \t [0.05417464 0.70350398 0.86835061]. \t  3.125635007798178 \t 3.849099744519981\n",
      "71     \t [0.20431318 0.58351669 0.88645951]. \t  3.721372261068683 \t 3.849099744519981\n",
      "72     \t [0.09165687 0.58968278 0.82303397]. \t  3.717782661068955 \t 3.849099744519981\n",
      "73     \t [0.14480129 0.58865884 0.85346842]. \t  3.8105395635515844 \t 3.849099744519981\n",
      "74     \t [0.21702301 0.61749763 0.84623491]. \t  3.720459160095694 \t 3.849099744519981\n",
      "75     \t [0.21178461 0.57592574 0.85632612]. \t  3.8417125981345954 \t 3.849099744519981\n",
      "76     \t [0.01680615 0.53401619 0.88410096]. \t  3.6985486269778547 \t 3.849099744519981\n",
      "77     \t [0.67405208 0.51758176 0.9058413 ]. \t  3.4880254463615987 \t 3.849099744519981\n",
      "78     \t [0.03185566 0.51774397 0.87521116]. \t  3.7158471552575865 \t 3.849099744519981\n",
      "79     \t [0.66223202 0.54100328 0.90628869]. \t  3.5325252495176356 \t 3.849099744519981\n",
      "80     \t [0.006454   0.53308613 0.87277279]. \t  3.752931370225161 \t 3.849099744519981\n",
      "81     \t [0.13507434 0.63465684 0.84831915]. \t  3.6338206938276767 \t 3.849099744519981\n",
      "82     \t [0.32382865 0.52370181 0.87469713]. \t  3.77614318351931 \t 3.849099744519981\n",
      "83     \t [0.1159409  0.57531403 0.85934843]. \t  3.825433971972455 \t 3.849099744519981\n",
      "84     \t [0.22552974 0.49016478 0.88695282]. \t  3.5818072319414425 \t 3.849099744519981\n",
      "85     \t [0.02154614 0.52529277 0.81864917]. \t  3.69006345447001 \t 3.849099744519981\n",
      "86     \t [0.71990975 0.55496911 0.88775213]. \t  3.6824574460090265 \t 3.849099744519981\n",
      "87     \t [0.50990606 0.50798204 0.84600594]. \t  3.765493882712703 \t 3.849099744519981\n",
      "88     \t [0.48091724 0.58901157 0.80987201]. \t  3.607018879447114 \t 3.849099744519981\n",
      "89     \t [0.49288026 0.50953685 0.89080192]. \t  3.628602485004783 \t 3.849099744519981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     \t [0.65301349 0.55285517 0.80905556]. \t  3.594863726551615 \t 3.849099744519981\n",
      "91     \t [0.68008396 0.59925184 0.87592257]. \t  3.6867333777867333 \t 3.849099744519981\n",
      "92     \t [0.81014129 0.60786318 0.86603949]. \t  3.637166809322262 \t 3.849099744519981\n",
      "93     \t [0.00878    0.55640937 0.82292272]. \t  3.7340589874720105 \t 3.849099744519981\n",
      "94     \t [0.84202391 0.43925574 0.86159375]. \t  3.3225102790163 \t 3.849099744519981\n",
      "95     \t [0.09040439 0.58869752 0.77310347]. \t  3.3193398035283863 \t 3.849099744519981\n",
      "96     \t [0.21353801 0.48592161 0.89846447]. \t  3.467051440081773 \t 3.849099744519981\n",
      "97     \t [0.07172067 0.57194263 0.85674237]. \t  3.82183907749139 \t 3.849099744519981\n",
      "98     \t [0.32692265 0.63074226 0.84188908]. \t  3.6484987898276646 \t 3.849099744519981\n",
      "99     \t [0.9023587  0.54834923 0.91047516]. \t  3.424869542794152 \t 3.849099744519981\n",
      "100    \t [0.32549712 0.46916938 0.90100188]. \t  3.3625193003391223 \t 3.849099744519981\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \n",
    "\n",
    "np.random.seed(run_num_19)\n",
    "surrogate_winner_19 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_19 = GPGO(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_19.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.57051729 0.56452876 0.48844183]. \t  0.675391399411646 \t 0.675391399411646\n",
      "init   \t [0.33647775 0.37586818 0.53203587]. \t  0.5331349538596052 \t 0.675391399411646\n",
      "init   \t [0.06810629 0.58452906 0.23789776]. \t  0.14747335095307315 \t 0.675391399411646\n",
      "init   \t [0.16075658 0.15211915 0.12706922]. \t  0.48089725804912437 \t 0.675391399411646\n",
      "init   \t [0.32744117 0.69415387 0.35896647]. \t  0.6289408047543804 \t 0.675391399411646\n",
      "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 0.675391399411646\n",
      "2      \t [1. 1. 0.]. \t  3.7727185179443916e-05 \t 0.675391399411646\n",
      "3      \t [1. 0. 0.]. \t  0.03095471703300515 \t 0.675391399411646\n",
      "4      \t [1. 0. 1.]. \t  0.08848201872702738 \t 0.675391399411646\n",
      "5      \t [-1.73472348e-18  1.00000000e+00  1.00000000e+00]. \t  0.330219860606422 \t 0.675391399411646\n",
      "6      \t [0. 0. 1.]. \t  0.0902894676548261 \t 0.675391399411646\n",
      "7      \t [0. 1. 0.]. \t  0.0002735367680454459 \t 0.675391399411646\n",
      "8      \t [0.47858187 0.56338791 1.        ]. \t  \u001b[92m2.0856887154107113\u001b[0m \t 2.0856887154107113\n",
      "9      \t [0.54380825 0.49892802 0.        ]. \t  0.02491470899022172 \t 2.0856887154107113\n",
      "10     \t [0.51002687 1.         1.        ]. \t  0.33214111554173376 \t 2.0856887154107113\n",
      "11     \t [1.         0.50240231 1.        ]. \t  1.9277168302790972 \t 2.0856887154107113\n",
      "12     \t [0.         0.49801472 1.        ]. \t  1.9623040119850639 \t 2.0856887154107113\n",
      "13     \t [0.53334594 0.         0.        ]. \t  0.0942791506888482 \t 2.0856887154107113\n",
      "14     \t [1.        0.        0.4792198]. \t  0.08296445019867717 \t 2.0856887154107113\n",
      "15     \t [0.         0.         0.42094322]. \t  0.2905900843503387 \t 2.0856887154107113\n",
      "16     \t [0.53839197 1.         0.        ]. \t  0.00018819464700503032 \t 2.0856887154107113\n",
      "17     \t [0.00000000e+00 5.55111512e-17 0.00000000e+00]. \t  0.06797411659013229 \t 2.0856887154107113\n",
      "18     \t [1.         1.         0.51086196]. \t  0.23401013265019302 \t 2.0856887154107113\n",
      "19     \t [0.         1.         0.48954708]. \t  \u001b[92m2.177402200392964\u001b[0m \t 2.177402200392964\n",
      "20     \t [1.         0.46877319 0.17538599]. \t  0.06890381885826431 \t 2.177402200392964\n",
      "21     \t [ 5.50560139e-01 -1.38777878e-17  1.00000000e+00]. \t  0.091526527178105 \t 2.177402200392964\n",
      "22     \t [0.         0.77607793 0.76904068]. \t  \u001b[92m2.368080891804486\u001b[0m \t 2.368080891804486\n",
      "23     \t [1.         0.70947519 0.77364422]. \t  2.2876854828843207 \t 2.368080891804486\n",
      "24     \t [0.29314994 1.         0.68384667]. \t  1.5743291733448128 \t 2.368080891804486\n",
      "25     \t [1.         0.29240316 0.76523112]. \t  1.7913056609403029 \t 2.368080891804486\n",
      "26     \t [ 3.40584371e-01 -1.38777878e-17  6.68591774e-01]. \t  0.17249640355005408 \t 2.368080891804486\n",
      "27     \t [0.18836316 0.74584069 1.        ]. \t  1.524625660999919 \t 2.368080891804486\n",
      "28     \t [1.         0.67848189 0.        ]. \t  0.0015356937505809013 \t 2.368080891804486\n",
      "29     \t [0.         0.19721052 0.74650841]. \t  1.0628400453579927 \t 2.368080891804486\n",
      "30     \t [0.2574526  0.25493656 1.        ]. \t  0.8033001000846659 \t 2.368080891804486\n",
      "31     \t [0.7733713  0.90949917 0.80363349]. \t  1.0612868222173764 \t 2.368080891804486\n",
      "32     \t [0.17108397 0.66628516 0.        ]. \t  0.005291924824311742 \t 2.368080891804486\n",
      "33     \t [0.76879524 0.28143899 0.89735662]. \t  1.7488381811608391 \t 2.368080891804486\n",
      "34     \t [0.6555492  0.00530537 0.20521944]. \t  0.6145638679747936 \t 2.368080891804486\n",
      "35     \t [0.00000000e+00 3.40019601e-01 2.50517436e-09]. \t  0.047417254855913325 \t 2.368080891804486\n",
      "36     \t [0.96699167 0.26721627 0.        ]. \t  0.031992839770776105 \t 2.368080891804486\n",
      "37     \t [0.74481632 0.95420274 0.31013388]. \t  0.13960229083059247 \t 2.368080891804486\n",
      "38     \t [0.30837875 0.99525649 0.28900807]. \t  0.27649705586665735 \t 2.368080891804486\n",
      "39     \t [8.16001879e-01 1.88785706e-08 8.68044129e-01]. \t  0.21878592480512238 \t 2.368080891804486\n",
      "40     \t [0.77611758 0.7678821  0.99999894]. \t  1.3838290814834544 \t 2.368080891804486\n",
      "41     \t [0.2082948  0.65104097 0.81651647]. \t  \u001b[92m3.4478185442662728\u001b[0m \t 3.4478185442662728\n",
      "42     \t [0.41385107 0.68713783 0.78441566]. \t  2.9535922992353694 \t 3.4478185442662728\n",
      "43     \t [0.01185246 1.         0.76831477]. \t  1.0069607551021122 \t 3.4478185442662728\n",
      "44     \t [0.07477153 0.59148091 0.73383566]. \t  2.8998910894962693 \t 3.4478185442662728\n",
      "45     \t [2.32933642e-01 5.23191840e-05 2.59345703e-01]. \t  0.823698580285614 \t 3.4478185442662728\n",
      "46     \t [0.82232254 0.49966903 0.88286228]. \t  \u001b[92m3.573155066776939\u001b[0m \t 3.573155066776939\n",
      "47     \t [0.79994295 0.90477319 0.        ]. \t  0.0002049923393030311 \t 3.573155066776939\n",
      "48     \t [0.81514549 0.51451505 0.79642808]. \t  3.393701028003082 \t 3.573155066776939\n",
      "49     \t [0.38301014 0.60710968 0.81577657]. \t  \u001b[92m3.6230679726518957\u001b[0m \t 3.6230679726518957\n",
      "50     \t [0.77622412 0.59772272 0.85519632]. \t  \u001b[92m3.688160379656863\u001b[0m \t 3.688160379656863\n",
      "51     \t [0.71526654 0.56639767 0.82253466]. \t  3.6600522116215934 \t 3.688160379656863\n",
      "52     \t [0.54169909 0.52346547 0.86366298]. \t  \u001b[92m3.792670236466728\u001b[0m \t 3.792670236466728\n",
      "53     \t [0.6943604 0.5808523 0.8206886]. \t  3.6286818864841384 \t 3.792670236466728\n",
      "54     \t [0.5933932  0.54246812 0.85822891]. \t  \u001b[92m3.8166406064242597\u001b[0m \t 3.8166406064242597\n",
      "55     \t [0.96055508 0.56226606 0.91736883]. \t  3.3298616221369657 \t 3.8166406064242597\n",
      "56     \t [0.61141809 0.5984274  0.81285063]. \t  3.5533513685966582 \t 3.8166406064242597\n",
      "57     \t [0.722994   0.5357248  0.87699202]. \t  3.726851922254992 \t 3.8166406064242597\n",
      "58     \t [0.74181229 0.56463367 0.87632501]. \t  3.732746580080355 \t 3.8166406064242597\n",
      "59     \t [0.4628504  0.5440478  0.85971005]. \t  \u001b[92m3.8414162584818095\u001b[0m \t 3.8414162584818095\n",
      "60     \t [0.30256106 0.52304306 0.83106306]. \t  3.786381972433672 \t 3.8414162584818095\n",
      "61     \t [0.56563201 0.5909709  0.79629616]. \t  3.4464245272393548 \t 3.8414162584818095\n",
      "62     \t [0.32876217 0.5626572  0.87219302]. \t  3.8237337630781423 \t 3.8414162584818095\n",
      "63     \t [0.35402018 0.52216989 0.8061744 ]. \t  3.633988538411497 \t 3.8414162584818095\n",
      "64     \t [0.30781908 0.56171102 0.82078472]. \t  3.762115722020321 \t 3.8414162584818095\n",
      "65     \t [0.34129467 0.65034085 0.81731469]. \t  3.4388971867111247 \t 3.8414162584818095\n",
      "66     \t [0.4647774  0.58700358 0.87814182]. \t  3.7569278147865903 \t 3.8414162584818095\n",
      "67     \t [0.26362993 0.60575612 0.87257506]. \t  3.7389934734544736 \t 3.8414162584818095\n",
      "68     \t [0.72840082 0.57290497 0.83569913]. \t  3.717327853498455 \t 3.8414162584818095\n",
      "69     \t [0.63821813 0.55634848 0.86257507]. \t  3.8040915974159812 \t 3.8414162584818095\n",
      "70     \t [0.00973032 0.00480065 0.78240338]. \t  0.25823003066437145 \t 3.8414162584818095\n",
      "71     \t [0.88547272 0.590213   0.88291759]. \t  3.6115287063533854 \t 3.8414162584818095\n",
      "72     \t [0.01512732 0.83957032 0.1915667 ]. \t  0.05841897249502551 \t 3.8414162584818095\n",
      "73     \t [0.22832659 0.57593544 0.76153723]. \t  3.2183372280368263 \t 3.8414162584818095\n",
      "74     \t [0.74975679 0.48651473 0.88509301]. \t  3.5265540802479927 \t 3.8414162584818095\n",
      "75     \t [0.45373321 0.58623462 0.86229639]. \t  3.8072079760902846 \t 3.8414162584818095\n",
      "76     \t [0.81772461 0.49901898 0.86488292]. \t  3.6454770646355463 \t 3.8414162584818095\n",
      "77     \t [0.68026029 0.52366049 0.85063843]. \t  3.7684971873467563 \t 3.8414162584818095\n",
      "78     \t [0.55455103 0.49483785 0.84989516]. \t  3.7148701464546807 \t 3.8414162584818095\n",
      "79     \t [0.39725423 0.5261547  0.77963125]. \t  3.3783721916373786 \t 3.8414162584818095\n",
      "80     \t [0.24144565 0.51696176 0.83242375]. \t  3.776690395212916 \t 3.8414162584818095\n",
      "81     \t [0.1914493  0.54173381 0.82521261]. \t  3.7826566885562896 \t 3.8414162584818095\n",
      "82     \t [0.68131961 0.61501811 0.86098489]. \t  3.657743658239979 \t 3.8414162584818095\n",
      "83     \t [0.53236811 0.59647274 0.87525663]. \t  3.733906031019176 \t 3.8414162584818095\n",
      "84     \t [0.42804805 0.57731867 0.85816354]. \t  3.832333311735268 \t 3.8414162584818095\n",
      "85     \t [0.63321061 0.48104984 0.85464696]. \t  3.637771262531312 \t 3.8414162584818095\n",
      "86     \t [0.71104161 0.55581468 0.84860566]. \t  3.7804377310754544 \t 3.8414162584818095\n",
      "87     \t [0.6027241  0.49761061 0.81673337]. \t  3.597965090574125 \t 3.8414162584818095\n",
      "88     \t [0.30745707 0.65592443 0.78629863]. \t  3.19988468301404 \t 3.8414162584818095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     \t [0.28248767 0.4988328  0.79023621]. \t  3.4455982212587832 \t 3.8414162584818095\n",
      "90     \t [0.35168926 0.47924185 0.82501439]. \t  3.612971640011819 \t 3.8414162584818095\n",
      "91     \t [0.21732874 0.55758111 0.84414664]. \t  \u001b[92m3.8510374076303444\u001b[0m \t 3.8510374076303444\n",
      "92     \t [0.9966977  0.11281528 0.1773188 ]. \t  0.24041387552247068 \t 3.8510374076303444\n",
      "93     \t [0.88018412 0.51826977 0.79802255]. \t  3.3873958799103514 \t 3.8510374076303444\n",
      "94     \t [0.60402868 0.68133892 0.82989736]. \t  3.1998511303106216 \t 3.8510374076303444\n",
      "95     \t [0.84893624 0.59719001 0.84690433]. \t  3.6467732139539293 \t 3.8510374076303444\n",
      "96     \t [1.         0.90869234 0.26604961]. \t  0.023105278390936575 \t 3.8510374076303444\n",
      "97     \t [0.24996006 0.55364059 0.89693862]. \t  3.6654548512891054 \t 3.8510374076303444\n",
      "98     \t [0.39120084 0.55908836 0.84896964]. \t  \u001b[92m3.853760811958231\u001b[0m \t 3.853760811958231\n",
      "99     \t [0.35187913 0.54509214 0.90165918]. \t  3.6197106142506605 \t 3.853760811958231\n",
      "100    \t [0.37655113 0.57882966 0.80215087]. \t  3.5958014059591705 \t 3.853760811958231\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \n",
    "\n",
    "np.random.seed(run_num_20)\n",
    "surrogate_winner_20 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_20 = GPGO(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_20.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.2205577764509106, -3.295179624654029)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 1\n",
    "\n",
    "loser_output_1 = np.append(np.max(loser_1.GP.y[0:n_init]),loser_1.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_1 = np.append(np.max(winner_1.GP.y[0:n_init]),winner_1.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_1 = np.log(y_global_orig - loser_output_1)\n",
    "regret_winner_1 = np.log(y_global_orig - winner_output_1)\n",
    "\n",
    "train_regret_loser_1 = min_max_array(regret_loser_1)\n",
    "train_regret_winner_1 = min_max_array(regret_winner_1)\n",
    "\n",
    "min_train_regret_loser_1 = min(train_regret_loser_1)\n",
    "min_train_regret_winner_1 = min(train_regret_winner_1)\n",
    "\n",
    "min_train_regret_loser_1, min_train_regret_winner_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.3952980347555135, -3.4546729968392555)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 2\n",
    "\n",
    "loser_output_2 = np.append(np.max(loser_2.GP.y[0:n_init]),loser_2.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_2 = np.append(np.max(winner_2.GP.y[0:n_init]),winner_2.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_2 = np.log(y_global_orig - loser_output_2)\n",
    "regret_winner_2 = np.log(y_global_orig - winner_output_2)\n",
    "\n",
    "train_regret_loser_2 = min_max_array(regret_loser_2)\n",
    "train_regret_winner_2 = min_max_array(regret_winner_2)\n",
    "\n",
    "min_train_regret_loser_2 = min(train_regret_loser_2)\n",
    "min_train_regret_winner_2 = min(train_regret_winner_2)\n",
    "\n",
    "min_train_regret_loser_2, min_train_regret_winner_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.344254090624879, -5.140706141792039)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 3\n",
    "\n",
    "loser_output_3 = np.append(np.max(loser_3.GP.y[0:n_init]),loser_3.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_3 = np.append(np.max(winner_3.GP.y[0:n_init]),winner_3.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_3 = np.log(y_global_orig - loser_output_3)\n",
    "regret_winner_3 = np.log(y_global_orig - winner_output_3)\n",
    "\n",
    "train_regret_loser_3 = min_max_array(regret_loser_3)\n",
    "train_regret_winner_3 = min_max_array(regret_winner_3)\n",
    "\n",
    "min_train_regret_loser_3 = min(train_regret_loser_3)\n",
    "min_train_regret_winner_3 = min(train_regret_winner_3)\n",
    "\n",
    "min_train_regret_loser_3, min_train_regret_winner_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.045118598435262, -4.137088093770865)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 4\n",
    "\n",
    "loser_output_4 = np.append(np.max(loser_4.GP.y[0:n_init]),loser_4.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_4 = np.append(np.max(winner_4.GP.y[0:n_init]),winner_4.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_4 = np.log(y_global_orig - loser_output_4)\n",
    "regret_winner_4 = np.log(y_global_orig - winner_output_4)\n",
    "\n",
    "train_regret_loser_4 = min_max_array(regret_loser_4)\n",
    "train_regret_winner_4 = min_max_array(regret_winner_4)\n",
    "\n",
    "min_train_regret_loser_4 = min(train_regret_loser_4)\n",
    "min_train_regret_winner_4 = min(train_regret_winner_4)\n",
    "\n",
    "min_train_regret_loser_4, min_train_regret_winner_4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.2175760190503504, -3.5798621932854733)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 5\n",
    "\n",
    "loser_output_5 = np.append(np.max(loser_5.GP.y[0:n_init]),loser_5.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_5 = np.append(np.max(winner_5.GP.y[0:n_init]),winner_5.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_5 = np.log(y_global_orig - loser_output_5)\n",
    "regret_winner_5 = np.log(y_global_orig - winner_output_5)\n",
    "\n",
    "train_regret_loser_5 = min_max_array(regret_loser_5)\n",
    "train_regret_winner_5 = min_max_array(regret_winner_5)\n",
    "\n",
    "min_train_regret_loser_5 = min(train_regret_loser_5)\n",
    "min_train_regret_winner_5 = min(train_regret_winner_5)\n",
    "\n",
    "min_train_regret_loser_5, min_train_regret_winner_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.4621495814352967, -4.200433220053186)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 6\n",
    "\n",
    "loser_output_6 = np.append(np.max(loser_6.GP.y[0:n_init]),loser_6.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_6 = np.append(np.max(winner_6.GP.y[0:n_init]),winner_6.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_6 = np.log(y_global_orig - loser_output_6)\n",
    "regret_winner_6 = np.log(y_global_orig - winner_output_6)\n",
    "\n",
    "train_regret_loser_6 = min_max_array(regret_loser_6)\n",
    "train_regret_winner_6 = min_max_array(regret_winner_6)\n",
    "\n",
    "min_train_regret_loser_6 = min(train_regret_loser_6)\n",
    "min_train_regret_winner_6 = min(train_regret_winner_6)\n",
    "\n",
    "min_train_regret_loser_6, min_train_regret_winner_6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.667069349629895, -4.491341440397606)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 7\n",
    "\n",
    "loser_output_7 = np.append(np.max(loser_7.GP.y[0:n_init]),loser_7.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_7 = np.append(np.max(winner_7.GP.y[0:n_init]),winner_7.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_7 = np.log(y_global_orig - loser_output_7)\n",
    "regret_winner_7 = np.log(y_global_orig - winner_output_7)\n",
    "\n",
    "train_regret_loser_7 = min_max_array(regret_loser_7)\n",
    "train_regret_winner_7 = min_max_array(regret_winner_7)\n",
    "\n",
    "min_train_regret_loser_7 = min(train_regret_loser_7)\n",
    "min_train_regret_winner_7 = min(train_regret_winner_7)\n",
    "\n",
    "min_train_regret_loser_7, min_train_regret_winner_7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.3479486680449515, -4.437340845619253)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 8\n",
    "\n",
    "loser_output_8 = np.append(np.max(loser_8.GP.y[0:n_init]),loser_8.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_8 = np.append(np.max(winner_8.GP.y[0:n_init]),winner_8.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_8 = np.log(y_global_orig - loser_output_8)\n",
    "regret_winner_8 = np.log(y_global_orig - winner_output_8)\n",
    "\n",
    "train_regret_loser_8 = min_max_array(regret_loser_8)\n",
    "train_regret_winner_8 = min_max_array(regret_winner_8)\n",
    "\n",
    "min_train_regret_loser_8 = min(train_regret_loser_8)\n",
    "min_train_regret_winner_8 = min(train_regret_winner_8)\n",
    "\n",
    "min_train_regret_loser_8, min_train_regret_winner_8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.0258572440729457, -3.732056583522499)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 9\n",
    "\n",
    "loser_output_9 = np.append(np.max(loser_9.GP.y[0:n_init]),loser_9.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_9 = np.append(np.max(winner_9.GP.y[0:n_init]),winner_9.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_9 = np.log(y_global_orig - loser_output_9)\n",
    "regret_winner_9 = np.log(y_global_orig - winner_output_9)\n",
    "\n",
    "train_regret_loser_9 = min_max_array(regret_loser_9)\n",
    "train_regret_winner_9 = min_max_array(regret_winner_9)\n",
    "\n",
    "min_train_regret_loser_9 = min(train_regret_loser_9)\n",
    "min_train_regret_winner_9 = min(train_regret_winner_9)\n",
    "\n",
    "min_train_regret_loser_9, min_train_regret_winner_9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.3026705103183764, -6.23972149327984)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 10\n",
    "\n",
    "loser_output_10 = np.append(np.max(loser_10.GP.y[0:n_init]),loser_10.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_10 = np.append(np.max(winner_10.GP.y[0:n_init]),winner_10.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_10 = np.log(y_global_orig - loser_output_10)\n",
    "regret_winner_10 = np.log(y_global_orig - winner_output_10)\n",
    "\n",
    "train_regret_loser_10 = min_max_array(regret_loser_10)\n",
    "train_regret_winner_10 = min_max_array(regret_winner_10)\n",
    "\n",
    "min_train_regret_loser_10 = min(train_regret_loser_10)\n",
    "min_train_regret_winner_10 = min(train_regret_winner_10)\n",
    "\n",
    "min_train_regret_loser_10, min_train_regret_winner_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.786336501531502, -4.451291654399481)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 11\n",
    "\n",
    "loser_output_11 = np.append(np.max(loser_11.GP.y[0:n_init]),loser_11.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_11 = np.append(np.max(winner_11.GP.y[0:n_init]),winner_11.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_11 = np.log(y_global_orig - loser_output_11)\n",
    "regret_winner_11 = np.log(y_global_orig - winner_output_11)\n",
    "\n",
    "train_regret_loser_11 = min_max_array(regret_loser_11)\n",
    "train_regret_winner_11 = min_max_array(regret_winner_11)\n",
    "\n",
    "min_train_regret_loser_11 = min(train_regret_loser_11)\n",
    "min_train_regret_winner_11 = min(train_regret_winner_11)\n",
    "\n",
    "min_train_regret_loser_11, min_train_regret_winner_11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.719191058071372, -4.324248471569495)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 12\n",
    "\n",
    "loser_output_12 = np.append(np.max(loser_12.GP.y[0:n_init]),loser_12.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_12 = np.append(np.max(winner_12.GP.y[0:n_init]),winner_12.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_12 = np.log(y_global_orig - loser_output_12)\n",
    "regret_winner_12 = np.log(y_global_orig - winner_output_12)\n",
    "\n",
    "train_regret_loser_12 = min_max_array(regret_loser_12)\n",
    "train_regret_winner_12 = min_max_array(regret_winner_12)\n",
    "\n",
    "min_train_regret_loser_12 = min(train_regret_loser_12)\n",
    "min_train_regret_winner_12 = min(train_regret_winner_12)\n",
    "\n",
    "min_train_regret_loser_12, min_train_regret_winner_12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.9435462090920615, -3.879771013593048)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 13\n",
    "\n",
    "loser_output_13 = np.append(np.max(loser_13.GP.y[0:n_init]),loser_13.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_13 = np.append(np.max(winner_13.GP.y[0:n_init]),winner_13.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_13 = np.log(y_global_orig - loser_output_13)\n",
    "regret_winner_13 = np.log(y_global_orig - winner_output_13)\n",
    "\n",
    "train_regret_loser_13 = min_max_array(regret_loser_13)\n",
    "train_regret_winner_13 = min_max_array(regret_winner_13)\n",
    "\n",
    "min_train_regret_loser_13 = min(train_regret_loser_13)\n",
    "min_train_regret_winner_13 = min(train_regret_winner_13)\n",
    "\n",
    "min_train_regret_loser_13, min_train_regret_winner_13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.904931863891832, -4.1099834541932925)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 14\n",
    "\n",
    "loser_output_14 = np.append(np.max(loser_14.GP.y[0:n_init]),loser_14.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_14 = np.append(np.max(winner_14.GP.y[0:n_init]),winner_14.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_14 = np.log(y_global_orig - loser_output_14)\n",
    "regret_winner_14 = np.log(y_global_orig - winner_output_14)\n",
    "\n",
    "train_regret_loser_14 = min_max_array(regret_loser_14)\n",
    "train_regret_winner_14 = min_max_array(regret_winner_14)\n",
    "\n",
    "min_train_regret_loser_14 = min(train_regret_loser_14)\n",
    "min_train_regret_winner_14 = min(train_regret_winner_14)\n",
    "\n",
    "min_train_regret_loser_14, min_train_regret_winner_14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.687574601537175, -3.9202771857447423)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 15\n",
    "\n",
    "loser_output_15 = np.append(np.max(loser_15.GP.y[0:n_init]),loser_15.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_15 = np.append(np.max(winner_15.GP.y[0:n_init]),winner_15.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_15 = np.log(y_global_orig - loser_output_15)\n",
    "regret_winner_15 = np.log(y_global_orig - winner_output_15)\n",
    "\n",
    "train_regret_loser_15 = min_max_array(regret_loser_15)\n",
    "train_regret_winner_15 = min_max_array(regret_winner_15)\n",
    "\n",
    "min_train_regret_loser_15 = min(train_regret_loser_15)\n",
    "min_train_regret_winner_15 = min(train_regret_winner_15)\n",
    "\n",
    "min_train_regret_loser_15, min_train_regret_winner_15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.061350856474767, -4.760762616180313)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 16\n",
    "\n",
    "loser_output_16 = np.append(np.max(loser_16.GP.y[0:n_init]),loser_16.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_16 = np.append(np.max(winner_16.GP.y[0:n_init]),winner_16.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_16 = np.log(y_global_orig - loser_output_16)\n",
    "regret_winner_16 = np.log(y_global_orig - winner_output_16)\n",
    "\n",
    "train_regret_loser_16 = min_max_array(regret_loser_16)\n",
    "train_regret_winner_16 = min_max_array(regret_winner_16)\n",
    "\n",
    "min_train_regret_loser_16 = min(train_regret_loser_16)\n",
    "min_train_regret_winner_16 = min(train_regret_winner_16)\n",
    "\n",
    "min_train_regret_loser_16, min_train_regret_winner_16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.6579528660276637, -4.129216088688163)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 17\n",
    "\n",
    "loser_output_17 = np.append(np.max(loser_17.GP.y[0:n_init]),loser_17.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_17 = np.append(np.max(winner_17.GP.y[0:n_init]),winner_17.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_17 = np.log(y_global_orig - loser_output_17)\n",
    "regret_winner_17 = np.log(y_global_orig - winner_output_17)\n",
    "\n",
    "train_regret_loser_17 = min_max_array(regret_loser_17)\n",
    "train_regret_winner_17 = min_max_array(regret_winner_17)\n",
    "\n",
    "min_train_regret_loser_17 = min(train_regret_loser_17)\n",
    "min_train_regret_winner_17 = min(train_regret_winner_17)\n",
    "\n",
    "min_train_regret_loser_17, min_train_regret_winner_17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.2682057916786316, -5.647701008554616)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 18\n",
    "\n",
    "loser_output_18 = np.append(np.max(loser_18.GP.y[0:n_init]),loser_18.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_18 = np.append(np.max(winner_18.GP.y[0:n_init]),winner_18.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_18 = np.log(y_global_orig - loser_output_18)\n",
    "regret_winner_18 = np.log(y_global_orig - winner_output_18)\n",
    "\n",
    "train_regret_loser_18 = min_max_array(regret_loser_18)\n",
    "train_regret_winner_18 = min_max_array(regret_winner_18)\n",
    "\n",
    "min_train_regret_loser_18 = min(train_regret_loser_18)\n",
    "min_train_regret_winner_18 = min(train_regret_winner_18)\n",
    "\n",
    "min_train_regret_loser_18, min_train_regret_winner_18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.1478049858514476, -4.291801691522143)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 19\n",
    "\n",
    "loser_output_19 = np.append(np.max(loser_19.GP.y[0:n_init]),loser_19.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_19 = np.append(np.max(winner_19.GP.y[0:n_init]),winner_19.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_19 = np.log(y_global_orig - loser_output_19)\n",
    "regret_winner_19 = np.log(y_global_orig - winner_output_19)\n",
    "\n",
    "train_regret_loser_19 = min_max_array(regret_loser_19)\n",
    "train_regret_winner_19 = min_max_array(regret_winner_19)\n",
    "\n",
    "min_train_regret_loser_19 = min(train_regret_loser_19)\n",
    "min_train_regret_winner_19 = min(train_regret_winner_19)\n",
    "\n",
    "min_train_regret_loser_19, min_train_regret_winner_19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.1123460797014895, -4.708400966501698)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 20\n",
    "\n",
    "loser_output_20 = np.append(np.max(loser_20.GP.y[0:n_init]),loser_20.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_20 = np.append(np.max(winner_20.GP.y[0:n_init]),winner_20.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_20 = np.log(y_global_orig - loser_output_20)\n",
    "regret_winner_20 = np.log(y_global_orig - winner_output_20)\n",
    "\n",
    "train_regret_loser_20 = min_max_array(regret_loser_20)\n",
    "train_regret_winner_20 = min_max_array(regret_winner_20)\n",
    "\n",
    "min_train_regret_loser_20 = min(train_regret_loser_20)\n",
    "min_train_regret_winner_20 = min(train_regret_winner_20)\n",
    "\n",
    "min_train_regret_loser_20, min_train_regret_winner_20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration1 :\n",
    "\n",
    "slice1 = 0\n",
    "\n",
    "loser1 = [train_regret_loser_1[slice1],\n",
    "       train_regret_loser_2[slice1],\n",
    "       train_regret_loser_3[slice1],\n",
    "       train_regret_loser_4[slice1],\n",
    "       train_regret_loser_5[slice1],\n",
    "       train_regret_loser_6[slice1],\n",
    "       train_regret_loser_7[slice1],\n",
    "       train_regret_loser_8[slice1],\n",
    "       train_regret_loser_9[slice1],\n",
    "       train_regret_loser_10[slice1],\n",
    "       train_regret_loser_11[slice1],\n",
    "       train_regret_loser_12[slice1],\n",
    "       train_regret_loser_13[slice1],\n",
    "       train_regret_loser_14[slice1],\n",
    "       train_regret_loser_15[slice1],\n",
    "       train_regret_loser_16[slice1],\n",
    "       train_regret_loser_17[slice1],\n",
    "       train_regret_loser_18[slice1],\n",
    "       train_regret_loser_19[slice1],\n",
    "       train_regret_loser_20[slice1]]\n",
    "\n",
    "winner1 = [train_regret_winner_1[slice1],\n",
    "       train_regret_winner_2[slice1],\n",
    "       train_regret_winner_3[slice1],\n",
    "       train_regret_winner_4[slice1],\n",
    "       train_regret_winner_5[slice1],\n",
    "       train_regret_winner_6[slice1],\n",
    "       train_regret_winner_7[slice1],\n",
    "       train_regret_winner_8[slice1],\n",
    "       train_regret_winner_9[slice1],\n",
    "       train_regret_winner_10[slice1],\n",
    "       train_regret_winner_11[slice1],\n",
    "       train_regret_winner_12[slice1],\n",
    "       train_regret_winner_13[slice1],\n",
    "       train_regret_winner_14[slice1],\n",
    "       train_regret_winner_15[slice1],\n",
    "       train_regret_winner_16[slice1],\n",
    "       train_regret_winner_17[slice1],\n",
    "       train_regret_winner_18[slice1],\n",
    "       train_regret_winner_19[slice1],\n",
    "       train_regret_winner_20[slice1]]\n",
    "\n",
    "loser1_results = pd.DataFrame(loser1).sort_values(by=[0], ascending=False)\n",
    "winner1_results = pd.DataFrame(winner1).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser1 = np.asarray(loser1_results[4:5][0])[0]\n",
    "median_loser1 = np.asarray(loser1_results[9:10][0])[0]\n",
    "upper_loser1 = np.asarray(loser1_results[14:15][0])[0]\n",
    "\n",
    "lower_winner1 = np.asarray(winner1_results[4:5][0])[0]\n",
    "median_winner1 = np.asarray(winner1_results[9:10][0])[0]\n",
    "upper_winner1 = np.asarray(winner1_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration11 :\n",
    "\n",
    "slice11 = 10\n",
    "\n",
    "loser11 = [train_regret_loser_1[slice11],\n",
    "       train_regret_loser_2[slice11],\n",
    "       train_regret_loser_3[slice11],\n",
    "       train_regret_loser_4[slice11],\n",
    "       train_regret_loser_5[slice11],\n",
    "       train_regret_loser_6[slice11],\n",
    "       train_regret_loser_7[slice11],\n",
    "       train_regret_loser_8[slice11],\n",
    "       train_regret_loser_9[slice11],\n",
    "       train_regret_loser_10[slice11],\n",
    "       train_regret_loser_11[slice11],\n",
    "       train_regret_loser_12[slice11],\n",
    "       train_regret_loser_13[slice11],\n",
    "       train_regret_loser_14[slice11],\n",
    "       train_regret_loser_15[slice11],\n",
    "       train_regret_loser_16[slice11],\n",
    "       train_regret_loser_17[slice11],\n",
    "       train_regret_loser_18[slice11],\n",
    "       train_regret_loser_19[slice11],\n",
    "       train_regret_loser_20[slice11]]\n",
    "\n",
    "winner11 = [train_regret_winner_1[slice11],\n",
    "       train_regret_winner_2[slice11],\n",
    "       train_regret_winner_3[slice11],\n",
    "       train_regret_winner_4[slice11],\n",
    "       train_regret_winner_5[slice11],\n",
    "       train_regret_winner_6[slice11],\n",
    "       train_regret_winner_7[slice11],\n",
    "       train_regret_winner_8[slice11],\n",
    "       train_regret_winner_9[slice11],\n",
    "       train_regret_winner_10[slice11],\n",
    "       train_regret_winner_11[slice11],\n",
    "       train_regret_winner_12[slice11],\n",
    "       train_regret_winner_13[slice11],\n",
    "       train_regret_winner_14[slice11],\n",
    "       train_regret_winner_15[slice11],\n",
    "       train_regret_winner_16[slice11],\n",
    "       train_regret_winner_17[slice11],\n",
    "       train_regret_winner_18[slice11],\n",
    "       train_regret_winner_19[slice11],\n",
    "       train_regret_winner_20[slice11]]\n",
    "\n",
    "loser11_results = pd.DataFrame(loser11).sort_values(by=[0], ascending=False)\n",
    "winner11_results = pd.DataFrame(winner11).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser11 = np.asarray(loser11_results[4:5][0])[0]\n",
    "median_loser11 = np.asarray(loser11_results[9:10][0])[0]\n",
    "upper_loser11 = np.asarray(loser11_results[14:15][0])[0]\n",
    "\n",
    "lower_winner11 = np.asarray(winner11_results[4:5][0])[0]\n",
    "median_winner11 = np.asarray(winner11_results[9:10][0])[0]\n",
    "upper_winner11 = np.asarray(winner11_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration21 :\n",
    "\n",
    "slice21 = 20\n",
    "\n",
    "loser21 = [train_regret_loser_1[slice21],\n",
    "       train_regret_loser_2[slice21],\n",
    "       train_regret_loser_3[slice21],\n",
    "       train_regret_loser_4[slice21],\n",
    "       train_regret_loser_5[slice21],\n",
    "       train_regret_loser_6[slice21],\n",
    "       train_regret_loser_7[slice21],\n",
    "       train_regret_loser_8[slice21],\n",
    "       train_regret_loser_9[slice21],\n",
    "       train_regret_loser_10[slice21],\n",
    "       train_regret_loser_11[slice21],\n",
    "       train_regret_loser_12[slice21],\n",
    "       train_regret_loser_13[slice21],\n",
    "       train_regret_loser_14[slice21],\n",
    "       train_regret_loser_15[slice21],\n",
    "       train_regret_loser_16[slice21],\n",
    "       train_regret_loser_17[slice21],\n",
    "       train_regret_loser_18[slice21],\n",
    "       train_regret_loser_19[slice21],\n",
    "       train_regret_loser_20[slice21]]\n",
    "\n",
    "winner21 = [train_regret_winner_1[slice21],\n",
    "       train_regret_winner_2[slice21],\n",
    "       train_regret_winner_3[slice21],\n",
    "       train_regret_winner_4[slice21],\n",
    "       train_regret_winner_5[slice21],\n",
    "       train_regret_winner_6[slice21],\n",
    "       train_regret_winner_7[slice21],\n",
    "       train_regret_winner_8[slice21],\n",
    "       train_regret_winner_9[slice21],\n",
    "       train_regret_winner_10[slice21],\n",
    "       train_regret_winner_11[slice21],\n",
    "       train_regret_winner_12[slice21],\n",
    "       train_regret_winner_13[slice21],\n",
    "       train_regret_winner_14[slice21],\n",
    "       train_regret_winner_15[slice21],\n",
    "       train_regret_winner_16[slice21],\n",
    "       train_regret_winner_17[slice21],\n",
    "       train_regret_winner_18[slice21],\n",
    "       train_regret_winner_19[slice21],\n",
    "       train_regret_winner_20[slice21]]\n",
    "\n",
    "loser21_results = pd.DataFrame(loser21).sort_values(by=[0], ascending=False)\n",
    "winner21_results = pd.DataFrame(winner21).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser21 = np.asarray(loser21_results[4:5][0])[0]\n",
    "median_loser21 = np.asarray(loser21_results[9:10][0])[0]\n",
    "upper_loser21 = np.asarray(loser21_results[14:15][0])[0]\n",
    "\n",
    "lower_winner21 = np.asarray(winner21_results[4:5][0])[0]\n",
    "median_winner21 = np.asarray(winner21_results[9:10][0])[0]\n",
    "upper_winner21 = np.asarray(winner21_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration31 :\n",
    "\n",
    "slice31 = 30\n",
    "\n",
    "loser31 = [train_regret_loser_1[slice31],\n",
    "       train_regret_loser_2[slice31],\n",
    "       train_regret_loser_3[slice31],\n",
    "       train_regret_loser_4[slice31],\n",
    "       train_regret_loser_5[slice31],\n",
    "       train_regret_loser_6[slice31],\n",
    "       train_regret_loser_7[slice31],\n",
    "       train_regret_loser_8[slice31],\n",
    "       train_regret_loser_9[slice31],\n",
    "       train_regret_loser_10[slice31],\n",
    "       train_regret_loser_11[slice31],\n",
    "       train_regret_loser_12[slice31],\n",
    "       train_regret_loser_13[slice31],\n",
    "       train_regret_loser_14[slice31],\n",
    "       train_regret_loser_15[slice31],\n",
    "       train_regret_loser_16[slice31],\n",
    "       train_regret_loser_17[slice31],\n",
    "       train_regret_loser_18[slice31],\n",
    "       train_regret_loser_19[slice31],\n",
    "       train_regret_loser_20[slice31]]\n",
    "\n",
    "winner31 = [train_regret_winner_1[slice31],\n",
    "       train_regret_winner_2[slice31],\n",
    "       train_regret_winner_3[slice31],\n",
    "       train_regret_winner_4[slice31],\n",
    "       train_regret_winner_5[slice31],\n",
    "       train_regret_winner_6[slice31],\n",
    "       train_regret_winner_7[slice31],\n",
    "       train_regret_winner_8[slice31],\n",
    "       train_regret_winner_9[slice31],\n",
    "       train_regret_winner_10[slice31],\n",
    "       train_regret_winner_11[slice31],\n",
    "       train_regret_winner_12[slice31],\n",
    "       train_regret_winner_13[slice31],\n",
    "       train_regret_winner_14[slice31],\n",
    "       train_regret_winner_15[slice31],\n",
    "       train_regret_winner_16[slice31],\n",
    "       train_regret_winner_17[slice31],\n",
    "       train_regret_winner_18[slice31],\n",
    "       train_regret_winner_19[slice31],\n",
    "       train_regret_winner_20[slice31]]\n",
    "\n",
    "loser31_results = pd.DataFrame(loser31).sort_values(by=[0], ascending=False)\n",
    "winner31_results = pd.DataFrame(winner31).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser31 = np.asarray(loser31_results[4:5][0])[0]\n",
    "median_loser31 = np.asarray(loser31_results[9:10][0])[0]\n",
    "upper_loser31 = np.asarray(loser31_results[14:15][0])[0]\n",
    "\n",
    "lower_winner31 = np.asarray(winner31_results[4:5][0])[0]\n",
    "median_winner31 = np.asarray(winner31_results[9:10][0])[0]\n",
    "upper_winner31 = np.asarray(winner31_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration41 :\n",
    "\n",
    "slice41 = 40\n",
    "\n",
    "loser41 = [train_regret_loser_1[slice41],\n",
    "       train_regret_loser_2[slice41],\n",
    "       train_regret_loser_3[slice41],\n",
    "       train_regret_loser_4[slice41],\n",
    "       train_regret_loser_5[slice41],\n",
    "       train_regret_loser_6[slice41],\n",
    "       train_regret_loser_7[slice41],\n",
    "       train_regret_loser_8[slice41],\n",
    "       train_regret_loser_9[slice41],\n",
    "       train_regret_loser_10[slice41],\n",
    "       train_regret_loser_11[slice41],\n",
    "       train_regret_loser_12[slice41],\n",
    "       train_regret_loser_13[slice41],\n",
    "       train_regret_loser_14[slice41],\n",
    "       train_regret_loser_15[slice41],\n",
    "       train_regret_loser_16[slice41],\n",
    "       train_regret_loser_17[slice41],\n",
    "       train_regret_loser_18[slice41],\n",
    "       train_regret_loser_19[slice41],\n",
    "       train_regret_loser_20[slice41]]\n",
    "\n",
    "winner41 = [train_regret_winner_1[slice41],\n",
    "       train_regret_winner_2[slice41],\n",
    "       train_regret_winner_3[slice41],\n",
    "       train_regret_winner_4[slice41],\n",
    "       train_regret_winner_5[slice41],\n",
    "       train_regret_winner_6[slice41],\n",
    "       train_regret_winner_7[slice41],\n",
    "       train_regret_winner_8[slice41],\n",
    "       train_regret_winner_9[slice41],\n",
    "       train_regret_winner_10[slice41],\n",
    "       train_regret_winner_11[slice41],\n",
    "       train_regret_winner_12[slice41],\n",
    "       train_regret_winner_13[slice41],\n",
    "       train_regret_winner_14[slice41],\n",
    "       train_regret_winner_15[slice41],\n",
    "       train_regret_winner_16[slice41],\n",
    "       train_regret_winner_17[slice41],\n",
    "       train_regret_winner_18[slice41],\n",
    "       train_regret_winner_19[slice41],\n",
    "       train_regret_winner_20[slice41]]\n",
    "\n",
    "loser41_results = pd.DataFrame(loser41).sort_values(by=[0], ascending=False)\n",
    "winner41_results = pd.DataFrame(winner41).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser41 = np.asarray(loser41_results[4:5][0])[0]\n",
    "median_loser41 = np.asarray(loser41_results[9:10][0])[0]\n",
    "upper_loser41 = np.asarray(loser41_results[14:15][0])[0]\n",
    "\n",
    "lower_winner41 = np.asarray(winner41_results[4:5][0])[0]\n",
    "median_winner41 = np.asarray(winner41_results[9:10][0])[0]\n",
    "upper_winner41 = np.asarray(winner41_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration51 :\n",
    "\n",
    "slice51 = 50\n",
    "\n",
    "loser51 = [train_regret_loser_1[slice51],\n",
    "       train_regret_loser_2[slice51],\n",
    "       train_regret_loser_3[slice51],\n",
    "       train_regret_loser_4[slice51],\n",
    "       train_regret_loser_5[slice51],\n",
    "       train_regret_loser_6[slice51],\n",
    "       train_regret_loser_7[slice51],\n",
    "       train_regret_loser_8[slice51],\n",
    "       train_regret_loser_9[slice51],\n",
    "       train_regret_loser_10[slice51],\n",
    "       train_regret_loser_11[slice51],\n",
    "       train_regret_loser_12[slice51],\n",
    "       train_regret_loser_13[slice51],\n",
    "       train_regret_loser_14[slice51],\n",
    "       train_regret_loser_15[slice51],\n",
    "       train_regret_loser_16[slice51],\n",
    "       train_regret_loser_17[slice51],\n",
    "       train_regret_loser_18[slice51],\n",
    "       train_regret_loser_19[slice51],\n",
    "       train_regret_loser_20[slice51]]\n",
    "\n",
    "winner51 = [train_regret_winner_1[slice51],\n",
    "       train_regret_winner_2[slice51],\n",
    "       train_regret_winner_3[slice51],\n",
    "       train_regret_winner_4[slice51],\n",
    "       train_regret_winner_5[slice51],\n",
    "       train_regret_winner_6[slice51],\n",
    "       train_regret_winner_7[slice51],\n",
    "       train_regret_winner_8[slice51],\n",
    "       train_regret_winner_9[slice51],\n",
    "       train_regret_winner_10[slice51],\n",
    "       train_regret_winner_11[slice51],\n",
    "       train_regret_winner_12[slice51],\n",
    "       train_regret_winner_13[slice51],\n",
    "       train_regret_winner_14[slice51],\n",
    "       train_regret_winner_15[slice51],\n",
    "       train_regret_winner_16[slice51],\n",
    "       train_regret_winner_17[slice51],\n",
    "       train_regret_winner_18[slice51],\n",
    "       train_regret_winner_19[slice51],\n",
    "       train_regret_winner_20[slice51]]\n",
    "\n",
    "loser51_results = pd.DataFrame(loser51).sort_values(by=[0], ascending=False)\n",
    "winner51_results = pd.DataFrame(winner51).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser51 = np.asarray(loser51_results[4:5][0])[0]\n",
    "median_loser51 = np.asarray(loser51_results[9:10][0])[0]\n",
    "upper_loser51 = np.asarray(loser51_results[14:15][0])[0]\n",
    "\n",
    "lower_winner51 = np.asarray(winner51_results[4:5][0])[0]\n",
    "median_winner51 = np.asarray(winner51_results[9:10][0])[0]\n",
    "upper_winner51 = np.asarray(winner51_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration61 :\n",
    "\n",
    "slice61 = 60\n",
    "\n",
    "loser61 = [train_regret_loser_1[slice61],\n",
    "       train_regret_loser_2[slice61],\n",
    "       train_regret_loser_3[slice61],\n",
    "       train_regret_loser_4[slice61],\n",
    "       train_regret_loser_5[slice61],\n",
    "       train_regret_loser_6[slice61],\n",
    "       train_regret_loser_7[slice61],\n",
    "       train_regret_loser_8[slice61],\n",
    "       train_regret_loser_9[slice61],\n",
    "       train_regret_loser_10[slice61],\n",
    "       train_regret_loser_11[slice61],\n",
    "       train_regret_loser_12[slice61],\n",
    "       train_regret_loser_13[slice61],\n",
    "       train_regret_loser_14[slice61],\n",
    "       train_regret_loser_15[slice61],\n",
    "       train_regret_loser_16[slice61],\n",
    "       train_regret_loser_17[slice61],\n",
    "       train_regret_loser_18[slice61],\n",
    "       train_regret_loser_19[slice61],\n",
    "       train_regret_loser_20[slice61]]\n",
    "\n",
    "winner61 = [train_regret_winner_1[slice61],\n",
    "       train_regret_winner_2[slice61],\n",
    "       train_regret_winner_3[slice61],\n",
    "       train_regret_winner_4[slice61],\n",
    "       train_regret_winner_5[slice61],\n",
    "       train_regret_winner_6[slice61],\n",
    "       train_regret_winner_7[slice61],\n",
    "       train_regret_winner_8[slice61],\n",
    "       train_regret_winner_9[slice61],\n",
    "       train_regret_winner_10[slice61],\n",
    "       train_regret_winner_11[slice61],\n",
    "       train_regret_winner_12[slice61],\n",
    "       train_regret_winner_13[slice61],\n",
    "       train_regret_winner_14[slice61],\n",
    "       train_regret_winner_15[slice61],\n",
    "       train_regret_winner_16[slice61],\n",
    "       train_regret_winner_17[slice61],\n",
    "       train_regret_winner_18[slice61],\n",
    "       train_regret_winner_19[slice61],\n",
    "       train_regret_winner_20[slice61]]\n",
    "\n",
    "loser61_results = pd.DataFrame(loser61).sort_values(by=[0], ascending=False)\n",
    "winner61_results = pd.DataFrame(winner61).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser61 = np.asarray(loser61_results[4:5][0])[0]\n",
    "median_loser61 = np.asarray(loser61_results[9:10][0])[0]\n",
    "upper_loser61 = np.asarray(loser61_results[14:15][0])[0]\n",
    "\n",
    "lower_winner61 = np.asarray(winner61_results[4:5][0])[0]\n",
    "median_winner61 = np.asarray(winner61_results[9:10][0])[0]\n",
    "upper_winner61 = np.asarray(winner61_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration71 :\n",
    "\n",
    "slice71 = 70\n",
    "\n",
    "loser71 = [train_regret_loser_1[slice71],\n",
    "       train_regret_loser_2[slice71],\n",
    "       train_regret_loser_3[slice71],\n",
    "       train_regret_loser_4[slice71],\n",
    "       train_regret_loser_5[slice71],\n",
    "       train_regret_loser_6[slice71],\n",
    "       train_regret_loser_7[slice71],\n",
    "       train_regret_loser_8[slice71],\n",
    "       train_regret_loser_9[slice71],\n",
    "       train_regret_loser_10[slice71],\n",
    "       train_regret_loser_11[slice71],\n",
    "       train_regret_loser_12[slice71],\n",
    "       train_regret_loser_13[slice71],\n",
    "       train_regret_loser_14[slice71],\n",
    "       train_regret_loser_15[slice71],\n",
    "       train_regret_loser_16[slice71],\n",
    "       train_regret_loser_17[slice71],\n",
    "       train_regret_loser_18[slice71],\n",
    "       train_regret_loser_19[slice71],\n",
    "       train_regret_loser_20[slice71]]\n",
    "\n",
    "winner71 = [train_regret_winner_1[slice71],\n",
    "       train_regret_winner_2[slice71],\n",
    "       train_regret_winner_3[slice71],\n",
    "       train_regret_winner_4[slice71],\n",
    "       train_regret_winner_5[slice71],\n",
    "       train_regret_winner_6[slice71],\n",
    "       train_regret_winner_7[slice71],\n",
    "       train_regret_winner_8[slice71],\n",
    "       train_regret_winner_9[slice71],\n",
    "       train_regret_winner_10[slice71],\n",
    "       train_regret_winner_11[slice71],\n",
    "       train_regret_winner_12[slice71],\n",
    "       train_regret_winner_13[slice71],\n",
    "       train_regret_winner_14[slice71],\n",
    "       train_regret_winner_15[slice71],\n",
    "       train_regret_winner_16[slice71],\n",
    "       train_regret_winner_17[slice71],\n",
    "       train_regret_winner_18[slice71],\n",
    "       train_regret_winner_19[slice71],\n",
    "       train_regret_winner_20[slice71]]\n",
    "\n",
    "loser71_results = pd.DataFrame(loser71).sort_values(by=[0], ascending=False)\n",
    "winner71_results = pd.DataFrame(winner71).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser71 = np.asarray(loser71_results[4:5][0])[0]\n",
    "median_loser71 = np.asarray(loser71_results[9:10][0])[0]\n",
    "upper_loser71 = np.asarray(loser71_results[14:15][0])[0]\n",
    "\n",
    "lower_winner71 = np.asarray(winner71_results[4:5][0])[0]\n",
    "median_winner71 = np.asarray(winner71_results[9:10][0])[0]\n",
    "upper_winner71 = np.asarray(winner71_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration81 :\n",
    "\n",
    "slice81 = 80\n",
    "\n",
    "loser81 = [train_regret_loser_1[slice81],\n",
    "       train_regret_loser_2[slice81],\n",
    "       train_regret_loser_3[slice81],\n",
    "       train_regret_loser_4[slice81],\n",
    "       train_regret_loser_5[slice81],\n",
    "       train_regret_loser_6[slice81],\n",
    "       train_regret_loser_7[slice81],\n",
    "       train_regret_loser_8[slice81],\n",
    "       train_regret_loser_9[slice81],\n",
    "       train_regret_loser_10[slice81],\n",
    "       train_regret_loser_11[slice81],\n",
    "       train_regret_loser_12[slice81],\n",
    "       train_regret_loser_13[slice81],\n",
    "       train_regret_loser_14[slice81],\n",
    "       train_regret_loser_15[slice81],\n",
    "       train_regret_loser_16[slice81],\n",
    "       train_regret_loser_17[slice81],\n",
    "       train_regret_loser_18[slice81],\n",
    "       train_regret_loser_19[slice81],\n",
    "       train_regret_loser_20[slice81]]\n",
    "\n",
    "winner81 = [train_regret_winner_1[slice81],\n",
    "       train_regret_winner_2[slice81],\n",
    "       train_regret_winner_3[slice81],\n",
    "       train_regret_winner_4[slice81],\n",
    "       train_regret_winner_5[slice81],\n",
    "       train_regret_winner_6[slice81],\n",
    "       train_regret_winner_7[slice81],\n",
    "       train_regret_winner_8[slice81],\n",
    "       train_regret_winner_9[slice81],\n",
    "       train_regret_winner_10[slice81],\n",
    "       train_regret_winner_11[slice81],\n",
    "       train_regret_winner_12[slice81],\n",
    "       train_regret_winner_13[slice81],\n",
    "       train_regret_winner_14[slice81],\n",
    "       train_regret_winner_15[slice81],\n",
    "       train_regret_winner_16[slice81],\n",
    "       train_regret_winner_17[slice81],\n",
    "       train_regret_winner_18[slice81],\n",
    "       train_regret_winner_19[slice81],\n",
    "       train_regret_winner_20[slice81]]\n",
    "\n",
    "loser81_results = pd.DataFrame(loser81).sort_values(by=[0], ascending=False)\n",
    "winner81_results = pd.DataFrame(winner81).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser81 = np.asarray(loser81_results[4:5][0])[0]\n",
    "median_loser81 = np.asarray(loser81_results[9:10][0])[0]\n",
    "upper_loser81 = np.asarray(loser81_results[14:15][0])[0]\n",
    "\n",
    "lower_winner81 = np.asarray(winner81_results[4:5][0])[0]\n",
    "median_winner81 = np.asarray(winner81_results[9:10][0])[0]\n",
    "upper_winner81 = np.asarray(winner81_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration91 :\n",
    "\n",
    "slice91 = 90\n",
    "\n",
    "loser91 = [train_regret_loser_1[slice91],\n",
    "       train_regret_loser_2[slice91],\n",
    "       train_regret_loser_3[slice91],\n",
    "       train_regret_loser_4[slice91],\n",
    "       train_regret_loser_5[slice91],\n",
    "       train_regret_loser_6[slice91],\n",
    "       train_regret_loser_7[slice91],\n",
    "       train_regret_loser_8[slice91],\n",
    "       train_regret_loser_9[slice91],\n",
    "       train_regret_loser_10[slice91],\n",
    "       train_regret_loser_11[slice91],\n",
    "       train_regret_loser_12[slice91],\n",
    "       train_regret_loser_13[slice91],\n",
    "       train_regret_loser_14[slice91],\n",
    "       train_regret_loser_15[slice91],\n",
    "       train_regret_loser_16[slice91],\n",
    "       train_regret_loser_17[slice91],\n",
    "       train_regret_loser_18[slice91],\n",
    "       train_regret_loser_19[slice91],\n",
    "       train_regret_loser_20[slice91]]\n",
    "\n",
    "winner91 = [train_regret_winner_1[slice91],\n",
    "       train_regret_winner_2[slice91],\n",
    "       train_regret_winner_3[slice91],\n",
    "       train_regret_winner_4[slice91],\n",
    "       train_regret_winner_5[slice91],\n",
    "       train_regret_winner_6[slice91],\n",
    "       train_regret_winner_7[slice91],\n",
    "       train_regret_winner_8[slice91],\n",
    "       train_regret_winner_9[slice91],\n",
    "       train_regret_winner_10[slice91],\n",
    "       train_regret_winner_11[slice91],\n",
    "       train_regret_winner_12[slice91],\n",
    "       train_regret_winner_13[slice91],\n",
    "       train_regret_winner_14[slice91],\n",
    "       train_regret_winner_15[slice91],\n",
    "       train_regret_winner_16[slice91],\n",
    "       train_regret_winner_17[slice91],\n",
    "       train_regret_winner_18[slice91],\n",
    "       train_regret_winner_19[slice91],\n",
    "       train_regret_winner_20[slice91]]\n",
    "\n",
    "loser91_results = pd.DataFrame(loser91).sort_values(by=[0], ascending=False)\n",
    "winner91_results = pd.DataFrame(winner91).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser91 = np.asarray(loser91_results[4:5][0])[0]\n",
    "median_loser91 = np.asarray(loser91_results[9:10][0])[0]\n",
    "upper_loser91 = np.asarray(loser91_results[14:15][0])[0]\n",
    "\n",
    "lower_winner91 = np.asarray(winner91_results[4:5][0])[0]\n",
    "median_winner91 = np.asarray(winner91_results[9:10][0])[0]\n",
    "upper_winner91 = np.asarray(winner91_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration101 :\n",
    "\n",
    "slice101 = 100\n",
    "\n",
    "loser101 = [train_regret_loser_1[slice101],\n",
    "       train_regret_loser_2[slice101],\n",
    "       train_regret_loser_3[slice101],\n",
    "       train_regret_loser_4[slice101],\n",
    "       train_regret_loser_5[slice101],\n",
    "       train_regret_loser_6[slice101],\n",
    "       train_regret_loser_7[slice101],\n",
    "       train_regret_loser_8[slice101],\n",
    "       train_regret_loser_9[slice101],\n",
    "       train_regret_loser_10[slice101],\n",
    "       train_regret_loser_11[slice101],\n",
    "       train_regret_loser_12[slice101],\n",
    "       train_regret_loser_13[slice101],\n",
    "       train_regret_loser_14[slice101],\n",
    "       train_regret_loser_15[slice101],\n",
    "       train_regret_loser_16[slice101],\n",
    "       train_regret_loser_17[slice101],\n",
    "       train_regret_loser_18[slice101],\n",
    "       train_regret_loser_19[slice101],\n",
    "       train_regret_loser_20[slice101]]\n",
    "\n",
    "winner101 = [train_regret_winner_1[slice101],\n",
    "       train_regret_winner_2[slice101],\n",
    "       train_regret_winner_3[slice101],\n",
    "       train_regret_winner_4[slice101],\n",
    "       train_regret_winner_5[slice101],\n",
    "       train_regret_winner_6[slice101],\n",
    "       train_regret_winner_7[slice101],\n",
    "       train_regret_winner_8[slice101],\n",
    "       train_regret_winner_9[slice101],\n",
    "       train_regret_winner_10[slice101],\n",
    "       train_regret_winner_11[slice101],\n",
    "       train_regret_winner_12[slice101],\n",
    "       train_regret_winner_13[slice101],\n",
    "       train_regret_winner_14[slice101],\n",
    "       train_regret_winner_15[slice101],\n",
    "       train_regret_winner_16[slice101],\n",
    "       train_regret_winner_17[slice101],\n",
    "       train_regret_winner_18[slice101],\n",
    "       train_regret_winner_19[slice101],\n",
    "       train_regret_winner_20[slice101]]\n",
    "\n",
    "loser101_results = pd.DataFrame(loser101).sort_values(by=[0], ascending=False)\n",
    "winner101_results = pd.DataFrame(winner101).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser101 = np.asarray(loser101_results[4:5][0])[0]\n",
    "median_loser101 = np.asarray(loser101_results[9:10][0])[0]\n",
    "upper_loser101 = np.asarray(loser101_results[14:15][0])[0]\n",
    "\n",
    "lower_winner101 = np.asarray(winner101_results[4:5][0])[0]\n",
    "median_winner101 = np.asarray(winner101_results[9:10][0])[0]\n",
    "upper_winner101 = np.asarray(winner101_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration2 :\n",
    "\n",
    "slice2 = 1\n",
    "\n",
    "loser2 = [train_regret_loser_1[slice2],\n",
    "       train_regret_loser_2[slice2],\n",
    "       train_regret_loser_3[slice2],\n",
    "       train_regret_loser_4[slice2],\n",
    "       train_regret_loser_5[slice2],\n",
    "       train_regret_loser_6[slice2],\n",
    "       train_regret_loser_7[slice2],\n",
    "       train_regret_loser_8[slice2],\n",
    "       train_regret_loser_9[slice2],\n",
    "       train_regret_loser_10[slice2],\n",
    "       train_regret_loser_11[slice2],\n",
    "       train_regret_loser_12[slice2],\n",
    "       train_regret_loser_13[slice2],\n",
    "       train_regret_loser_14[slice2],\n",
    "       train_regret_loser_15[slice2],\n",
    "       train_regret_loser_16[slice2],\n",
    "       train_regret_loser_17[slice2],\n",
    "       train_regret_loser_18[slice2],\n",
    "       train_regret_loser_19[slice2],\n",
    "       train_regret_loser_20[slice2]]\n",
    "\n",
    "winner2 = [train_regret_winner_1[slice2],\n",
    "       train_regret_winner_2[slice2],\n",
    "       train_regret_winner_3[slice2],\n",
    "       train_regret_winner_4[slice2],\n",
    "       train_regret_winner_5[slice2],\n",
    "       train_regret_winner_6[slice2],\n",
    "       train_regret_winner_7[slice2],\n",
    "       train_regret_winner_8[slice2],\n",
    "       train_regret_winner_9[slice2],\n",
    "       train_regret_winner_10[slice2],\n",
    "       train_regret_winner_11[slice2],\n",
    "       train_regret_winner_12[slice2],\n",
    "       train_regret_winner_13[slice2],\n",
    "       train_regret_winner_14[slice2],\n",
    "       train_regret_winner_15[slice2],\n",
    "       train_regret_winner_16[slice2],\n",
    "       train_regret_winner_17[slice2],\n",
    "       train_regret_winner_18[slice2],\n",
    "       train_regret_winner_19[slice2],\n",
    "       train_regret_winner_20[slice2]]\n",
    "\n",
    "loser2_results = pd.DataFrame(loser2).sort_values(by=[0], ascending=False)\n",
    "winner2_results = pd.DataFrame(winner2).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser2 = np.asarray(loser2_results[4:5][0])[0]\n",
    "median_loser2 = np.asarray(loser2_results[9:10][0])[0]\n",
    "upper_loser2 = np.asarray(loser2_results[14:15][0])[0]\n",
    "\n",
    "lower_winner2 = np.asarray(winner2_results[4:5][0])[0]\n",
    "median_winner2 = np.asarray(winner2_results[9:10][0])[0]\n",
    "upper_winner2 = np.asarray(winner2_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration12 :\n",
    "\n",
    "slice12 = 11\n",
    "\n",
    "loser12 = [train_regret_loser_1[slice12],\n",
    "       train_regret_loser_2[slice12],\n",
    "       train_regret_loser_3[slice12],\n",
    "       train_regret_loser_4[slice12],\n",
    "       train_regret_loser_5[slice12],\n",
    "       train_regret_loser_6[slice12],\n",
    "       train_regret_loser_7[slice12],\n",
    "       train_regret_loser_8[slice12],\n",
    "       train_regret_loser_9[slice12],\n",
    "       train_regret_loser_10[slice12],\n",
    "       train_regret_loser_11[slice12],\n",
    "       train_regret_loser_12[slice12],\n",
    "       train_regret_loser_13[slice12],\n",
    "       train_regret_loser_14[slice12],\n",
    "       train_regret_loser_15[slice12],\n",
    "       train_regret_loser_16[slice12],\n",
    "       train_regret_loser_17[slice12],\n",
    "       train_regret_loser_18[slice12],\n",
    "       train_regret_loser_19[slice12],\n",
    "       train_regret_loser_20[slice12]]\n",
    "\n",
    "winner12 = [train_regret_winner_1[slice12],\n",
    "       train_regret_winner_2[slice12],\n",
    "       train_regret_winner_3[slice12],\n",
    "       train_regret_winner_4[slice12],\n",
    "       train_regret_winner_5[slice12],\n",
    "       train_regret_winner_6[slice12],\n",
    "       train_regret_winner_7[slice12],\n",
    "       train_regret_winner_8[slice12],\n",
    "       train_regret_winner_9[slice12],\n",
    "       train_regret_winner_10[slice12],\n",
    "       train_regret_winner_11[slice12],\n",
    "       train_regret_winner_12[slice12],\n",
    "       train_regret_winner_13[slice12],\n",
    "       train_regret_winner_14[slice12],\n",
    "       train_regret_winner_15[slice12],\n",
    "       train_regret_winner_16[slice12],\n",
    "       train_regret_winner_17[slice12],\n",
    "       train_regret_winner_18[slice12],\n",
    "       train_regret_winner_19[slice12],\n",
    "       train_regret_winner_20[slice12]]\n",
    "\n",
    "loser12_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\n",
    "winner12_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser12 = np.asarray(loser12_results[4:5][0])[0]\n",
    "median_loser12 = np.asarray(loser12_results[9:10][0])[0]\n",
    "upper_loser12 = np.asarray(loser12_results[14:15][0])[0]\n",
    "\n",
    "lower_winner12 = np.asarray(winner12_results[4:5][0])[0]\n",
    "median_winner12 = np.asarray(winner12_results[9:10][0])[0]\n",
    "upper_winner12 = np.asarray(winner12_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration22 :\n",
    "\n",
    "slice22 = 21\n",
    "\n",
    "loser22 = [train_regret_loser_1[slice22],\n",
    "       train_regret_loser_2[slice22],\n",
    "       train_regret_loser_3[slice22],\n",
    "       train_regret_loser_4[slice22],\n",
    "       train_regret_loser_5[slice22],\n",
    "       train_regret_loser_6[slice22],\n",
    "       train_regret_loser_7[slice22],\n",
    "       train_regret_loser_8[slice22],\n",
    "       train_regret_loser_9[slice22],\n",
    "       train_regret_loser_10[slice22],\n",
    "       train_regret_loser_11[slice22],\n",
    "       train_regret_loser_12[slice22],\n",
    "       train_regret_loser_13[slice22],\n",
    "       train_regret_loser_14[slice22],\n",
    "       train_regret_loser_15[slice22],\n",
    "       train_regret_loser_16[slice22],\n",
    "       train_regret_loser_17[slice22],\n",
    "       train_regret_loser_18[slice22],\n",
    "       train_regret_loser_19[slice22],\n",
    "       train_regret_loser_20[slice22]]\n",
    "\n",
    "winner22 = [train_regret_winner_1[slice22],\n",
    "       train_regret_winner_2[slice22],\n",
    "       train_regret_winner_3[slice22],\n",
    "       train_regret_winner_4[slice22],\n",
    "       train_regret_winner_5[slice22],\n",
    "       train_regret_winner_6[slice22],\n",
    "       train_regret_winner_7[slice22],\n",
    "       train_regret_winner_8[slice22],\n",
    "       train_regret_winner_9[slice22],\n",
    "       train_regret_winner_10[slice22],\n",
    "       train_regret_winner_11[slice22],\n",
    "       train_regret_winner_12[slice22],\n",
    "       train_regret_winner_13[slice22],\n",
    "       train_regret_winner_14[slice22],\n",
    "       train_regret_winner_15[slice22],\n",
    "       train_regret_winner_16[slice22],\n",
    "       train_regret_winner_17[slice22],\n",
    "       train_regret_winner_18[slice22],\n",
    "       train_regret_winner_19[slice22],\n",
    "       train_regret_winner_20[slice22]]\n",
    "\n",
    "loser22_results = pd.DataFrame(loser22).sort_values(by=[0], ascending=False)\n",
    "winner22_results = pd.DataFrame(winner22).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser22 = np.asarray(loser22_results[4:5][0])[0]\n",
    "median_loser22 = np.asarray(loser22_results[9:10][0])[0]\n",
    "upper_loser22 = np.asarray(loser22_results[14:15][0])[0]\n",
    "\n",
    "lower_winner22 = np.asarray(winner22_results[4:5][0])[0]\n",
    "median_winner22 = np.asarray(winner22_results[9:10][0])[0]\n",
    "upper_winner22 = np.asarray(winner22_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration32 :\n",
    "\n",
    "slice32 = 31\n",
    "\n",
    "loser32 = [train_regret_loser_1[slice32],\n",
    "       train_regret_loser_2[slice32],\n",
    "       train_regret_loser_3[slice32],\n",
    "       train_regret_loser_4[slice32],\n",
    "       train_regret_loser_5[slice32],\n",
    "       train_regret_loser_6[slice32],\n",
    "       train_regret_loser_7[slice32],\n",
    "       train_regret_loser_8[slice32],\n",
    "       train_regret_loser_9[slice32],\n",
    "       train_regret_loser_10[slice32],\n",
    "       train_regret_loser_11[slice32],\n",
    "       train_regret_loser_12[slice32],\n",
    "       train_regret_loser_13[slice32],\n",
    "       train_regret_loser_14[slice32],\n",
    "       train_regret_loser_15[slice32],\n",
    "       train_regret_loser_16[slice32],\n",
    "       train_regret_loser_17[slice32],\n",
    "       train_regret_loser_18[slice32],\n",
    "       train_regret_loser_19[slice32],\n",
    "       train_regret_loser_20[slice32]]\n",
    "\n",
    "winner32 = [train_regret_winner_1[slice32],\n",
    "       train_regret_winner_2[slice32],\n",
    "       train_regret_winner_3[slice32],\n",
    "       train_regret_winner_4[slice32],\n",
    "       train_regret_winner_5[slice32],\n",
    "       train_regret_winner_6[slice32],\n",
    "       train_regret_winner_7[slice32],\n",
    "       train_regret_winner_8[slice32],\n",
    "       train_regret_winner_9[slice32],\n",
    "       train_regret_winner_10[slice32],\n",
    "       train_regret_winner_11[slice32],\n",
    "       train_regret_winner_12[slice32],\n",
    "       train_regret_winner_13[slice32],\n",
    "       train_regret_winner_14[slice32],\n",
    "       train_regret_winner_15[slice32],\n",
    "       train_regret_winner_16[slice32],\n",
    "       train_regret_winner_17[slice32],\n",
    "       train_regret_winner_18[slice32],\n",
    "       train_regret_winner_19[slice32],\n",
    "       train_regret_winner_20[slice32]]\n",
    "\n",
    "loser32_results = pd.DataFrame(loser32).sort_values(by=[0], ascending=False)\n",
    "winner32_results = pd.DataFrame(winner32).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser32 = np.asarray(loser32_results[4:5][0])[0]\n",
    "median_loser32 = np.asarray(loser32_results[9:10][0])[0]\n",
    "upper_loser32 = np.asarray(loser32_results[14:15][0])[0]\n",
    "\n",
    "lower_winner32 = np.asarray(winner32_results[4:5][0])[0]\n",
    "median_winner32 = np.asarray(winner32_results[9:10][0])[0]\n",
    "upper_winner32 = np.asarray(winner32_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration42 :\n",
    "\n",
    "slice42 = 41\n",
    "\n",
    "loser42 = [train_regret_loser_1[slice42],\n",
    "       train_regret_loser_2[slice42],\n",
    "       train_regret_loser_3[slice42],\n",
    "       train_regret_loser_4[slice42],\n",
    "       train_regret_loser_5[slice42],\n",
    "       train_regret_loser_6[slice42],\n",
    "       train_regret_loser_7[slice42],\n",
    "       train_regret_loser_8[slice42],\n",
    "       train_regret_loser_9[slice42],\n",
    "       train_regret_loser_10[slice42],\n",
    "       train_regret_loser_11[slice42],\n",
    "       train_regret_loser_12[slice42],\n",
    "       train_regret_loser_13[slice42],\n",
    "       train_regret_loser_14[slice42],\n",
    "       train_regret_loser_15[slice42],\n",
    "       train_regret_loser_16[slice42],\n",
    "       train_regret_loser_17[slice42],\n",
    "       train_regret_loser_18[slice42],\n",
    "       train_regret_loser_19[slice42],\n",
    "       train_regret_loser_20[slice42]]\n",
    "\n",
    "winner42 = [train_regret_winner_1[slice42],\n",
    "       train_regret_winner_2[slice42],\n",
    "       train_regret_winner_3[slice42],\n",
    "       train_regret_winner_4[slice42],\n",
    "       train_regret_winner_5[slice42],\n",
    "       train_regret_winner_6[slice42],\n",
    "       train_regret_winner_7[slice42],\n",
    "       train_regret_winner_8[slice42],\n",
    "       train_regret_winner_9[slice42],\n",
    "       train_regret_winner_10[slice42],\n",
    "       train_regret_winner_11[slice42],\n",
    "       train_regret_winner_12[slice42],\n",
    "       train_regret_winner_13[slice42],\n",
    "       train_regret_winner_14[slice42],\n",
    "       train_regret_winner_15[slice42],\n",
    "       train_regret_winner_16[slice42],\n",
    "       train_regret_winner_17[slice42],\n",
    "       train_regret_winner_18[slice42],\n",
    "       train_regret_winner_19[slice42],\n",
    "       train_regret_winner_20[slice42]]\n",
    "\n",
    "loser42_results = pd.DataFrame(loser42).sort_values(by=[0], ascending=False)\n",
    "winner42_results = pd.DataFrame(winner42).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser42 = np.asarray(loser42_results[4:5][0])[0]\n",
    "median_loser42 = np.asarray(loser42_results[9:10][0])[0]\n",
    "upper_loser42 = np.asarray(loser42_results[14:15][0])[0]\n",
    "\n",
    "lower_winner42 = np.asarray(winner42_results[4:5][0])[0]\n",
    "median_winner42 = np.asarray(winner42_results[9:10][0])[0]\n",
    "upper_winner42 = np.asarray(winner42_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration52 :\n",
    "\n",
    "slice52 = 51\n",
    "\n",
    "loser52 = [train_regret_loser_1[slice52],\n",
    "       train_regret_loser_2[slice52],\n",
    "       train_regret_loser_3[slice52],\n",
    "       train_regret_loser_4[slice52],\n",
    "       train_regret_loser_5[slice52],\n",
    "       train_regret_loser_6[slice52],\n",
    "       train_regret_loser_7[slice52],\n",
    "       train_regret_loser_8[slice52],\n",
    "       train_regret_loser_9[slice52],\n",
    "       train_regret_loser_10[slice52],\n",
    "       train_regret_loser_11[slice52],\n",
    "       train_regret_loser_12[slice52],\n",
    "       train_regret_loser_13[slice52],\n",
    "       train_regret_loser_14[slice52],\n",
    "       train_regret_loser_15[slice52],\n",
    "       train_regret_loser_16[slice52],\n",
    "       train_regret_loser_17[slice52],\n",
    "       train_regret_loser_18[slice52],\n",
    "       train_regret_loser_19[slice52],\n",
    "       train_regret_loser_20[slice52]]\n",
    "\n",
    "winner52 = [train_regret_winner_1[slice52],\n",
    "       train_regret_winner_2[slice52],\n",
    "       train_regret_winner_3[slice52],\n",
    "       train_regret_winner_4[slice52],\n",
    "       train_regret_winner_5[slice52],\n",
    "       train_regret_winner_6[slice52],\n",
    "       train_regret_winner_7[slice52],\n",
    "       train_regret_winner_8[slice52],\n",
    "       train_regret_winner_9[slice52],\n",
    "       train_regret_winner_10[slice52],\n",
    "       train_regret_winner_11[slice52],\n",
    "       train_regret_winner_12[slice52],\n",
    "       train_regret_winner_13[slice52],\n",
    "       train_regret_winner_14[slice52],\n",
    "       train_regret_winner_15[slice52],\n",
    "       train_regret_winner_16[slice52],\n",
    "       train_regret_winner_17[slice52],\n",
    "       train_regret_winner_18[slice52],\n",
    "       train_regret_winner_19[slice52],\n",
    "       train_regret_winner_20[slice52]]\n",
    "\n",
    "loser52_results = pd.DataFrame(loser52).sort_values(by=[0], ascending=False)\n",
    "winner52_results = pd.DataFrame(winner52).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser52 = np.asarray(loser52_results[4:5][0])[0]\n",
    "median_loser52 = np.asarray(loser52_results[9:10][0])[0]\n",
    "upper_loser52 = np.asarray(loser52_results[14:15][0])[0]\n",
    "\n",
    "lower_winner52 = np.asarray(winner52_results[4:5][0])[0]\n",
    "median_winner52 = np.asarray(winner52_results[9:10][0])[0]\n",
    "upper_winner52 = np.asarray(winner52_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration62 :\n",
    "\n",
    "slice62 = 61\n",
    "\n",
    "loser62 = [train_regret_loser_1[slice62],\n",
    "       train_regret_loser_2[slice62],\n",
    "       train_regret_loser_3[slice62],\n",
    "       train_regret_loser_4[slice62],\n",
    "       train_regret_loser_5[slice62],\n",
    "       train_regret_loser_6[slice62],\n",
    "       train_regret_loser_7[slice62],\n",
    "       train_regret_loser_8[slice62],\n",
    "       train_regret_loser_9[slice62],\n",
    "       train_regret_loser_10[slice62],\n",
    "       train_regret_loser_11[slice62],\n",
    "       train_regret_loser_12[slice62],\n",
    "       train_regret_loser_13[slice62],\n",
    "       train_regret_loser_14[slice62],\n",
    "       train_regret_loser_15[slice62],\n",
    "       train_regret_loser_16[slice62],\n",
    "       train_regret_loser_17[slice62],\n",
    "       train_regret_loser_18[slice62],\n",
    "       train_regret_loser_19[slice62],\n",
    "       train_regret_loser_20[slice62]]\n",
    "\n",
    "winner62 = [train_regret_winner_1[slice62],\n",
    "       train_regret_winner_2[slice62],\n",
    "       train_regret_winner_3[slice62],\n",
    "       train_regret_winner_4[slice62],\n",
    "       train_regret_winner_5[slice62],\n",
    "       train_regret_winner_6[slice62],\n",
    "       train_regret_winner_7[slice62],\n",
    "       train_regret_winner_8[slice62],\n",
    "       train_regret_winner_9[slice62],\n",
    "       train_regret_winner_10[slice62],\n",
    "       train_regret_winner_11[slice62],\n",
    "       train_regret_winner_12[slice62],\n",
    "       train_regret_winner_13[slice62],\n",
    "       train_regret_winner_14[slice62],\n",
    "       train_regret_winner_15[slice62],\n",
    "       train_regret_winner_16[slice62],\n",
    "       train_regret_winner_17[slice62],\n",
    "       train_regret_winner_18[slice62],\n",
    "       train_regret_winner_19[slice62],\n",
    "       train_regret_winner_20[slice62]]\n",
    "\n",
    "loser62_results = pd.DataFrame(loser62).sort_values(by=[0], ascending=False)\n",
    "winner62_results = pd.DataFrame(winner62).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser62 = np.asarray(loser62_results[4:5][0])[0]\n",
    "median_loser62 = np.asarray(loser62_results[9:10][0])[0]\n",
    "upper_loser62 = np.asarray(loser62_results[14:15][0])[0]\n",
    "\n",
    "lower_winner62 = np.asarray(winner62_results[4:5][0])[0]\n",
    "median_winner62 = np.asarray(winner62_results[9:10][0])[0]\n",
    "upper_winner62 = np.asarray(winner62_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration72 :\n",
    "\n",
    "slice72 = 71\n",
    "\n",
    "loser72 = [train_regret_loser_1[slice72],\n",
    "       train_regret_loser_2[slice72],\n",
    "       train_regret_loser_3[slice72],\n",
    "       train_regret_loser_4[slice72],\n",
    "       train_regret_loser_5[slice72],\n",
    "       train_regret_loser_6[slice72],\n",
    "       train_regret_loser_7[slice72],\n",
    "       train_regret_loser_8[slice72],\n",
    "       train_regret_loser_9[slice72],\n",
    "       train_regret_loser_10[slice72],\n",
    "       train_regret_loser_11[slice72],\n",
    "       train_regret_loser_12[slice72],\n",
    "       train_regret_loser_13[slice72],\n",
    "       train_regret_loser_14[slice72],\n",
    "       train_regret_loser_15[slice72],\n",
    "       train_regret_loser_16[slice72],\n",
    "       train_regret_loser_17[slice72],\n",
    "       train_regret_loser_18[slice72],\n",
    "       train_regret_loser_19[slice72],\n",
    "       train_regret_loser_20[slice72]]\n",
    "\n",
    "winner72 = [train_regret_winner_1[slice72],\n",
    "       train_regret_winner_2[slice72],\n",
    "       train_regret_winner_3[slice72],\n",
    "       train_regret_winner_4[slice72],\n",
    "       train_regret_winner_5[slice72],\n",
    "       train_regret_winner_6[slice72],\n",
    "       train_regret_winner_7[slice72],\n",
    "       train_regret_winner_8[slice72],\n",
    "       train_regret_winner_9[slice72],\n",
    "       train_regret_winner_10[slice72],\n",
    "       train_regret_winner_11[slice72],\n",
    "       train_regret_winner_12[slice72],\n",
    "       train_regret_winner_13[slice72],\n",
    "       train_regret_winner_14[slice72],\n",
    "       train_regret_winner_15[slice72],\n",
    "       train_regret_winner_16[slice72],\n",
    "       train_regret_winner_17[slice72],\n",
    "       train_regret_winner_18[slice72],\n",
    "       train_regret_winner_19[slice72],\n",
    "       train_regret_winner_20[slice72]]\n",
    "\n",
    "loser72_results = pd.DataFrame(loser72).sort_values(by=[0], ascending=False)\n",
    "winner72_results = pd.DataFrame(winner72).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser72 = np.asarray(loser72_results[4:5][0])[0]\n",
    "median_loser72 = np.asarray(loser72_results[9:10][0])[0]\n",
    "upper_loser72 = np.asarray(loser72_results[14:15][0])[0]\n",
    "\n",
    "lower_winner72 = np.asarray(winner72_results[4:5][0])[0]\n",
    "median_winner72 = np.asarray(winner72_results[9:10][0])[0]\n",
    "upper_winner72 = np.asarray(winner72_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration82 :\n",
    "\n",
    "slice82 = 81\n",
    "\n",
    "loser82 = [train_regret_loser_1[slice82],\n",
    "       train_regret_loser_2[slice82],\n",
    "       train_regret_loser_3[slice82],\n",
    "       train_regret_loser_4[slice82],\n",
    "       train_regret_loser_5[slice82],\n",
    "       train_regret_loser_6[slice82],\n",
    "       train_regret_loser_7[slice82],\n",
    "       train_regret_loser_8[slice82],\n",
    "       train_regret_loser_9[slice82],\n",
    "       train_regret_loser_10[slice82],\n",
    "       train_regret_loser_11[slice82],\n",
    "       train_regret_loser_12[slice82],\n",
    "       train_regret_loser_13[slice82],\n",
    "       train_regret_loser_14[slice82],\n",
    "       train_regret_loser_15[slice82],\n",
    "       train_regret_loser_16[slice82],\n",
    "       train_regret_loser_17[slice82],\n",
    "       train_regret_loser_18[slice82],\n",
    "       train_regret_loser_19[slice82],\n",
    "       train_regret_loser_20[slice82]]\n",
    "\n",
    "winner82 = [train_regret_winner_1[slice82],\n",
    "       train_regret_winner_2[slice82],\n",
    "       train_regret_winner_3[slice82],\n",
    "       train_regret_winner_4[slice82],\n",
    "       train_regret_winner_5[slice82],\n",
    "       train_regret_winner_6[slice82],\n",
    "       train_regret_winner_7[slice82],\n",
    "       train_regret_winner_8[slice82],\n",
    "       train_regret_winner_9[slice82],\n",
    "       train_regret_winner_10[slice82],\n",
    "       train_regret_winner_11[slice82],\n",
    "       train_regret_winner_12[slice82],\n",
    "       train_regret_winner_13[slice82],\n",
    "       train_regret_winner_14[slice82],\n",
    "       train_regret_winner_15[slice82],\n",
    "       train_regret_winner_16[slice82],\n",
    "       train_regret_winner_17[slice82],\n",
    "       train_regret_winner_18[slice82],\n",
    "       train_regret_winner_19[slice82],\n",
    "       train_regret_winner_20[slice82]]\n",
    "\n",
    "loser82_results = pd.DataFrame(loser82).sort_values(by=[0], ascending=False)\n",
    "winner82_results = pd.DataFrame(winner82).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser82 = np.asarray(loser82_results[4:5][0])[0]\n",
    "median_loser82 = np.asarray(loser82_results[9:10][0])[0]\n",
    "upper_loser82 = np.asarray(loser82_results[14:15][0])[0]\n",
    "\n",
    "lower_winner82 = np.asarray(winner82_results[4:5][0])[0]\n",
    "median_winner82 = np.asarray(winner82_results[9:10][0])[0]\n",
    "upper_winner82 = np.asarray(winner82_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration92 :\n",
    "\n",
    "slice92 = 91\n",
    "\n",
    "loser92 = [train_regret_loser_1[slice92],\n",
    "       train_regret_loser_2[slice92],\n",
    "       train_regret_loser_3[slice92],\n",
    "       train_regret_loser_4[slice92],\n",
    "       train_regret_loser_5[slice92],\n",
    "       train_regret_loser_6[slice92],\n",
    "       train_regret_loser_7[slice92],\n",
    "       train_regret_loser_8[slice92],\n",
    "       train_regret_loser_9[slice92],\n",
    "       train_regret_loser_10[slice92],\n",
    "       train_regret_loser_11[slice92],\n",
    "       train_regret_loser_12[slice92],\n",
    "       train_regret_loser_13[slice92],\n",
    "       train_regret_loser_14[slice92],\n",
    "       train_regret_loser_15[slice92],\n",
    "       train_regret_loser_16[slice92],\n",
    "       train_regret_loser_17[slice92],\n",
    "       train_regret_loser_18[slice92],\n",
    "       train_regret_loser_19[slice92],\n",
    "       train_regret_loser_20[slice92]]\n",
    "\n",
    "winner92 = [train_regret_winner_1[slice92],\n",
    "       train_regret_winner_2[slice92],\n",
    "       train_regret_winner_3[slice92],\n",
    "       train_regret_winner_4[slice92],\n",
    "       train_regret_winner_5[slice92],\n",
    "       train_regret_winner_6[slice92],\n",
    "       train_regret_winner_7[slice92],\n",
    "       train_regret_winner_8[slice92],\n",
    "       train_regret_winner_9[slice92],\n",
    "       train_regret_winner_10[slice92],\n",
    "       train_regret_winner_11[slice92],\n",
    "       train_regret_winner_12[slice92],\n",
    "       train_regret_winner_13[slice92],\n",
    "       train_regret_winner_14[slice92],\n",
    "       train_regret_winner_15[slice92],\n",
    "       train_regret_winner_16[slice92],\n",
    "       train_regret_winner_17[slice92],\n",
    "       train_regret_winner_18[slice92],\n",
    "       train_regret_winner_19[slice92],\n",
    "       train_regret_winner_20[slice92]]\n",
    "\n",
    "loser92_results = pd.DataFrame(loser92).sort_values(by=[0], ascending=False)\n",
    "winner92_results = pd.DataFrame(winner92).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser92 = np.asarray(loser92_results[4:5][0])[0]\n",
    "median_loser92 = np.asarray(loser92_results[9:10][0])[0]\n",
    "upper_loser92 = np.asarray(loser92_results[14:15][0])[0]\n",
    "\n",
    "lower_winner92 = np.asarray(winner92_results[4:5][0])[0]\n",
    "median_winner92 = np.asarray(winner92_results[9:10][0])[0]\n",
    "upper_winner92 = np.asarray(winner92_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration3 :\n",
    "\n",
    "slice3 = 2\n",
    "\n",
    "loser3 = [train_regret_loser_1[slice3],\n",
    "       train_regret_loser_2[slice3],\n",
    "       train_regret_loser_3[slice3],\n",
    "       train_regret_loser_4[slice3],\n",
    "       train_regret_loser_5[slice3],\n",
    "       train_regret_loser_6[slice3],\n",
    "       train_regret_loser_7[slice3],\n",
    "       train_regret_loser_8[slice3],\n",
    "       train_regret_loser_9[slice3],\n",
    "       train_regret_loser_10[slice3],\n",
    "       train_regret_loser_11[slice3],\n",
    "       train_regret_loser_12[slice3],\n",
    "       train_regret_loser_13[slice3],\n",
    "       train_regret_loser_14[slice3],\n",
    "       train_regret_loser_15[slice3],\n",
    "       train_regret_loser_16[slice3],\n",
    "       train_regret_loser_17[slice3],\n",
    "       train_regret_loser_18[slice3],\n",
    "       train_regret_loser_19[slice3],\n",
    "       train_regret_loser_20[slice3]]\n",
    "\n",
    "winner3 = [train_regret_winner_1[slice3],\n",
    "       train_regret_winner_2[slice3],\n",
    "       train_regret_winner_3[slice3],\n",
    "       train_regret_winner_4[slice3],\n",
    "       train_regret_winner_5[slice3],\n",
    "       train_regret_winner_6[slice3],\n",
    "       train_regret_winner_7[slice3],\n",
    "       train_regret_winner_8[slice3],\n",
    "       train_regret_winner_9[slice3],\n",
    "       train_regret_winner_10[slice3],\n",
    "       train_regret_winner_11[slice3],\n",
    "       train_regret_winner_12[slice3],\n",
    "       train_regret_winner_13[slice3],\n",
    "       train_regret_winner_14[slice3],\n",
    "       train_regret_winner_15[slice3],\n",
    "       train_regret_winner_16[slice3],\n",
    "       train_regret_winner_17[slice3],\n",
    "       train_regret_winner_18[slice3],\n",
    "       train_regret_winner_19[slice3],\n",
    "       train_regret_winner_20[slice3]]\n",
    "\n",
    "loser3_results = pd.DataFrame(loser3).sort_values(by=[0], ascending=False)\n",
    "winner3_results = pd.DataFrame(winner3).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser3 = np.asarray(loser3_results[4:5][0])[0]\n",
    "median_loser3 = np.asarray(loser3_results[9:10][0])[0]\n",
    "upper_loser3 = np.asarray(loser3_results[14:15][0])[0]\n",
    "\n",
    "lower_winner3 = np.asarray(winner3_results[4:5][0])[0]\n",
    "median_winner3 = np.asarray(winner3_results[9:10][0])[0]\n",
    "upper_winner3 = np.asarray(winner3_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration13 :\n",
    "\n",
    "slice13 = 12\n",
    "\n",
    "loser13 = [train_regret_loser_1[slice13],\n",
    "       train_regret_loser_2[slice13],\n",
    "       train_regret_loser_3[slice13],\n",
    "       train_regret_loser_4[slice13],\n",
    "       train_regret_loser_5[slice13],\n",
    "       train_regret_loser_6[slice13],\n",
    "       train_regret_loser_7[slice13],\n",
    "       train_regret_loser_8[slice13],\n",
    "       train_regret_loser_9[slice13],\n",
    "       train_regret_loser_10[slice13],\n",
    "       train_regret_loser_11[slice13],\n",
    "       train_regret_loser_12[slice13],\n",
    "       train_regret_loser_13[slice13],\n",
    "       train_regret_loser_14[slice13],\n",
    "       train_regret_loser_15[slice13],\n",
    "       train_regret_loser_16[slice13],\n",
    "       train_regret_loser_17[slice13],\n",
    "       train_regret_loser_18[slice13],\n",
    "       train_regret_loser_19[slice13],\n",
    "       train_regret_loser_20[slice13]]\n",
    "\n",
    "winner13 = [train_regret_winner_1[slice13],\n",
    "       train_regret_winner_2[slice13],\n",
    "       train_regret_winner_3[slice13],\n",
    "       train_regret_winner_4[slice13],\n",
    "       train_regret_winner_5[slice13],\n",
    "       train_regret_winner_6[slice13],\n",
    "       train_regret_winner_7[slice13],\n",
    "       train_regret_winner_8[slice13],\n",
    "       train_regret_winner_9[slice13],\n",
    "       train_regret_winner_10[slice13],\n",
    "       train_regret_winner_11[slice13],\n",
    "       train_regret_winner_12[slice13],\n",
    "       train_regret_winner_13[slice13],\n",
    "       train_regret_winner_14[slice13],\n",
    "       train_regret_winner_15[slice13],\n",
    "       train_regret_winner_16[slice13],\n",
    "       train_regret_winner_17[slice13],\n",
    "       train_regret_winner_18[slice13],\n",
    "       train_regret_winner_19[slice13],\n",
    "       train_regret_winner_20[slice13]]\n",
    "\n",
    "loser13_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\n",
    "winner13_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser13 = np.asarray(loser13_results[4:5][0])[0]\n",
    "median_loser13 = np.asarray(loser13_results[9:10][0])[0]\n",
    "upper_loser13 = np.asarray(loser13_results[14:15][0])[0]\n",
    "\n",
    "lower_winner13 = np.asarray(winner13_results[4:5][0])[0]\n",
    "median_winner13 = np.asarray(winner13_results[9:10][0])[0]\n",
    "upper_winner13 = np.asarray(winner13_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration23 :\n",
    "\n",
    "slice23 = 22\n",
    "\n",
    "loser23 = [train_regret_loser_1[slice23],\n",
    "       train_regret_loser_2[slice23],\n",
    "       train_regret_loser_3[slice23],\n",
    "       train_regret_loser_4[slice23],\n",
    "       train_regret_loser_5[slice23],\n",
    "       train_regret_loser_6[slice23],\n",
    "       train_regret_loser_7[slice23],\n",
    "       train_regret_loser_8[slice23],\n",
    "       train_regret_loser_9[slice23],\n",
    "       train_regret_loser_10[slice23],\n",
    "       train_regret_loser_11[slice23],\n",
    "       train_regret_loser_12[slice23],\n",
    "       train_regret_loser_13[slice23],\n",
    "       train_regret_loser_14[slice23],\n",
    "       train_regret_loser_15[slice23],\n",
    "       train_regret_loser_16[slice23],\n",
    "       train_regret_loser_17[slice23],\n",
    "       train_regret_loser_18[slice23],\n",
    "       train_regret_loser_19[slice23],\n",
    "       train_regret_loser_20[slice23]]\n",
    "\n",
    "winner23 = [train_regret_winner_1[slice23],\n",
    "       train_regret_winner_2[slice23],\n",
    "       train_regret_winner_3[slice23],\n",
    "       train_regret_winner_4[slice23],\n",
    "       train_regret_winner_5[slice23],\n",
    "       train_regret_winner_6[slice23],\n",
    "       train_regret_winner_7[slice23],\n",
    "       train_regret_winner_8[slice23],\n",
    "       train_regret_winner_9[slice23],\n",
    "       train_regret_winner_10[slice23],\n",
    "       train_regret_winner_11[slice23],\n",
    "       train_regret_winner_12[slice23],\n",
    "       train_regret_winner_13[slice23],\n",
    "       train_regret_winner_14[slice23],\n",
    "       train_regret_winner_15[slice23],\n",
    "       train_regret_winner_16[slice23],\n",
    "       train_regret_winner_17[slice23],\n",
    "       train_regret_winner_18[slice23],\n",
    "       train_regret_winner_19[slice23],\n",
    "       train_regret_winner_20[slice23]]\n",
    "\n",
    "loser23_results = pd.DataFrame(loser23).sort_values(by=[0], ascending=False)\n",
    "winner23_results = pd.DataFrame(winner23).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser23 = np.asarray(loser23_results[4:5][0])[0]\n",
    "median_loser23 = np.asarray(loser23_results[9:10][0])[0]\n",
    "upper_loser23 = np.asarray(loser23_results[14:15][0])[0]\n",
    "\n",
    "lower_winner23 = np.asarray(winner23_results[4:5][0])[0]\n",
    "median_winner23 = np.asarray(winner23_results[9:10][0])[0]\n",
    "upper_winner23 = np.asarray(winner23_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration33 :\n",
    "\n",
    "slice33 = 32\n",
    "\n",
    "loser33 = [train_regret_loser_1[slice33],\n",
    "       train_regret_loser_2[slice33],\n",
    "       train_regret_loser_3[slice33],\n",
    "       train_regret_loser_4[slice33],\n",
    "       train_regret_loser_5[slice33],\n",
    "       train_regret_loser_6[slice33],\n",
    "       train_regret_loser_7[slice33],\n",
    "       train_regret_loser_8[slice33],\n",
    "       train_regret_loser_9[slice33],\n",
    "       train_regret_loser_10[slice33],\n",
    "       train_regret_loser_11[slice33],\n",
    "       train_regret_loser_12[slice33],\n",
    "       train_regret_loser_13[slice33],\n",
    "       train_regret_loser_14[slice33],\n",
    "       train_regret_loser_15[slice33],\n",
    "       train_regret_loser_16[slice33],\n",
    "       train_regret_loser_17[slice33],\n",
    "       train_regret_loser_18[slice33],\n",
    "       train_regret_loser_19[slice33],\n",
    "       train_regret_loser_20[slice33]]\n",
    "\n",
    "winner33 = [train_regret_winner_1[slice33],\n",
    "       train_regret_winner_2[slice33],\n",
    "       train_regret_winner_3[slice33],\n",
    "       train_regret_winner_4[slice33],\n",
    "       train_regret_winner_5[slice33],\n",
    "       train_regret_winner_6[slice33],\n",
    "       train_regret_winner_7[slice33],\n",
    "       train_regret_winner_8[slice33],\n",
    "       train_regret_winner_9[slice33],\n",
    "       train_regret_winner_10[slice33],\n",
    "       train_regret_winner_11[slice33],\n",
    "       train_regret_winner_12[slice33],\n",
    "       train_regret_winner_13[slice33],\n",
    "       train_regret_winner_14[slice33],\n",
    "       train_regret_winner_15[slice33],\n",
    "       train_regret_winner_16[slice33],\n",
    "       train_regret_winner_17[slice33],\n",
    "       train_regret_winner_18[slice33],\n",
    "       train_regret_winner_19[slice33],\n",
    "       train_regret_winner_20[slice33]]\n",
    "\n",
    "loser33_results = pd.DataFrame(loser33).sort_values(by=[0], ascending=False)\n",
    "winner33_results = pd.DataFrame(winner33).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser33 = np.asarray(loser33_results[4:5][0])[0]\n",
    "median_loser33 = np.asarray(loser33_results[9:10][0])[0]\n",
    "upper_loser33 = np.asarray(loser33_results[14:15][0])[0]\n",
    "\n",
    "lower_winner33 = np.asarray(winner33_results[4:5][0])[0]\n",
    "median_winner33 = np.asarray(winner33_results[9:10][0])[0]\n",
    "upper_winner33 = np.asarray(winner33_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration43 :\n",
    "\n",
    "slice43 = 42\n",
    "\n",
    "loser43 = [train_regret_loser_1[slice43],\n",
    "       train_regret_loser_2[slice43],\n",
    "       train_regret_loser_3[slice43],\n",
    "       train_regret_loser_4[slice43],\n",
    "       train_regret_loser_5[slice43],\n",
    "       train_regret_loser_6[slice43],\n",
    "       train_regret_loser_7[slice43],\n",
    "       train_regret_loser_8[slice43],\n",
    "       train_regret_loser_9[slice43],\n",
    "       train_regret_loser_10[slice43],\n",
    "       train_regret_loser_11[slice43],\n",
    "       train_regret_loser_12[slice43],\n",
    "       train_regret_loser_13[slice43],\n",
    "       train_regret_loser_14[slice43],\n",
    "       train_regret_loser_15[slice43],\n",
    "       train_regret_loser_16[slice43],\n",
    "       train_regret_loser_17[slice43],\n",
    "       train_regret_loser_18[slice43],\n",
    "       train_regret_loser_19[slice43],\n",
    "       train_regret_loser_20[slice43]]\n",
    "\n",
    "winner43 = [train_regret_winner_1[slice43],\n",
    "       train_regret_winner_2[slice43],\n",
    "       train_regret_winner_3[slice43],\n",
    "       train_regret_winner_4[slice43],\n",
    "       train_regret_winner_5[slice43],\n",
    "       train_regret_winner_6[slice43],\n",
    "       train_regret_winner_7[slice43],\n",
    "       train_regret_winner_8[slice43],\n",
    "       train_regret_winner_9[slice43],\n",
    "       train_regret_winner_10[slice43],\n",
    "       train_regret_winner_11[slice43],\n",
    "       train_regret_winner_12[slice43],\n",
    "       train_regret_winner_13[slice43],\n",
    "       train_regret_winner_14[slice43],\n",
    "       train_regret_winner_15[slice43],\n",
    "       train_regret_winner_16[slice43],\n",
    "       train_regret_winner_17[slice43],\n",
    "       train_regret_winner_18[slice43],\n",
    "       train_regret_winner_19[slice43],\n",
    "       train_regret_winner_20[slice43]]\n",
    "\n",
    "loser43_results = pd.DataFrame(loser43).sort_values(by=[0], ascending=False)\n",
    "winner43_results = pd.DataFrame(winner43).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser43 = np.asarray(loser43_results[4:5][0])[0]\n",
    "median_loser43 = np.asarray(loser43_results[9:10][0])[0]\n",
    "upper_loser43 = np.asarray(loser43_results[14:15][0])[0]\n",
    "\n",
    "lower_winner43 = np.asarray(winner43_results[4:5][0])[0]\n",
    "median_winner43 = np.asarray(winner43_results[9:10][0])[0]\n",
    "upper_winner43 = np.asarray(winner43_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration53 :\n",
    "\n",
    "slice53 = 52\n",
    "\n",
    "loser53 = [train_regret_loser_1[slice53],\n",
    "       train_regret_loser_2[slice53],\n",
    "       train_regret_loser_3[slice53],\n",
    "       train_regret_loser_4[slice53],\n",
    "       train_regret_loser_5[slice53],\n",
    "       train_regret_loser_6[slice53],\n",
    "       train_regret_loser_7[slice53],\n",
    "       train_regret_loser_8[slice53],\n",
    "       train_regret_loser_9[slice53],\n",
    "       train_regret_loser_10[slice53],\n",
    "       train_regret_loser_11[slice53],\n",
    "       train_regret_loser_12[slice53],\n",
    "       train_regret_loser_13[slice53],\n",
    "       train_regret_loser_14[slice53],\n",
    "       train_regret_loser_15[slice53],\n",
    "       train_regret_loser_16[slice53],\n",
    "       train_regret_loser_17[slice53],\n",
    "       train_regret_loser_18[slice53],\n",
    "       train_regret_loser_19[slice53],\n",
    "       train_regret_loser_20[slice53]]\n",
    "\n",
    "winner53 = [train_regret_winner_1[slice53],\n",
    "       train_regret_winner_2[slice53],\n",
    "       train_regret_winner_3[slice53],\n",
    "       train_regret_winner_4[slice53],\n",
    "       train_regret_winner_5[slice53],\n",
    "       train_regret_winner_6[slice53],\n",
    "       train_regret_winner_7[slice53],\n",
    "       train_regret_winner_8[slice53],\n",
    "       train_regret_winner_9[slice53],\n",
    "       train_regret_winner_10[slice53],\n",
    "       train_regret_winner_11[slice53],\n",
    "       train_regret_winner_12[slice53],\n",
    "       train_regret_winner_13[slice53],\n",
    "       train_regret_winner_14[slice53],\n",
    "       train_regret_winner_15[slice53],\n",
    "       train_regret_winner_16[slice53],\n",
    "       train_regret_winner_17[slice53],\n",
    "       train_regret_winner_18[slice53],\n",
    "       train_regret_winner_19[slice53],\n",
    "       train_regret_winner_20[slice53]]\n",
    "\n",
    "loser53_results = pd.DataFrame(loser53).sort_values(by=[0], ascending=False)\n",
    "winner53_results = pd.DataFrame(winner53).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser53 = np.asarray(loser53_results[4:5][0])[0]\n",
    "median_loser53 = np.asarray(loser53_results[9:10][0])[0]\n",
    "upper_loser53 = np.asarray(loser53_results[14:15][0])[0]\n",
    "\n",
    "lower_winner53 = np.asarray(winner53_results[4:5][0])[0]\n",
    "median_winner53 = np.asarray(winner53_results[9:10][0])[0]\n",
    "upper_winner53 = np.asarray(winner53_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration63 :\n",
    "\n",
    "slice63 = 62\n",
    "\n",
    "loser63 = [train_regret_loser_1[slice63],\n",
    "       train_regret_loser_2[slice63],\n",
    "       train_regret_loser_3[slice63],\n",
    "       train_regret_loser_4[slice63],\n",
    "       train_regret_loser_5[slice63],\n",
    "       train_regret_loser_6[slice63],\n",
    "       train_regret_loser_7[slice63],\n",
    "       train_regret_loser_8[slice63],\n",
    "       train_regret_loser_9[slice63],\n",
    "       train_regret_loser_10[slice63],\n",
    "       train_regret_loser_11[slice63],\n",
    "       train_regret_loser_12[slice63],\n",
    "       train_regret_loser_13[slice63],\n",
    "       train_regret_loser_14[slice63],\n",
    "       train_regret_loser_15[slice63],\n",
    "       train_regret_loser_16[slice63],\n",
    "       train_regret_loser_17[slice63],\n",
    "       train_regret_loser_18[slice63],\n",
    "       train_regret_loser_19[slice63],\n",
    "       train_regret_loser_20[slice63]]\n",
    "\n",
    "winner63 = [train_regret_winner_1[slice63],\n",
    "       train_regret_winner_2[slice63],\n",
    "       train_regret_winner_3[slice63],\n",
    "       train_regret_winner_4[slice63],\n",
    "       train_regret_winner_5[slice63],\n",
    "       train_regret_winner_6[slice63],\n",
    "       train_regret_winner_7[slice63],\n",
    "       train_regret_winner_8[slice63],\n",
    "       train_regret_winner_9[slice63],\n",
    "       train_regret_winner_10[slice63],\n",
    "       train_regret_winner_11[slice63],\n",
    "       train_regret_winner_12[slice63],\n",
    "       train_regret_winner_13[slice63],\n",
    "       train_regret_winner_14[slice63],\n",
    "       train_regret_winner_15[slice63],\n",
    "       train_regret_winner_16[slice63],\n",
    "       train_regret_winner_17[slice63],\n",
    "       train_regret_winner_18[slice63],\n",
    "       train_regret_winner_19[slice63],\n",
    "       train_regret_winner_20[slice63]]\n",
    "\n",
    "loser63_results = pd.DataFrame(loser63).sort_values(by=[0], ascending=False)\n",
    "winner63_results = pd.DataFrame(winner63).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser63 = np.asarray(loser63_results[4:5][0])[0]\n",
    "median_loser63 = np.asarray(loser63_results[9:10][0])[0]\n",
    "upper_loser63 = np.asarray(loser63_results[14:15][0])[0]\n",
    "\n",
    "lower_winner63 = np.asarray(winner63_results[4:5][0])[0]\n",
    "median_winner63 = np.asarray(winner63_results[9:10][0])[0]\n",
    "upper_winner63 = np.asarray(winner63_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration73 :\n",
    "\n",
    "slice73 = 72\n",
    "\n",
    "loser73 = [train_regret_loser_1[slice73],\n",
    "       train_regret_loser_2[slice73],\n",
    "       train_regret_loser_3[slice73],\n",
    "       train_regret_loser_4[slice73],\n",
    "       train_regret_loser_5[slice73],\n",
    "       train_regret_loser_6[slice73],\n",
    "       train_regret_loser_7[slice73],\n",
    "       train_regret_loser_8[slice73],\n",
    "       train_regret_loser_9[slice73],\n",
    "       train_regret_loser_10[slice73],\n",
    "       train_regret_loser_11[slice73],\n",
    "       train_regret_loser_12[slice73],\n",
    "       train_regret_loser_13[slice73],\n",
    "       train_regret_loser_14[slice73],\n",
    "       train_regret_loser_15[slice73],\n",
    "       train_regret_loser_16[slice73],\n",
    "       train_regret_loser_17[slice73],\n",
    "       train_regret_loser_18[slice73],\n",
    "       train_regret_loser_19[slice73],\n",
    "       train_regret_loser_20[slice73]]\n",
    "\n",
    "winner73 = [train_regret_winner_1[slice73],\n",
    "       train_regret_winner_2[slice73],\n",
    "       train_regret_winner_3[slice73],\n",
    "       train_regret_winner_4[slice73],\n",
    "       train_regret_winner_5[slice73],\n",
    "       train_regret_winner_6[slice73],\n",
    "       train_regret_winner_7[slice73],\n",
    "       train_regret_winner_8[slice73],\n",
    "       train_regret_winner_9[slice73],\n",
    "       train_regret_winner_10[slice73],\n",
    "       train_regret_winner_11[slice73],\n",
    "       train_regret_winner_12[slice73],\n",
    "       train_regret_winner_13[slice73],\n",
    "       train_regret_winner_14[slice73],\n",
    "       train_regret_winner_15[slice73],\n",
    "       train_regret_winner_16[slice73],\n",
    "       train_regret_winner_17[slice73],\n",
    "       train_regret_winner_18[slice73],\n",
    "       train_regret_winner_19[slice73],\n",
    "       train_regret_winner_20[slice73]]\n",
    "\n",
    "loser73_results = pd.DataFrame(loser73).sort_values(by=[0], ascending=False)\n",
    "winner73_results = pd.DataFrame(winner73).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser73 = np.asarray(loser73_results[4:5][0])[0]\n",
    "median_loser73 = np.asarray(loser73_results[9:10][0])[0]\n",
    "upper_loser73 = np.asarray(loser73_results[14:15][0])[0]\n",
    "\n",
    "lower_winner73 = np.asarray(winner73_results[4:5][0])[0]\n",
    "median_winner73 = np.asarray(winner73_results[9:10][0])[0]\n",
    "upper_winner73 = np.asarray(winner73_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration83 :\n",
    "\n",
    "slice83 = 82\n",
    "\n",
    "loser83 = [train_regret_loser_1[slice83],\n",
    "       train_regret_loser_2[slice83],\n",
    "       train_regret_loser_3[slice83],\n",
    "       train_regret_loser_4[slice83],\n",
    "       train_regret_loser_5[slice83],\n",
    "       train_regret_loser_6[slice83],\n",
    "       train_regret_loser_7[slice83],\n",
    "       train_regret_loser_8[slice83],\n",
    "       train_regret_loser_9[slice83],\n",
    "       train_regret_loser_10[slice83],\n",
    "       train_regret_loser_11[slice83],\n",
    "       train_regret_loser_12[slice83],\n",
    "       train_regret_loser_13[slice83],\n",
    "       train_regret_loser_14[slice83],\n",
    "       train_regret_loser_15[slice83],\n",
    "       train_regret_loser_16[slice83],\n",
    "       train_regret_loser_17[slice83],\n",
    "       train_regret_loser_18[slice83],\n",
    "       train_regret_loser_19[slice83],\n",
    "       train_regret_loser_20[slice83]]\n",
    "\n",
    "winner83 = [train_regret_winner_1[slice83],\n",
    "       train_regret_winner_2[slice83],\n",
    "       train_regret_winner_3[slice83],\n",
    "       train_regret_winner_4[slice83],\n",
    "       train_regret_winner_5[slice83],\n",
    "       train_regret_winner_6[slice83],\n",
    "       train_regret_winner_7[slice83],\n",
    "       train_regret_winner_8[slice83],\n",
    "       train_regret_winner_9[slice83],\n",
    "       train_regret_winner_10[slice83],\n",
    "       train_regret_winner_11[slice83],\n",
    "       train_regret_winner_12[slice83],\n",
    "       train_regret_winner_13[slice83],\n",
    "       train_regret_winner_14[slice83],\n",
    "       train_regret_winner_15[slice83],\n",
    "       train_regret_winner_16[slice83],\n",
    "       train_regret_winner_17[slice83],\n",
    "       train_regret_winner_18[slice83],\n",
    "       train_regret_winner_19[slice83],\n",
    "       train_regret_winner_20[slice83]]\n",
    "\n",
    "loser83_results = pd.DataFrame(loser83).sort_values(by=[0], ascending=False)\n",
    "winner83_results = pd.DataFrame(winner83).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser83 = np.asarray(loser83_results[4:5][0])[0]\n",
    "median_loser83 = np.asarray(loser83_results[9:10][0])[0]\n",
    "upper_loser83 = np.asarray(loser83_results[14:15][0])[0]\n",
    "\n",
    "lower_winner83 = np.asarray(winner83_results[4:5][0])[0]\n",
    "median_winner83 = np.asarray(winner83_results[9:10][0])[0]\n",
    "upper_winner83 = np.asarray(winner83_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration93 :\n",
    "\n",
    "slice93 = 92\n",
    "\n",
    "loser93 = [train_regret_loser_1[slice93],\n",
    "       train_regret_loser_2[slice93],\n",
    "       train_regret_loser_3[slice93],\n",
    "       train_regret_loser_4[slice93],\n",
    "       train_regret_loser_5[slice93],\n",
    "       train_regret_loser_6[slice93],\n",
    "       train_regret_loser_7[slice93],\n",
    "       train_regret_loser_8[slice93],\n",
    "       train_regret_loser_9[slice93],\n",
    "       train_regret_loser_10[slice93],\n",
    "       train_regret_loser_11[slice93],\n",
    "       train_regret_loser_12[slice93],\n",
    "       train_regret_loser_13[slice93],\n",
    "       train_regret_loser_14[slice93],\n",
    "       train_regret_loser_15[slice93],\n",
    "       train_regret_loser_16[slice93],\n",
    "       train_regret_loser_17[slice93],\n",
    "       train_regret_loser_18[slice93],\n",
    "       train_regret_loser_19[slice93],\n",
    "       train_regret_loser_20[slice93]]\n",
    "\n",
    "winner93 = [train_regret_winner_1[slice93],\n",
    "       train_regret_winner_2[slice93],\n",
    "       train_regret_winner_3[slice93],\n",
    "       train_regret_winner_4[slice93],\n",
    "       train_regret_winner_5[slice93],\n",
    "       train_regret_winner_6[slice93],\n",
    "       train_regret_winner_7[slice93],\n",
    "       train_regret_winner_8[slice93],\n",
    "       train_regret_winner_9[slice93],\n",
    "       train_regret_winner_10[slice93],\n",
    "       train_regret_winner_11[slice93],\n",
    "       train_regret_winner_12[slice93],\n",
    "       train_regret_winner_13[slice93],\n",
    "       train_regret_winner_14[slice93],\n",
    "       train_regret_winner_15[slice93],\n",
    "       train_regret_winner_16[slice93],\n",
    "       train_regret_winner_17[slice93],\n",
    "       train_regret_winner_18[slice93],\n",
    "       train_regret_winner_19[slice93],\n",
    "       train_regret_winner_20[slice93]]\n",
    "\n",
    "loser93_results = pd.DataFrame(loser93).sort_values(by=[0], ascending=False)\n",
    "winner93_results = pd.DataFrame(winner93).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser93 = np.asarray(loser93_results[4:5][0])[0]\n",
    "median_loser93 = np.asarray(loser93_results[9:10][0])[0]\n",
    "upper_loser93 = np.asarray(loser93_results[14:15][0])[0]\n",
    "\n",
    "lower_winner93 = np.asarray(winner93_results[4:5][0])[0]\n",
    "median_winner93 = np.asarray(winner93_results[9:10][0])[0]\n",
    "upper_winner93 = np.asarray(winner93_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration4 :\n",
    "\n",
    "slice4 = 3\n",
    "\n",
    "loser4 = [train_regret_loser_1[slice4],\n",
    "       train_regret_loser_2[slice4],\n",
    "       train_regret_loser_3[slice4],\n",
    "       train_regret_loser_4[slice4],\n",
    "       train_regret_loser_5[slice4],\n",
    "       train_regret_loser_6[slice4],\n",
    "       train_regret_loser_7[slice4],\n",
    "       train_regret_loser_8[slice4],\n",
    "       train_regret_loser_9[slice4],\n",
    "       train_regret_loser_10[slice4],\n",
    "       train_regret_loser_11[slice4],\n",
    "       train_regret_loser_12[slice4],\n",
    "       train_regret_loser_13[slice4],\n",
    "       train_regret_loser_14[slice4],\n",
    "       train_regret_loser_15[slice4],\n",
    "       train_regret_loser_16[slice4],\n",
    "       train_regret_loser_17[slice4],\n",
    "       train_regret_loser_18[slice4],\n",
    "       train_regret_loser_19[slice4],\n",
    "       train_regret_loser_20[slice4]]\n",
    "\n",
    "winner4 = [train_regret_winner_1[slice4],\n",
    "       train_regret_winner_2[slice4],\n",
    "       train_regret_winner_3[slice4],\n",
    "       train_regret_winner_4[slice4],\n",
    "       train_regret_winner_5[slice4],\n",
    "       train_regret_winner_6[slice4],\n",
    "       train_regret_winner_7[slice4],\n",
    "       train_regret_winner_8[slice4],\n",
    "       train_regret_winner_9[slice4],\n",
    "       train_regret_winner_10[slice4],\n",
    "       train_regret_winner_11[slice4],\n",
    "       train_regret_winner_12[slice4],\n",
    "       train_regret_winner_13[slice4],\n",
    "       train_regret_winner_14[slice4],\n",
    "       train_regret_winner_15[slice4],\n",
    "       train_regret_winner_16[slice4],\n",
    "       train_regret_winner_17[slice4],\n",
    "       train_regret_winner_18[slice4],\n",
    "       train_regret_winner_19[slice4],\n",
    "       train_regret_winner_20[slice4]]\n",
    "\n",
    "loser4_results = pd.DataFrame(loser4).sort_values(by=[0], ascending=False)\n",
    "winner4_results = pd.DataFrame(winner4).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser4 = np.asarray(loser4_results[4:5][0])[0]\n",
    "median_loser4 = np.asarray(loser4_results[9:10][0])[0]\n",
    "upper_loser4 = np.asarray(loser4_results[14:15][0])[0]\n",
    "\n",
    "lower_winner4 = np.asarray(winner4_results[4:5][0])[0]\n",
    "median_winner4 = np.asarray(winner4_results[9:10][0])[0]\n",
    "upper_winner4 = np.asarray(winner4_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration14 :\n",
    "\n",
    "slice14 = 13\n",
    "\n",
    "loser14 = [train_regret_loser_1[slice14],\n",
    "       train_regret_loser_2[slice14],\n",
    "       train_regret_loser_3[slice14],\n",
    "       train_regret_loser_4[slice14],\n",
    "       train_regret_loser_5[slice14],\n",
    "       train_regret_loser_6[slice14],\n",
    "       train_regret_loser_7[slice14],\n",
    "       train_regret_loser_8[slice14],\n",
    "       train_regret_loser_9[slice14],\n",
    "       train_regret_loser_10[slice14],\n",
    "       train_regret_loser_11[slice14],\n",
    "       train_regret_loser_12[slice14],\n",
    "       train_regret_loser_13[slice14],\n",
    "       train_regret_loser_14[slice14],\n",
    "       train_regret_loser_15[slice14],\n",
    "       train_regret_loser_16[slice14],\n",
    "       train_regret_loser_17[slice14],\n",
    "       train_regret_loser_18[slice14],\n",
    "       train_regret_loser_19[slice14],\n",
    "       train_regret_loser_20[slice14]]\n",
    "\n",
    "winner14 = [train_regret_winner_1[slice14],\n",
    "       train_regret_winner_2[slice14],\n",
    "       train_regret_winner_3[slice14],\n",
    "       train_regret_winner_4[slice14],\n",
    "       train_regret_winner_5[slice14],\n",
    "       train_regret_winner_6[slice14],\n",
    "       train_regret_winner_7[slice14],\n",
    "       train_regret_winner_8[slice14],\n",
    "       train_regret_winner_9[slice14],\n",
    "       train_regret_winner_10[slice14],\n",
    "       train_regret_winner_11[slice14],\n",
    "       train_regret_winner_12[slice14],\n",
    "       train_regret_winner_13[slice14],\n",
    "       train_regret_winner_14[slice14],\n",
    "       train_regret_winner_15[slice14],\n",
    "       train_regret_winner_16[slice14],\n",
    "       train_regret_winner_17[slice14],\n",
    "       train_regret_winner_18[slice14],\n",
    "       train_regret_winner_19[slice14],\n",
    "       train_regret_winner_20[slice14]]\n",
    "\n",
    "loser14_results = pd.DataFrame(loser14).sort_values(by=[0], ascending=False)\n",
    "winner14_results = pd.DataFrame(winner14).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser14 = np.asarray(loser14_results[4:5][0])[0]\n",
    "median_loser14 = np.asarray(loser14_results[9:10][0])[0]\n",
    "upper_loser14 = np.asarray(loser14_results[14:15][0])[0]\n",
    "\n",
    "lower_winner14 = np.asarray(winner14_results[4:5][0])[0]\n",
    "median_winner14 = np.asarray(winner14_results[9:10][0])[0]\n",
    "upper_winner14 = np.asarray(winner14_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration24 :\n",
    "\n",
    "slice24 = 23\n",
    "\n",
    "loser24 = [train_regret_loser_1[slice24],\n",
    "       train_regret_loser_2[slice24],\n",
    "       train_regret_loser_3[slice24],\n",
    "       train_regret_loser_4[slice24],\n",
    "       train_regret_loser_5[slice24],\n",
    "       train_regret_loser_6[slice24],\n",
    "       train_regret_loser_7[slice24],\n",
    "       train_regret_loser_8[slice24],\n",
    "       train_regret_loser_9[slice24],\n",
    "       train_regret_loser_10[slice24],\n",
    "       train_regret_loser_11[slice24],\n",
    "       train_regret_loser_12[slice24],\n",
    "       train_regret_loser_13[slice24],\n",
    "       train_regret_loser_14[slice24],\n",
    "       train_regret_loser_15[slice24],\n",
    "       train_regret_loser_16[slice24],\n",
    "       train_regret_loser_17[slice24],\n",
    "       train_regret_loser_18[slice24],\n",
    "       train_regret_loser_19[slice24],\n",
    "       train_regret_loser_20[slice24]]\n",
    "\n",
    "winner24 = [train_regret_winner_1[slice24],\n",
    "       train_regret_winner_2[slice24],\n",
    "       train_regret_winner_3[slice24],\n",
    "       train_regret_winner_4[slice24],\n",
    "       train_regret_winner_5[slice24],\n",
    "       train_regret_winner_6[slice24],\n",
    "       train_regret_winner_7[slice24],\n",
    "       train_regret_winner_8[slice24],\n",
    "       train_regret_winner_9[slice24],\n",
    "       train_regret_winner_10[slice24],\n",
    "       train_regret_winner_11[slice24],\n",
    "       train_regret_winner_12[slice24],\n",
    "       train_regret_winner_13[slice24],\n",
    "       train_regret_winner_14[slice24],\n",
    "       train_regret_winner_15[slice24],\n",
    "       train_regret_winner_16[slice24],\n",
    "       train_regret_winner_17[slice24],\n",
    "       train_regret_winner_18[slice24],\n",
    "       train_regret_winner_19[slice24],\n",
    "       train_regret_winner_20[slice24]]\n",
    "\n",
    "loser24_results = pd.DataFrame(loser24).sort_values(by=[0], ascending=False)\n",
    "winner24_results = pd.DataFrame(winner24).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser24 = np.asarray(loser24_results[4:5][0])[0]\n",
    "median_loser24 = np.asarray(loser24_results[9:10][0])[0]\n",
    "upper_loser24 = np.asarray(loser24_results[14:15][0])[0]\n",
    "\n",
    "lower_winner24 = np.asarray(winner24_results[4:5][0])[0]\n",
    "median_winner24 = np.asarray(winner24_results[9:10][0])[0]\n",
    "upper_winner24 = np.asarray(winner24_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration34 :\n",
    "\n",
    "slice34 = 33\n",
    "\n",
    "loser34 = [train_regret_loser_1[slice34],\n",
    "       train_regret_loser_2[slice34],\n",
    "       train_regret_loser_3[slice34],\n",
    "       train_regret_loser_4[slice34],\n",
    "       train_regret_loser_5[slice34],\n",
    "       train_regret_loser_6[slice34],\n",
    "       train_regret_loser_7[slice34],\n",
    "       train_regret_loser_8[slice34],\n",
    "       train_regret_loser_9[slice34],\n",
    "       train_regret_loser_10[slice34],\n",
    "       train_regret_loser_11[slice34],\n",
    "       train_regret_loser_12[slice34],\n",
    "       train_regret_loser_13[slice34],\n",
    "       train_regret_loser_14[slice34],\n",
    "       train_regret_loser_15[slice34],\n",
    "       train_regret_loser_16[slice34],\n",
    "       train_regret_loser_17[slice34],\n",
    "       train_regret_loser_18[slice34],\n",
    "       train_regret_loser_19[slice34],\n",
    "       train_regret_loser_20[slice34]]\n",
    "\n",
    "winner34 = [train_regret_winner_1[slice34],\n",
    "       train_regret_winner_2[slice34],\n",
    "       train_regret_winner_3[slice34],\n",
    "       train_regret_winner_4[slice34],\n",
    "       train_regret_winner_5[slice34],\n",
    "       train_regret_winner_6[slice34],\n",
    "       train_regret_winner_7[slice34],\n",
    "       train_regret_winner_8[slice34],\n",
    "       train_regret_winner_9[slice34],\n",
    "       train_regret_winner_10[slice34],\n",
    "       train_regret_winner_11[slice34],\n",
    "       train_regret_winner_12[slice34],\n",
    "       train_regret_winner_13[slice34],\n",
    "       train_regret_winner_14[slice34],\n",
    "       train_regret_winner_15[slice34],\n",
    "       train_regret_winner_16[slice34],\n",
    "       train_regret_winner_17[slice34],\n",
    "       train_regret_winner_18[slice34],\n",
    "       train_regret_winner_19[slice34],\n",
    "       train_regret_winner_20[slice34]]\n",
    "\n",
    "loser34_results = pd.DataFrame(loser34).sort_values(by=[0], ascending=False)\n",
    "winner34_results = pd.DataFrame(winner34).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser34 = np.asarray(loser34_results[4:5][0])[0]\n",
    "median_loser34 = np.asarray(loser34_results[9:10][0])[0]\n",
    "upper_loser34 = np.asarray(loser34_results[14:15][0])[0]\n",
    "\n",
    "lower_winner34 = np.asarray(winner34_results[4:5][0])[0]\n",
    "median_winner34 = np.asarray(winner34_results[9:10][0])[0]\n",
    "upper_winner34 = np.asarray(winner34_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration44 :\n",
    "\n",
    "slice44 = 43\n",
    "\n",
    "loser44 = [train_regret_loser_1[slice44],\n",
    "       train_regret_loser_2[slice44],\n",
    "       train_regret_loser_3[slice44],\n",
    "       train_regret_loser_4[slice44],\n",
    "       train_regret_loser_5[slice44],\n",
    "       train_regret_loser_6[slice44],\n",
    "       train_regret_loser_7[slice44],\n",
    "       train_regret_loser_8[slice44],\n",
    "       train_regret_loser_9[slice44],\n",
    "       train_regret_loser_10[slice44],\n",
    "       train_regret_loser_11[slice44],\n",
    "       train_regret_loser_12[slice44],\n",
    "       train_regret_loser_13[slice44],\n",
    "       train_regret_loser_14[slice44],\n",
    "       train_regret_loser_15[slice44],\n",
    "       train_regret_loser_16[slice44],\n",
    "       train_regret_loser_17[slice44],\n",
    "       train_regret_loser_18[slice44],\n",
    "       train_regret_loser_19[slice44],\n",
    "       train_regret_loser_20[slice44]]\n",
    "\n",
    "winner44 = [train_regret_winner_1[slice44],\n",
    "       train_regret_winner_2[slice44],\n",
    "       train_regret_winner_3[slice44],\n",
    "       train_regret_winner_4[slice44],\n",
    "       train_regret_winner_5[slice44],\n",
    "       train_regret_winner_6[slice44],\n",
    "       train_regret_winner_7[slice44],\n",
    "       train_regret_winner_8[slice44],\n",
    "       train_regret_winner_9[slice44],\n",
    "       train_regret_winner_10[slice44],\n",
    "       train_regret_winner_11[slice44],\n",
    "       train_regret_winner_12[slice44],\n",
    "       train_regret_winner_13[slice44],\n",
    "       train_regret_winner_14[slice44],\n",
    "       train_regret_winner_15[slice44],\n",
    "       train_regret_winner_16[slice44],\n",
    "       train_regret_winner_17[slice44],\n",
    "       train_regret_winner_18[slice44],\n",
    "       train_regret_winner_19[slice44],\n",
    "       train_regret_winner_20[slice44]]\n",
    "\n",
    "loser44_results = pd.DataFrame(loser44).sort_values(by=[0], ascending=False)\n",
    "winner44_results = pd.DataFrame(winner44).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser44 = np.asarray(loser44_results[4:5][0])[0]\n",
    "median_loser44 = np.asarray(loser44_results[9:10][0])[0]\n",
    "upper_loser44 = np.asarray(loser44_results[14:15][0])[0]\n",
    "\n",
    "lower_winner44 = np.asarray(winner44_results[4:5][0])[0]\n",
    "median_winner44 = np.asarray(winner44_results[9:10][0])[0]\n",
    "upper_winner44 = np.asarray(winner44_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration54 :\n",
    "\n",
    "slice54 = 53\n",
    "\n",
    "loser54 = [train_regret_loser_1[slice54],\n",
    "       train_regret_loser_2[slice54],\n",
    "       train_regret_loser_3[slice54],\n",
    "       train_regret_loser_4[slice54],\n",
    "       train_regret_loser_5[slice54],\n",
    "       train_regret_loser_6[slice54],\n",
    "       train_regret_loser_7[slice54],\n",
    "       train_regret_loser_8[slice54],\n",
    "       train_regret_loser_9[slice54],\n",
    "       train_regret_loser_10[slice54],\n",
    "       train_regret_loser_11[slice54],\n",
    "       train_regret_loser_12[slice54],\n",
    "       train_regret_loser_13[slice54],\n",
    "       train_regret_loser_14[slice54],\n",
    "       train_regret_loser_15[slice54],\n",
    "       train_regret_loser_16[slice54],\n",
    "       train_regret_loser_17[slice54],\n",
    "       train_regret_loser_18[slice54],\n",
    "       train_regret_loser_19[slice54],\n",
    "       train_regret_loser_20[slice54]]\n",
    "\n",
    "winner54 = [train_regret_winner_1[slice54],\n",
    "       train_regret_winner_2[slice54],\n",
    "       train_regret_winner_3[slice54],\n",
    "       train_regret_winner_4[slice54],\n",
    "       train_regret_winner_5[slice54],\n",
    "       train_regret_winner_6[slice54],\n",
    "       train_regret_winner_7[slice54],\n",
    "       train_regret_winner_8[slice54],\n",
    "       train_regret_winner_9[slice54],\n",
    "       train_regret_winner_10[slice54],\n",
    "       train_regret_winner_11[slice54],\n",
    "       train_regret_winner_12[slice54],\n",
    "       train_regret_winner_13[slice54],\n",
    "       train_regret_winner_14[slice54],\n",
    "       train_regret_winner_15[slice54],\n",
    "       train_regret_winner_16[slice54],\n",
    "       train_regret_winner_17[slice54],\n",
    "       train_regret_winner_18[slice54],\n",
    "       train_regret_winner_19[slice54],\n",
    "       train_regret_winner_20[slice54]]\n",
    "\n",
    "loser54_results = pd.DataFrame(loser54).sort_values(by=[0], ascending=False)\n",
    "winner54_results = pd.DataFrame(winner54).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser54 = np.asarray(loser54_results[4:5][0])[0]\n",
    "median_loser54 = np.asarray(loser54_results[9:10][0])[0]\n",
    "upper_loser54 = np.asarray(loser54_results[14:15][0])[0]\n",
    "\n",
    "lower_winner54 = np.asarray(winner54_results[4:5][0])[0]\n",
    "median_winner54 = np.asarray(winner54_results[9:10][0])[0]\n",
    "upper_winner54 = np.asarray(winner54_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration64 :\n",
    "\n",
    "slice64 = 63\n",
    "\n",
    "loser64 = [train_regret_loser_1[slice64],\n",
    "       train_regret_loser_2[slice64],\n",
    "       train_regret_loser_3[slice64],\n",
    "       train_regret_loser_4[slice64],\n",
    "       train_regret_loser_5[slice64],\n",
    "       train_regret_loser_6[slice64],\n",
    "       train_regret_loser_7[slice64],\n",
    "       train_regret_loser_8[slice64],\n",
    "       train_regret_loser_9[slice64],\n",
    "       train_regret_loser_10[slice64],\n",
    "       train_regret_loser_11[slice64],\n",
    "       train_regret_loser_12[slice64],\n",
    "       train_regret_loser_13[slice64],\n",
    "       train_regret_loser_14[slice64],\n",
    "       train_regret_loser_15[slice64],\n",
    "       train_regret_loser_16[slice64],\n",
    "       train_regret_loser_17[slice64],\n",
    "       train_regret_loser_18[slice64],\n",
    "       train_regret_loser_19[slice64],\n",
    "       train_regret_loser_20[slice64]]\n",
    "\n",
    "winner64 = [train_regret_winner_1[slice64],\n",
    "       train_regret_winner_2[slice64],\n",
    "       train_regret_winner_3[slice64],\n",
    "       train_regret_winner_4[slice64],\n",
    "       train_regret_winner_5[slice64],\n",
    "       train_regret_winner_6[slice64],\n",
    "       train_regret_winner_7[slice64],\n",
    "       train_regret_winner_8[slice64],\n",
    "       train_regret_winner_9[slice64],\n",
    "       train_regret_winner_10[slice64],\n",
    "       train_regret_winner_11[slice64],\n",
    "       train_regret_winner_12[slice64],\n",
    "       train_regret_winner_13[slice64],\n",
    "       train_regret_winner_14[slice64],\n",
    "       train_regret_winner_15[slice64],\n",
    "       train_regret_winner_16[slice64],\n",
    "       train_regret_winner_17[slice64],\n",
    "       train_regret_winner_18[slice64],\n",
    "       train_regret_winner_19[slice64],\n",
    "       train_regret_winner_20[slice64]]\n",
    "\n",
    "loser64_results = pd.DataFrame(loser64).sort_values(by=[0], ascending=False)\n",
    "winner64_results = pd.DataFrame(winner64).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser64 = np.asarray(loser64_results[4:5][0])[0]\n",
    "median_loser64 = np.asarray(loser64_results[9:10][0])[0]\n",
    "upper_loser64 = np.asarray(loser64_results[14:15][0])[0]\n",
    "\n",
    "lower_winner64 = np.asarray(winner64_results[4:5][0])[0]\n",
    "median_winner64 = np.asarray(winner64_results[9:10][0])[0]\n",
    "upper_winner64 = np.asarray(winner64_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration74 :\n",
    "\n",
    "slice74 = 73\n",
    "\n",
    "loser74 = [train_regret_loser_1[slice74],\n",
    "       train_regret_loser_2[slice74],\n",
    "       train_regret_loser_3[slice74],\n",
    "       train_regret_loser_4[slice74],\n",
    "       train_regret_loser_5[slice74],\n",
    "       train_regret_loser_6[slice74],\n",
    "       train_regret_loser_7[slice74],\n",
    "       train_regret_loser_8[slice74],\n",
    "       train_regret_loser_9[slice74],\n",
    "       train_regret_loser_10[slice74],\n",
    "       train_regret_loser_11[slice74],\n",
    "       train_regret_loser_12[slice74],\n",
    "       train_regret_loser_13[slice74],\n",
    "       train_regret_loser_14[slice74],\n",
    "       train_regret_loser_15[slice74],\n",
    "       train_regret_loser_16[slice74],\n",
    "       train_regret_loser_17[slice74],\n",
    "       train_regret_loser_18[slice74],\n",
    "       train_regret_loser_19[slice74],\n",
    "       train_regret_loser_20[slice74]]\n",
    "\n",
    "winner74 = [train_regret_winner_1[slice74],\n",
    "       train_regret_winner_2[slice74],\n",
    "       train_regret_winner_3[slice74],\n",
    "       train_regret_winner_4[slice74],\n",
    "       train_regret_winner_5[slice74],\n",
    "       train_regret_winner_6[slice74],\n",
    "       train_regret_winner_7[slice74],\n",
    "       train_regret_winner_8[slice74],\n",
    "       train_regret_winner_9[slice74],\n",
    "       train_regret_winner_10[slice74],\n",
    "       train_regret_winner_11[slice74],\n",
    "       train_regret_winner_12[slice74],\n",
    "       train_regret_winner_13[slice74],\n",
    "       train_regret_winner_14[slice74],\n",
    "       train_regret_winner_15[slice74],\n",
    "       train_regret_winner_16[slice74],\n",
    "       train_regret_winner_17[slice74],\n",
    "       train_regret_winner_18[slice74],\n",
    "       train_regret_winner_19[slice74],\n",
    "       train_regret_winner_20[slice74]]\n",
    "\n",
    "loser74_results = pd.DataFrame(loser74).sort_values(by=[0], ascending=False)\n",
    "winner74_results = pd.DataFrame(winner74).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser74 = np.asarray(loser74_results[4:5][0])[0]\n",
    "median_loser74 = np.asarray(loser74_results[9:10][0])[0]\n",
    "upper_loser74 = np.asarray(loser74_results[14:15][0])[0]\n",
    "\n",
    "lower_winner74 = np.asarray(winner74_results[4:5][0])[0]\n",
    "median_winner74 = np.asarray(winner74_results[9:10][0])[0]\n",
    "upper_winner74 = np.asarray(winner74_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration84 :\n",
    "\n",
    "slice84 = 83\n",
    "\n",
    "loser84 = [train_regret_loser_1[slice84],\n",
    "       train_regret_loser_2[slice84],\n",
    "       train_regret_loser_3[slice84],\n",
    "       train_regret_loser_4[slice84],\n",
    "       train_regret_loser_5[slice84],\n",
    "       train_regret_loser_6[slice84],\n",
    "       train_regret_loser_7[slice84],\n",
    "       train_regret_loser_8[slice84],\n",
    "       train_regret_loser_9[slice84],\n",
    "       train_regret_loser_10[slice84],\n",
    "       train_regret_loser_11[slice84],\n",
    "       train_regret_loser_12[slice84],\n",
    "       train_regret_loser_13[slice84],\n",
    "       train_regret_loser_14[slice84],\n",
    "       train_regret_loser_15[slice84],\n",
    "       train_regret_loser_16[slice84],\n",
    "       train_regret_loser_17[slice84],\n",
    "       train_regret_loser_18[slice84],\n",
    "       train_regret_loser_19[slice84],\n",
    "       train_regret_loser_20[slice84]]\n",
    "\n",
    "winner84 = [train_regret_winner_1[slice84],\n",
    "       train_regret_winner_2[slice84],\n",
    "       train_regret_winner_3[slice84],\n",
    "       train_regret_winner_4[slice84],\n",
    "       train_regret_winner_5[slice84],\n",
    "       train_regret_winner_6[slice84],\n",
    "       train_regret_winner_7[slice84],\n",
    "       train_regret_winner_8[slice84],\n",
    "       train_regret_winner_9[slice84],\n",
    "       train_regret_winner_10[slice84],\n",
    "       train_regret_winner_11[slice84],\n",
    "       train_regret_winner_12[slice84],\n",
    "       train_regret_winner_13[slice84],\n",
    "       train_regret_winner_14[slice84],\n",
    "       train_regret_winner_15[slice84],\n",
    "       train_regret_winner_16[slice84],\n",
    "       train_regret_winner_17[slice84],\n",
    "       train_regret_winner_18[slice84],\n",
    "       train_regret_winner_19[slice84],\n",
    "       train_regret_winner_20[slice84]]\n",
    "\n",
    "loser84_results = pd.DataFrame(loser84).sort_values(by=[0], ascending=False)\n",
    "winner84_results = pd.DataFrame(winner84).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser84 = np.asarray(loser84_results[4:5][0])[0]\n",
    "median_loser84 = np.asarray(loser84_results[9:10][0])[0]\n",
    "upper_loser84 = np.asarray(loser84_results[14:15][0])[0]\n",
    "\n",
    "lower_winner84 = np.asarray(winner84_results[4:5][0])[0]\n",
    "median_winner84 = np.asarray(winner84_results[9:10][0])[0]\n",
    "upper_winner84 = np.asarray(winner84_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration94 :\n",
    "\n",
    "slice94 = 93\n",
    "\n",
    "loser94 = [train_regret_loser_1[slice94],\n",
    "       train_regret_loser_2[slice94],\n",
    "       train_regret_loser_3[slice94],\n",
    "       train_regret_loser_4[slice94],\n",
    "       train_regret_loser_5[slice94],\n",
    "       train_regret_loser_6[slice94],\n",
    "       train_regret_loser_7[slice94],\n",
    "       train_regret_loser_8[slice94],\n",
    "       train_regret_loser_9[slice94],\n",
    "       train_regret_loser_10[slice94],\n",
    "       train_regret_loser_11[slice94],\n",
    "       train_regret_loser_12[slice94],\n",
    "       train_regret_loser_13[slice94],\n",
    "       train_regret_loser_14[slice94],\n",
    "       train_regret_loser_15[slice94],\n",
    "       train_regret_loser_16[slice94],\n",
    "       train_regret_loser_17[slice94],\n",
    "       train_regret_loser_18[slice94],\n",
    "       train_regret_loser_19[slice94],\n",
    "       train_regret_loser_20[slice94]]\n",
    "\n",
    "winner94 = [train_regret_winner_1[slice94],\n",
    "       train_regret_winner_2[slice94],\n",
    "       train_regret_winner_3[slice94],\n",
    "       train_regret_winner_4[slice94],\n",
    "       train_regret_winner_5[slice94],\n",
    "       train_regret_winner_6[slice94],\n",
    "       train_regret_winner_7[slice94],\n",
    "       train_regret_winner_8[slice94],\n",
    "       train_regret_winner_9[slice94],\n",
    "       train_regret_winner_10[slice94],\n",
    "       train_regret_winner_11[slice94],\n",
    "       train_regret_winner_12[slice94],\n",
    "       train_regret_winner_13[slice94],\n",
    "       train_regret_winner_14[slice94],\n",
    "       train_regret_winner_15[slice94],\n",
    "       train_regret_winner_16[slice94],\n",
    "       train_regret_winner_17[slice94],\n",
    "       train_regret_winner_18[slice94],\n",
    "       train_regret_winner_19[slice94],\n",
    "       train_regret_winner_20[slice94]]\n",
    "\n",
    "loser94_results = pd.DataFrame(loser94).sort_values(by=[0], ascending=False)\n",
    "winner94_results = pd.DataFrame(winner94).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser94 = np.asarray(loser94_results[4:5][0])[0]\n",
    "median_loser94 = np.asarray(loser94_results[9:10][0])[0]\n",
    "upper_loser94 = np.asarray(loser94_results[14:15][0])[0]\n",
    "\n",
    "lower_winner94 = np.asarray(winner94_results[4:5][0])[0]\n",
    "median_winner94 = np.asarray(winner94_results[9:10][0])[0]\n",
    "upper_winner94 = np.asarray(winner94_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration5 :\n",
    "\n",
    "slice5 = 4\n",
    "\n",
    "loser5 = [train_regret_loser_1[slice5],\n",
    "       train_regret_loser_2[slice5],\n",
    "       train_regret_loser_3[slice5],\n",
    "       train_regret_loser_4[slice5],\n",
    "       train_regret_loser_5[slice5],\n",
    "       train_regret_loser_6[slice5],\n",
    "       train_regret_loser_7[slice5],\n",
    "       train_regret_loser_8[slice5],\n",
    "       train_regret_loser_9[slice5],\n",
    "       train_regret_loser_10[slice5],\n",
    "       train_regret_loser_11[slice5],\n",
    "       train_regret_loser_12[slice5],\n",
    "       train_regret_loser_13[slice5],\n",
    "       train_regret_loser_14[slice5],\n",
    "       train_regret_loser_15[slice5],\n",
    "       train_regret_loser_16[slice5],\n",
    "       train_regret_loser_17[slice5],\n",
    "       train_regret_loser_18[slice5],\n",
    "       train_regret_loser_19[slice5],\n",
    "       train_regret_loser_20[slice5]]\n",
    "\n",
    "winner5 = [train_regret_winner_1[slice5],\n",
    "       train_regret_winner_2[slice5],\n",
    "       train_regret_winner_3[slice5],\n",
    "       train_regret_winner_4[slice5],\n",
    "       train_regret_winner_5[slice5],\n",
    "       train_regret_winner_6[slice5],\n",
    "       train_regret_winner_7[slice5],\n",
    "       train_regret_winner_8[slice5],\n",
    "       train_regret_winner_9[slice5],\n",
    "       train_regret_winner_10[slice5],\n",
    "       train_regret_winner_11[slice5],\n",
    "       train_regret_winner_12[slice5],\n",
    "       train_regret_winner_13[slice5],\n",
    "       train_regret_winner_14[slice5],\n",
    "       train_regret_winner_15[slice5],\n",
    "       train_regret_winner_16[slice5],\n",
    "       train_regret_winner_17[slice5],\n",
    "       train_regret_winner_18[slice5],\n",
    "       train_regret_winner_19[slice5],\n",
    "       train_regret_winner_20[slice5]]\n",
    "\n",
    "loser5_results = pd.DataFrame(loser5).sort_values(by=[0], ascending=False)\n",
    "winner5_results = pd.DataFrame(winner5).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser5 = np.asarray(loser5_results[4:5][0])[0]\n",
    "median_loser5 = np.asarray(loser5_results[9:10][0])[0]\n",
    "upper_loser5 = np.asarray(loser5_results[14:15][0])[0]\n",
    "\n",
    "lower_winner5 = np.asarray(winner5_results[4:5][0])[0]\n",
    "median_winner5 = np.asarray(winner5_results[9:10][0])[0]\n",
    "upper_winner5 = np.asarray(winner5_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration15 :\n",
    "\n",
    "slice15 = 14\n",
    "\n",
    "loser15 = [train_regret_loser_1[slice15],\n",
    "       train_regret_loser_2[slice15],\n",
    "       train_regret_loser_3[slice15],\n",
    "       train_regret_loser_4[slice15],\n",
    "       train_regret_loser_5[slice15],\n",
    "       train_regret_loser_6[slice15],\n",
    "       train_regret_loser_7[slice15],\n",
    "       train_regret_loser_8[slice15],\n",
    "       train_regret_loser_9[slice15],\n",
    "       train_regret_loser_10[slice15],\n",
    "       train_regret_loser_11[slice15],\n",
    "       train_regret_loser_12[slice15],\n",
    "       train_regret_loser_13[slice15],\n",
    "       train_regret_loser_14[slice15],\n",
    "       train_regret_loser_15[slice15],\n",
    "       train_regret_loser_16[slice15],\n",
    "       train_regret_loser_17[slice15],\n",
    "       train_regret_loser_18[slice15],\n",
    "       train_regret_loser_19[slice15],\n",
    "       train_regret_loser_20[slice15]]\n",
    "\n",
    "winner15 = [train_regret_winner_1[slice15],\n",
    "       train_regret_winner_2[slice15],\n",
    "       train_regret_winner_3[slice15],\n",
    "       train_regret_winner_4[slice15],\n",
    "       train_regret_winner_5[slice15],\n",
    "       train_regret_winner_6[slice15],\n",
    "       train_regret_winner_7[slice15],\n",
    "       train_regret_winner_8[slice15],\n",
    "       train_regret_winner_9[slice15],\n",
    "       train_regret_winner_10[slice15],\n",
    "       train_regret_winner_11[slice15],\n",
    "       train_regret_winner_12[slice15],\n",
    "       train_regret_winner_13[slice15],\n",
    "       train_regret_winner_14[slice15],\n",
    "       train_regret_winner_15[slice15],\n",
    "       train_regret_winner_16[slice15],\n",
    "       train_regret_winner_17[slice15],\n",
    "       train_regret_winner_18[slice15],\n",
    "       train_regret_winner_19[slice15],\n",
    "       train_regret_winner_20[slice15]]\n",
    "\n",
    "loser15_results = pd.DataFrame(loser15).sort_values(by=[0], ascending=False)\n",
    "winner15_results = pd.DataFrame(winner15).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser15 = np.asarray(loser15_results[4:5][0])[0]\n",
    "median_loser15 = np.asarray(loser15_results[9:10][0])[0]\n",
    "upper_loser15 = np.asarray(loser15_results[14:15][0])[0]\n",
    "\n",
    "lower_winner15 = np.asarray(winner15_results[4:5][0])[0]\n",
    "median_winner15 = np.asarray(winner15_results[9:10][0])[0]\n",
    "upper_winner15 = np.asarray(winner15_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration25 :\n",
    "\n",
    "slice25 = 24\n",
    "\n",
    "loser25 = [train_regret_loser_1[slice25],\n",
    "       train_regret_loser_2[slice25],\n",
    "       train_regret_loser_3[slice25],\n",
    "       train_regret_loser_4[slice25],\n",
    "       train_regret_loser_5[slice25],\n",
    "       train_regret_loser_6[slice25],\n",
    "       train_regret_loser_7[slice25],\n",
    "       train_regret_loser_8[slice25],\n",
    "       train_regret_loser_9[slice25],\n",
    "       train_regret_loser_10[slice25],\n",
    "       train_regret_loser_11[slice25],\n",
    "       train_regret_loser_12[slice25],\n",
    "       train_regret_loser_13[slice25],\n",
    "       train_regret_loser_14[slice25],\n",
    "       train_regret_loser_15[slice25],\n",
    "       train_regret_loser_16[slice25],\n",
    "       train_regret_loser_17[slice25],\n",
    "       train_regret_loser_18[slice25],\n",
    "       train_regret_loser_19[slice25],\n",
    "       train_regret_loser_20[slice25]]\n",
    "\n",
    "winner25 = [train_regret_winner_1[slice25],\n",
    "       train_regret_winner_2[slice25],\n",
    "       train_regret_winner_3[slice25],\n",
    "       train_regret_winner_4[slice25],\n",
    "       train_regret_winner_5[slice25],\n",
    "       train_regret_winner_6[slice25],\n",
    "       train_regret_winner_7[slice25],\n",
    "       train_regret_winner_8[slice25],\n",
    "       train_regret_winner_9[slice25],\n",
    "       train_regret_winner_10[slice25],\n",
    "       train_regret_winner_11[slice25],\n",
    "       train_regret_winner_12[slice25],\n",
    "       train_regret_winner_13[slice25],\n",
    "       train_regret_winner_14[slice25],\n",
    "       train_regret_winner_15[slice25],\n",
    "       train_regret_winner_16[slice25],\n",
    "       train_regret_winner_17[slice25],\n",
    "       train_regret_winner_18[slice25],\n",
    "       train_regret_winner_19[slice25],\n",
    "       train_regret_winner_20[slice25]]\n",
    "\n",
    "loser25_results = pd.DataFrame(loser25).sort_values(by=[0], ascending=False)\n",
    "winner25_results = pd.DataFrame(winner25).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser25 = np.asarray(loser25_results[4:5][0])[0]\n",
    "median_loser25 = np.asarray(loser25_results[9:10][0])[0]\n",
    "upper_loser25 = np.asarray(loser25_results[14:15][0])[0]\n",
    "\n",
    "lower_winner25 = np.asarray(winner25_results[4:5][0])[0]\n",
    "median_winner25 = np.asarray(winner25_results[9:10][0])[0]\n",
    "upper_winner25= np.asarray(winner25_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration35 :\n",
    "\n",
    "slice35 = 34\n",
    "\n",
    "loser35 = [train_regret_loser_1[slice35],\n",
    "       train_regret_loser_2[slice35],\n",
    "       train_regret_loser_3[slice35],\n",
    "       train_regret_loser_4[slice35],\n",
    "       train_regret_loser_5[slice35],\n",
    "       train_regret_loser_6[slice35],\n",
    "       train_regret_loser_7[slice35],\n",
    "       train_regret_loser_8[slice35],\n",
    "       train_regret_loser_9[slice35],\n",
    "       train_regret_loser_10[slice35],\n",
    "       train_regret_loser_11[slice35],\n",
    "       train_regret_loser_12[slice35],\n",
    "       train_regret_loser_13[slice35],\n",
    "       train_regret_loser_14[slice35],\n",
    "       train_regret_loser_15[slice35],\n",
    "       train_regret_loser_16[slice35],\n",
    "       train_regret_loser_17[slice35],\n",
    "       train_regret_loser_18[slice35],\n",
    "       train_regret_loser_19[slice35],\n",
    "       train_regret_loser_20[slice35]]\n",
    "\n",
    "winner35 = [train_regret_winner_1[slice35],\n",
    "       train_regret_winner_2[slice35],\n",
    "       train_regret_winner_3[slice35],\n",
    "       train_regret_winner_4[slice35],\n",
    "       train_regret_winner_5[slice35],\n",
    "       train_regret_winner_6[slice35],\n",
    "       train_regret_winner_7[slice35],\n",
    "       train_regret_winner_8[slice35],\n",
    "       train_regret_winner_9[slice35],\n",
    "       train_regret_winner_10[slice35],\n",
    "       train_regret_winner_11[slice35],\n",
    "       train_regret_winner_12[slice35],\n",
    "       train_regret_winner_13[slice35],\n",
    "       train_regret_winner_14[slice35],\n",
    "       train_regret_winner_15[slice35],\n",
    "       train_regret_winner_16[slice35],\n",
    "       train_regret_winner_17[slice35],\n",
    "       train_regret_winner_18[slice35],\n",
    "       train_regret_winner_19[slice35],\n",
    "       train_regret_winner_20[slice35]]\n",
    "\n",
    "loser35_results = pd.DataFrame(loser35).sort_values(by=[0], ascending=False)\n",
    "winner35_results = pd.DataFrame(winner35).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser35 = np.asarray(loser35_results[4:5][0])[0]\n",
    "median_loser35 = np.asarray(loser35_results[9:10][0])[0]\n",
    "upper_loser35 = np.asarray(loser35_results[14:15][0])[0]\n",
    "\n",
    "lower_winner35 = np.asarray(winner35_results[4:5][0])[0]\n",
    "median_winner35 = np.asarray(winner35_results[9:10][0])[0]\n",
    "upper_winner35 = np.asarray(winner35_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration45 :\n",
    "\n",
    "slice45 = 44\n",
    "\n",
    "loser45 = [train_regret_loser_1[slice45],\n",
    "       train_regret_loser_2[slice45],\n",
    "       train_regret_loser_3[slice45],\n",
    "       train_regret_loser_4[slice45],\n",
    "       train_regret_loser_5[slice45],\n",
    "       train_regret_loser_6[slice45],\n",
    "       train_regret_loser_7[slice45],\n",
    "       train_regret_loser_8[slice45],\n",
    "       train_regret_loser_9[slice45],\n",
    "       train_regret_loser_10[slice45],\n",
    "       train_regret_loser_11[slice45],\n",
    "       train_regret_loser_12[slice45],\n",
    "       train_regret_loser_13[slice45],\n",
    "       train_regret_loser_14[slice45],\n",
    "       train_regret_loser_15[slice45],\n",
    "       train_regret_loser_16[slice45],\n",
    "       train_regret_loser_17[slice45],\n",
    "       train_regret_loser_18[slice45],\n",
    "       train_regret_loser_19[slice45],\n",
    "       train_regret_loser_20[slice45]]\n",
    "\n",
    "winner45 = [train_regret_winner_1[slice45],\n",
    "       train_regret_winner_2[slice45],\n",
    "       train_regret_winner_3[slice45],\n",
    "       train_regret_winner_4[slice45],\n",
    "       train_regret_winner_5[slice45],\n",
    "       train_regret_winner_6[slice45],\n",
    "       train_regret_winner_7[slice45],\n",
    "       train_regret_winner_8[slice45],\n",
    "       train_regret_winner_9[slice45],\n",
    "       train_regret_winner_10[slice45],\n",
    "       train_regret_winner_11[slice45],\n",
    "       train_regret_winner_12[slice45],\n",
    "       train_regret_winner_13[slice45],\n",
    "       train_regret_winner_14[slice45],\n",
    "       train_regret_winner_15[slice45],\n",
    "       train_regret_winner_16[slice45],\n",
    "       train_regret_winner_17[slice45],\n",
    "       train_regret_winner_18[slice45],\n",
    "       train_regret_winner_19[slice45],\n",
    "       train_regret_winner_20[slice45]]\n",
    "\n",
    "loser45_results = pd.DataFrame(loser45).sort_values(by=[0], ascending=False)\n",
    "winner45_results = pd.DataFrame(winner45).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser45 = np.asarray(loser45_results[4:5][0])[0]\n",
    "median_loser45 = np.asarray(loser45_results[9:10][0])[0]\n",
    "upper_loser45 = np.asarray(loser45_results[14:15][0])[0]\n",
    "\n",
    "lower_winner45 = np.asarray(winner45_results[4:5][0])[0]\n",
    "median_winner45 = np.asarray(winner45_results[9:10][0])[0]\n",
    "upper_winner45 = np.asarray(winner45_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration55 :\n",
    "\n",
    "slice55 = 54\n",
    "\n",
    "loser55 = [train_regret_loser_1[slice55],\n",
    "       train_regret_loser_2[slice55],\n",
    "       train_regret_loser_3[slice55],\n",
    "       train_regret_loser_4[slice55],\n",
    "       train_regret_loser_5[slice55],\n",
    "       train_regret_loser_6[slice55],\n",
    "       train_regret_loser_7[slice55],\n",
    "       train_regret_loser_8[slice55],\n",
    "       train_regret_loser_9[slice55],\n",
    "       train_regret_loser_10[slice55],\n",
    "       train_regret_loser_11[slice55],\n",
    "       train_regret_loser_12[slice55],\n",
    "       train_regret_loser_13[slice55],\n",
    "       train_regret_loser_14[slice55],\n",
    "       train_regret_loser_15[slice55],\n",
    "       train_regret_loser_16[slice55],\n",
    "       train_regret_loser_17[slice55],\n",
    "       train_regret_loser_18[slice55],\n",
    "       train_regret_loser_19[slice55],\n",
    "       train_regret_loser_20[slice55]]\n",
    "\n",
    "winner55 = [train_regret_winner_1[slice55],\n",
    "       train_regret_winner_2[slice55],\n",
    "       train_regret_winner_3[slice55],\n",
    "       train_regret_winner_4[slice55],\n",
    "       train_regret_winner_5[slice55],\n",
    "       train_regret_winner_6[slice55],\n",
    "       train_regret_winner_7[slice55],\n",
    "       train_regret_winner_8[slice55],\n",
    "       train_regret_winner_9[slice55],\n",
    "       train_regret_winner_10[slice55],\n",
    "       train_regret_winner_11[slice55],\n",
    "       train_regret_winner_12[slice55],\n",
    "       train_regret_winner_13[slice55],\n",
    "       train_regret_winner_14[slice55],\n",
    "       train_regret_winner_15[slice55],\n",
    "       train_regret_winner_16[slice55],\n",
    "       train_regret_winner_17[slice55],\n",
    "       train_regret_winner_18[slice55],\n",
    "       train_regret_winner_19[slice55],\n",
    "       train_regret_winner_20[slice55]]\n",
    "\n",
    "loser55_results = pd.DataFrame(loser55).sort_values(by=[0], ascending=False)\n",
    "winner55_results = pd.DataFrame(winner55).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser55 = np.asarray(loser55_results[4:5][0])[0]\n",
    "median_loser55 = np.asarray(loser55_results[9:10][0])[0]\n",
    "upper_loser55 = np.asarray(loser55_results[14:15][0])[0]\n",
    "\n",
    "lower_winner55 = np.asarray(winner55_results[4:5][0])[0]\n",
    "median_winner55 = np.asarray(winner55_results[9:10][0])[0]\n",
    "upper_winner55 = np.asarray(winner55_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration65 :\n",
    "\n",
    "slice65 = 64\n",
    "\n",
    "loser65 = [train_regret_loser_1[slice65],\n",
    "       train_regret_loser_2[slice65],\n",
    "       train_regret_loser_3[slice65],\n",
    "       train_regret_loser_4[slice65],\n",
    "       train_regret_loser_5[slice65],\n",
    "       train_regret_loser_6[slice65],\n",
    "       train_regret_loser_7[slice65],\n",
    "       train_regret_loser_8[slice65],\n",
    "       train_regret_loser_9[slice65],\n",
    "       train_regret_loser_10[slice65],\n",
    "       train_regret_loser_11[slice65],\n",
    "       train_regret_loser_12[slice65],\n",
    "       train_regret_loser_13[slice65],\n",
    "       train_regret_loser_14[slice65],\n",
    "       train_regret_loser_15[slice65],\n",
    "       train_regret_loser_16[slice65],\n",
    "       train_regret_loser_17[slice65],\n",
    "       train_regret_loser_18[slice65],\n",
    "       train_regret_loser_19[slice65],\n",
    "       train_regret_loser_20[slice65]]\n",
    "\n",
    "winner65 = [train_regret_winner_1[slice65],\n",
    "       train_regret_winner_2[slice65],\n",
    "       train_regret_winner_3[slice65],\n",
    "       train_regret_winner_4[slice65],\n",
    "       train_regret_winner_5[slice65],\n",
    "       train_regret_winner_6[slice65],\n",
    "       train_regret_winner_7[slice65],\n",
    "       train_regret_winner_8[slice65],\n",
    "       train_regret_winner_9[slice65],\n",
    "       train_regret_winner_10[slice65],\n",
    "       train_regret_winner_11[slice65],\n",
    "       train_regret_winner_12[slice65],\n",
    "       train_regret_winner_13[slice65],\n",
    "       train_regret_winner_14[slice65],\n",
    "       train_regret_winner_15[slice65],\n",
    "       train_regret_winner_16[slice65],\n",
    "       train_regret_winner_17[slice65],\n",
    "       train_regret_winner_18[slice65],\n",
    "       train_regret_winner_19[slice65],\n",
    "       train_regret_winner_20[slice65]]\n",
    "\n",
    "loser65_results = pd.DataFrame(loser65).sort_values(by=[0], ascending=False)\n",
    "winner65_results = pd.DataFrame(winner65).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser65 = np.asarray(loser65_results[4:5][0])[0]\n",
    "median_loser65 = np.asarray(loser65_results[9:10][0])[0]\n",
    "upper_loser65 = np.asarray(loser65_results[14:15][0])[0]\n",
    "\n",
    "lower_winner65 = np.asarray(winner65_results[4:5][0])[0]\n",
    "median_winner65 = np.asarray(winner65_results[9:10][0])[0]\n",
    "upper_winner65 = np.asarray(winner65_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration75 :\n",
    "\n",
    "slice75 = 74\n",
    "\n",
    "loser75 = [train_regret_loser_1[slice75],\n",
    "       train_regret_loser_2[slice75],\n",
    "       train_regret_loser_3[slice75],\n",
    "       train_regret_loser_4[slice75],\n",
    "       train_regret_loser_5[slice75],\n",
    "       train_regret_loser_6[slice75],\n",
    "       train_regret_loser_7[slice75],\n",
    "       train_regret_loser_8[slice75],\n",
    "       train_regret_loser_9[slice75],\n",
    "       train_regret_loser_10[slice75],\n",
    "       train_regret_loser_11[slice75],\n",
    "       train_regret_loser_12[slice75],\n",
    "       train_regret_loser_13[slice75],\n",
    "       train_regret_loser_14[slice75],\n",
    "       train_regret_loser_15[slice75],\n",
    "       train_regret_loser_16[slice75],\n",
    "       train_regret_loser_17[slice75],\n",
    "       train_regret_loser_18[slice75],\n",
    "       train_regret_loser_19[slice75],\n",
    "       train_regret_loser_20[slice75]]\n",
    "\n",
    "winner75 = [train_regret_winner_1[slice75],\n",
    "       train_regret_winner_2[slice75],\n",
    "       train_regret_winner_3[slice75],\n",
    "       train_regret_winner_4[slice75],\n",
    "       train_regret_winner_5[slice75],\n",
    "       train_regret_winner_6[slice75],\n",
    "       train_regret_winner_7[slice75],\n",
    "       train_regret_winner_8[slice75],\n",
    "       train_regret_winner_9[slice75],\n",
    "       train_regret_winner_10[slice75],\n",
    "       train_regret_winner_11[slice75],\n",
    "       train_regret_winner_12[slice75],\n",
    "       train_regret_winner_13[slice75],\n",
    "       train_regret_winner_14[slice75],\n",
    "       train_regret_winner_15[slice75],\n",
    "       train_regret_winner_16[slice75],\n",
    "       train_regret_winner_17[slice75],\n",
    "       train_regret_winner_18[slice75],\n",
    "       train_regret_winner_19[slice75],\n",
    "       train_regret_winner_20[slice75]]\n",
    "\n",
    "loser75_results = pd.DataFrame(loser75).sort_values(by=[0], ascending=False)\n",
    "winner75_results = pd.DataFrame(winner75).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser75 = np.asarray(loser75_results[4:5][0])[0]\n",
    "median_loser75 = np.asarray(loser75_results[9:10][0])[0]\n",
    "upper_loser75 = np.asarray(loser75_results[14:15][0])[0]\n",
    "\n",
    "lower_winner75 = np.asarray(winner75_results[4:5][0])[0]\n",
    "median_winner75 = np.asarray(winner75_results[9:10][0])[0]\n",
    "upper_winner75 = np.asarray(winner75_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration85 :\n",
    "\n",
    "slice85 = 84\n",
    "\n",
    "loser85 = [train_regret_loser_1[slice85],\n",
    "       train_regret_loser_2[slice85],\n",
    "       train_regret_loser_3[slice85],\n",
    "       train_regret_loser_4[slice85],\n",
    "       train_regret_loser_5[slice85],\n",
    "       train_regret_loser_6[slice85],\n",
    "       train_regret_loser_7[slice85],\n",
    "       train_regret_loser_8[slice85],\n",
    "       train_regret_loser_9[slice85],\n",
    "       train_regret_loser_10[slice85],\n",
    "       train_regret_loser_11[slice85],\n",
    "       train_regret_loser_12[slice85],\n",
    "       train_regret_loser_13[slice85],\n",
    "       train_regret_loser_14[slice85],\n",
    "       train_regret_loser_15[slice85],\n",
    "       train_regret_loser_16[slice85],\n",
    "       train_regret_loser_17[slice85],\n",
    "       train_regret_loser_18[slice85],\n",
    "       train_regret_loser_19[slice85],\n",
    "       train_regret_loser_20[slice85]]\n",
    "\n",
    "winner85 = [train_regret_winner_1[slice85],\n",
    "       train_regret_winner_2[slice85],\n",
    "       train_regret_winner_3[slice85],\n",
    "       train_regret_winner_4[slice85],\n",
    "       train_regret_winner_5[slice85],\n",
    "       train_regret_winner_6[slice85],\n",
    "       train_regret_winner_7[slice85],\n",
    "       train_regret_winner_8[slice85],\n",
    "       train_regret_winner_9[slice85],\n",
    "       train_regret_winner_10[slice85],\n",
    "       train_regret_winner_11[slice85],\n",
    "       train_regret_winner_12[slice85],\n",
    "       train_regret_winner_13[slice85],\n",
    "       train_regret_winner_14[slice85],\n",
    "       train_regret_winner_15[slice85],\n",
    "       train_regret_winner_16[slice85],\n",
    "       train_regret_winner_17[slice85],\n",
    "       train_regret_winner_18[slice85],\n",
    "       train_regret_winner_19[slice85],\n",
    "       train_regret_winner_20[slice85]]\n",
    "\n",
    "loser85_results = pd.DataFrame(loser85).sort_values(by=[0], ascending=False)\n",
    "winner85_results = pd.DataFrame(winner85).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser85 = np.asarray(loser85_results[4:5][0])[0]\n",
    "median_loser85 = np.asarray(loser85_results[9:10][0])[0]\n",
    "upper_loser85 = np.asarray(loser85_results[14:15][0])[0]\n",
    "\n",
    "lower_winner85 = np.asarray(winner85_results[4:5][0])[0]\n",
    "median_winner85 = np.asarray(winner85_results[9:10][0])[0]\n",
    "upper_winner85 = np.asarray(winner85_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration95 :\n",
    "\n",
    "slice95 = 94\n",
    "\n",
    "loser95 = [train_regret_loser_1[slice95],\n",
    "       train_regret_loser_2[slice95],\n",
    "       train_regret_loser_3[slice95],\n",
    "       train_regret_loser_4[slice95],\n",
    "       train_regret_loser_5[slice95],\n",
    "       train_regret_loser_6[slice95],\n",
    "       train_regret_loser_7[slice95],\n",
    "       train_regret_loser_8[slice95],\n",
    "       train_regret_loser_9[slice95],\n",
    "       train_regret_loser_10[slice95],\n",
    "       train_regret_loser_11[slice95],\n",
    "       train_regret_loser_12[slice95],\n",
    "       train_regret_loser_13[slice95],\n",
    "       train_regret_loser_14[slice95],\n",
    "       train_regret_loser_15[slice95],\n",
    "       train_regret_loser_16[slice95],\n",
    "       train_regret_loser_17[slice95],\n",
    "       train_regret_loser_18[slice95],\n",
    "       train_regret_loser_19[slice95],\n",
    "       train_regret_loser_20[slice95]]\n",
    "\n",
    "winner95 = [train_regret_winner_1[slice95],\n",
    "       train_regret_winner_2[slice95],\n",
    "       train_regret_winner_3[slice95],\n",
    "       train_regret_winner_4[slice95],\n",
    "       train_regret_winner_5[slice95],\n",
    "       train_regret_winner_6[slice95],\n",
    "       train_regret_winner_7[slice95],\n",
    "       train_regret_winner_8[slice95],\n",
    "       train_regret_winner_9[slice95],\n",
    "       train_regret_winner_10[slice95],\n",
    "       train_regret_winner_11[slice95],\n",
    "       train_regret_winner_12[slice95],\n",
    "       train_regret_winner_13[slice95],\n",
    "       train_regret_winner_14[slice95],\n",
    "       train_regret_winner_15[slice95],\n",
    "       train_regret_winner_16[slice95],\n",
    "       train_regret_winner_17[slice95],\n",
    "       train_regret_winner_18[slice95],\n",
    "       train_regret_winner_19[slice95],\n",
    "       train_regret_winner_20[slice95]]\n",
    "\n",
    "loser95_results = pd.DataFrame(loser95).sort_values(by=[0], ascending=False)\n",
    "winner95_results = pd.DataFrame(winner95).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser95 = np.asarray(loser95_results[4:5][0])[0]\n",
    "median_loser95 = np.asarray(loser95_results[9:10][0])[0]\n",
    "upper_loser95 = np.asarray(loser95_results[14:15][0])[0]\n",
    "\n",
    "lower_winner95 = np.asarray(winner95_results[4:5][0])[0]\n",
    "median_winner95 = np.asarray(winner95_results[9:10][0])[0]\n",
    "upper_winner95 = np.asarray(winner95_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration6 :\n",
    "\n",
    "slice6 = 5\n",
    "\n",
    "loser6 = [train_regret_loser_1[slice6],\n",
    "       train_regret_loser_2[slice6],\n",
    "       train_regret_loser_3[slice6],\n",
    "       train_regret_loser_4[slice6],\n",
    "       train_regret_loser_5[slice6],\n",
    "       train_regret_loser_6[slice6],\n",
    "       train_regret_loser_7[slice6],\n",
    "       train_regret_loser_8[slice6],\n",
    "       train_regret_loser_9[slice6],\n",
    "       train_regret_loser_10[slice6],\n",
    "       train_regret_loser_11[slice6],\n",
    "       train_regret_loser_12[slice6],\n",
    "       train_regret_loser_13[slice6],\n",
    "       train_regret_loser_14[slice6],\n",
    "       train_regret_loser_15[slice6],\n",
    "       train_regret_loser_16[slice6],\n",
    "       train_regret_loser_17[slice6],\n",
    "       train_regret_loser_18[slice6],\n",
    "       train_regret_loser_19[slice6],\n",
    "       train_regret_loser_20[slice6]]\n",
    "\n",
    "winner6 = [train_regret_winner_1[slice6],\n",
    "       train_regret_winner_2[slice6],\n",
    "       train_regret_winner_3[slice6],\n",
    "       train_regret_winner_4[slice6],\n",
    "       train_regret_winner_5[slice6],\n",
    "       train_regret_winner_6[slice6],\n",
    "       train_regret_winner_7[slice6],\n",
    "       train_regret_winner_8[slice6],\n",
    "       train_regret_winner_9[slice6],\n",
    "       train_regret_winner_10[slice6],\n",
    "       train_regret_winner_11[slice6],\n",
    "       train_regret_winner_12[slice6],\n",
    "       train_regret_winner_13[slice6],\n",
    "       train_regret_winner_14[slice6],\n",
    "       train_regret_winner_15[slice6],\n",
    "       train_regret_winner_16[slice6],\n",
    "       train_regret_winner_17[slice6],\n",
    "       train_regret_winner_18[slice6],\n",
    "       train_regret_winner_19[slice6],\n",
    "       train_regret_winner_20[slice6]]\n",
    "\n",
    "loser6_results = pd.DataFrame(loser6).sort_values(by=[0], ascending=False)\n",
    "winner6_results = pd.DataFrame(winner6).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser6 = np.asarray(loser6_results[4:5][0])[0]\n",
    "median_loser6 = np.asarray(loser6_results[9:10][0])[0]\n",
    "upper_loser6 = np.asarray(loser6_results[14:15][0])[0]\n",
    "\n",
    "lower_winner6 = np.asarray(winner6_results[4:5][0])[0]\n",
    "median_winner6 = np.asarray(winner6_results[9:10][0])[0]\n",
    "upper_winner6 = np.asarray(winner6_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration16 :\n",
    "\n",
    "slice16 = 15\n",
    "\n",
    "loser16 = [train_regret_loser_1[slice16],\n",
    "       train_regret_loser_2[slice16],\n",
    "       train_regret_loser_3[slice16],\n",
    "       train_regret_loser_4[slice16],\n",
    "       train_regret_loser_5[slice16],\n",
    "       train_regret_loser_6[slice16],\n",
    "       train_regret_loser_7[slice16],\n",
    "       train_regret_loser_8[slice16],\n",
    "       train_regret_loser_9[slice16],\n",
    "       train_regret_loser_10[slice16],\n",
    "       train_regret_loser_11[slice16],\n",
    "       train_regret_loser_12[slice16],\n",
    "       train_regret_loser_13[slice16],\n",
    "       train_regret_loser_14[slice16],\n",
    "       train_regret_loser_15[slice16],\n",
    "       train_regret_loser_16[slice16],\n",
    "       train_regret_loser_17[slice16],\n",
    "       train_regret_loser_18[slice16],\n",
    "       train_regret_loser_19[slice16],\n",
    "       train_regret_loser_20[slice16]]\n",
    "\n",
    "winner16 = [train_regret_winner_1[slice16],\n",
    "       train_regret_winner_2[slice16],\n",
    "       train_regret_winner_3[slice16],\n",
    "       train_regret_winner_4[slice16],\n",
    "       train_regret_winner_5[slice16],\n",
    "       train_regret_winner_6[slice16],\n",
    "       train_regret_winner_7[slice16],\n",
    "       train_regret_winner_8[slice16],\n",
    "       train_regret_winner_9[slice16],\n",
    "       train_regret_winner_10[slice16],\n",
    "       train_regret_winner_11[slice16],\n",
    "       train_regret_winner_12[slice16],\n",
    "       train_regret_winner_13[slice16],\n",
    "       train_regret_winner_14[slice16],\n",
    "       train_regret_winner_15[slice16],\n",
    "       train_regret_winner_16[slice16],\n",
    "       train_regret_winner_17[slice16],\n",
    "       train_regret_winner_18[slice16],\n",
    "       train_regret_winner_19[slice16],\n",
    "       train_regret_winner_20[slice16]]\n",
    "\n",
    "loser16_results = pd.DataFrame(loser16).sort_values(by=[0], ascending=False)\n",
    "winner16_results = pd.DataFrame(winner16).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser16 = np.asarray(loser16_results[4:5][0])[0]\n",
    "median_loser16 = np.asarray(loser16_results[9:10][0])[0]\n",
    "upper_loser16 = np.asarray(loser16_results[14:15][0])[0]\n",
    "\n",
    "lower_winner16 = np.asarray(winner16_results[4:5][0])[0]\n",
    "median_winner16 = np.asarray(winner16_results[9:10][0])[0]\n",
    "upper_winner16 = np.asarray(winner16_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration26 :\n",
    "\n",
    "slice26 = 25\n",
    "\n",
    "loser26 = [train_regret_loser_1[slice26],\n",
    "       train_regret_loser_2[slice26],\n",
    "       train_regret_loser_3[slice26],\n",
    "       train_regret_loser_4[slice26],\n",
    "       train_regret_loser_5[slice26],\n",
    "       train_regret_loser_6[slice26],\n",
    "       train_regret_loser_7[slice26],\n",
    "       train_regret_loser_8[slice26],\n",
    "       train_regret_loser_9[slice26],\n",
    "       train_regret_loser_10[slice26],\n",
    "       train_regret_loser_11[slice26],\n",
    "       train_regret_loser_12[slice26],\n",
    "       train_regret_loser_13[slice26],\n",
    "       train_regret_loser_14[slice26],\n",
    "       train_regret_loser_15[slice26],\n",
    "       train_regret_loser_16[slice26],\n",
    "       train_regret_loser_17[slice26],\n",
    "       train_regret_loser_18[slice26],\n",
    "       train_regret_loser_19[slice26],\n",
    "       train_regret_loser_20[slice26]]\n",
    "\n",
    "winner26 = [train_regret_winner_1[slice26],\n",
    "       train_regret_winner_2[slice26],\n",
    "       train_regret_winner_3[slice26],\n",
    "       train_regret_winner_4[slice26],\n",
    "       train_regret_winner_5[slice26],\n",
    "       train_regret_winner_6[slice26],\n",
    "       train_regret_winner_7[slice26],\n",
    "       train_regret_winner_8[slice26],\n",
    "       train_regret_winner_9[slice26],\n",
    "       train_regret_winner_10[slice26],\n",
    "       train_regret_winner_11[slice26],\n",
    "       train_regret_winner_12[slice26],\n",
    "       train_regret_winner_13[slice26],\n",
    "       train_regret_winner_14[slice26],\n",
    "       train_regret_winner_15[slice26],\n",
    "       train_regret_winner_16[slice26],\n",
    "       train_regret_winner_17[slice26],\n",
    "       train_regret_winner_18[slice26],\n",
    "       train_regret_winner_19[slice26],\n",
    "       train_regret_winner_20[slice26]]\n",
    "\n",
    "loser26_results = pd.DataFrame(loser26).sort_values(by=[0], ascending=False)\n",
    "winner26_results = pd.DataFrame(winner26).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser26 = np.asarray(loser26_results[4:5][0])[0]\n",
    "median_loser26 = np.asarray(loser26_results[9:10][0])[0]\n",
    "upper_loser26 = np.asarray(loser26_results[14:15][0])[0]\n",
    "\n",
    "lower_winner26 = np.asarray(winner26_results[4:5][0])[0]\n",
    "median_winner26 = np.asarray(winner26_results[9:10][0])[0]\n",
    "upper_winner26 = np.asarray(winner26_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration36 :\n",
    "\n",
    "slice36 = 35\n",
    "\n",
    "loser36 = [train_regret_loser_1[slice36],\n",
    "       train_regret_loser_2[slice36],\n",
    "       train_regret_loser_3[slice36],\n",
    "       train_regret_loser_4[slice36],\n",
    "       train_regret_loser_5[slice36],\n",
    "       train_regret_loser_6[slice36],\n",
    "       train_regret_loser_7[slice36],\n",
    "       train_regret_loser_8[slice36],\n",
    "       train_regret_loser_9[slice36],\n",
    "       train_regret_loser_10[slice36],\n",
    "       train_regret_loser_11[slice36],\n",
    "       train_regret_loser_12[slice36],\n",
    "       train_regret_loser_13[slice36],\n",
    "       train_regret_loser_14[slice36],\n",
    "       train_regret_loser_15[slice36],\n",
    "       train_regret_loser_16[slice36],\n",
    "       train_regret_loser_17[slice36],\n",
    "       train_regret_loser_18[slice36],\n",
    "       train_regret_loser_19[slice36],\n",
    "       train_regret_loser_20[slice36]]\n",
    "\n",
    "winner36 = [train_regret_winner_1[slice36],\n",
    "       train_regret_winner_2[slice36],\n",
    "       train_regret_winner_3[slice36],\n",
    "       train_regret_winner_4[slice36],\n",
    "       train_regret_winner_5[slice36],\n",
    "       train_regret_winner_6[slice36],\n",
    "       train_regret_winner_7[slice36],\n",
    "       train_regret_winner_8[slice36],\n",
    "       train_regret_winner_9[slice36],\n",
    "       train_regret_winner_10[slice36],\n",
    "       train_regret_winner_11[slice36],\n",
    "       train_regret_winner_12[slice36],\n",
    "       train_regret_winner_13[slice36],\n",
    "       train_regret_winner_14[slice36],\n",
    "       train_regret_winner_15[slice36],\n",
    "       train_regret_winner_16[slice36],\n",
    "       train_regret_winner_17[slice36],\n",
    "       train_regret_winner_18[slice36],\n",
    "       train_regret_winner_19[slice36],\n",
    "       train_regret_winner_20[slice36]]\n",
    "\n",
    "loser36_results = pd.DataFrame(loser36).sort_values(by=[0], ascending=False)\n",
    "winner36_results = pd.DataFrame(winner36).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser36 = np.asarray(loser36_results[4:5][0])[0]\n",
    "median_loser36 = np.asarray(loser36_results[9:10][0])[0]\n",
    "upper_loser36 = np.asarray(loser36_results[14:15][0])[0]\n",
    "\n",
    "lower_winner36 = np.asarray(winner36_results[4:5][0])[0]\n",
    "median_winner36 = np.asarray(winner36_results[9:10][0])[0]\n",
    "upper_winner36 = np.asarray(winner36_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration46 :\n",
    "\n",
    "slice46 = 45\n",
    "\n",
    "loser46 = [train_regret_loser_1[slice46],\n",
    "       train_regret_loser_2[slice46],\n",
    "       train_regret_loser_3[slice46],\n",
    "       train_regret_loser_4[slice46],\n",
    "       train_regret_loser_5[slice46],\n",
    "       train_regret_loser_6[slice46],\n",
    "       train_regret_loser_7[slice46],\n",
    "       train_regret_loser_8[slice46],\n",
    "       train_regret_loser_9[slice46],\n",
    "       train_regret_loser_10[slice46],\n",
    "       train_regret_loser_11[slice46],\n",
    "       train_regret_loser_12[slice46],\n",
    "       train_regret_loser_13[slice46],\n",
    "       train_regret_loser_14[slice46],\n",
    "       train_regret_loser_15[slice46],\n",
    "       train_regret_loser_16[slice46],\n",
    "       train_regret_loser_17[slice46],\n",
    "       train_regret_loser_18[slice46],\n",
    "       train_regret_loser_19[slice46],\n",
    "       train_regret_loser_20[slice46]]\n",
    "\n",
    "winner46 = [train_regret_winner_1[slice46],\n",
    "       train_regret_winner_2[slice46],\n",
    "       train_regret_winner_3[slice46],\n",
    "       train_regret_winner_4[slice46],\n",
    "       train_regret_winner_5[slice46],\n",
    "       train_regret_winner_6[slice46],\n",
    "       train_regret_winner_7[slice46],\n",
    "       train_regret_winner_8[slice46],\n",
    "       train_regret_winner_9[slice46],\n",
    "       train_regret_winner_10[slice46],\n",
    "       train_regret_winner_11[slice46],\n",
    "       train_regret_winner_12[slice46],\n",
    "       train_regret_winner_13[slice46],\n",
    "       train_regret_winner_14[slice46],\n",
    "       train_regret_winner_15[slice46],\n",
    "       train_regret_winner_16[slice46],\n",
    "       train_regret_winner_17[slice46],\n",
    "       train_regret_winner_18[slice46],\n",
    "       train_regret_winner_19[slice46],\n",
    "       train_regret_winner_20[slice46]]\n",
    "\n",
    "loser46_results = pd.DataFrame(loser46).sort_values(by=[0], ascending=False)\n",
    "winner46_results = pd.DataFrame(winner46).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser46 = np.asarray(loser46_results[4:5][0])[0]\n",
    "median_loser46 = np.asarray(loser46_results[9:10][0])[0]\n",
    "upper_loser46 = np.asarray(loser46_results[14:15][0])[0]\n",
    "\n",
    "lower_winner46 = np.asarray(winner46_results[4:5][0])[0]\n",
    "median_winner46 = np.asarray(winner46_results[9:10][0])[0]\n",
    "upper_winner46 = np.asarray(winner46_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration56 :\n",
    "\n",
    "slice56 = 55\n",
    "\n",
    "loser56 = [train_regret_loser_1[slice56],\n",
    "       train_regret_loser_2[slice56],\n",
    "       train_regret_loser_3[slice56],\n",
    "       train_regret_loser_4[slice56],\n",
    "       train_regret_loser_5[slice56],\n",
    "       train_regret_loser_6[slice56],\n",
    "       train_regret_loser_7[slice56],\n",
    "       train_regret_loser_8[slice56],\n",
    "       train_regret_loser_9[slice56],\n",
    "       train_regret_loser_10[slice56],\n",
    "       train_regret_loser_11[slice56],\n",
    "       train_regret_loser_12[slice56],\n",
    "       train_regret_loser_13[slice56],\n",
    "       train_regret_loser_14[slice56],\n",
    "       train_regret_loser_15[slice56],\n",
    "       train_regret_loser_16[slice56],\n",
    "       train_regret_loser_17[slice56],\n",
    "       train_regret_loser_18[slice56],\n",
    "       train_regret_loser_19[slice56],\n",
    "       train_regret_loser_20[slice56]]\n",
    "\n",
    "winner56 = [train_regret_winner_1[slice56],\n",
    "       train_regret_winner_2[slice56],\n",
    "       train_regret_winner_3[slice56],\n",
    "       train_regret_winner_4[slice56],\n",
    "       train_regret_winner_5[slice56],\n",
    "       train_regret_winner_6[slice56],\n",
    "       train_regret_winner_7[slice56],\n",
    "       train_regret_winner_8[slice56],\n",
    "       train_regret_winner_9[slice56],\n",
    "       train_regret_winner_10[slice56],\n",
    "       train_regret_winner_11[slice56],\n",
    "       train_regret_winner_12[slice56],\n",
    "       train_regret_winner_13[slice56],\n",
    "       train_regret_winner_14[slice56],\n",
    "       train_regret_winner_15[slice56],\n",
    "       train_regret_winner_16[slice56],\n",
    "       train_regret_winner_17[slice56],\n",
    "       train_regret_winner_18[slice56],\n",
    "       train_regret_winner_19[slice56],\n",
    "       train_regret_winner_20[slice56]]\n",
    "\n",
    "loser56_results = pd.DataFrame(loser56).sort_values(by=[0], ascending=False)\n",
    "winner56_results = pd.DataFrame(winner56).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser56 = np.asarray(loser56_results[4:5][0])[0]\n",
    "median_loser56 = np.asarray(loser56_results[9:10][0])[0]\n",
    "upper_loser56 = np.asarray(loser56_results[14:15][0])[0]\n",
    "\n",
    "lower_winner56 = np.asarray(winner56_results[4:5][0])[0]\n",
    "median_winner56 = np.asarray(winner56_results[9:10][0])[0]\n",
    "upper_winner56 = np.asarray(winner56_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration66 :\n",
    "\n",
    "slice66 = 65\n",
    "\n",
    "loser66 = [train_regret_loser_1[slice66],\n",
    "       train_regret_loser_2[slice66],\n",
    "       train_regret_loser_3[slice66],\n",
    "       train_regret_loser_4[slice66],\n",
    "       train_regret_loser_5[slice66],\n",
    "       train_regret_loser_6[slice66],\n",
    "       train_regret_loser_7[slice66],\n",
    "       train_regret_loser_8[slice66],\n",
    "       train_regret_loser_9[slice66],\n",
    "       train_regret_loser_10[slice66],\n",
    "       train_regret_loser_11[slice66],\n",
    "       train_regret_loser_12[slice66],\n",
    "       train_regret_loser_13[slice66],\n",
    "       train_regret_loser_14[slice66],\n",
    "       train_regret_loser_15[slice66],\n",
    "       train_regret_loser_16[slice66],\n",
    "       train_regret_loser_17[slice66],\n",
    "       train_regret_loser_18[slice66],\n",
    "       train_regret_loser_19[slice66],\n",
    "       train_regret_loser_20[slice66]]\n",
    "\n",
    "winner66 = [train_regret_winner_1[slice66],\n",
    "       train_regret_winner_2[slice66],\n",
    "       train_regret_winner_3[slice66],\n",
    "       train_regret_winner_4[slice66],\n",
    "       train_regret_winner_5[slice66],\n",
    "       train_regret_winner_6[slice66],\n",
    "       train_regret_winner_7[slice66],\n",
    "       train_regret_winner_8[slice66],\n",
    "       train_regret_winner_9[slice66],\n",
    "       train_regret_winner_10[slice66],\n",
    "       train_regret_winner_11[slice66],\n",
    "       train_regret_winner_12[slice66],\n",
    "       train_regret_winner_13[slice66],\n",
    "       train_regret_winner_14[slice66],\n",
    "       train_regret_winner_15[slice66],\n",
    "       train_regret_winner_16[slice66],\n",
    "       train_regret_winner_17[slice66],\n",
    "       train_regret_winner_18[slice66],\n",
    "       train_regret_winner_19[slice66],\n",
    "       train_regret_winner_20[slice66]]\n",
    "\n",
    "loser66_results = pd.DataFrame(loser66).sort_values(by=[0], ascending=False)\n",
    "winner66_results = pd.DataFrame(winner66).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser66 = np.asarray(loser66_results[4:5][0])[0]\n",
    "median_loser66 = np.asarray(loser66_results[9:10][0])[0]\n",
    "upper_loser66 = np.asarray(loser66_results[14:15][0])[0]\n",
    "\n",
    "lower_winner66 = np.asarray(winner66_results[4:5][0])[0]\n",
    "median_winner66 = np.asarray(winner66_results[9:10][0])[0]\n",
    "upper_winner66 = np.asarray(winner66_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration76 :\n",
    "\n",
    "slice76 = 75\n",
    "\n",
    "loser76 = [train_regret_loser_1[slice76],\n",
    "       train_regret_loser_2[slice76],\n",
    "       train_regret_loser_3[slice76],\n",
    "       train_regret_loser_4[slice76],\n",
    "       train_regret_loser_5[slice76],\n",
    "       train_regret_loser_6[slice76],\n",
    "       train_regret_loser_7[slice76],\n",
    "       train_regret_loser_8[slice76],\n",
    "       train_regret_loser_9[slice76],\n",
    "       train_regret_loser_10[slice76],\n",
    "       train_regret_loser_11[slice76],\n",
    "       train_regret_loser_12[slice76],\n",
    "       train_regret_loser_13[slice76],\n",
    "       train_regret_loser_14[slice76],\n",
    "       train_regret_loser_15[slice76],\n",
    "       train_regret_loser_16[slice76],\n",
    "       train_regret_loser_17[slice76],\n",
    "       train_regret_loser_18[slice76],\n",
    "       train_regret_loser_19[slice76],\n",
    "       train_regret_loser_20[slice76]]\n",
    "\n",
    "winner76 = [train_regret_winner_1[slice76],\n",
    "       train_regret_winner_2[slice76],\n",
    "       train_regret_winner_3[slice76],\n",
    "       train_regret_winner_4[slice76],\n",
    "       train_regret_winner_5[slice76],\n",
    "       train_regret_winner_6[slice76],\n",
    "       train_regret_winner_7[slice76],\n",
    "       train_regret_winner_8[slice76],\n",
    "       train_regret_winner_9[slice76],\n",
    "       train_regret_winner_10[slice76],\n",
    "       train_regret_winner_11[slice76],\n",
    "       train_regret_winner_12[slice76],\n",
    "       train_regret_winner_13[slice76],\n",
    "       train_regret_winner_14[slice76],\n",
    "       train_regret_winner_15[slice76],\n",
    "       train_regret_winner_16[slice76],\n",
    "       train_regret_winner_17[slice76],\n",
    "       train_regret_winner_18[slice76],\n",
    "       train_regret_winner_19[slice76],\n",
    "       train_regret_winner_20[slice76]]\n",
    "\n",
    "loser76_results = pd.DataFrame(loser76).sort_values(by=[0], ascending=False)\n",
    "winner76_results = pd.DataFrame(winner76).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser76 = np.asarray(loser76_results[4:5][0])[0]\n",
    "median_loser76 = np.asarray(loser76_results[9:10][0])[0]\n",
    "upper_loser76 = np.asarray(loser76_results[14:15][0])[0]\n",
    "\n",
    "lower_winner76 = np.asarray(winner76_results[4:5][0])[0]\n",
    "median_winner76 = np.asarray(winner76_results[9:10][0])[0]\n",
    "upper_winner76 = np.asarray(winner76_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration86 :\n",
    "\n",
    "slice86 = 85\n",
    "\n",
    "loser86 = [train_regret_loser_1[slice86],\n",
    "       train_regret_loser_2[slice86],\n",
    "       train_regret_loser_3[slice86],\n",
    "       train_regret_loser_4[slice86],\n",
    "       train_regret_loser_5[slice86],\n",
    "       train_regret_loser_6[slice86],\n",
    "       train_regret_loser_7[slice86],\n",
    "       train_regret_loser_8[slice86],\n",
    "       train_regret_loser_9[slice86],\n",
    "       train_regret_loser_10[slice86],\n",
    "       train_regret_loser_11[slice86],\n",
    "       train_regret_loser_12[slice86],\n",
    "       train_regret_loser_13[slice86],\n",
    "       train_regret_loser_14[slice86],\n",
    "       train_regret_loser_15[slice86],\n",
    "       train_regret_loser_16[slice86],\n",
    "       train_regret_loser_17[slice86],\n",
    "       train_regret_loser_18[slice86],\n",
    "       train_regret_loser_19[slice86],\n",
    "       train_regret_loser_20[slice86]]\n",
    "\n",
    "winner86 = [train_regret_winner_1[slice86],\n",
    "       train_regret_winner_2[slice86],\n",
    "       train_regret_winner_3[slice86],\n",
    "       train_regret_winner_4[slice86],\n",
    "       train_regret_winner_5[slice86],\n",
    "       train_regret_winner_6[slice86],\n",
    "       train_regret_winner_7[slice86],\n",
    "       train_regret_winner_8[slice86],\n",
    "       train_regret_winner_9[slice86],\n",
    "       train_regret_winner_10[slice86],\n",
    "       train_regret_winner_11[slice86],\n",
    "       train_regret_winner_12[slice86],\n",
    "       train_regret_winner_13[slice86],\n",
    "       train_regret_winner_14[slice86],\n",
    "       train_regret_winner_15[slice86],\n",
    "       train_regret_winner_16[slice86],\n",
    "       train_regret_winner_17[slice86],\n",
    "       train_regret_winner_18[slice86],\n",
    "       train_regret_winner_19[slice86],\n",
    "       train_regret_winner_20[slice86]]\n",
    "\n",
    "loser86_results = pd.DataFrame(loser86).sort_values(by=[0], ascending=False)\n",
    "winner86_results = pd.DataFrame(winner86).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser86 = np.asarray(loser86_results[4:5][0])[0]\n",
    "median_loser86 = np.asarray(loser86_results[9:10][0])[0]\n",
    "upper_loser86 = np.asarray(loser86_results[14:15][0])[0]\n",
    "\n",
    "lower_winner86 = np.asarray(winner86_results[4:5][0])[0]\n",
    "median_winner86 = np.asarray(winner86_results[9:10][0])[0]\n",
    "upper_winner86 = np.asarray(winner86_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration96 :\n",
    "\n",
    "slice96 = 95\n",
    "\n",
    "loser96 = [train_regret_loser_1[slice96],\n",
    "       train_regret_loser_2[slice96],\n",
    "       train_regret_loser_3[slice96],\n",
    "       train_regret_loser_4[slice96],\n",
    "       train_regret_loser_5[slice96],\n",
    "       train_regret_loser_6[slice96],\n",
    "       train_regret_loser_7[slice96],\n",
    "       train_regret_loser_8[slice96],\n",
    "       train_regret_loser_9[slice96],\n",
    "       train_regret_loser_10[slice96],\n",
    "       train_regret_loser_11[slice96],\n",
    "       train_regret_loser_12[slice96],\n",
    "       train_regret_loser_13[slice96],\n",
    "       train_regret_loser_14[slice96],\n",
    "       train_regret_loser_15[slice96],\n",
    "       train_regret_loser_16[slice96],\n",
    "       train_regret_loser_17[slice96],\n",
    "       train_regret_loser_18[slice96],\n",
    "       train_regret_loser_19[slice96],\n",
    "       train_regret_loser_20[slice96]]\n",
    "\n",
    "winner96 = [train_regret_winner_1[slice96],\n",
    "       train_regret_winner_2[slice96],\n",
    "       train_regret_winner_3[slice96],\n",
    "       train_regret_winner_4[slice96],\n",
    "       train_regret_winner_5[slice96],\n",
    "       train_regret_winner_6[slice96],\n",
    "       train_regret_winner_7[slice96],\n",
    "       train_regret_winner_8[slice96],\n",
    "       train_regret_winner_9[slice96],\n",
    "       train_regret_winner_10[slice96],\n",
    "       train_regret_winner_11[slice96],\n",
    "       train_regret_winner_12[slice96],\n",
    "       train_regret_winner_13[slice96],\n",
    "       train_regret_winner_14[slice96],\n",
    "       train_regret_winner_15[slice96],\n",
    "       train_regret_winner_16[slice96],\n",
    "       train_regret_winner_17[slice96],\n",
    "       train_regret_winner_18[slice96],\n",
    "       train_regret_winner_19[slice96],\n",
    "       train_regret_winner_20[slice96]]\n",
    "\n",
    "loser96_results = pd.DataFrame(loser96).sort_values(by=[0], ascending=False)\n",
    "winner96_results = pd.DataFrame(winner96).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser96 = np.asarray(loser96_results[4:5][0])[0]\n",
    "median_loser96 = np.asarray(loser96_results[9:10][0])[0]\n",
    "upper_loser96 = np.asarray(loser96_results[14:15][0])[0]\n",
    "\n",
    "lower_winner96 = np.asarray(winner96_results[4:5][0])[0]\n",
    "median_winner96 = np.asarray(winner96_results[9:10][0])[0]\n",
    "upper_winner96 = np.asarray(winner96_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration7 :\n",
    "\n",
    "slice7 = 6\n",
    "\n",
    "loser7 = [train_regret_loser_1[slice7],\n",
    "       train_regret_loser_2[slice7],\n",
    "       train_regret_loser_3[slice7],\n",
    "       train_regret_loser_4[slice7],\n",
    "       train_regret_loser_5[slice7],\n",
    "       train_regret_loser_6[slice7],\n",
    "       train_regret_loser_7[slice7],\n",
    "       train_regret_loser_8[slice7],\n",
    "       train_regret_loser_9[slice7],\n",
    "       train_regret_loser_10[slice7],\n",
    "       train_regret_loser_11[slice7],\n",
    "       train_regret_loser_12[slice7],\n",
    "       train_regret_loser_13[slice7],\n",
    "       train_regret_loser_14[slice7],\n",
    "       train_regret_loser_15[slice7],\n",
    "       train_regret_loser_16[slice7],\n",
    "       train_regret_loser_17[slice7],\n",
    "       train_regret_loser_18[slice7],\n",
    "       train_regret_loser_19[slice7],\n",
    "       train_regret_loser_20[slice7]]\n",
    "\n",
    "winner7 = [train_regret_winner_1[slice7],\n",
    "       train_regret_winner_2[slice7],\n",
    "       train_regret_winner_3[slice7],\n",
    "       train_regret_winner_4[slice7],\n",
    "       train_regret_winner_5[slice7],\n",
    "       train_regret_winner_6[slice7],\n",
    "       train_regret_winner_7[slice7],\n",
    "       train_regret_winner_8[slice7],\n",
    "       train_regret_winner_9[slice7],\n",
    "       train_regret_winner_10[slice7],\n",
    "       train_regret_winner_11[slice7],\n",
    "       train_regret_winner_12[slice7],\n",
    "       train_regret_winner_13[slice7],\n",
    "       train_regret_winner_14[slice7],\n",
    "       train_regret_winner_15[slice7],\n",
    "       train_regret_winner_16[slice7],\n",
    "       train_regret_winner_17[slice7],\n",
    "       train_regret_winner_18[slice7],\n",
    "       train_regret_winner_19[slice7],\n",
    "       train_regret_winner_20[slice7]]\n",
    "\n",
    "loser7_results = pd.DataFrame(loser7).sort_values(by=[0], ascending=False)\n",
    "winner7_results = pd.DataFrame(winner7).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser7 = np.asarray(loser7_results[4:5][0])[0]\n",
    "median_loser7 = np.asarray(loser7_results[9:10][0])[0]\n",
    "upper_loser7 = np.asarray(loser7_results[14:15][0])[0]\n",
    "\n",
    "lower_winner7 = np.asarray(winner7_results[4:5][0])[0]\n",
    "median_winner7 = np.asarray(winner7_results[9:10][0])[0]\n",
    "upper_winner7 = np.asarray(winner7_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration17 :\n",
    "\n",
    "slice17 = 16\n",
    "\n",
    "loser17 = [train_regret_loser_1[slice17],\n",
    "       train_regret_loser_2[slice17],\n",
    "       train_regret_loser_3[slice17],\n",
    "       train_regret_loser_4[slice17],\n",
    "       train_regret_loser_5[slice17],\n",
    "       train_regret_loser_6[slice17],\n",
    "       train_regret_loser_7[slice17],\n",
    "       train_regret_loser_8[slice17],\n",
    "       train_regret_loser_9[slice17],\n",
    "       train_regret_loser_10[slice17],\n",
    "       train_regret_loser_11[slice17],\n",
    "       train_regret_loser_12[slice17],\n",
    "       train_regret_loser_13[slice17],\n",
    "       train_regret_loser_14[slice17],\n",
    "       train_regret_loser_15[slice17],\n",
    "       train_regret_loser_16[slice17],\n",
    "       train_regret_loser_17[slice17],\n",
    "       train_regret_loser_18[slice17],\n",
    "       train_regret_loser_19[slice17],\n",
    "       train_regret_loser_20[slice17]]\n",
    "\n",
    "winner17 = [train_regret_winner_1[slice17],\n",
    "       train_regret_winner_2[slice17],\n",
    "       train_regret_winner_3[slice17],\n",
    "       train_regret_winner_4[slice17],\n",
    "       train_regret_winner_5[slice17],\n",
    "       train_regret_winner_6[slice17],\n",
    "       train_regret_winner_7[slice17],\n",
    "       train_regret_winner_8[slice17],\n",
    "       train_regret_winner_9[slice17],\n",
    "       train_regret_winner_10[slice17],\n",
    "       train_regret_winner_11[slice17],\n",
    "       train_regret_winner_12[slice17],\n",
    "       train_regret_winner_13[slice17],\n",
    "       train_regret_winner_14[slice17],\n",
    "       train_regret_winner_15[slice17],\n",
    "       train_regret_winner_16[slice17],\n",
    "       train_regret_winner_17[slice17],\n",
    "       train_regret_winner_18[slice17],\n",
    "       train_regret_winner_19[slice17],\n",
    "       train_regret_winner_20[slice17]]\n",
    "\n",
    "loser17_results = pd.DataFrame(loser17).sort_values(by=[0], ascending=False)\n",
    "winner17_results = pd.DataFrame(winner17).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser17 = np.asarray(loser17_results[4:5][0])[0]\n",
    "median_loser17 = np.asarray(loser17_results[9:10][0])[0]\n",
    "upper_loser17 = np.asarray(loser17_results[14:15][0])[0]\n",
    "\n",
    "lower_winner17 = np.asarray(winner17_results[4:5][0])[0]\n",
    "median_winner17 = np.asarray(winner17_results[9:10][0])[0]\n",
    "upper_winner17 = np.asarray(winner17_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration27 :\n",
    "\n",
    "slice27 = 26\n",
    "\n",
    "loser27 = [train_regret_loser_1[slice27],\n",
    "       train_regret_loser_2[slice27],\n",
    "       train_regret_loser_3[slice27],\n",
    "       train_regret_loser_4[slice27],\n",
    "       train_regret_loser_5[slice27],\n",
    "       train_regret_loser_6[slice27],\n",
    "       train_regret_loser_7[slice27],\n",
    "       train_regret_loser_8[slice27],\n",
    "       train_regret_loser_9[slice27],\n",
    "       train_regret_loser_10[slice27],\n",
    "       train_regret_loser_11[slice27],\n",
    "       train_regret_loser_12[slice27],\n",
    "       train_regret_loser_13[slice27],\n",
    "       train_regret_loser_14[slice27],\n",
    "       train_regret_loser_15[slice27],\n",
    "       train_regret_loser_16[slice27],\n",
    "       train_regret_loser_17[slice27],\n",
    "       train_regret_loser_18[slice27],\n",
    "       train_regret_loser_19[slice27],\n",
    "       train_regret_loser_20[slice27]]\n",
    "\n",
    "winner27 = [train_regret_winner_1[slice27],\n",
    "       train_regret_winner_2[slice27],\n",
    "       train_regret_winner_3[slice27],\n",
    "       train_regret_winner_4[slice27],\n",
    "       train_regret_winner_5[slice27],\n",
    "       train_regret_winner_6[slice27],\n",
    "       train_regret_winner_7[slice27],\n",
    "       train_regret_winner_8[slice27],\n",
    "       train_regret_winner_9[slice27],\n",
    "       train_regret_winner_10[slice27],\n",
    "       train_regret_winner_11[slice27],\n",
    "       train_regret_winner_12[slice27],\n",
    "       train_regret_winner_13[slice27],\n",
    "       train_regret_winner_14[slice27],\n",
    "       train_regret_winner_15[slice27],\n",
    "       train_regret_winner_16[slice27],\n",
    "       train_regret_winner_17[slice27],\n",
    "       train_regret_winner_18[slice27],\n",
    "       train_regret_winner_19[slice27],\n",
    "       train_regret_winner_20[slice27]]\n",
    "\n",
    "loser27_results = pd.DataFrame(loser27).sort_values(by=[0], ascending=False)\n",
    "winner27_results = pd.DataFrame(winner27).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser27 = np.asarray(loser27_results[4:5][0])[0]\n",
    "median_loser27 = np.asarray(loser27_results[9:10][0])[0]\n",
    "upper_loser27 = np.asarray(loser27_results[14:15][0])[0]\n",
    "\n",
    "lower_winner27 = np.asarray(winner27_results[4:5][0])[0]\n",
    "median_winner27 = np.asarray(winner27_results[9:10][0])[0]\n",
    "upper_winner27 = np.asarray(winner27_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration37 :\n",
    "\n",
    "slice37 = 36\n",
    "\n",
    "loser37 = [train_regret_loser_1[slice37],\n",
    "       train_regret_loser_2[slice37],\n",
    "       train_regret_loser_3[slice37],\n",
    "       train_regret_loser_4[slice37],\n",
    "       train_regret_loser_5[slice37],\n",
    "       train_regret_loser_6[slice37],\n",
    "       train_regret_loser_7[slice37],\n",
    "       train_regret_loser_8[slice37],\n",
    "       train_regret_loser_9[slice37],\n",
    "       train_regret_loser_10[slice37],\n",
    "       train_regret_loser_11[slice37],\n",
    "       train_regret_loser_12[slice37],\n",
    "       train_regret_loser_13[slice37],\n",
    "       train_regret_loser_14[slice37],\n",
    "       train_regret_loser_15[slice37],\n",
    "       train_regret_loser_16[slice37],\n",
    "       train_regret_loser_17[slice37],\n",
    "       train_regret_loser_18[slice37],\n",
    "       train_regret_loser_19[slice37],\n",
    "       train_regret_loser_20[slice37]]\n",
    "\n",
    "winner37 = [train_regret_winner_1[slice37],\n",
    "       train_regret_winner_2[slice37],\n",
    "       train_regret_winner_3[slice37],\n",
    "       train_regret_winner_4[slice37],\n",
    "       train_regret_winner_5[slice37],\n",
    "       train_regret_winner_6[slice37],\n",
    "       train_regret_winner_7[slice37],\n",
    "       train_regret_winner_8[slice37],\n",
    "       train_regret_winner_9[slice37],\n",
    "       train_regret_winner_10[slice37],\n",
    "       train_regret_winner_11[slice37],\n",
    "       train_regret_winner_12[slice37],\n",
    "       train_regret_winner_13[slice37],\n",
    "       train_regret_winner_14[slice37],\n",
    "       train_regret_winner_15[slice37],\n",
    "       train_regret_winner_16[slice37],\n",
    "       train_regret_winner_17[slice37],\n",
    "       train_regret_winner_18[slice37],\n",
    "       train_regret_winner_19[slice37],\n",
    "       train_regret_winner_20[slice37]]\n",
    "\n",
    "loser37_results = pd.DataFrame(loser37).sort_values(by=[0], ascending=False)\n",
    "winner37_results = pd.DataFrame(winner37).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser37 = np.asarray(loser37_results[4:5][0])[0]\n",
    "median_loser37 = np.asarray(loser37_results[9:10][0])[0]\n",
    "upper_loser37 = np.asarray(loser37_results[14:15][0])[0]\n",
    "\n",
    "lower_winner37 = np.asarray(winner37_results[4:5][0])[0]\n",
    "median_winner37 = np.asarray(winner37_results[9:10][0])[0]\n",
    "upper_winner37 = np.asarray(winner37_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration47 :\n",
    "\n",
    "slice47 = 46\n",
    "\n",
    "loser47 = [train_regret_loser_1[slice47],\n",
    "       train_regret_loser_2[slice47],\n",
    "       train_regret_loser_3[slice47],\n",
    "       train_regret_loser_4[slice47],\n",
    "       train_regret_loser_5[slice47],\n",
    "       train_regret_loser_6[slice47],\n",
    "       train_regret_loser_7[slice47],\n",
    "       train_regret_loser_8[slice47],\n",
    "       train_regret_loser_9[slice47],\n",
    "       train_regret_loser_10[slice47],\n",
    "       train_regret_loser_11[slice47],\n",
    "       train_regret_loser_12[slice47],\n",
    "       train_regret_loser_13[slice47],\n",
    "       train_regret_loser_14[slice47],\n",
    "       train_regret_loser_15[slice47],\n",
    "       train_regret_loser_16[slice47],\n",
    "       train_regret_loser_17[slice47],\n",
    "       train_regret_loser_18[slice47],\n",
    "       train_regret_loser_19[slice47],\n",
    "       train_regret_loser_20[slice47]]\n",
    "\n",
    "winner47 = [train_regret_winner_1[slice47],\n",
    "       train_regret_winner_2[slice47],\n",
    "       train_regret_winner_3[slice47],\n",
    "       train_regret_winner_4[slice47],\n",
    "       train_regret_winner_5[slice47],\n",
    "       train_regret_winner_6[slice47],\n",
    "       train_regret_winner_7[slice47],\n",
    "       train_regret_winner_8[slice47],\n",
    "       train_regret_winner_9[slice47],\n",
    "       train_regret_winner_10[slice47],\n",
    "       train_regret_winner_11[slice47],\n",
    "       train_regret_winner_12[slice47],\n",
    "       train_regret_winner_13[slice47],\n",
    "       train_regret_winner_14[slice47],\n",
    "       train_regret_winner_15[slice47],\n",
    "       train_regret_winner_16[slice47],\n",
    "       train_regret_winner_17[slice47],\n",
    "       train_regret_winner_18[slice47],\n",
    "       train_regret_winner_19[slice47],\n",
    "       train_regret_winner_20[slice47]]\n",
    "\n",
    "loser47_results = pd.DataFrame(loser47).sort_values(by=[0], ascending=False)\n",
    "winner47_results = pd.DataFrame(winner47).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser47 = np.asarray(loser47_results[4:5][0])[0]\n",
    "median_loser47 = np.asarray(loser47_results[9:10][0])[0]\n",
    "upper_loser47 = np.asarray(loser47_results[14:15][0])[0]\n",
    "\n",
    "lower_winner47 = np.asarray(winner47_results[4:5][0])[0]\n",
    "median_winner47 = np.asarray(winner47_results[9:10][0])[0]\n",
    "upper_winner47 = np.asarray(winner47_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration57 :\n",
    "\n",
    "slice57 = 56\n",
    "\n",
    "loser57 = [train_regret_loser_1[slice57],\n",
    "       train_regret_loser_2[slice57],\n",
    "       train_regret_loser_3[slice57],\n",
    "       train_regret_loser_4[slice57],\n",
    "       train_regret_loser_5[slice57],\n",
    "       train_regret_loser_6[slice57],\n",
    "       train_regret_loser_7[slice57],\n",
    "       train_regret_loser_8[slice57],\n",
    "       train_regret_loser_9[slice57],\n",
    "       train_regret_loser_10[slice57],\n",
    "       train_regret_loser_11[slice57],\n",
    "       train_regret_loser_12[slice57],\n",
    "       train_regret_loser_13[slice57],\n",
    "       train_regret_loser_14[slice57],\n",
    "       train_regret_loser_15[slice57],\n",
    "       train_regret_loser_16[slice57],\n",
    "       train_regret_loser_17[slice57],\n",
    "       train_regret_loser_18[slice57],\n",
    "       train_regret_loser_19[slice57],\n",
    "       train_regret_loser_20[slice57]]\n",
    "\n",
    "winner57 = [train_regret_winner_1[slice57],\n",
    "       train_regret_winner_2[slice57],\n",
    "       train_regret_winner_3[slice57],\n",
    "       train_regret_winner_4[slice57],\n",
    "       train_regret_winner_5[slice57],\n",
    "       train_regret_winner_6[slice57],\n",
    "       train_regret_winner_7[slice57],\n",
    "       train_regret_winner_8[slice57],\n",
    "       train_regret_winner_9[slice57],\n",
    "       train_regret_winner_10[slice57],\n",
    "       train_regret_winner_11[slice57],\n",
    "       train_regret_winner_12[slice57],\n",
    "       train_regret_winner_13[slice57],\n",
    "       train_regret_winner_14[slice57],\n",
    "       train_regret_winner_15[slice57],\n",
    "       train_regret_winner_16[slice57],\n",
    "       train_regret_winner_17[slice57],\n",
    "       train_regret_winner_18[slice57],\n",
    "       train_regret_winner_19[slice57],\n",
    "       train_regret_winner_20[slice57]]\n",
    "\n",
    "loser57_results = pd.DataFrame(loser57).sort_values(by=[0], ascending=False)\n",
    "winner57_results = pd.DataFrame(winner57).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser57 = np.asarray(loser57_results[4:5][0])[0]\n",
    "median_loser57 = np.asarray(loser57_results[9:10][0])[0]\n",
    "upper_loser57 = np.asarray(loser57_results[14:15][0])[0]\n",
    "\n",
    "lower_winner57 = np.asarray(winner57_results[4:5][0])[0]\n",
    "median_winner57 = np.asarray(winner57_results[9:10][0])[0]\n",
    "upper_winner57 = np.asarray(winner57_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration67 :\n",
    "\n",
    "slice67 = 66\n",
    "\n",
    "loser67 = [train_regret_loser_1[slice67],\n",
    "       train_regret_loser_2[slice67],\n",
    "       train_regret_loser_3[slice67],\n",
    "       train_regret_loser_4[slice67],\n",
    "       train_regret_loser_5[slice67],\n",
    "       train_regret_loser_6[slice67],\n",
    "       train_regret_loser_7[slice67],\n",
    "       train_regret_loser_8[slice67],\n",
    "       train_regret_loser_9[slice67],\n",
    "       train_regret_loser_10[slice67],\n",
    "       train_regret_loser_11[slice67],\n",
    "       train_regret_loser_12[slice67],\n",
    "       train_regret_loser_13[slice67],\n",
    "       train_regret_loser_14[slice67],\n",
    "       train_regret_loser_15[slice67],\n",
    "       train_regret_loser_16[slice67],\n",
    "       train_regret_loser_17[slice67],\n",
    "       train_regret_loser_18[slice67],\n",
    "       train_regret_loser_19[slice67],\n",
    "       train_regret_loser_20[slice67]]\n",
    "\n",
    "winner67 = [train_regret_winner_1[slice67],\n",
    "       train_regret_winner_2[slice67],\n",
    "       train_regret_winner_3[slice67],\n",
    "       train_regret_winner_4[slice67],\n",
    "       train_regret_winner_5[slice67],\n",
    "       train_regret_winner_6[slice67],\n",
    "       train_regret_winner_7[slice67],\n",
    "       train_regret_winner_8[slice67],\n",
    "       train_regret_winner_9[slice67],\n",
    "       train_regret_winner_10[slice67],\n",
    "       train_regret_winner_11[slice67],\n",
    "       train_regret_winner_12[slice67],\n",
    "       train_regret_winner_13[slice67],\n",
    "       train_regret_winner_14[slice67],\n",
    "       train_regret_winner_15[slice67],\n",
    "       train_regret_winner_16[slice67],\n",
    "       train_regret_winner_17[slice67],\n",
    "       train_regret_winner_18[slice67],\n",
    "       train_regret_winner_19[slice67],\n",
    "       train_regret_winner_20[slice67]]\n",
    "\n",
    "loser67_results = pd.DataFrame(loser67).sort_values(by=[0], ascending=False)\n",
    "winner67_results = pd.DataFrame(winner67).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser67 = np.asarray(loser67_results[4:5][0])[0]\n",
    "median_loser67 = np.asarray(loser67_results[9:10][0])[0]\n",
    "upper_loser67 = np.asarray(loser67_results[14:15][0])[0]\n",
    "\n",
    "lower_winner67 = np.asarray(winner67_results[4:5][0])[0]\n",
    "median_winner67 = np.asarray(winner67_results[9:10][0])[0]\n",
    "upper_winner67 = np.asarray(winner67_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration77 :\n",
    "\n",
    "slice77 = 76\n",
    "\n",
    "loser77 = [train_regret_loser_1[slice77],\n",
    "       train_regret_loser_2[slice77],\n",
    "       train_regret_loser_3[slice77],\n",
    "       train_regret_loser_4[slice77],\n",
    "       train_regret_loser_5[slice77],\n",
    "       train_regret_loser_6[slice77],\n",
    "       train_regret_loser_7[slice77],\n",
    "       train_regret_loser_8[slice77],\n",
    "       train_regret_loser_9[slice77],\n",
    "       train_regret_loser_10[slice77],\n",
    "       train_regret_loser_11[slice77],\n",
    "       train_regret_loser_12[slice77],\n",
    "       train_regret_loser_13[slice77],\n",
    "       train_regret_loser_14[slice77],\n",
    "       train_regret_loser_15[slice77],\n",
    "       train_regret_loser_16[slice77],\n",
    "       train_regret_loser_17[slice77],\n",
    "       train_regret_loser_18[slice77],\n",
    "       train_regret_loser_19[slice77],\n",
    "       train_regret_loser_20[slice77]]\n",
    "\n",
    "winner77 = [train_regret_winner_1[slice77],\n",
    "       train_regret_winner_2[slice77],\n",
    "       train_regret_winner_3[slice77],\n",
    "       train_regret_winner_4[slice77],\n",
    "       train_regret_winner_5[slice77],\n",
    "       train_regret_winner_6[slice77],\n",
    "       train_regret_winner_7[slice77],\n",
    "       train_regret_winner_8[slice77],\n",
    "       train_regret_winner_9[slice77],\n",
    "       train_regret_winner_10[slice77],\n",
    "       train_regret_winner_11[slice77],\n",
    "       train_regret_winner_12[slice77],\n",
    "       train_regret_winner_13[slice77],\n",
    "       train_regret_winner_14[slice77],\n",
    "       train_regret_winner_15[slice77],\n",
    "       train_regret_winner_16[slice77],\n",
    "       train_regret_winner_17[slice77],\n",
    "       train_regret_winner_18[slice77],\n",
    "       train_regret_winner_19[slice77],\n",
    "       train_regret_winner_20[slice77]]\n",
    "\n",
    "loser77_results = pd.DataFrame(loser77).sort_values(by=[0], ascending=False)\n",
    "winner77_results = pd.DataFrame(winner77).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser77 = np.asarray(loser77_results[4:5][0])[0]\n",
    "median_loser77 = np.asarray(loser77_results[9:10][0])[0]\n",
    "upper_loser77 = np.asarray(loser77_results[14:15][0])[0]\n",
    "\n",
    "lower_winner77 = np.asarray(winner77_results[4:5][0])[0]\n",
    "median_winner77 = np.asarray(winner77_results[9:10][0])[0]\n",
    "upper_winner77 = np.asarray(winner77_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration87 :\n",
    "\n",
    "slice87 = 86\n",
    "\n",
    "loser87 = [train_regret_loser_1[slice87],\n",
    "       train_regret_loser_2[slice87],\n",
    "       train_regret_loser_3[slice87],\n",
    "       train_regret_loser_4[slice87],\n",
    "       train_regret_loser_5[slice87],\n",
    "       train_regret_loser_6[slice87],\n",
    "       train_regret_loser_7[slice87],\n",
    "       train_regret_loser_8[slice87],\n",
    "       train_regret_loser_9[slice87],\n",
    "       train_regret_loser_10[slice87],\n",
    "       train_regret_loser_11[slice87],\n",
    "       train_regret_loser_12[slice87],\n",
    "       train_regret_loser_13[slice87],\n",
    "       train_regret_loser_14[slice87],\n",
    "       train_regret_loser_15[slice87],\n",
    "       train_regret_loser_16[slice87],\n",
    "       train_regret_loser_17[slice87],\n",
    "       train_regret_loser_18[slice87],\n",
    "       train_regret_loser_19[slice87],\n",
    "       train_regret_loser_20[slice87]]\n",
    "\n",
    "winner87 = [train_regret_winner_1[slice87],\n",
    "       train_regret_winner_2[slice87],\n",
    "       train_regret_winner_3[slice87],\n",
    "       train_regret_winner_4[slice87],\n",
    "       train_regret_winner_5[slice87],\n",
    "       train_regret_winner_6[slice87],\n",
    "       train_regret_winner_7[slice87],\n",
    "       train_regret_winner_8[slice87],\n",
    "       train_regret_winner_9[slice87],\n",
    "       train_regret_winner_10[slice87],\n",
    "       train_regret_winner_11[slice87],\n",
    "       train_regret_winner_12[slice87],\n",
    "       train_regret_winner_13[slice87],\n",
    "       train_regret_winner_14[slice87],\n",
    "       train_regret_winner_15[slice87],\n",
    "       train_regret_winner_16[slice87],\n",
    "       train_regret_winner_17[slice87],\n",
    "       train_regret_winner_18[slice87],\n",
    "       train_regret_winner_19[slice87],\n",
    "       train_regret_winner_20[slice87]]\n",
    "\n",
    "loser87_results = pd.DataFrame(loser87).sort_values(by=[0], ascending=False)\n",
    "winner87_results = pd.DataFrame(winner87).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser87 = np.asarray(loser87_results[4:5][0])[0]\n",
    "median_loser87 = np.asarray(loser87_results[9:10][0])[0]\n",
    "upper_loser87 = np.asarray(loser87_results[14:15][0])[0]\n",
    "\n",
    "lower_winner87 = np.asarray(winner87_results[4:5][0])[0]\n",
    "median_winner87 = np.asarray(winner87_results[9:10][0])[0]\n",
    "upper_winner87 = np.asarray(winner87_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration97 :\n",
    "\n",
    "slice97 = 96\n",
    "\n",
    "loser97 = [train_regret_loser_1[slice97],\n",
    "       train_regret_loser_2[slice97],\n",
    "       train_regret_loser_3[slice97],\n",
    "       train_regret_loser_4[slice97],\n",
    "       train_regret_loser_5[slice97],\n",
    "       train_regret_loser_6[slice97],\n",
    "       train_regret_loser_7[slice97],\n",
    "       train_regret_loser_8[slice97],\n",
    "       train_regret_loser_9[slice97],\n",
    "       train_regret_loser_10[slice97],\n",
    "       train_regret_loser_11[slice97],\n",
    "       train_regret_loser_12[slice97],\n",
    "       train_regret_loser_13[slice97],\n",
    "       train_regret_loser_14[slice97],\n",
    "       train_regret_loser_15[slice97],\n",
    "       train_regret_loser_16[slice97],\n",
    "       train_regret_loser_17[slice97],\n",
    "       train_regret_loser_18[slice97],\n",
    "       train_regret_loser_19[slice97],\n",
    "       train_regret_loser_20[slice97]]\n",
    "\n",
    "winner97 = [train_regret_winner_1[slice97],\n",
    "       train_regret_winner_2[slice97],\n",
    "       train_regret_winner_3[slice97],\n",
    "       train_regret_winner_4[slice97],\n",
    "       train_regret_winner_5[slice97],\n",
    "       train_regret_winner_6[slice97],\n",
    "       train_regret_winner_7[slice97],\n",
    "       train_regret_winner_8[slice97],\n",
    "       train_regret_winner_9[slice97],\n",
    "       train_regret_winner_10[slice97],\n",
    "       train_regret_winner_11[slice97],\n",
    "       train_regret_winner_12[slice97],\n",
    "       train_regret_winner_13[slice97],\n",
    "       train_regret_winner_14[slice97],\n",
    "       train_regret_winner_15[slice97],\n",
    "       train_regret_winner_16[slice97],\n",
    "       train_regret_winner_17[slice97],\n",
    "       train_regret_winner_18[slice97],\n",
    "       train_regret_winner_19[slice97],\n",
    "       train_regret_winner_20[slice97]]\n",
    "\n",
    "loser97_results = pd.DataFrame(loser97).sort_values(by=[0], ascending=False)\n",
    "winner97_results = pd.DataFrame(winner97).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser97 = np.asarray(loser97_results[4:5][0])[0]\n",
    "median_loser97 = np.asarray(loser97_results[9:10][0])[0]\n",
    "upper_loser97 = np.asarray(loser97_results[14:15][0])[0]\n",
    "\n",
    "lower_winner97 = np.asarray(winner97_results[4:5][0])[0]\n",
    "median_winner97 = np.asarray(winner97_results[9:10][0])[0]\n",
    "upper_winner97 = np.asarray(winner97_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration8 :\n",
    "\n",
    "slice8 = 7\n",
    "\n",
    "loser8 = [train_regret_loser_1[slice8],\n",
    "       train_regret_loser_2[slice8],\n",
    "       train_regret_loser_3[slice8],\n",
    "       train_regret_loser_4[slice8],\n",
    "       train_regret_loser_5[slice8],\n",
    "       train_regret_loser_6[slice8],\n",
    "       train_regret_loser_7[slice8],\n",
    "       train_regret_loser_8[slice8],\n",
    "       train_regret_loser_9[slice8],\n",
    "       train_regret_loser_10[slice8],\n",
    "       train_regret_loser_11[slice8],\n",
    "       train_regret_loser_12[slice8],\n",
    "       train_regret_loser_13[slice8],\n",
    "       train_regret_loser_14[slice8],\n",
    "       train_regret_loser_15[slice8],\n",
    "       train_regret_loser_16[slice8],\n",
    "       train_regret_loser_17[slice8],\n",
    "       train_regret_loser_18[slice8],\n",
    "       train_regret_loser_19[slice8],\n",
    "       train_regret_loser_20[slice8]]\n",
    "\n",
    "winner8 = [train_regret_winner_1[slice8],\n",
    "       train_regret_winner_2[slice8],\n",
    "       train_regret_winner_3[slice8],\n",
    "       train_regret_winner_4[slice8],\n",
    "       train_regret_winner_5[slice8],\n",
    "       train_regret_winner_6[slice8],\n",
    "       train_regret_winner_7[slice8],\n",
    "       train_regret_winner_8[slice8],\n",
    "       train_regret_winner_9[slice8],\n",
    "       train_regret_winner_10[slice8],\n",
    "       train_regret_winner_11[slice8],\n",
    "       train_regret_winner_12[slice8],\n",
    "       train_regret_winner_13[slice8],\n",
    "       train_regret_winner_14[slice8],\n",
    "       train_regret_winner_15[slice8],\n",
    "       train_regret_winner_16[slice8],\n",
    "       train_regret_winner_17[slice8],\n",
    "       train_regret_winner_18[slice8],\n",
    "       train_regret_winner_19[slice8],\n",
    "       train_regret_winner_20[slice8]]\n",
    "\n",
    "loser8_results = pd.DataFrame(loser8).sort_values(by=[0], ascending=False)\n",
    "winner8_results = pd.DataFrame(winner8).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser8 = np.asarray(loser8_results[4:5][0])[0]\n",
    "median_loser8 = np.asarray(loser8_results[9:10][0])[0]\n",
    "upper_loser8 = np.asarray(loser8_results[14:15][0])[0]\n",
    "\n",
    "lower_winner8 = np.asarray(winner8_results[4:5][0])[0]\n",
    "median_winner8 = np.asarray(winner8_results[9:10][0])[0]\n",
    "upper_winner8 = np.asarray(winner8_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration18 :\n",
    "\n",
    "slice18 = 17\n",
    "\n",
    "loser18 = [train_regret_loser_1[slice18],\n",
    "       train_regret_loser_2[slice18],\n",
    "       train_regret_loser_3[slice18],\n",
    "       train_regret_loser_4[slice18],\n",
    "       train_regret_loser_5[slice18],\n",
    "       train_regret_loser_6[slice18],\n",
    "       train_regret_loser_7[slice18],\n",
    "       train_regret_loser_8[slice18],\n",
    "       train_regret_loser_9[slice18],\n",
    "       train_regret_loser_10[slice18],\n",
    "       train_regret_loser_11[slice18],\n",
    "       train_regret_loser_12[slice18],\n",
    "       train_regret_loser_13[slice18],\n",
    "       train_regret_loser_14[slice18],\n",
    "       train_regret_loser_15[slice18],\n",
    "       train_regret_loser_16[slice18],\n",
    "       train_regret_loser_17[slice18],\n",
    "       train_regret_loser_18[slice18],\n",
    "       train_regret_loser_19[slice18],\n",
    "       train_regret_loser_20[slice18]]\n",
    "\n",
    "winner18 = [train_regret_winner_1[slice18],\n",
    "       train_regret_winner_2[slice18],\n",
    "       train_regret_winner_3[slice18],\n",
    "       train_regret_winner_4[slice18],\n",
    "       train_regret_winner_5[slice18],\n",
    "       train_regret_winner_6[slice18],\n",
    "       train_regret_winner_7[slice18],\n",
    "       train_regret_winner_8[slice18],\n",
    "       train_regret_winner_9[slice18],\n",
    "       train_regret_winner_10[slice18],\n",
    "       train_regret_winner_11[slice18],\n",
    "       train_regret_winner_12[slice18],\n",
    "       train_regret_winner_13[slice18],\n",
    "       train_regret_winner_14[slice18],\n",
    "       train_regret_winner_15[slice18],\n",
    "       train_regret_winner_16[slice18],\n",
    "       train_regret_winner_17[slice18],\n",
    "       train_regret_winner_18[slice18],\n",
    "       train_regret_winner_19[slice18],\n",
    "       train_regret_winner_20[slice18]]\n",
    "\n",
    "loser18_results = pd.DataFrame(loser18).sort_values(by=[0], ascending=False)\n",
    "winner18_results = pd.DataFrame(winner18).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser18 = np.asarray(loser18_results[4:5][0])[0]\n",
    "median_loser18 = np.asarray(loser18_results[9:10][0])[0]\n",
    "upper_loser18 = np.asarray(loser18_results[14:15][0])[0]\n",
    "\n",
    "lower_winner18 = np.asarray(winner18_results[4:5][0])[0]\n",
    "median_winner18 = np.asarray(winner18_results[9:10][0])[0]\n",
    "upper_winner18 = np.asarray(winner18_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration28 :\n",
    "\n",
    "slice28 = 27\n",
    "\n",
    "loser28 = [train_regret_loser_1[slice28],\n",
    "       train_regret_loser_2[slice28],\n",
    "       train_regret_loser_3[slice28],\n",
    "       train_regret_loser_4[slice28],\n",
    "       train_regret_loser_5[slice28],\n",
    "       train_regret_loser_6[slice28],\n",
    "       train_regret_loser_7[slice28],\n",
    "       train_regret_loser_8[slice28],\n",
    "       train_regret_loser_9[slice28],\n",
    "       train_regret_loser_10[slice28],\n",
    "       train_regret_loser_11[slice28],\n",
    "       train_regret_loser_12[slice28],\n",
    "       train_regret_loser_13[slice28],\n",
    "       train_regret_loser_14[slice28],\n",
    "       train_regret_loser_15[slice28],\n",
    "       train_regret_loser_16[slice28],\n",
    "       train_regret_loser_17[slice28],\n",
    "       train_regret_loser_18[slice28],\n",
    "       train_regret_loser_19[slice28],\n",
    "       train_regret_loser_20[slice28]]\n",
    "\n",
    "winner28 = [train_regret_winner_1[slice28],\n",
    "       train_regret_winner_2[slice28],\n",
    "       train_regret_winner_3[slice28],\n",
    "       train_regret_winner_4[slice28],\n",
    "       train_regret_winner_5[slice28],\n",
    "       train_regret_winner_6[slice28],\n",
    "       train_regret_winner_7[slice28],\n",
    "       train_regret_winner_8[slice28],\n",
    "       train_regret_winner_9[slice28],\n",
    "       train_regret_winner_10[slice28],\n",
    "       train_regret_winner_11[slice28],\n",
    "       train_regret_winner_12[slice28],\n",
    "       train_regret_winner_13[slice28],\n",
    "       train_regret_winner_14[slice28],\n",
    "       train_regret_winner_15[slice28],\n",
    "       train_regret_winner_16[slice28],\n",
    "       train_regret_winner_17[slice28],\n",
    "       train_regret_winner_18[slice28],\n",
    "       train_regret_winner_19[slice28],\n",
    "       train_regret_winner_20[slice28]]\n",
    "\n",
    "loser28_results = pd.DataFrame(loser28).sort_values(by=[0], ascending=False)\n",
    "winner28_results = pd.DataFrame(winner28).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser28 = np.asarray(loser28_results[4:5][0])[0]\n",
    "median_loser28 = np.asarray(loser28_results[9:10][0])[0]\n",
    "upper_loser28 = np.asarray(loser28_results[14:15][0])[0]\n",
    "\n",
    "lower_winner28 = np.asarray(winner28_results[4:5][0])[0]\n",
    "median_winner28 = np.asarray(winner28_results[9:10][0])[0]\n",
    "upper_winner28 = np.asarray(winner28_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration38 :\n",
    "\n",
    "slice38 = 37\n",
    "\n",
    "loser38 = [train_regret_loser_1[slice38],\n",
    "       train_regret_loser_2[slice38],\n",
    "       train_regret_loser_3[slice38],\n",
    "       train_regret_loser_4[slice38],\n",
    "       train_regret_loser_5[slice38],\n",
    "       train_regret_loser_6[slice38],\n",
    "       train_regret_loser_7[slice38],\n",
    "       train_regret_loser_8[slice38],\n",
    "       train_regret_loser_9[slice38],\n",
    "       train_regret_loser_10[slice38],\n",
    "       train_regret_loser_11[slice38],\n",
    "       train_regret_loser_12[slice38],\n",
    "       train_regret_loser_13[slice38],\n",
    "       train_regret_loser_14[slice38],\n",
    "       train_regret_loser_15[slice38],\n",
    "       train_regret_loser_16[slice38],\n",
    "       train_regret_loser_17[slice38],\n",
    "       train_regret_loser_18[slice38],\n",
    "       train_regret_loser_19[slice38],\n",
    "       train_regret_loser_20[slice38]]\n",
    "\n",
    "winner38 = [train_regret_winner_1[slice38],\n",
    "       train_regret_winner_2[slice38],\n",
    "       train_regret_winner_3[slice38],\n",
    "       train_regret_winner_4[slice38],\n",
    "       train_regret_winner_5[slice38],\n",
    "       train_regret_winner_6[slice38],\n",
    "       train_regret_winner_7[slice38],\n",
    "       train_regret_winner_8[slice38],\n",
    "       train_regret_winner_9[slice38],\n",
    "       train_regret_winner_10[slice38],\n",
    "       train_regret_winner_11[slice38],\n",
    "       train_regret_winner_12[slice38],\n",
    "       train_regret_winner_13[slice38],\n",
    "       train_regret_winner_14[slice38],\n",
    "       train_regret_winner_15[slice38],\n",
    "       train_regret_winner_16[slice38],\n",
    "       train_regret_winner_17[slice38],\n",
    "       train_regret_winner_18[slice38],\n",
    "       train_regret_winner_19[slice38],\n",
    "       train_regret_winner_20[slice38]]\n",
    "\n",
    "loser38_results = pd.DataFrame(loser38).sort_values(by=[0], ascending=False)\n",
    "winner38_results = pd.DataFrame(winner38).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser38 = np.asarray(loser38_results[4:5][0])[0]\n",
    "median_loser38 = np.asarray(loser38_results[9:10][0])[0]\n",
    "upper_loser38 = np.asarray(loser38_results[14:15][0])[0]\n",
    "\n",
    "lower_winner38 = np.asarray(winner38_results[4:5][0])[0]\n",
    "median_winner38 = np.asarray(winner38_results[9:10][0])[0]\n",
    "upper_winner38 = np.asarray(winner38_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration48 :\n",
    "\n",
    "slice48 = 47\n",
    "\n",
    "loser48 = [train_regret_loser_1[slice48],\n",
    "       train_regret_loser_2[slice48],\n",
    "       train_regret_loser_3[slice48],\n",
    "       train_regret_loser_4[slice48],\n",
    "       train_regret_loser_5[slice48],\n",
    "       train_regret_loser_6[slice48],\n",
    "       train_regret_loser_7[slice48],\n",
    "       train_regret_loser_8[slice48],\n",
    "       train_regret_loser_9[slice48],\n",
    "       train_regret_loser_10[slice48],\n",
    "       train_regret_loser_11[slice48],\n",
    "       train_regret_loser_12[slice48],\n",
    "       train_regret_loser_13[slice48],\n",
    "       train_regret_loser_14[slice48],\n",
    "       train_regret_loser_15[slice48],\n",
    "       train_regret_loser_16[slice48],\n",
    "       train_regret_loser_17[slice48],\n",
    "       train_regret_loser_18[slice48],\n",
    "       train_regret_loser_19[slice48],\n",
    "       train_regret_loser_20[slice48]]\n",
    "\n",
    "winner48 = [train_regret_winner_1[slice48],\n",
    "       train_regret_winner_2[slice48],\n",
    "       train_regret_winner_3[slice48],\n",
    "       train_regret_winner_4[slice48],\n",
    "       train_regret_winner_5[slice48],\n",
    "       train_regret_winner_6[slice48],\n",
    "       train_regret_winner_7[slice48],\n",
    "       train_regret_winner_8[slice48],\n",
    "       train_regret_winner_9[slice48],\n",
    "       train_regret_winner_10[slice48],\n",
    "       train_regret_winner_11[slice48],\n",
    "       train_regret_winner_12[slice48],\n",
    "       train_regret_winner_13[slice48],\n",
    "       train_regret_winner_14[slice48],\n",
    "       train_regret_winner_15[slice48],\n",
    "       train_regret_winner_16[slice48],\n",
    "       train_regret_winner_17[slice48],\n",
    "       train_regret_winner_18[slice48],\n",
    "       train_regret_winner_19[slice48],\n",
    "       train_regret_winner_20[slice48]]\n",
    "\n",
    "loser48_results = pd.DataFrame(loser48).sort_values(by=[0], ascending=False)\n",
    "winner48_results = pd.DataFrame(winner48).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser48 = np.asarray(loser48_results[4:5][0])[0]\n",
    "median_loser48 = np.asarray(loser48_results[9:10][0])[0]\n",
    "upper_loser48 = np.asarray(loser48_results[14:15][0])[0]\n",
    "\n",
    "lower_winner48 = np.asarray(winner48_results[4:5][0])[0]\n",
    "median_winner48 = np.asarray(winner48_results[9:10][0])[0]\n",
    "upper_winner48 = np.asarray(winner48_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration58 :\n",
    "\n",
    "slice58 = 57\n",
    "\n",
    "loser58 = [train_regret_loser_1[slice58],\n",
    "       train_regret_loser_2[slice58],\n",
    "       train_regret_loser_3[slice58],\n",
    "       train_regret_loser_4[slice58],\n",
    "       train_regret_loser_5[slice58],\n",
    "       train_regret_loser_6[slice58],\n",
    "       train_regret_loser_7[slice58],\n",
    "       train_regret_loser_8[slice58],\n",
    "       train_regret_loser_9[slice58],\n",
    "       train_regret_loser_10[slice58],\n",
    "       train_regret_loser_11[slice58],\n",
    "       train_regret_loser_12[slice58],\n",
    "       train_regret_loser_13[slice58],\n",
    "       train_regret_loser_14[slice58],\n",
    "       train_regret_loser_15[slice58],\n",
    "       train_regret_loser_16[slice58],\n",
    "       train_regret_loser_17[slice58],\n",
    "       train_regret_loser_18[slice58],\n",
    "       train_regret_loser_19[slice58],\n",
    "       train_regret_loser_20[slice58]]\n",
    "\n",
    "winner58 = [train_regret_winner_1[slice58],\n",
    "       train_regret_winner_2[slice58],\n",
    "       train_regret_winner_3[slice58],\n",
    "       train_regret_winner_4[slice58],\n",
    "       train_regret_winner_5[slice58],\n",
    "       train_regret_winner_6[slice58],\n",
    "       train_regret_winner_7[slice58],\n",
    "       train_regret_winner_8[slice58],\n",
    "       train_regret_winner_9[slice58],\n",
    "       train_regret_winner_10[slice58],\n",
    "       train_regret_winner_11[slice58],\n",
    "       train_regret_winner_12[slice58],\n",
    "       train_regret_winner_13[slice58],\n",
    "       train_regret_winner_14[slice58],\n",
    "       train_regret_winner_15[slice58],\n",
    "       train_regret_winner_16[slice58],\n",
    "       train_regret_winner_17[slice58],\n",
    "       train_regret_winner_18[slice58],\n",
    "       train_regret_winner_19[slice58],\n",
    "       train_regret_winner_20[slice58]]\n",
    "\n",
    "loser58_results = pd.DataFrame(loser58).sort_values(by=[0], ascending=False)\n",
    "winner58_results = pd.DataFrame(winner58).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser58 = np.asarray(loser58_results[4:5][0])[0]\n",
    "median_loser58 = np.asarray(loser58_results[9:10][0])[0]\n",
    "upper_loser58 = np.asarray(loser58_results[14:15][0])[0]\n",
    "\n",
    "lower_winner58 = np.asarray(winner58_results[4:5][0])[0]\n",
    "median_winner58 = np.asarray(winner58_results[9:10][0])[0]\n",
    "upper_winner58 = np.asarray(winner58_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration68 :\n",
    "\n",
    "slice68 = 67\n",
    "\n",
    "loser68 = [train_regret_loser_1[slice68],\n",
    "       train_regret_loser_2[slice68],\n",
    "       train_regret_loser_3[slice68],\n",
    "       train_regret_loser_4[slice68],\n",
    "       train_regret_loser_5[slice68],\n",
    "       train_regret_loser_6[slice68],\n",
    "       train_regret_loser_7[slice68],\n",
    "       train_regret_loser_8[slice68],\n",
    "       train_regret_loser_9[slice68],\n",
    "       train_regret_loser_10[slice68],\n",
    "       train_regret_loser_11[slice68],\n",
    "       train_regret_loser_12[slice68],\n",
    "       train_regret_loser_13[slice68],\n",
    "       train_regret_loser_14[slice68],\n",
    "       train_regret_loser_15[slice68],\n",
    "       train_regret_loser_16[slice68],\n",
    "       train_regret_loser_17[slice68],\n",
    "       train_regret_loser_18[slice68],\n",
    "       train_regret_loser_19[slice68],\n",
    "       train_regret_loser_20[slice68]]\n",
    "\n",
    "winner68 = [train_regret_winner_1[slice68],\n",
    "       train_regret_winner_2[slice68],\n",
    "       train_regret_winner_3[slice68],\n",
    "       train_regret_winner_4[slice68],\n",
    "       train_regret_winner_5[slice68],\n",
    "       train_regret_winner_6[slice68],\n",
    "       train_regret_winner_7[slice68],\n",
    "       train_regret_winner_8[slice68],\n",
    "       train_regret_winner_9[slice68],\n",
    "       train_regret_winner_10[slice68],\n",
    "       train_regret_winner_11[slice68],\n",
    "       train_regret_winner_12[slice68],\n",
    "       train_regret_winner_13[slice68],\n",
    "       train_regret_winner_14[slice68],\n",
    "       train_regret_winner_15[slice68],\n",
    "       train_regret_winner_16[slice68],\n",
    "       train_regret_winner_17[slice68],\n",
    "       train_regret_winner_18[slice68],\n",
    "       train_regret_winner_19[slice68],\n",
    "       train_regret_winner_20[slice68]]\n",
    "\n",
    "loser68_results = pd.DataFrame(loser68).sort_values(by=[0], ascending=False)\n",
    "winner68_results = pd.DataFrame(winner68).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser68 = np.asarray(loser68_results[4:5][0])[0]\n",
    "median_loser68 = np.asarray(loser68_results[9:10][0])[0]\n",
    "upper_loser68 = np.asarray(loser68_results[14:15][0])[0]\n",
    "\n",
    "lower_winner68 = np.asarray(winner68_results[4:5][0])[0]\n",
    "median_winner68 = np.asarray(winner68_results[9:10][0])[0]\n",
    "upper_winner68 = np.asarray(winner68_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration78 :\n",
    "\n",
    "slice78 = 77\n",
    "\n",
    "loser78 = [train_regret_loser_1[slice78],\n",
    "       train_regret_loser_2[slice78],\n",
    "       train_regret_loser_3[slice78],\n",
    "       train_regret_loser_4[slice78],\n",
    "       train_regret_loser_5[slice78],\n",
    "       train_regret_loser_6[slice78],\n",
    "       train_regret_loser_7[slice78],\n",
    "       train_regret_loser_8[slice78],\n",
    "       train_regret_loser_9[slice78],\n",
    "       train_regret_loser_10[slice78],\n",
    "       train_regret_loser_11[slice78],\n",
    "       train_regret_loser_12[slice78],\n",
    "       train_regret_loser_13[slice78],\n",
    "       train_regret_loser_14[slice78],\n",
    "       train_regret_loser_15[slice78],\n",
    "       train_regret_loser_16[slice78],\n",
    "       train_regret_loser_17[slice78],\n",
    "       train_regret_loser_18[slice78],\n",
    "       train_regret_loser_19[slice78],\n",
    "       train_regret_loser_20[slice78]]\n",
    "\n",
    "winner78 = [train_regret_winner_1[slice78],\n",
    "       train_regret_winner_2[slice78],\n",
    "       train_regret_winner_3[slice78],\n",
    "       train_regret_winner_4[slice78],\n",
    "       train_regret_winner_5[slice78],\n",
    "       train_regret_winner_6[slice78],\n",
    "       train_regret_winner_7[slice78],\n",
    "       train_regret_winner_8[slice78],\n",
    "       train_regret_winner_9[slice78],\n",
    "       train_regret_winner_10[slice78],\n",
    "       train_regret_winner_11[slice78],\n",
    "       train_regret_winner_12[slice78],\n",
    "       train_regret_winner_13[slice78],\n",
    "       train_regret_winner_14[slice78],\n",
    "       train_regret_winner_15[slice78],\n",
    "       train_regret_winner_16[slice78],\n",
    "       train_regret_winner_17[slice78],\n",
    "       train_regret_winner_18[slice78],\n",
    "       train_regret_winner_19[slice78],\n",
    "       train_regret_winner_20[slice78]]\n",
    "\n",
    "loser78_results = pd.DataFrame(loser78).sort_values(by=[0], ascending=False)\n",
    "winner78_results = pd.DataFrame(winner78).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser78 = np.asarray(loser78_results[4:5][0])[0]\n",
    "median_loser78 = np.asarray(loser78_results[9:10][0])[0]\n",
    "upper_loser78 = np.asarray(loser78_results[14:15][0])[0]\n",
    "\n",
    "lower_winner78 = np.asarray(winner78_results[4:5][0])[0]\n",
    "median_winner78 = np.asarray(winner78_results[9:10][0])[0]\n",
    "upper_winner78 = np.asarray(winner78_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration88 :\n",
    "\n",
    "slice88 = 87\n",
    "\n",
    "loser88 = [train_regret_loser_1[slice88],\n",
    "       train_regret_loser_2[slice88],\n",
    "       train_regret_loser_3[slice88],\n",
    "       train_regret_loser_4[slice88],\n",
    "       train_regret_loser_5[slice88],\n",
    "       train_regret_loser_6[slice88],\n",
    "       train_regret_loser_7[slice88],\n",
    "       train_regret_loser_8[slice88],\n",
    "       train_regret_loser_9[slice88],\n",
    "       train_regret_loser_10[slice88],\n",
    "       train_regret_loser_11[slice88],\n",
    "       train_regret_loser_12[slice88],\n",
    "       train_regret_loser_13[slice88],\n",
    "       train_regret_loser_14[slice88],\n",
    "       train_regret_loser_15[slice88],\n",
    "       train_regret_loser_16[slice88],\n",
    "       train_regret_loser_17[slice88],\n",
    "       train_regret_loser_18[slice88],\n",
    "       train_regret_loser_19[slice88],\n",
    "       train_regret_loser_20[slice88]]\n",
    "\n",
    "winner88 = [train_regret_winner_1[slice88],\n",
    "       train_regret_winner_2[slice88],\n",
    "       train_regret_winner_3[slice88],\n",
    "       train_regret_winner_4[slice88],\n",
    "       train_regret_winner_5[slice88],\n",
    "       train_regret_winner_6[slice88],\n",
    "       train_regret_winner_7[slice88],\n",
    "       train_regret_winner_8[slice88],\n",
    "       train_regret_winner_9[slice88],\n",
    "       train_regret_winner_10[slice88],\n",
    "       train_regret_winner_11[slice88],\n",
    "       train_regret_winner_12[slice88],\n",
    "       train_regret_winner_13[slice88],\n",
    "       train_regret_winner_14[slice88],\n",
    "       train_regret_winner_15[slice88],\n",
    "       train_regret_winner_16[slice88],\n",
    "       train_regret_winner_17[slice88],\n",
    "       train_regret_winner_18[slice88],\n",
    "       train_regret_winner_19[slice88],\n",
    "       train_regret_winner_20[slice88]]\n",
    "\n",
    "loser88_results = pd.DataFrame(loser88).sort_values(by=[0], ascending=False)\n",
    "winner88_results = pd.DataFrame(winner88).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser88 = np.asarray(loser88_results[4:5][0])[0]\n",
    "median_loser88 = np.asarray(loser88_results[9:10][0])[0]\n",
    "upper_loser88 = np.asarray(loser88_results[14:15][0])[0]\n",
    "\n",
    "lower_winner88 = np.asarray(winner88_results[4:5][0])[0]\n",
    "median_winner88 = np.asarray(winner88_results[9:10][0])[0]\n",
    "upper_winner88 = np.asarray(winner88_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration98 :\n",
    "\n",
    "slice98 = 97\n",
    "\n",
    "loser98 = [train_regret_loser_1[slice98],\n",
    "       train_regret_loser_2[slice98],\n",
    "       train_regret_loser_3[slice98],\n",
    "       train_regret_loser_4[slice98],\n",
    "       train_regret_loser_5[slice98],\n",
    "       train_regret_loser_6[slice98],\n",
    "       train_regret_loser_7[slice98],\n",
    "       train_regret_loser_8[slice98],\n",
    "       train_regret_loser_9[slice98],\n",
    "       train_regret_loser_10[slice98],\n",
    "       train_regret_loser_11[slice98],\n",
    "       train_regret_loser_12[slice98],\n",
    "       train_regret_loser_13[slice98],\n",
    "       train_regret_loser_14[slice98],\n",
    "       train_regret_loser_15[slice98],\n",
    "       train_regret_loser_16[slice98],\n",
    "       train_regret_loser_17[slice98],\n",
    "       train_regret_loser_18[slice98],\n",
    "       train_regret_loser_19[slice98],\n",
    "       train_regret_loser_20[slice98]]\n",
    "\n",
    "winner98 = [train_regret_winner_1[slice98],\n",
    "       train_regret_winner_2[slice98],\n",
    "       train_regret_winner_3[slice98],\n",
    "       train_regret_winner_4[slice98],\n",
    "       train_regret_winner_5[slice98],\n",
    "       train_regret_winner_6[slice98],\n",
    "       train_regret_winner_7[slice98],\n",
    "       train_regret_winner_8[slice98],\n",
    "       train_regret_winner_9[slice98],\n",
    "       train_regret_winner_10[slice98],\n",
    "       train_regret_winner_11[slice98],\n",
    "       train_regret_winner_12[slice98],\n",
    "       train_regret_winner_13[slice98],\n",
    "       train_regret_winner_14[slice98],\n",
    "       train_regret_winner_15[slice98],\n",
    "       train_regret_winner_16[slice98],\n",
    "       train_regret_winner_17[slice98],\n",
    "       train_regret_winner_18[slice98],\n",
    "       train_regret_winner_19[slice98],\n",
    "       train_regret_winner_20[slice98]]\n",
    "\n",
    "loser98_results = pd.DataFrame(loser98).sort_values(by=[0], ascending=False)\n",
    "winner98_results = pd.DataFrame(winner98).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser98 = np.asarray(loser98_results[4:5][0])[0]\n",
    "median_loser98 = np.asarray(loser98_results[9:10][0])[0]\n",
    "upper_loser98 = np.asarray(loser98_results[14:15][0])[0]\n",
    "\n",
    "lower_winner98 = np.asarray(winner98_results[4:5][0])[0]\n",
    "median_winner98 = np.asarray(winner98_results[9:10][0])[0]\n",
    "upper_winner98 = np.asarray(winner98_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration9 :\n",
    "\n",
    "slice9 = 8\n",
    "\n",
    "loser9 = [train_regret_loser_1[slice9],\n",
    "       train_regret_loser_2[slice9],\n",
    "       train_regret_loser_3[slice9],\n",
    "       train_regret_loser_4[slice9],\n",
    "       train_regret_loser_5[slice9],\n",
    "       train_regret_loser_6[slice9],\n",
    "       train_regret_loser_7[slice9],\n",
    "       train_regret_loser_8[slice9],\n",
    "       train_regret_loser_9[slice9],\n",
    "       train_regret_loser_10[slice9],\n",
    "       train_regret_loser_11[slice9],\n",
    "       train_regret_loser_12[slice9],\n",
    "       train_regret_loser_13[slice9],\n",
    "       train_regret_loser_14[slice9],\n",
    "       train_regret_loser_15[slice9],\n",
    "       train_regret_loser_16[slice9],\n",
    "       train_regret_loser_17[slice9],\n",
    "       train_regret_loser_18[slice9],\n",
    "       train_regret_loser_19[slice9],\n",
    "       train_regret_loser_20[slice9]]\n",
    "\n",
    "winner9 = [train_regret_winner_1[slice9],\n",
    "       train_regret_winner_2[slice9],\n",
    "       train_regret_winner_3[slice9],\n",
    "       train_regret_winner_4[slice9],\n",
    "       train_regret_winner_5[slice9],\n",
    "       train_regret_winner_6[slice9],\n",
    "       train_regret_winner_7[slice9],\n",
    "       train_regret_winner_8[slice9],\n",
    "       train_regret_winner_9[slice9],\n",
    "       train_regret_winner_10[slice9],\n",
    "       train_regret_winner_11[slice9],\n",
    "       train_regret_winner_12[slice9],\n",
    "       train_regret_winner_13[slice9],\n",
    "       train_regret_winner_14[slice9],\n",
    "       train_regret_winner_15[slice9],\n",
    "       train_regret_winner_16[slice9],\n",
    "       train_regret_winner_17[slice9],\n",
    "       train_regret_winner_18[slice9],\n",
    "       train_regret_winner_19[slice9],\n",
    "       train_regret_winner_20[slice9]]\n",
    "\n",
    "loser9_results = pd.DataFrame(loser9).sort_values(by=[0], ascending=False)\n",
    "winner9_results = pd.DataFrame(winner9).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser9 = np.asarray(loser9_results[4:5][0])[0]\n",
    "median_loser9 = np.asarray(loser9_results[9:10][0])[0]\n",
    "upper_loser9 = np.asarray(loser9_results[14:15][0])[0]\n",
    "\n",
    "lower_winner9 = np.asarray(winner9_results[4:5][0])[0]\n",
    "median_winner9 = np.asarray(winner9_results[9:10][0])[0]\n",
    "upper_winner9 = np.asarray(winner9_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration19 :\n",
    "\n",
    "slice19 = 18\n",
    "\n",
    "loser19 = [train_regret_loser_1[slice19],\n",
    "       train_regret_loser_2[slice19],\n",
    "       train_regret_loser_3[slice19],\n",
    "       train_regret_loser_4[slice19],\n",
    "       train_regret_loser_5[slice19],\n",
    "       train_regret_loser_6[slice19],\n",
    "       train_regret_loser_7[slice19],\n",
    "       train_regret_loser_8[slice19],\n",
    "       train_regret_loser_9[slice19],\n",
    "       train_regret_loser_10[slice19],\n",
    "       train_regret_loser_11[slice19],\n",
    "       train_regret_loser_12[slice19],\n",
    "       train_regret_loser_13[slice19],\n",
    "       train_regret_loser_14[slice19],\n",
    "       train_regret_loser_15[slice19],\n",
    "       train_regret_loser_16[slice19],\n",
    "       train_regret_loser_17[slice19],\n",
    "       train_regret_loser_18[slice19],\n",
    "       train_regret_loser_19[slice19],\n",
    "       train_regret_loser_20[slice19]]\n",
    "\n",
    "winner19 = [train_regret_winner_1[slice19],\n",
    "       train_regret_winner_2[slice19],\n",
    "       train_regret_winner_3[slice19],\n",
    "       train_regret_winner_4[slice19],\n",
    "       train_regret_winner_5[slice19],\n",
    "       train_regret_winner_6[slice19],\n",
    "       train_regret_winner_7[slice19],\n",
    "       train_regret_winner_8[slice19],\n",
    "       train_regret_winner_9[slice19],\n",
    "       train_regret_winner_10[slice19],\n",
    "       train_regret_winner_11[slice19],\n",
    "       train_regret_winner_12[slice19],\n",
    "       train_regret_winner_13[slice19],\n",
    "       train_regret_winner_14[slice19],\n",
    "       train_regret_winner_15[slice19],\n",
    "       train_regret_winner_16[slice19],\n",
    "       train_regret_winner_17[slice19],\n",
    "       train_regret_winner_18[slice19],\n",
    "       train_regret_winner_19[slice19],\n",
    "       train_regret_winner_20[slice19]]\n",
    "\n",
    "loser19_results = pd.DataFrame(loser19).sort_values(by=[0], ascending=False)\n",
    "winner19_results = pd.DataFrame(winner19).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser19 = np.asarray(loser19_results[4:5][0])[0]\n",
    "median_loser19 = np.asarray(loser19_results[9:10][0])[0]\n",
    "upper_loser19 = np.asarray(loser19_results[14:15][0])[0]\n",
    "\n",
    "lower_winner19 = np.asarray(winner19_results[4:5][0])[0]\n",
    "median_winner19 = np.asarray(winner19_results[9:10][0])[0]\n",
    "upper_winner19 = np.asarray(winner19_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration29 :\n",
    "\n",
    "slice29 = 28\n",
    "\n",
    "loser29 = [train_regret_loser_1[slice29],\n",
    "       train_regret_loser_2[slice29],\n",
    "       train_regret_loser_3[slice29],\n",
    "       train_regret_loser_4[slice29],\n",
    "       train_regret_loser_5[slice29],\n",
    "       train_regret_loser_6[slice29],\n",
    "       train_regret_loser_7[slice29],\n",
    "       train_regret_loser_8[slice29],\n",
    "       train_regret_loser_9[slice29],\n",
    "       train_regret_loser_10[slice29],\n",
    "       train_regret_loser_11[slice29],\n",
    "       train_regret_loser_12[slice29],\n",
    "       train_regret_loser_13[slice29],\n",
    "       train_regret_loser_14[slice29],\n",
    "       train_regret_loser_15[slice29],\n",
    "       train_regret_loser_16[slice29],\n",
    "       train_regret_loser_17[slice29],\n",
    "       train_regret_loser_18[slice29],\n",
    "       train_regret_loser_19[slice29],\n",
    "       train_regret_loser_20[slice29]]\n",
    "\n",
    "winner29 = [train_regret_winner_1[slice29],\n",
    "       train_regret_winner_2[slice29],\n",
    "       train_regret_winner_3[slice29],\n",
    "       train_regret_winner_4[slice29],\n",
    "       train_regret_winner_5[slice29],\n",
    "       train_regret_winner_6[slice29],\n",
    "       train_regret_winner_7[slice29],\n",
    "       train_regret_winner_8[slice29],\n",
    "       train_regret_winner_9[slice29],\n",
    "       train_regret_winner_10[slice29],\n",
    "       train_regret_winner_11[slice29],\n",
    "       train_regret_winner_12[slice29],\n",
    "       train_regret_winner_13[slice29],\n",
    "       train_regret_winner_14[slice29],\n",
    "       train_regret_winner_15[slice29],\n",
    "       train_regret_winner_16[slice29],\n",
    "       train_regret_winner_17[slice29],\n",
    "       train_regret_winner_18[slice29],\n",
    "       train_regret_winner_19[slice29],\n",
    "       train_regret_winner_20[slice29]]\n",
    "\n",
    "loser29_results = pd.DataFrame(loser29).sort_values(by=[0], ascending=False)\n",
    "winner29_results = pd.DataFrame(winner29).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser29 = np.asarray(loser29_results[4:5][0])[0]\n",
    "median_loser29 = np.asarray(loser29_results[9:10][0])[0]\n",
    "upper_loser29 = np.asarray(loser29_results[14:15][0])[0]\n",
    "\n",
    "lower_winner29 = np.asarray(winner29_results[4:5][0])[0]\n",
    "median_winner29 = np.asarray(winner29_results[9:10][0])[0]\n",
    "upper_winner29 = np.asarray(winner29_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration39 :\n",
    "\n",
    "slice39 = 38\n",
    "\n",
    "loser39 = [train_regret_loser_1[slice39],\n",
    "       train_regret_loser_2[slice39],\n",
    "       train_regret_loser_3[slice39],\n",
    "       train_regret_loser_4[slice39],\n",
    "       train_regret_loser_5[slice39],\n",
    "       train_regret_loser_6[slice39],\n",
    "       train_regret_loser_7[slice39],\n",
    "       train_regret_loser_8[slice39],\n",
    "       train_regret_loser_9[slice39],\n",
    "       train_regret_loser_10[slice39],\n",
    "       train_regret_loser_11[slice39],\n",
    "       train_regret_loser_12[slice39],\n",
    "       train_regret_loser_13[slice39],\n",
    "       train_regret_loser_14[slice39],\n",
    "       train_regret_loser_15[slice39],\n",
    "       train_regret_loser_16[slice39],\n",
    "       train_regret_loser_17[slice39],\n",
    "       train_regret_loser_18[slice39],\n",
    "       train_regret_loser_19[slice39],\n",
    "       train_regret_loser_20[slice39]]\n",
    "\n",
    "winner39 = [train_regret_winner_1[slice39],\n",
    "       train_regret_winner_2[slice39],\n",
    "       train_regret_winner_3[slice39],\n",
    "       train_regret_winner_4[slice39],\n",
    "       train_regret_winner_5[slice39],\n",
    "       train_regret_winner_6[slice39],\n",
    "       train_regret_winner_7[slice39],\n",
    "       train_regret_winner_8[slice39],\n",
    "       train_regret_winner_9[slice39],\n",
    "       train_regret_winner_10[slice39],\n",
    "       train_regret_winner_11[slice39],\n",
    "       train_regret_winner_12[slice39],\n",
    "       train_regret_winner_13[slice39],\n",
    "       train_regret_winner_14[slice39],\n",
    "       train_regret_winner_15[slice39],\n",
    "       train_regret_winner_16[slice39],\n",
    "       train_regret_winner_17[slice39],\n",
    "       train_regret_winner_18[slice39],\n",
    "       train_regret_winner_19[slice39],\n",
    "       train_regret_winner_20[slice39]]\n",
    "\n",
    "loser39_results = pd.DataFrame(loser39).sort_values(by=[0], ascending=False)\n",
    "winner39_results = pd.DataFrame(winner39).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser39 = np.asarray(loser39_results[4:5][0])[0]\n",
    "median_loser39 = np.asarray(loser39_results[9:10][0])[0]\n",
    "upper_loser39 = np.asarray(loser39_results[14:15][0])[0]\n",
    "\n",
    "lower_winner39 = np.asarray(winner39_results[4:5][0])[0]\n",
    "median_winner39 = np.asarray(winner39_results[9:10][0])[0]\n",
    "upper_winner39 = np.asarray(winner39_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration49 :\n",
    "\n",
    "slice49 = 48\n",
    "\n",
    "loser49 = [train_regret_loser_1[slice49],\n",
    "       train_regret_loser_2[slice49],\n",
    "       train_regret_loser_3[slice49],\n",
    "       train_regret_loser_4[slice49],\n",
    "       train_regret_loser_5[slice49],\n",
    "       train_regret_loser_6[slice49],\n",
    "       train_regret_loser_7[slice49],\n",
    "       train_regret_loser_8[slice49],\n",
    "       train_regret_loser_9[slice49],\n",
    "       train_regret_loser_10[slice49],\n",
    "       train_regret_loser_11[slice49],\n",
    "       train_regret_loser_12[slice49],\n",
    "       train_regret_loser_13[slice49],\n",
    "       train_regret_loser_14[slice49],\n",
    "       train_regret_loser_15[slice49],\n",
    "       train_regret_loser_16[slice49],\n",
    "       train_regret_loser_17[slice49],\n",
    "       train_regret_loser_18[slice49],\n",
    "       train_regret_loser_19[slice49],\n",
    "       train_regret_loser_20[slice49]]\n",
    "\n",
    "winner49 = [train_regret_winner_1[slice49],\n",
    "       train_regret_winner_2[slice49],\n",
    "       train_regret_winner_3[slice49],\n",
    "       train_regret_winner_4[slice49],\n",
    "       train_regret_winner_5[slice49],\n",
    "       train_regret_winner_6[slice49],\n",
    "       train_regret_winner_7[slice49],\n",
    "       train_regret_winner_8[slice49],\n",
    "       train_regret_winner_9[slice49],\n",
    "       train_regret_winner_10[slice49],\n",
    "       train_regret_winner_11[slice49],\n",
    "       train_regret_winner_12[slice49],\n",
    "       train_regret_winner_13[slice49],\n",
    "       train_regret_winner_14[slice49],\n",
    "       train_regret_winner_15[slice49],\n",
    "       train_regret_winner_16[slice49],\n",
    "       train_regret_winner_17[slice49],\n",
    "       train_regret_winner_18[slice49],\n",
    "       train_regret_winner_19[slice49],\n",
    "       train_regret_winner_20[slice49]]\n",
    "\n",
    "loser49_results = pd.DataFrame(loser49).sort_values(by=[0], ascending=False)\n",
    "winner49_results = pd.DataFrame(winner49).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser49 = np.asarray(loser49_results[4:5][0])[0]\n",
    "median_loser49 = np.asarray(loser49_results[9:10][0])[0]\n",
    "upper_loser49 = np.asarray(loser49_results[14:15][0])[0]\n",
    "\n",
    "lower_winner49 = np.asarray(winner49_results[4:5][0])[0]\n",
    "median_winner49 = np.asarray(winner49_results[9:10][0])[0]\n",
    "upper_winner49 = np.asarray(winner49_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration59 :\n",
    "\n",
    "slice59 = 58\n",
    "\n",
    "loser59 = [train_regret_loser_1[slice59],\n",
    "       train_regret_loser_2[slice59],\n",
    "       train_regret_loser_3[slice59],\n",
    "       train_regret_loser_4[slice59],\n",
    "       train_regret_loser_5[slice59],\n",
    "       train_regret_loser_6[slice59],\n",
    "       train_regret_loser_7[slice59],\n",
    "       train_regret_loser_8[slice59],\n",
    "       train_regret_loser_9[slice59],\n",
    "       train_regret_loser_10[slice59],\n",
    "       train_regret_loser_11[slice59],\n",
    "       train_regret_loser_12[slice59],\n",
    "       train_regret_loser_13[slice59],\n",
    "       train_regret_loser_14[slice59],\n",
    "       train_regret_loser_15[slice59],\n",
    "       train_regret_loser_16[slice59],\n",
    "       train_regret_loser_17[slice59],\n",
    "       train_regret_loser_18[slice59],\n",
    "       train_regret_loser_19[slice59],\n",
    "       train_regret_loser_20[slice59]]\n",
    "\n",
    "winner59 = [train_regret_winner_1[slice59],\n",
    "       train_regret_winner_2[slice59],\n",
    "       train_regret_winner_3[slice59],\n",
    "       train_regret_winner_4[slice59],\n",
    "       train_regret_winner_5[slice59],\n",
    "       train_regret_winner_6[slice59],\n",
    "       train_regret_winner_7[slice59],\n",
    "       train_regret_winner_8[slice59],\n",
    "       train_regret_winner_9[slice59],\n",
    "       train_regret_winner_10[slice59],\n",
    "       train_regret_winner_11[slice59],\n",
    "       train_regret_winner_12[slice59],\n",
    "       train_regret_winner_13[slice59],\n",
    "       train_regret_winner_14[slice59],\n",
    "       train_regret_winner_15[slice59],\n",
    "       train_regret_winner_16[slice59],\n",
    "       train_regret_winner_17[slice59],\n",
    "       train_regret_winner_18[slice59],\n",
    "       train_regret_winner_19[slice59],\n",
    "       train_regret_winner_20[slice59]]\n",
    "\n",
    "loser59_results = pd.DataFrame(loser59).sort_values(by=[0], ascending=False)\n",
    "winner59_results = pd.DataFrame(winner59).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser59 = np.asarray(loser59_results[4:5][0])[0]\n",
    "median_loser59 = np.asarray(loser59_results[9:10][0])[0]\n",
    "upper_loser59 = np.asarray(loser59_results[14:15][0])[0]\n",
    "\n",
    "lower_winner59 = np.asarray(winner59_results[4:5][0])[0]\n",
    "median_winner59 = np.asarray(winner59_results[9:10][0])[0]\n",
    "upper_winner59 = np.asarray(winner59_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration69 :\n",
    "\n",
    "slice69 = 68\n",
    "\n",
    "loser69 = [train_regret_loser_1[slice69],\n",
    "       train_regret_loser_2[slice69],\n",
    "       train_regret_loser_3[slice69],\n",
    "       train_regret_loser_4[slice69],\n",
    "       train_regret_loser_5[slice69],\n",
    "       train_regret_loser_6[slice69],\n",
    "       train_regret_loser_7[slice69],\n",
    "       train_regret_loser_8[slice69],\n",
    "       train_regret_loser_9[slice69],\n",
    "       train_regret_loser_10[slice69],\n",
    "       train_regret_loser_11[slice69],\n",
    "       train_regret_loser_12[slice69],\n",
    "       train_regret_loser_13[slice69],\n",
    "       train_regret_loser_14[slice69],\n",
    "       train_regret_loser_15[slice69],\n",
    "       train_regret_loser_16[slice69],\n",
    "       train_regret_loser_17[slice69],\n",
    "       train_regret_loser_18[slice69],\n",
    "       train_regret_loser_19[slice69],\n",
    "       train_regret_loser_20[slice69]]\n",
    "\n",
    "winner69 = [train_regret_winner_1[slice69],\n",
    "       train_regret_winner_2[slice69],\n",
    "       train_regret_winner_3[slice69],\n",
    "       train_regret_winner_4[slice69],\n",
    "       train_regret_winner_5[slice69],\n",
    "       train_regret_winner_6[slice69],\n",
    "       train_regret_winner_7[slice69],\n",
    "       train_regret_winner_8[slice69],\n",
    "       train_regret_winner_9[slice69],\n",
    "       train_regret_winner_10[slice69],\n",
    "       train_regret_winner_11[slice69],\n",
    "       train_regret_winner_12[slice69],\n",
    "       train_regret_winner_13[slice69],\n",
    "       train_regret_winner_14[slice69],\n",
    "       train_regret_winner_15[slice69],\n",
    "       train_regret_winner_16[slice69],\n",
    "       train_regret_winner_17[slice69],\n",
    "       train_regret_winner_18[slice69],\n",
    "       train_regret_winner_19[slice69],\n",
    "       train_regret_winner_20[slice69]]\n",
    "\n",
    "loser69_results = pd.DataFrame(loser69).sort_values(by=[0], ascending=False)\n",
    "winner69_results = pd.DataFrame(winner69).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser69 = np.asarray(loser69_results[4:5][0])[0]\n",
    "median_loser69 = np.asarray(loser69_results[9:10][0])[0]\n",
    "upper_loser69 = np.asarray(loser69_results[14:15][0])[0]\n",
    "\n",
    "lower_winner69 = np.asarray(winner69_results[4:5][0])[0]\n",
    "median_winner69 = np.asarray(winner69_results[9:10][0])[0]\n",
    "upper_winner69 = np.asarray(winner69_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration79 :\n",
    "\n",
    "slice79 = 78\n",
    "\n",
    "loser79 = [train_regret_loser_1[slice79],\n",
    "       train_regret_loser_2[slice79],\n",
    "       train_regret_loser_3[slice79],\n",
    "       train_regret_loser_4[slice79],\n",
    "       train_regret_loser_5[slice79],\n",
    "       train_regret_loser_6[slice79],\n",
    "       train_regret_loser_7[slice79],\n",
    "       train_regret_loser_8[slice79],\n",
    "       train_regret_loser_9[slice79],\n",
    "       train_regret_loser_10[slice79],\n",
    "       train_regret_loser_11[slice79],\n",
    "       train_regret_loser_12[slice79],\n",
    "       train_regret_loser_13[slice79],\n",
    "       train_regret_loser_14[slice79],\n",
    "       train_regret_loser_15[slice79],\n",
    "       train_regret_loser_16[slice79],\n",
    "       train_regret_loser_17[slice79],\n",
    "       train_regret_loser_18[slice79],\n",
    "       train_regret_loser_19[slice79],\n",
    "       train_regret_loser_20[slice79]]\n",
    "\n",
    "winner79 = [train_regret_winner_1[slice79],\n",
    "       train_regret_winner_2[slice79],\n",
    "       train_regret_winner_3[slice79],\n",
    "       train_regret_winner_4[slice79],\n",
    "       train_regret_winner_5[slice79],\n",
    "       train_regret_winner_6[slice79],\n",
    "       train_regret_winner_7[slice79],\n",
    "       train_regret_winner_8[slice79],\n",
    "       train_regret_winner_9[slice79],\n",
    "       train_regret_winner_10[slice79],\n",
    "       train_regret_winner_11[slice79],\n",
    "       train_regret_winner_12[slice79],\n",
    "       train_regret_winner_13[slice79],\n",
    "       train_regret_winner_14[slice79],\n",
    "       train_regret_winner_15[slice79],\n",
    "       train_regret_winner_16[slice79],\n",
    "       train_regret_winner_17[slice79],\n",
    "       train_regret_winner_18[slice79],\n",
    "       train_regret_winner_19[slice79],\n",
    "       train_regret_winner_20[slice79]]\n",
    "\n",
    "loser79_results = pd.DataFrame(loser79).sort_values(by=[0], ascending=False)\n",
    "winner79_results = pd.DataFrame(winner79).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser79 = np.asarray(loser79_results[4:5][0])[0]\n",
    "median_loser79 = np.asarray(loser79_results[9:10][0])[0]\n",
    "upper_loser79 = np.asarray(loser79_results[14:15][0])[0]\n",
    "\n",
    "lower_winner79 = np.asarray(winner79_results[4:5][0])[0]\n",
    "median_winner79 = np.asarray(winner79_results[9:10][0])[0]\n",
    "upper_winner79 = np.asarray(winner79_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration89 :\n",
    "\n",
    "slice89 = 88\n",
    "\n",
    "loser89 = [train_regret_loser_1[slice89],\n",
    "       train_regret_loser_2[slice89],\n",
    "       train_regret_loser_3[slice89],\n",
    "       train_regret_loser_4[slice89],\n",
    "       train_regret_loser_5[slice89],\n",
    "       train_regret_loser_6[slice89],\n",
    "       train_regret_loser_7[slice89],\n",
    "       train_regret_loser_8[slice89],\n",
    "       train_regret_loser_9[slice89],\n",
    "       train_regret_loser_10[slice89],\n",
    "       train_regret_loser_11[slice89],\n",
    "       train_regret_loser_12[slice89],\n",
    "       train_regret_loser_13[slice89],\n",
    "       train_regret_loser_14[slice89],\n",
    "       train_regret_loser_15[slice89],\n",
    "       train_regret_loser_16[slice89],\n",
    "       train_regret_loser_17[slice89],\n",
    "       train_regret_loser_18[slice89],\n",
    "       train_regret_loser_19[slice89],\n",
    "       train_regret_loser_20[slice89]]\n",
    "\n",
    "winner89 = [train_regret_winner_1[slice89],\n",
    "       train_regret_winner_2[slice89],\n",
    "       train_regret_winner_3[slice89],\n",
    "       train_regret_winner_4[slice89],\n",
    "       train_regret_winner_5[slice89],\n",
    "       train_regret_winner_6[slice89],\n",
    "       train_regret_winner_7[slice89],\n",
    "       train_regret_winner_8[slice89],\n",
    "       train_regret_winner_9[slice89],\n",
    "       train_regret_winner_10[slice89],\n",
    "       train_regret_winner_11[slice89],\n",
    "       train_regret_winner_12[slice89],\n",
    "       train_regret_winner_13[slice89],\n",
    "       train_regret_winner_14[slice89],\n",
    "       train_regret_winner_15[slice89],\n",
    "       train_regret_winner_16[slice89],\n",
    "       train_regret_winner_17[slice89],\n",
    "       train_regret_winner_18[slice89],\n",
    "       train_regret_winner_19[slice89],\n",
    "       train_regret_winner_20[slice89]]\n",
    "\n",
    "loser89_results = pd.DataFrame(loser89).sort_values(by=[0], ascending=False)\n",
    "winner89_results = pd.DataFrame(winner89).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser89 = np.asarray(loser89_results[4:5][0])[0]\n",
    "median_loser89 = np.asarray(loser89_results[9:10][0])[0]\n",
    "upper_loser89 = np.asarray(loser89_results[14:15][0])[0]\n",
    "\n",
    "lower_winner89 = np.asarray(winner89_results[4:5][0])[0]\n",
    "median_winner89 = np.asarray(winner89_results[9:10][0])[0]\n",
    "upper_winner89 = np.asarray(winner89_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.3026705103183764, -3.772524122829952)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iteration99 :\n",
    "\n",
    "slice99 = 98\n",
    "\n",
    "loser99 = [train_regret_loser_1[slice99],\n",
    "       train_regret_loser_2[slice99],\n",
    "       train_regret_loser_3[slice99],\n",
    "       train_regret_loser_4[slice99],\n",
    "       train_regret_loser_5[slice99],\n",
    "       train_regret_loser_6[slice99],\n",
    "       train_regret_loser_7[slice99],\n",
    "       train_regret_loser_8[slice99],\n",
    "       train_regret_loser_9[slice99],\n",
    "       train_regret_loser_10[slice99],\n",
    "       train_regret_loser_11[slice99],\n",
    "       train_regret_loser_12[slice99],\n",
    "       train_regret_loser_13[slice99],\n",
    "       train_regret_loser_14[slice99],\n",
    "       train_regret_loser_15[slice99],\n",
    "       train_regret_loser_16[slice99],\n",
    "       train_regret_loser_17[slice99],\n",
    "       train_regret_loser_18[slice99],\n",
    "       train_regret_loser_19[slice99],\n",
    "       train_regret_loser_20[slice99]]\n",
    "\n",
    "winner99 = [train_regret_winner_1[slice99],\n",
    "       train_regret_winner_2[slice99],\n",
    "       train_regret_winner_3[slice99],\n",
    "       train_regret_winner_4[slice99],\n",
    "       train_regret_winner_5[slice99],\n",
    "       train_regret_winner_6[slice99],\n",
    "       train_regret_winner_7[slice99],\n",
    "       train_regret_winner_8[slice99],\n",
    "       train_regret_winner_9[slice99],\n",
    "       train_regret_winner_10[slice99],\n",
    "       train_regret_winner_11[slice99],\n",
    "       train_regret_winner_12[slice99],\n",
    "       train_regret_winner_13[slice99],\n",
    "       train_regret_winner_14[slice99],\n",
    "       train_regret_winner_15[slice99],\n",
    "       train_regret_winner_16[slice99],\n",
    "       train_regret_winner_17[slice99],\n",
    "       train_regret_winner_18[slice99],\n",
    "       train_regret_winner_19[slice99],\n",
    "       train_regret_winner_20[slice99]]\n",
    "\n",
    "loser99_results = pd.DataFrame(loser99).sort_values(by=[0], ascending=False)\n",
    "winner99_results = pd.DataFrame(winner99).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser99 = np.asarray(loser99_results[4:5][0])[0]\n",
    "median_loser99 = np.asarray(loser99_results[9:10][0])[0]\n",
    "upper_loser99 = np.asarray(loser99_results[14:15][0])[0]\n",
    "\n",
    "lower_winner99 = np.asarray(winner99_results[4:5][0])[0]\n",
    "median_winner99 = np.asarray(winner99_results[9:10][0])[0]\n",
    "upper_winner99 = np.asarray(winner99_results[14:15][0])[0]\n",
    "\n",
    "lower_loser99, lower_winner99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration10 :\n",
    "\n",
    "slice10 = 9\n",
    "\n",
    "loser10 = [train_regret_loser_1[slice10],\n",
    "       train_regret_loser_2[slice10],\n",
    "       train_regret_loser_3[slice10],\n",
    "       train_regret_loser_4[slice10],\n",
    "       train_regret_loser_5[slice10],\n",
    "       train_regret_loser_6[slice10],\n",
    "       train_regret_loser_7[slice10],\n",
    "       train_regret_loser_8[slice10],\n",
    "       train_regret_loser_9[slice10],\n",
    "       train_regret_loser_10[slice10],\n",
    "       train_regret_loser_11[slice10],\n",
    "       train_regret_loser_12[slice10],\n",
    "       train_regret_loser_13[slice10],\n",
    "       train_regret_loser_14[slice10],\n",
    "       train_regret_loser_15[slice10],\n",
    "       train_regret_loser_16[slice10],\n",
    "       train_regret_loser_17[slice10],\n",
    "       train_regret_loser_18[slice10],\n",
    "       train_regret_loser_19[slice10],\n",
    "       train_regret_loser_20[slice10]]\n",
    "\n",
    "winner10 = [train_regret_winner_1[slice10],\n",
    "       train_regret_winner_2[slice10],\n",
    "       train_regret_winner_3[slice10],\n",
    "       train_regret_winner_4[slice10],\n",
    "       train_regret_winner_5[slice10],\n",
    "       train_regret_winner_6[slice10],\n",
    "       train_regret_winner_7[slice10],\n",
    "       train_regret_winner_8[slice10],\n",
    "       train_regret_winner_9[slice10],\n",
    "       train_regret_winner_10[slice10],\n",
    "       train_regret_winner_11[slice10],\n",
    "       train_regret_winner_12[slice10],\n",
    "       train_regret_winner_13[slice10],\n",
    "       train_regret_winner_14[slice10],\n",
    "       train_regret_winner_15[slice10],\n",
    "       train_regret_winner_16[slice10],\n",
    "       train_regret_winner_17[slice10],\n",
    "       train_regret_winner_18[slice10],\n",
    "       train_regret_winner_19[slice10],\n",
    "       train_regret_winner_20[slice10]]\n",
    "\n",
    "loser10_results = pd.DataFrame(loser10).sort_values(by=[0], ascending=False)\n",
    "winner10_results = pd.DataFrame(winner10).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser10 = np.asarray(loser10_results[4:5][0])[0]\n",
    "median_loser10 = np.asarray(loser10_results[9:10][0])[0]\n",
    "upper_loser10 = np.asarray(loser10_results[14:15][0])[0]\n",
    "\n",
    "lower_winner10 = np.asarray(winner10_results[4:5][0])[0]\n",
    "median_winner10 = np.asarray(winner10_results[9:10][0])[0]\n",
    "upper_winner10 = np.asarray(winner10_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration20 :\n",
    "\n",
    "slice20 = 19\n",
    "\n",
    "loser20 = [train_regret_loser_1[slice20],\n",
    "       train_regret_loser_2[slice20],\n",
    "       train_regret_loser_3[slice20],\n",
    "       train_regret_loser_4[slice20],\n",
    "       train_regret_loser_5[slice20],\n",
    "       train_regret_loser_6[slice20],\n",
    "       train_regret_loser_7[slice20],\n",
    "       train_regret_loser_8[slice20],\n",
    "       train_regret_loser_9[slice20],\n",
    "       train_regret_loser_10[slice20],\n",
    "       train_regret_loser_11[slice20],\n",
    "       train_regret_loser_12[slice20],\n",
    "       train_regret_loser_13[slice20],\n",
    "       train_regret_loser_14[slice20],\n",
    "       train_regret_loser_15[slice20],\n",
    "       train_regret_loser_16[slice20],\n",
    "       train_regret_loser_17[slice20],\n",
    "       train_regret_loser_18[slice20],\n",
    "       train_regret_loser_19[slice20],\n",
    "       train_regret_loser_20[slice20]]\n",
    "\n",
    "winner20 = [train_regret_winner_1[slice20],\n",
    "       train_regret_winner_2[slice20],\n",
    "       train_regret_winner_3[slice20],\n",
    "       train_regret_winner_4[slice20],\n",
    "       train_regret_winner_5[slice20],\n",
    "       train_regret_winner_6[slice20],\n",
    "       train_regret_winner_7[slice20],\n",
    "       train_regret_winner_8[slice20],\n",
    "       train_regret_winner_9[slice20],\n",
    "       train_regret_winner_10[slice20],\n",
    "       train_regret_winner_11[slice20],\n",
    "       train_regret_winner_12[slice20],\n",
    "       train_regret_winner_13[slice20],\n",
    "       train_regret_winner_14[slice20],\n",
    "       train_regret_winner_15[slice20],\n",
    "       train_regret_winner_16[slice20],\n",
    "       train_regret_winner_17[slice20],\n",
    "       train_regret_winner_18[slice20],\n",
    "       train_regret_winner_19[slice20],\n",
    "       train_regret_winner_20[slice20]]\n",
    "\n",
    "loser20_results = pd.DataFrame(loser20).sort_values(by=[0], ascending=False)\n",
    "winner20_results = pd.DataFrame(winner20).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser20 = np.asarray(loser20_results[4:5][0])[0]\n",
    "median_loser20 = np.asarray(loser20_results[9:10][0])[0]\n",
    "upper_loser20 = np.asarray(loser20_results[14:15][0])[0]\n",
    "\n",
    "lower_winner20 = np.asarray(winner20_results[4:5][0])[0]\n",
    "median_winner20 = np.asarray(winner20_results[9:10][0])[0]\n",
    "upper_winner20 = np.asarray(winner20_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration30 :\n",
    "\n",
    "slice30 = 29\n",
    "\n",
    "loser30 = [train_regret_loser_1[slice30],\n",
    "       train_regret_loser_2[slice30],\n",
    "       train_regret_loser_3[slice30],\n",
    "       train_regret_loser_4[slice30],\n",
    "       train_regret_loser_5[slice30],\n",
    "       train_regret_loser_6[slice30],\n",
    "       train_regret_loser_7[slice30],\n",
    "       train_regret_loser_8[slice30],\n",
    "       train_regret_loser_9[slice30],\n",
    "       train_regret_loser_10[slice30],\n",
    "       train_regret_loser_11[slice30],\n",
    "       train_regret_loser_12[slice30],\n",
    "       train_regret_loser_13[slice30],\n",
    "       train_regret_loser_14[slice30],\n",
    "       train_regret_loser_15[slice30],\n",
    "       train_regret_loser_16[slice30],\n",
    "       train_regret_loser_17[slice30],\n",
    "       train_regret_loser_18[slice30],\n",
    "       train_regret_loser_19[slice30],\n",
    "       train_regret_loser_20[slice30]]\n",
    "\n",
    "winner30 = [train_regret_winner_1[slice30],\n",
    "       train_regret_winner_2[slice30],\n",
    "       train_regret_winner_3[slice30],\n",
    "       train_regret_winner_4[slice30],\n",
    "       train_regret_winner_5[slice30],\n",
    "       train_regret_winner_6[slice30],\n",
    "       train_regret_winner_7[slice30],\n",
    "       train_regret_winner_8[slice30],\n",
    "       train_regret_winner_9[slice30],\n",
    "       train_regret_winner_10[slice30],\n",
    "       train_regret_winner_11[slice30],\n",
    "       train_regret_winner_12[slice30],\n",
    "       train_regret_winner_13[slice30],\n",
    "       train_regret_winner_14[slice30],\n",
    "       train_regret_winner_15[slice30],\n",
    "       train_regret_winner_16[slice30],\n",
    "       train_regret_winner_17[slice30],\n",
    "       train_regret_winner_18[slice30],\n",
    "       train_regret_winner_19[slice30],\n",
    "       train_regret_winner_20[slice30]]\n",
    "\n",
    "loser30_results = pd.DataFrame(loser30).sort_values(by=[0], ascending=False)\n",
    "winner30_results = pd.DataFrame(winner30).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser30 = np.asarray(loser30_results[4:5][0])[0]\n",
    "median_loser30 = np.asarray(loser30_results[9:10][0])[0]\n",
    "upper_loser30 = np.asarray(loser30_results[14:15][0])[0]\n",
    "\n",
    "lower_winner30 = np.asarray(winner30_results[4:5][0])[0]\n",
    "median_winner30 = np.asarray(winner30_results[9:10][0])[0]\n",
    "upper_winner30 = np.asarray(winner30_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration40 :\n",
    "\n",
    "slice40 = 39\n",
    "\n",
    "loser40 = [train_regret_loser_1[slice40],\n",
    "       train_regret_loser_2[slice40],\n",
    "       train_regret_loser_3[slice40],\n",
    "       train_regret_loser_4[slice40],\n",
    "       train_regret_loser_5[slice40],\n",
    "       train_regret_loser_6[slice40],\n",
    "       train_regret_loser_7[slice40],\n",
    "       train_regret_loser_8[slice40],\n",
    "       train_regret_loser_9[slice40],\n",
    "       train_regret_loser_10[slice40],\n",
    "       train_regret_loser_11[slice40],\n",
    "       train_regret_loser_12[slice40],\n",
    "       train_regret_loser_13[slice40],\n",
    "       train_regret_loser_14[slice40],\n",
    "       train_regret_loser_15[slice40],\n",
    "       train_regret_loser_16[slice40],\n",
    "       train_regret_loser_17[slice40],\n",
    "       train_regret_loser_18[slice40],\n",
    "       train_regret_loser_19[slice40],\n",
    "       train_regret_loser_20[slice40]]\n",
    "\n",
    "winner40 = [train_regret_winner_1[slice40],\n",
    "       train_regret_winner_2[slice40],\n",
    "       train_regret_winner_3[slice40],\n",
    "       train_regret_winner_4[slice40],\n",
    "       train_regret_winner_5[slice40],\n",
    "       train_regret_winner_6[slice40],\n",
    "       train_regret_winner_7[slice40],\n",
    "       train_regret_winner_8[slice40],\n",
    "       train_regret_winner_9[slice40],\n",
    "       train_regret_winner_10[slice40],\n",
    "       train_regret_winner_11[slice40],\n",
    "       train_regret_winner_12[slice40],\n",
    "       train_regret_winner_13[slice40],\n",
    "       train_regret_winner_14[slice40],\n",
    "       train_regret_winner_15[slice40],\n",
    "       train_regret_winner_16[slice40],\n",
    "       train_regret_winner_17[slice40],\n",
    "       train_regret_winner_18[slice40],\n",
    "       train_regret_winner_19[slice40],\n",
    "       train_regret_winner_20[slice40]]\n",
    "\n",
    "loser40_results = pd.DataFrame(loser40).sort_values(by=[0], ascending=False)\n",
    "winner40_results = pd.DataFrame(winner40).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser40 = np.asarray(loser40_results[4:5][0])[0]\n",
    "median_loser40 = np.asarray(loser40_results[9:10][0])[0]\n",
    "upper_loser40 = np.asarray(loser40_results[14:15][0])[0]\n",
    "\n",
    "lower_winner40 = np.asarray(winner40_results[4:5][0])[0]\n",
    "median_winner40 = np.asarray(winner40_results[9:10][0])[0]\n",
    "upper_winner40 = np.asarray(winner40_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration50 :\n",
    "\n",
    "slice50 = 49\n",
    "\n",
    "loser50 = [train_regret_loser_1[slice50],\n",
    "       train_regret_loser_2[slice50],\n",
    "       train_regret_loser_3[slice50],\n",
    "       train_regret_loser_4[slice50],\n",
    "       train_regret_loser_5[slice50],\n",
    "       train_regret_loser_6[slice50],\n",
    "       train_regret_loser_7[slice50],\n",
    "       train_regret_loser_8[slice50],\n",
    "       train_regret_loser_9[slice50],\n",
    "       train_regret_loser_10[slice50],\n",
    "       train_regret_loser_11[slice50],\n",
    "       train_regret_loser_12[slice50],\n",
    "       train_regret_loser_13[slice50],\n",
    "       train_regret_loser_14[slice50],\n",
    "       train_regret_loser_15[slice50],\n",
    "       train_regret_loser_16[slice50],\n",
    "       train_regret_loser_17[slice50],\n",
    "       train_regret_loser_18[slice50],\n",
    "       train_regret_loser_19[slice50],\n",
    "       train_regret_loser_20[slice50]]\n",
    "\n",
    "winner50 = [train_regret_winner_1[slice50],\n",
    "       train_regret_winner_2[slice50],\n",
    "       train_regret_winner_3[slice50],\n",
    "       train_regret_winner_4[slice50],\n",
    "       train_regret_winner_5[slice50],\n",
    "       train_regret_winner_6[slice50],\n",
    "       train_regret_winner_7[slice50],\n",
    "       train_regret_winner_8[slice50],\n",
    "       train_regret_winner_9[slice50],\n",
    "       train_regret_winner_10[slice50],\n",
    "       train_regret_winner_11[slice50],\n",
    "       train_regret_winner_12[slice50],\n",
    "       train_regret_winner_13[slice50],\n",
    "       train_regret_winner_14[slice50],\n",
    "       train_regret_winner_15[slice50],\n",
    "       train_regret_winner_16[slice50],\n",
    "       train_regret_winner_17[slice50],\n",
    "       train_regret_winner_18[slice50],\n",
    "       train_regret_winner_19[slice50],\n",
    "       train_regret_winner_20[slice50]]\n",
    "\n",
    "loser50_results = pd.DataFrame(loser50).sort_values(by=[0], ascending=False)\n",
    "winner50_results = pd.DataFrame(winner50).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser50 = np.asarray(loser50_results[4:5][0])[0]\n",
    "median_loser50 = np.asarray(loser50_results[9:10][0])[0]\n",
    "upper_loser50 = np.asarray(loser50_results[14:15][0])[0]\n",
    "\n",
    "lower_winner50 = np.asarray(winner50_results[4:5][0])[0]\n",
    "median_winner50 = np.asarray(winner50_results[9:10][0])[0]\n",
    "upper_winner50 = np.asarray(winner50_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration60 :\n",
    "\n",
    "slice60 = 59\n",
    "\n",
    "loser60 = [train_regret_loser_1[slice60],\n",
    "       train_regret_loser_2[slice60],\n",
    "       train_regret_loser_3[slice60],\n",
    "       train_regret_loser_4[slice60],\n",
    "       train_regret_loser_5[slice60],\n",
    "       train_regret_loser_6[slice60],\n",
    "       train_regret_loser_7[slice60],\n",
    "       train_regret_loser_8[slice60],\n",
    "       train_regret_loser_9[slice60],\n",
    "       train_regret_loser_10[slice60],\n",
    "       train_regret_loser_11[slice60],\n",
    "       train_regret_loser_12[slice60],\n",
    "       train_regret_loser_13[slice60],\n",
    "       train_regret_loser_14[slice60],\n",
    "       train_regret_loser_15[slice60],\n",
    "       train_regret_loser_16[slice60],\n",
    "       train_regret_loser_17[slice60],\n",
    "       train_regret_loser_18[slice60],\n",
    "       train_regret_loser_19[slice60],\n",
    "       train_regret_loser_20[slice60]]\n",
    "\n",
    "winner60 = [train_regret_winner_1[slice60],\n",
    "       train_regret_winner_2[slice60],\n",
    "       train_regret_winner_3[slice60],\n",
    "       train_regret_winner_4[slice60],\n",
    "       train_regret_winner_5[slice60],\n",
    "       train_regret_winner_6[slice60],\n",
    "       train_regret_winner_7[slice60],\n",
    "       train_regret_winner_8[slice60],\n",
    "       train_regret_winner_9[slice60],\n",
    "       train_regret_winner_10[slice60],\n",
    "       train_regret_winner_11[slice60],\n",
    "       train_regret_winner_12[slice60],\n",
    "       train_regret_winner_13[slice60],\n",
    "       train_regret_winner_14[slice60],\n",
    "       train_regret_winner_15[slice60],\n",
    "       train_regret_winner_16[slice60],\n",
    "       train_regret_winner_17[slice60],\n",
    "       train_regret_winner_18[slice60],\n",
    "       train_regret_winner_19[slice60],\n",
    "       train_regret_winner_20[slice60]]\n",
    "\n",
    "loser60_results = pd.DataFrame(loser60).sort_values(by=[0], ascending=False)\n",
    "winner60_results = pd.DataFrame(winner60).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser60 = np.asarray(loser60_results[4:5][0])[0]\n",
    "median_loser60 = np.asarray(loser60_results[9:10][0])[0]\n",
    "upper_loser60 = np.asarray(loser60_results[14:15][0])[0]\n",
    "\n",
    "lower_winner60 = np.asarray(winner60_results[4:5][0])[0]\n",
    "median_winner60 = np.asarray(winner60_results[9:10][0])[0]\n",
    "upper_winner60 = np.asarray(winner60_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration70 :\n",
    "\n",
    "slice70 = 69\n",
    "\n",
    "loser70 = [train_regret_loser_1[slice70],\n",
    "       train_regret_loser_2[slice70],\n",
    "       train_regret_loser_3[slice70],\n",
    "       train_regret_loser_4[slice70],\n",
    "       train_regret_loser_5[slice70],\n",
    "       train_regret_loser_6[slice70],\n",
    "       train_regret_loser_7[slice70],\n",
    "       train_regret_loser_8[slice70],\n",
    "       train_regret_loser_9[slice70],\n",
    "       train_regret_loser_10[slice70],\n",
    "       train_regret_loser_11[slice70],\n",
    "       train_regret_loser_12[slice70],\n",
    "       train_regret_loser_13[slice70],\n",
    "       train_regret_loser_14[slice70],\n",
    "       train_regret_loser_15[slice70],\n",
    "       train_regret_loser_16[slice70],\n",
    "       train_regret_loser_17[slice70],\n",
    "       train_regret_loser_18[slice70],\n",
    "       train_regret_loser_19[slice70],\n",
    "       train_regret_loser_20[slice70]]\n",
    "\n",
    "winner70 = [train_regret_winner_1[slice70],\n",
    "       train_regret_winner_2[slice70],\n",
    "       train_regret_winner_3[slice70],\n",
    "       train_regret_winner_4[slice70],\n",
    "       train_regret_winner_5[slice70],\n",
    "       train_regret_winner_6[slice70],\n",
    "       train_regret_winner_7[slice70],\n",
    "       train_regret_winner_8[slice70],\n",
    "       train_regret_winner_9[slice70],\n",
    "       train_regret_winner_10[slice70],\n",
    "       train_regret_winner_11[slice70],\n",
    "       train_regret_winner_12[slice70],\n",
    "       train_regret_winner_13[slice70],\n",
    "       train_regret_winner_14[slice70],\n",
    "       train_regret_winner_15[slice70],\n",
    "       train_regret_winner_16[slice70],\n",
    "       train_regret_winner_17[slice70],\n",
    "       train_regret_winner_18[slice70],\n",
    "       train_regret_winner_19[slice70],\n",
    "       train_regret_winner_20[slice70]]\n",
    "\n",
    "loser70_results = pd.DataFrame(loser70).sort_values(by=[0], ascending=False)\n",
    "winner70_results = pd.DataFrame(winner70).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser70 = np.asarray(loser70_results[4:5][0])[0]\n",
    "median_loser70 = np.asarray(loser70_results[9:10][0])[0]\n",
    "upper_loser70 = np.asarray(loser70_results[14:15][0])[0]\n",
    "\n",
    "lower_winner70 = np.asarray(winner70_results[4:5][0])[0]\n",
    "median_winner70 = np.asarray(winner70_results[9:10][0])[0]\n",
    "upper_winner70 = np.asarray(winner70_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration80 :\n",
    "\n",
    "slice80 = 79\n",
    "\n",
    "loser80 = [train_regret_loser_1[slice80],\n",
    "       train_regret_loser_2[slice80],\n",
    "       train_regret_loser_3[slice80],\n",
    "       train_regret_loser_4[slice80],\n",
    "       train_regret_loser_5[slice80],\n",
    "       train_regret_loser_6[slice80],\n",
    "       train_regret_loser_7[slice80],\n",
    "       train_regret_loser_8[slice80],\n",
    "       train_regret_loser_9[slice80],\n",
    "       train_regret_loser_10[slice80],\n",
    "       train_regret_loser_11[slice80],\n",
    "       train_regret_loser_12[slice80],\n",
    "       train_regret_loser_13[slice80],\n",
    "       train_regret_loser_14[slice80],\n",
    "       train_regret_loser_15[slice80],\n",
    "       train_regret_loser_16[slice80],\n",
    "       train_regret_loser_17[slice80],\n",
    "       train_regret_loser_18[slice80],\n",
    "       train_regret_loser_19[slice80],\n",
    "       train_regret_loser_20[slice80]]\n",
    "\n",
    "winner80 = [train_regret_winner_1[slice80],\n",
    "       train_regret_winner_2[slice80],\n",
    "       train_regret_winner_3[slice80],\n",
    "       train_regret_winner_4[slice80],\n",
    "       train_regret_winner_5[slice80],\n",
    "       train_regret_winner_6[slice80],\n",
    "       train_regret_winner_7[slice80],\n",
    "       train_regret_winner_8[slice80],\n",
    "       train_regret_winner_9[slice80],\n",
    "       train_regret_winner_10[slice80],\n",
    "       train_regret_winner_11[slice80],\n",
    "       train_regret_winner_12[slice80],\n",
    "       train_regret_winner_13[slice80],\n",
    "       train_regret_winner_14[slice80],\n",
    "       train_regret_winner_15[slice80],\n",
    "       train_regret_winner_16[slice80],\n",
    "       train_regret_winner_17[slice80],\n",
    "       train_regret_winner_18[slice80],\n",
    "       train_regret_winner_19[slice80],\n",
    "       train_regret_winner_20[slice80]]\n",
    "\n",
    "loser80_results = pd.DataFrame(loser80).sort_values(by=[0], ascending=False)\n",
    "winner80_results = pd.DataFrame(winner80).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser80 = np.asarray(loser80_results[4:5][0])[0]\n",
    "median_loser80 = np.asarray(loser80_results[9:10][0])[0]\n",
    "upper_loser80 = np.asarray(loser80_results[14:15][0])[0]\n",
    "\n",
    "lower_winner80 = np.asarray(winner80_results[4:5][0])[0]\n",
    "median_winner80 = np.asarray(winner80_results[9:10][0])[0]\n",
    "upper_winner80 = np.asarray(winner80_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration90 :\n",
    "\n",
    "slice90 = 89\n",
    "\n",
    "loser90 = [train_regret_loser_1[slice90],\n",
    "       train_regret_loser_2[slice90],\n",
    "       train_regret_loser_3[slice90],\n",
    "       train_regret_loser_4[slice90],\n",
    "       train_regret_loser_5[slice90],\n",
    "       train_regret_loser_6[slice90],\n",
    "       train_regret_loser_7[slice90],\n",
    "       train_regret_loser_8[slice90],\n",
    "       train_regret_loser_9[slice90],\n",
    "       train_regret_loser_10[slice90],\n",
    "       train_regret_loser_11[slice90],\n",
    "       train_regret_loser_12[slice90],\n",
    "       train_regret_loser_13[slice90],\n",
    "       train_regret_loser_14[slice90],\n",
    "       train_regret_loser_15[slice90],\n",
    "       train_regret_loser_16[slice90],\n",
    "       train_regret_loser_17[slice90],\n",
    "       train_regret_loser_18[slice90],\n",
    "       train_regret_loser_19[slice90],\n",
    "       train_regret_loser_20[slice90]]\n",
    "\n",
    "winner90 = [train_regret_winner_1[slice90],\n",
    "       train_regret_winner_2[slice90],\n",
    "       train_regret_winner_3[slice90],\n",
    "       train_regret_winner_4[slice90],\n",
    "       train_regret_winner_5[slice90],\n",
    "       train_regret_winner_6[slice90],\n",
    "       train_regret_winner_7[slice90],\n",
    "       train_regret_winner_8[slice90],\n",
    "       train_regret_winner_9[slice90],\n",
    "       train_regret_winner_10[slice90],\n",
    "       train_regret_winner_11[slice90],\n",
    "       train_regret_winner_12[slice90],\n",
    "       train_regret_winner_13[slice90],\n",
    "       train_regret_winner_14[slice90],\n",
    "       train_regret_winner_15[slice90],\n",
    "       train_regret_winner_16[slice90],\n",
    "       train_regret_winner_17[slice90],\n",
    "       train_regret_winner_18[slice90],\n",
    "       train_regret_winner_19[slice90],\n",
    "       train_regret_winner_20[slice90]]\n",
    "\n",
    "loser90_results = pd.DataFrame(loser90).sort_values(by=[0], ascending=False)\n",
    "winner90_results = pd.DataFrame(winner90).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser90 = np.asarray(loser90_results[4:5][0])[0]\n",
    "median_loser90 = np.asarray(loser90_results[9:10][0])[0]\n",
    "upper_loser90 = np.asarray(loser90_results[14:15][0])[0]\n",
    "\n",
    "lower_winner90 = np.asarray(winner90_results[4:5][0])[0]\n",
    "median_winner90 = np.asarray(winner90_results[9:10][0])[0]\n",
    "upper_winner90 = np.asarray(winner90_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration100 :\n",
    "\n",
    "slice100 = 99\n",
    "\n",
    "loser100 = [train_regret_loser_1[slice100],\n",
    "       train_regret_loser_2[slice100],\n",
    "       train_regret_loser_3[slice100],\n",
    "       train_regret_loser_4[slice100],\n",
    "       train_regret_loser_5[slice100],\n",
    "       train_regret_loser_6[slice100],\n",
    "       train_regret_loser_7[slice100],\n",
    "       train_regret_loser_8[slice100],\n",
    "       train_regret_loser_9[slice100],\n",
    "       train_regret_loser_10[slice100],\n",
    "       train_regret_loser_11[slice100],\n",
    "       train_regret_loser_12[slice100],\n",
    "       train_regret_loser_13[slice100],\n",
    "       train_regret_loser_14[slice100],\n",
    "       train_regret_loser_15[slice100],\n",
    "       train_regret_loser_16[slice100],\n",
    "       train_regret_loser_17[slice100],\n",
    "       train_regret_loser_18[slice100],\n",
    "       train_regret_loser_19[slice100],\n",
    "       train_regret_loser_20[slice100]]\n",
    "\n",
    "winner100 = [train_regret_winner_1[slice100],\n",
    "       train_regret_winner_2[slice100],\n",
    "       train_regret_winner_3[slice100],\n",
    "       train_regret_winner_4[slice100],\n",
    "       train_regret_winner_5[slice100],\n",
    "       train_regret_winner_6[slice100],\n",
    "       train_regret_winner_7[slice100],\n",
    "       train_regret_winner_8[slice100],\n",
    "       train_regret_winner_9[slice100],\n",
    "       train_regret_winner_10[slice100],\n",
    "       train_regret_winner_11[slice100],\n",
    "       train_regret_winner_12[slice100],\n",
    "       train_regret_winner_13[slice100],\n",
    "       train_regret_winner_14[slice100],\n",
    "       train_regret_winner_15[slice100],\n",
    "       train_regret_winner_16[slice100],\n",
    "       train_regret_winner_17[slice100],\n",
    "       train_regret_winner_18[slice100],\n",
    "       train_regret_winner_19[slice100],\n",
    "       train_regret_winner_20[slice100]]\n",
    "\n",
    "loser100_results = pd.DataFrame(loser100).sort_values(by=[0], ascending=False)\n",
    "winner100_results = pd.DataFrame(winner100).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser100 = np.asarray(loser100_results[4:5][0])[0]\n",
    "median_loser100 = np.asarray(loser100_results[9:10][0])[0]\n",
    "upper_loser100 = np.asarray(loser100_results[14:15][0])[0]\n",
    "\n",
    "lower_winner100 = np.asarray(winner100_results[4:5][0])[0]\n",
    "median_winner100 = np.asarray(winner100_results[9:10][0])[0]\n",
    "upper_winner100 = np.asarray(winner100_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summarise arrays: 'Loser'\n",
    "\n",
    "lower_loser = [lower_loser1,\n",
    "            lower_loser2,\n",
    "            lower_loser3,\n",
    "            lower_loser4,\n",
    "            lower_loser5,\n",
    "            lower_loser6,\n",
    "            lower_loser7,\n",
    "            lower_loser8,\n",
    "            lower_loser9,\n",
    "            lower_loser10,\n",
    "            lower_loser11,\n",
    "            lower_loser12,\n",
    "            lower_loser13,\n",
    "            lower_loser14,\n",
    "            lower_loser15,\n",
    "            lower_loser16,\n",
    "            lower_loser17,\n",
    "            lower_loser18,\n",
    "            lower_loser19,\n",
    "            lower_loser20,\n",
    "            lower_loser21,\n",
    "            lower_loser22,\n",
    "            lower_loser23,\n",
    "            lower_loser24,\n",
    "            lower_loser25,\n",
    "            lower_loser26,\n",
    "            lower_loser27,\n",
    "            lower_loser28,\n",
    "            lower_loser29,\n",
    "            lower_loser30,\n",
    "            lower_loser31,\n",
    "            lower_loser32,\n",
    "            lower_loser33,\n",
    "            lower_loser34,\n",
    "            lower_loser35,\n",
    "            lower_loser36,\n",
    "            lower_loser37,\n",
    "            lower_loser38,\n",
    "            lower_loser39,\n",
    "            lower_loser40,\n",
    "            lower_loser41,\n",
    "            lower_loser42,\n",
    "            lower_loser43,\n",
    "            lower_loser44,\n",
    "            lower_loser45,\n",
    "            lower_loser46,\n",
    "            lower_loser47,\n",
    "            lower_loser48,\n",
    "            lower_loser49,\n",
    "            lower_loser50,\n",
    "            lower_loser51,\n",
    "            lower_loser52,\n",
    "            lower_loser53,\n",
    "            lower_loser54,\n",
    "            lower_loser55,\n",
    "            lower_loser56,\n",
    "            lower_loser57,\n",
    "            lower_loser58,\n",
    "            lower_loser59,\n",
    "            lower_loser60,\n",
    "            lower_loser61,\n",
    "            lower_loser62,\n",
    "            lower_loser63,\n",
    "            lower_loser64,\n",
    "            lower_loser65,\n",
    "            lower_loser66,\n",
    "            lower_loser67,\n",
    "            lower_loser68,\n",
    "            lower_loser69,\n",
    "            lower_loser70,\n",
    "            lower_loser71,\n",
    "            lower_loser72,\n",
    "            lower_loser73,\n",
    "            lower_loser74,\n",
    "            lower_loser75,\n",
    "            lower_loser76,\n",
    "            lower_loser77,\n",
    "            lower_loser78,\n",
    "            lower_loser79,\n",
    "            lower_loser80,\n",
    "            lower_loser81,\n",
    "            lower_loser82,\n",
    "            lower_loser83,\n",
    "            lower_loser84,\n",
    "            lower_loser85,\n",
    "            lower_loser86,\n",
    "            lower_loser87,\n",
    "            lower_loser88,\n",
    "            lower_loser89,\n",
    "            lower_loser90,\n",
    "            lower_loser91,\n",
    "            lower_loser92,\n",
    "            lower_loser93,\n",
    "            lower_loser94,\n",
    "            lower_loser95,\n",
    "            lower_loser96,\n",
    "            lower_loser97,\n",
    "            lower_loser98,\n",
    "            lower_loser99,\n",
    "            lower_loser100,\n",
    "            lower_loser101]\n",
    "\n",
    "median_loser = [median_loser1,\n",
    "            median_loser2,\n",
    "            median_loser3,\n",
    "            median_loser4,\n",
    "            median_loser5,\n",
    "            median_loser6,\n",
    "            median_loser7,\n",
    "            median_loser8,\n",
    "            median_loser9,\n",
    "            median_loser10,\n",
    "            median_loser11,\n",
    "            median_loser12,\n",
    "            median_loser13,\n",
    "            median_loser14,\n",
    "            median_loser15,\n",
    "            median_loser16,\n",
    "            median_loser17,\n",
    "            median_loser18,\n",
    "            median_loser19,\n",
    "            median_loser20,\n",
    "            median_loser21,\n",
    "            median_loser22,\n",
    "            median_loser23,\n",
    "            median_loser24,\n",
    "            median_loser25,\n",
    "            median_loser26,\n",
    "            median_loser27,\n",
    "            median_loser28,\n",
    "            median_loser29,\n",
    "            median_loser30,\n",
    "            median_loser31,\n",
    "            median_loser32,\n",
    "            median_loser33,\n",
    "            median_loser34,\n",
    "            median_loser35,\n",
    "            median_loser36,\n",
    "            median_loser37,\n",
    "            median_loser38,\n",
    "            median_loser39,\n",
    "            median_loser40,\n",
    "            median_loser41,\n",
    "            median_loser42,\n",
    "            median_loser43,\n",
    "            median_loser44,\n",
    "            median_loser45,\n",
    "            median_loser46,\n",
    "            median_loser47,\n",
    "            median_loser48,\n",
    "            median_loser49,\n",
    "            median_loser50,\n",
    "            median_loser51,\n",
    "            median_loser52,\n",
    "            median_loser53,\n",
    "            median_loser54,\n",
    "            median_loser55,\n",
    "            median_loser56,\n",
    "            median_loser57,\n",
    "            median_loser58,\n",
    "            median_loser59,\n",
    "            median_loser60,\n",
    "            median_loser61,\n",
    "            median_loser62,\n",
    "            median_loser63,\n",
    "            median_loser64,\n",
    "            median_loser65,\n",
    "            median_loser66,\n",
    "            median_loser67,\n",
    "            median_loser68,\n",
    "            median_loser69,\n",
    "            median_loser70,\n",
    "            median_loser71,\n",
    "            median_loser72,\n",
    "            median_loser73,\n",
    "            median_loser74,\n",
    "            median_loser75,\n",
    "            median_loser76,\n",
    "            median_loser77,\n",
    "            median_loser78,\n",
    "            median_loser79,\n",
    "            median_loser80,\n",
    "            median_loser81,\n",
    "            median_loser82,\n",
    "            median_loser83,\n",
    "            median_loser84,\n",
    "            median_loser85,\n",
    "            median_loser86,\n",
    "            median_loser87,\n",
    "            median_loser88,\n",
    "            median_loser89,\n",
    "            median_loser90,\n",
    "            median_loser91,\n",
    "            median_loser92,\n",
    "            median_loser93,\n",
    "            median_loser94,\n",
    "            median_loser95,\n",
    "            median_loser96,\n",
    "            median_loser97,\n",
    "            median_loser98,\n",
    "            median_loser99,\n",
    "            median_loser100,\n",
    "            median_loser101]\n",
    "\n",
    "upper_loser = [upper_loser1,\n",
    "            upper_loser2,\n",
    "            upper_loser3,\n",
    "            upper_loser4,\n",
    "            upper_loser5,\n",
    "            upper_loser6,\n",
    "            upper_loser7,\n",
    "            upper_loser8,\n",
    "            upper_loser9,\n",
    "            upper_loser10,\n",
    "            upper_loser11,\n",
    "            upper_loser12,\n",
    "            upper_loser13,\n",
    "            upper_loser14,\n",
    "            upper_loser15,\n",
    "            upper_loser16,\n",
    "            upper_loser17,\n",
    "            upper_loser18,\n",
    "            upper_loser19,\n",
    "            upper_loser20,\n",
    "            upper_loser21,\n",
    "            upper_loser22,\n",
    "            upper_loser23,\n",
    "            upper_loser24,\n",
    "            upper_loser25,\n",
    "            upper_loser26,\n",
    "            upper_loser27,\n",
    "            upper_loser28,\n",
    "            upper_loser29,\n",
    "            upper_loser30,\n",
    "            upper_loser31,\n",
    "            upper_loser32,\n",
    "            upper_loser33,\n",
    "            upper_loser34,\n",
    "            upper_loser35,\n",
    "            upper_loser36,\n",
    "            upper_loser37,\n",
    "            upper_loser38,\n",
    "            upper_loser39,\n",
    "            upper_loser40,\n",
    "            upper_loser41,\n",
    "            upper_loser42,\n",
    "            upper_loser43,\n",
    "            upper_loser44,\n",
    "            upper_loser45,\n",
    "            upper_loser46,\n",
    "            upper_loser47,\n",
    "            upper_loser48,\n",
    "            upper_loser49,\n",
    "            upper_loser50,\n",
    "            upper_loser51,\n",
    "            upper_loser52,\n",
    "            upper_loser53,\n",
    "            upper_loser54,\n",
    "            upper_loser55,\n",
    "            upper_loser56,\n",
    "            upper_loser57,\n",
    "            upper_loser58,\n",
    "            upper_loser59,\n",
    "            upper_loser60,\n",
    "            upper_loser61,\n",
    "            upper_loser62,\n",
    "            upper_loser63,\n",
    "            upper_loser64,\n",
    "            upper_loser65,\n",
    "            upper_loser66,\n",
    "            upper_loser67,\n",
    "            upper_loser68,\n",
    "            upper_loser69,\n",
    "            upper_loser70,\n",
    "            upper_loser71,\n",
    "            upper_loser72,\n",
    "            upper_loser73,\n",
    "            upper_loser74,\n",
    "            upper_loser75,\n",
    "            upper_loser76,\n",
    "            upper_loser77,\n",
    "            upper_loser78,\n",
    "            upper_loser79,\n",
    "            upper_loser80,\n",
    "            upper_loser81,\n",
    "            upper_loser82,\n",
    "            upper_loser83,\n",
    "            upper_loser84,\n",
    "            upper_loser85,\n",
    "            upper_loser86,\n",
    "            upper_loser87,\n",
    "            upper_loser88,\n",
    "            upper_loser89,\n",
    "            upper_loser90,\n",
    "            upper_loser91,\n",
    "            upper_loser92,\n",
    "            upper_loser93,\n",
    "            upper_loser94,\n",
    "            upper_loser95,\n",
    "            upper_loser96,\n",
    "            upper_loser97,\n",
    "            upper_loser98,\n",
    "            upper_loser99,\n",
    "            upper_loser100,\n",
    "            upper_loser101]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summarise arrays: 'Winner'\n",
    "\n",
    "lower_winner = [lower_winner1,\n",
    "            lower_winner2,\n",
    "            lower_winner3,\n",
    "            lower_winner4,\n",
    "            lower_winner5,\n",
    "            lower_winner6,\n",
    "            lower_winner7,\n",
    "            lower_winner8,\n",
    "            lower_winner9,\n",
    "            lower_winner10,\n",
    "            lower_winner11,\n",
    "            lower_winner12,\n",
    "            lower_winner13,\n",
    "            lower_winner14,\n",
    "            lower_winner15,\n",
    "            lower_winner16,\n",
    "            lower_winner17,\n",
    "            lower_winner18,\n",
    "            lower_winner19,\n",
    "            lower_winner20,\n",
    "            lower_winner21,\n",
    "            lower_winner22,\n",
    "            lower_winner23,\n",
    "            lower_winner24,\n",
    "            lower_winner25,\n",
    "            lower_winner26,\n",
    "            lower_winner27,\n",
    "            lower_winner28,\n",
    "            lower_winner29,\n",
    "            lower_winner30,\n",
    "            lower_winner31,\n",
    "            lower_winner32,\n",
    "            lower_winner33,\n",
    "            lower_winner34,\n",
    "            lower_winner35,\n",
    "            lower_winner36,\n",
    "            lower_winner37,\n",
    "            lower_winner38,\n",
    "            lower_winner39,\n",
    "            lower_winner40,\n",
    "            lower_winner41,\n",
    "            lower_winner42,\n",
    "            lower_winner43,\n",
    "            lower_winner44,\n",
    "            lower_winner45,\n",
    "            lower_winner46,\n",
    "            lower_winner47,\n",
    "            lower_winner48,\n",
    "            lower_winner49,\n",
    "            lower_winner50,\n",
    "            lower_winner51,\n",
    "            lower_winner52,\n",
    "            lower_winner53,\n",
    "            lower_winner54,\n",
    "            lower_winner55,\n",
    "            lower_winner56,\n",
    "            lower_winner57,\n",
    "            lower_winner58,\n",
    "            lower_winner59,\n",
    "            lower_winner60,\n",
    "            lower_winner61,\n",
    "            lower_winner62,\n",
    "            lower_winner63,\n",
    "            lower_winner64,\n",
    "            lower_winner65,\n",
    "            lower_winner66,\n",
    "            lower_winner67,\n",
    "            lower_winner68,\n",
    "            lower_winner69,\n",
    "            lower_winner70,\n",
    "            lower_winner71,\n",
    "            lower_winner72,\n",
    "            lower_winner73,\n",
    "            lower_winner74,\n",
    "            lower_winner75,\n",
    "            lower_winner76,\n",
    "            lower_winner77,\n",
    "            lower_winner78,\n",
    "            lower_winner79,\n",
    "            lower_winner80,\n",
    "            lower_winner81,\n",
    "            lower_winner82,\n",
    "            lower_winner83,\n",
    "            lower_winner84,\n",
    "            lower_winner85,\n",
    "            lower_winner86,\n",
    "            lower_winner87,\n",
    "            lower_winner88,\n",
    "            lower_winner89,\n",
    "            lower_winner90,\n",
    "            lower_winner91,\n",
    "            lower_winner92,\n",
    "            lower_winner93,\n",
    "            lower_winner94,\n",
    "            lower_winner95,\n",
    "            lower_winner96,\n",
    "            lower_winner97,\n",
    "            lower_winner98,\n",
    "            lower_winner99,\n",
    "            lower_winner100,\n",
    "            lower_winner101]\n",
    "\n",
    "median_winner = [median_winner1,\n",
    "            median_winner2,\n",
    "            median_winner3,\n",
    "            median_winner4,\n",
    "            median_winner5,\n",
    "            median_winner6,\n",
    "            median_winner7,\n",
    "            median_winner8,\n",
    "            median_winner9,\n",
    "            median_winner10,\n",
    "            median_winner11,\n",
    "            median_winner12,\n",
    "            median_winner13,\n",
    "            median_winner14,\n",
    "            median_winner15,\n",
    "            median_winner16,\n",
    "            median_winner17,\n",
    "            median_winner18,\n",
    "            median_winner19,\n",
    "            median_winner20,\n",
    "            median_winner21,\n",
    "            median_winner22,\n",
    "            median_winner23,\n",
    "            median_winner24,\n",
    "            median_winner25,\n",
    "            median_winner26,\n",
    "            median_winner27,\n",
    "            median_winner28,\n",
    "            median_winner29,\n",
    "            median_winner30,\n",
    "            median_winner31,\n",
    "            median_winner32,\n",
    "            median_winner33,\n",
    "            median_winner34,\n",
    "            median_winner35,\n",
    "            median_winner36,\n",
    "            median_winner37,\n",
    "            median_winner38,\n",
    "            median_winner39,\n",
    "            median_winner40,\n",
    "            median_winner41,\n",
    "            median_winner42,\n",
    "            median_winner43,\n",
    "            median_winner44,\n",
    "            median_winner45,\n",
    "            median_winner46,\n",
    "            median_winner47,\n",
    "            median_winner48,\n",
    "            median_winner49,\n",
    "            median_winner50,\n",
    "            median_winner51,\n",
    "            median_winner52,\n",
    "            median_winner53,\n",
    "            median_winner54,\n",
    "            median_winner55,\n",
    "            median_winner56,\n",
    "            median_winner57,\n",
    "            median_winner58,\n",
    "            median_winner59,\n",
    "            median_winner60,\n",
    "            median_winner61,\n",
    "            median_winner62,\n",
    "            median_winner63,\n",
    "            median_winner64,\n",
    "            median_winner65,\n",
    "            median_winner66,\n",
    "            median_winner67,\n",
    "            median_winner68,\n",
    "            median_winner69,\n",
    "            median_winner70,\n",
    "            median_winner71,\n",
    "            median_winner72,\n",
    "            median_winner73,\n",
    "            median_winner74,\n",
    "            median_winner75,\n",
    "            median_winner76,\n",
    "            median_winner77,\n",
    "            median_winner78,\n",
    "            median_winner79,\n",
    "            median_winner80,\n",
    "            median_winner81,\n",
    "            median_winner82,\n",
    "            median_winner83,\n",
    "            median_winner84,\n",
    "            median_winner85,\n",
    "            median_winner86,\n",
    "            median_winner87,\n",
    "            median_winner88,\n",
    "            median_winner89,\n",
    "            median_winner90,\n",
    "            median_winner91,\n",
    "            median_winner92,\n",
    "            median_winner93,\n",
    "            median_winner94,\n",
    "            median_winner95,\n",
    "            median_winner96,\n",
    "            median_winner97,\n",
    "            median_winner98,\n",
    "            median_winner99,\n",
    "            median_winner100,\n",
    "            median_winner101]\n",
    "\n",
    "upper_winner = [upper_winner1,\n",
    "            upper_winner2,\n",
    "            upper_winner3,\n",
    "            upper_winner4,\n",
    "            upper_winner5,\n",
    "            upper_winner6,\n",
    "            upper_winner7,\n",
    "            upper_winner8,\n",
    "            upper_winner9,\n",
    "            upper_winner10,\n",
    "            upper_winner11,\n",
    "            upper_winner12,\n",
    "            upper_winner13,\n",
    "            upper_winner14,\n",
    "            upper_winner15,\n",
    "            upper_winner16,\n",
    "            upper_winner17,\n",
    "            upper_winner18,\n",
    "            upper_winner19,\n",
    "            upper_winner20,\n",
    "            upper_winner21,\n",
    "            upper_winner22,\n",
    "            upper_winner23,\n",
    "            upper_winner24,\n",
    "            upper_winner25,\n",
    "            upper_winner26,\n",
    "            upper_winner27,\n",
    "            upper_winner28,\n",
    "            upper_winner29,\n",
    "            upper_winner30,\n",
    "            upper_winner31,\n",
    "            upper_winner32,\n",
    "            upper_winner33,\n",
    "            upper_winner34,\n",
    "            upper_winner35,\n",
    "            upper_winner36,\n",
    "            upper_winner37,\n",
    "            upper_winner38,\n",
    "            upper_winner39,\n",
    "            upper_winner40,\n",
    "            upper_winner41,\n",
    "            upper_winner42,\n",
    "            upper_winner43,\n",
    "            upper_winner44,\n",
    "            upper_winner45,\n",
    "            upper_winner46,\n",
    "            upper_winner47,\n",
    "            upper_winner48,\n",
    "            upper_winner49,\n",
    "            upper_winner50,\n",
    "            upper_winner51,\n",
    "            upper_winner52,\n",
    "            upper_winner53,\n",
    "            upper_winner54,\n",
    "            upper_winner55,\n",
    "            upper_winner56,\n",
    "            upper_winner57,\n",
    "            upper_winner58,\n",
    "            upper_winner59,\n",
    "            upper_winner60,\n",
    "            upper_winner61,\n",
    "            upper_winner62,\n",
    "            upper_winner63,\n",
    "            upper_winner64,\n",
    "            upper_winner65,\n",
    "            upper_winner66,\n",
    "            upper_winner67,\n",
    "            upper_winner68,\n",
    "            upper_winner69,\n",
    "            upper_winner70,\n",
    "            upper_winner71,\n",
    "            upper_winner72,\n",
    "            upper_winner73,\n",
    "            upper_winner74,\n",
    "            upper_winner75,\n",
    "            upper_winner76,\n",
    "            upper_winner77,\n",
    "            upper_winner78,\n",
    "            upper_winner79,\n",
    "            upper_winner80,\n",
    "            upper_winner81,\n",
    "            upper_winner82,\n",
    "            upper_winner83,\n",
    "            upper_winner84,\n",
    "            upper_winner85,\n",
    "            upper_winner86,\n",
    "            upper_winner87,\n",
    "            upper_winner88,\n",
    "            upper_winner89,\n",
    "            upper_winner90,\n",
    "            upper_winner91,\n",
    "            upper_winner92,\n",
    "            upper_winner93,\n",
    "            upper_winner94,\n",
    "            upper_winner95,\n",
    "            upper_winner96,\n",
    "            upper_winner97,\n",
    "            upper_winner98,\n",
    "            upper_winner99,\n",
    "            upper_winner100,\n",
    "            upper_winner101]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Visualise!\n",
    "\n",
    "title = obj_func\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(median_loser, color = 'Red')\n",
    "plt.plot(median_winner, color = 'Blue')\n",
    "\n",
    "xstar = np.arange(0, max_iter+1, step=1)\n",
    "plt.fill_between(xstar, lower_loser, upper_loser, facecolor = 'Red', alpha=0.4, label='GP ERM Regret: IQR')\n",
    "plt.fill_between(xstar, lower_winner, upper_winner, facecolor = 'Blue', alpha=0.4, label='STP ERM Regret: IQR ' r'($\\nu$' ' = {})'.format(df))\n",
    "\n",
    "plt.title(title, weight = 'bold', family = 'Arial')\n",
    "plt.xlabel('(Post-initialization) iteration $\\it{k}$', weight = 'bold', family = 'Arial') # x-axis label\n",
    "plt.ylabel('ln(Regret)', weight = 'bold', family = 'Arial') # y-axis label\n",
    "plt.legend(loc=0) # add plot legend\n",
    "\n",
    "plt.show() #visualise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
