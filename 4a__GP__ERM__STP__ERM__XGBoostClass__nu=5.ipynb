{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost Classification - 'real-world' example: UCI Skin Segmentation dataset\n",
    "\n",
    "GP ERM versus STP nu = 5 ERM (winner)\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Skin+Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some default Python modules:\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import rc\n",
    "rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
    "rc('text', usetex=False)\n",
    "\n",
    "from collections import OrderedDict\n",
    "from numpy.linalg import slogdet\n",
    "from scipy.linalg import inv\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import gamma\n",
    "from scipy.stats import norm, t\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from pyGPGO.logger import EventLogger\n",
    "from pyGPGO.GPGO import GPGO\n",
    "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\n",
    "from pyGPGO.surrogates.tStudentProcess import tStudentProcess\n",
    "from pyGPGO.surrogates.tStudentProcess import logpdf\n",
    "from pyGPGO.acquisition import Acquisition\n",
    "from pyGPGO.covfunc import squaredExponential, matern32, matern52\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadData():\n",
    "    #Data in format [B G R Label] from\n",
    "    data = np.genfromtxt('/home/ulsterconorc/Downloads/Skin_NonSkin.txt', dtype=np.int32)\n",
    "\n",
    "    labels = data[:,3]\n",
    "    data = data[:,0:3]\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "data, labels = ReadData()\n",
    "\n",
    "X = data\n",
    "y = labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bayesian Optimization - inputs:\n",
    "\n",
    "obj_func = 'XGBoost'\n",
    "n_test = 50 # test points\n",
    "df = 5 # nu\n",
    "\n",
    "util_loser = 'RegretMinimized'\n",
    "util_winner = 'tRegretMinimized'\n",
    "n_init = 5 # random initialisations\n",
    "\n",
    "test_perc = 0.15\n",
    "train_perc = 1 - test_perc\n",
    "n_est = 2\n",
    "\n",
    "obj_classifier = 'binary:logistic'\n",
    "cov_func = squaredExponential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Objective function:\n",
    "\n",
    "if obj_func == 'XGBoost':\n",
    "    \n",
    "    # Constraints:\n",
    "    param_lb_alpha = 0\n",
    "    param_ub_alpha = 10\n",
    "    \n",
    "    param_lb_gamma = 0\n",
    "    param_ub_gamma = 10\n",
    "    \n",
    "    param_lb_max_depth = 5\n",
    "    param_ub_max_depth = 15\n",
    "    \n",
    "    param_lb_min_child_weight = 1\n",
    "    param_ub_min_child_weight = 20\n",
    "    \n",
    "    param_lb_subsample = .5\n",
    "    param_ub_subsample = 1\n",
    "    \n",
    "    param_lb_colsample = .1\n",
    "    param_ub_colsample = 1\n",
    "    \n",
    "    # 6-D inputs' parameter bounds:\n",
    "    param = { 'alpha':  ('cont', (param_lb_alpha, param_ub_alpha)),\n",
    "         'gamma':  ('cont', (param_lb_gamma, param_ub_gamma)),     \n",
    "         'max_depth':  ('int', (param_lb_max_depth, param_ub_max_depth)),\n",
    "         'subsample':  ('cont', (param_lb_subsample, param_ub_subsample)),\n",
    "          'min_child_weight':  ('int', (param_lb_min_child_weight, param_ub_min_child_weight)),\n",
    "            'colsample': ('cont', (param_lb_colsample, param_ub_colsample))\n",
    "        }\n",
    "       \n",
    "    # True y bounds:\n",
    "    y_global_orig = 1\n",
    "    dim = 6\n",
    "    \n",
    "    max_iter = 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cumulative Regret Calculator:\n",
    "\n",
    "def min_max_array(x):\n",
    "    new_list = []\n",
    "    for i, num in enumerate(x):\n",
    "            new_list.append(np.min(x[0:i+1]))\n",
    "    return new_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set-seeds:\n",
    "\n",
    "run_num_1 = 111\n",
    "run_num_2 = 113\n",
    "run_num_3 = 3333\n",
    "run_num_4 = 44444\n",
    "run_num_5 = 5555\n",
    "run_num_6 = 6\n",
    "run_num_7 = 7777\n",
    "run_num_8 = 887\n",
    "run_num_9 = 99\n",
    "run_num_10 = 1000\n",
    "run_num_11 = 1113\n",
    "run_num_12 = 1234\n",
    "run_num_13 = 2345\n",
    "run_num_14 = 88\n",
    "run_num_15 = 1557\n",
    "run_num_16 = 1666\n",
    "run_num_17 = 717\n",
    "run_num_18 = 8\n",
    "run_num_19 = 1998\n",
    "run_num_20 = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4a. Add new acquisition functions: add CBM & ERM (Nyugen and Osborne, 2019) method .\n",
    "\n",
    "### Inherits from class Acquisition()\n",
    "\n",
    "class Acquisition_new(Acquisition):    \n",
    "    def __init__(self, mode, eps=1e-06, **params):\n",
    "        \n",
    "        self.params = params\n",
    "        self.eps = eps\n",
    "\n",
    "        mode_dict = {\n",
    "            'RegretMinimized': self.RegretMinimized,\n",
    "            'tRegretMinimized': self.tRegretMinimized\n",
    "        }\n",
    "\n",
    "        self.f = mode_dict[mode]\n",
    "   \n",
    "    def RegretMinimized(self, tau, mean, std):\n",
    "            \n",
    "        z = (mean - y_global_orig - self.eps) / (std + self.eps)\n",
    "        return z * (std + self.eps) * norm.cdf(z) + (std + self.eps) * norm.pdf(z)[0]\n",
    "    \n",
    "    def tRegretMinimized(self, tau, mean, std, nu=3.0):\n",
    "        \n",
    "        gamma = (mean - y_global_orig - self.eps) / (std + self.eps)\n",
    "        return gamma * (std + self.eps) * t.cdf(gamma, df=nu) + (std + self.eps) * (nu + gamma ** 2)/(nu - 1) * t.pdf(gamma, df=nu)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.12170176  1.69069754 11.          0.92695323  3.          0.36579277]. \t  0.8890243937641641 \t 0.9846806025668978\n",
      "init   \t [ 1.49162957  0.22478325  6.          0.85613654 15.          0.19363322]. \t  0.8875985382508563 \t 0.9846806025668978\n",
      "init   \t [ 5.27957805  2.81819356 11.          0.81062146 14.          0.74826845]. \t  0.9846806025668978 \t 0.9846806025668978\n",
      "init   \t [3.81060005 6.24236569 7.         0.95038709 5.         0.9861745 ]. \t  0.9822369877610099 \t 0.9846806025668978\n",
      "init   \t [4.77531134 7.87990376 6.         0.78863691 6.         0.47918609]. \t  0.8869792272703285 \t 0.9846806025668978\n",
      "1      \t [4.26128727 7.00737964 6.53282638 0.87482164 5.4671733  0.74932303]. \t  0.9779738706123698 \t 0.9846806025668978\n",
      "2      \t [3.78128046 7.08001843 7.28349467 0.81606759 4.93438906 0.7105907 ]. \t  0.9823714094297742 \t 0.9846806025668978\n",
      "3      \t [4.43495488 6.67503667 7.45921469 0.91537911 5.27821457 0.94593868]. \t  0.9822225858715052 \t 0.9846806025668978\n",
      "4      \t [3.68016409 6.77604232 7.27169507 1.         5.69453543 1.        ]. \t  \u001b[92m0.9929283980981055\u001b[0m \t 0.9929283980981055\n",
      "5      \t [3.94377561 6.50688609 7.22839034 0.59466167 5.45018393 0.36502336]. \t  0.8875505296477145 \t 0.9929283980981055\n",
      "6      \t [4.41877541 7.76880846 7.06693971 0.82186923 5.8623705  0.62412751]. \t  0.8873728978160903 \t 0.9929283980981055\n",
      "7      \t [3.99951539 6.87954847 7.09827907 0.5        5.33772404 1.        ]. \t  0.9916033654914598 \t 0.9929283980981055\n",
      "8      \t [ 4.64029006  2.26793414 11.00000003  0.8568146  14.3343997   0.75219898]. \t  0.9844789695451731 \t 0.9929283980981055\n",
      "9      \t [ 4.77044407  2.79327663 10.29424364  0.86393244 14.21150674  0.77815176]. \t  0.9844933720569718 \t 0.9929283980981055\n",
      "10     \t [ 4.8460223   2.30338293 10.62960286  0.86832639 13.54957104  0.7557789 ]. \t  0.9845701846465542 \t 0.9929283980981055\n",
      "11     \t [ 5.34333438  2.1512391  10.48286015  0.87866378 14.24960578  0.66335567]. \t  0.8890099911832218 \t 0.9929283980981055\n",
      "12     \t [ 4.80527328  2.6015079  10.73877464  0.63627975 14.00199581  0.10030124]. \t  0.8886691301009156 \t 0.9929283980981055\n",
      "13     \t [ 4.88210432  2.50797511 10.70850992  0.5        14.03199066  1.        ]. \t  \u001b[92m0.9935525052371701\u001b[0m \t 0.9935525052371701\n",
      "14     \t [4.84119943 7.20068917 6.56654891 1.         6.48662855 0.7880537 ]. \t  0.9833267743402093 \t 0.9935525052371701\n",
      "15     \t [4.09272251 7.48345997 6.338479   1.         6.518583   0.41514376]. \t  0.8978770869776707 \t 0.9935525052371701\n",
      "16     \t [4.82211471 7.27326904 6.60600987 1.         6.11870979 0.1       ]. \t  0.8978434814394779 \t 0.9935525052371701\n",
      "17     \t [3.56631901 6.34991001 7.94205345 0.82996947 4.91444527 1.        ]. \t  0.9930196122314735 \t 0.9935525052371701\n",
      "18     \t [4.54225837 7.28613986 6.467682   0.5        6.25421075 0.61954201]. \t  0.88573100441837 \t 0.9935525052371701\n",
      "19     \t [ 4.37068563  3.01221855 11.03573364  1.         13.77771211  0.83735485]. \t  0.9904175735295638 \t 0.9935525052371701\n",
      "20     \t [ 4.80463126  2.63381063 10.79472848  1.         13.98774045  0.76314075]. \t  0.9900287090299001 \t 0.9935525052371701\n",
      "21     \t [ 4.80156716  3.43523697 10.54005175  0.52231805 13.27074067  0.72643107]. \t  0.9830147174517755 \t 0.9935525052371701\n",
      "22     \t [ 3.90493279  2.89823188 10.34960286  0.5171559  13.47710925  0.72281431]. \t  0.9838020526660776 \t 0.9935525052371701\n",
      "23     \t [ 4.3424758   2.81983964 11.19441984  0.5        13.01505795  0.66798185]. \t  0.9831347395127801 \t 0.9935525052371701\n",
      "24     \t [ 3.73078354  2.23902392 11.1937452   0.5        13.79003576  0.69222816]. \t  0.9835044045123827 \t 0.9935525052371701\n",
      "25     \t [ 4.31075141  2.92465313 10.93945422  0.5        13.76172864  0.66474749]. \t  0.8879057941406884 \t 0.9935525052371701\n",
      "26     \t [ 3.74102736  2.50928284 10.8796482   1.         13.0622096   1.        ]. \t  \u001b[92m0.994963945585367\u001b[0m \t 0.994963945585367\n",
      "27     \t [ 4.37639463  3.19336415 10.50511824  1.         12.85007647  1.        ]. \t  0.9948871325117782 \t 0.994963945585367\n",
      "28     \t [ 3.88510483  1.80872888 10.31609602  0.66360591 14.08921059  0.70681909]. \t  0.984320542606825 \t 0.994963945585367\n",
      "29     \t [ 3.88608939  1.95735921 10.64170936  0.59846577 13.15789777  0.27749738]. \t  0.8883138664376672 \t 0.994963945585367\n",
      "30     \t [ 4.22160398  1.38186915 11.17102336  0.5        13.82928026  0.88245964]. \t  0.9832019501051588 \t 0.994963945585367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05920696212231716"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\n",
    "\n",
    "np.random.seed(run_num_1)\n",
    "surrogate_loser_1 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=test_perc, random_state=run_num_1)\n",
    "\n",
    "def f_syn_polarity1(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, \n",
    "                       min_child_weight=min_child_weight,colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_1, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train1, y=y_train1).mean())\n",
    "    return  score\n",
    "\n",
    "loser_1 = GPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity1, param, n_jobs = -1) # define BayesOpt\n",
    "loser_1.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_1 = loser_1.getResult()[0]\n",
    "params_loser_1['max_depth'] = int(params_loser_1['max_depth'])\n",
    "params_loser_1['min_child_weight'] = int(params_loser_1['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train1 = xgb.DMatrix(X_train1, y_train1)\n",
    "dX_loser_test1 = xgb.DMatrix(X_test1, y_test1)\n",
    "model_loser_1 = xgb.train(params_loser_1, dX_loser_train1)\n",
    "pred_loser_1 = model_loser_1.predict(dX_loser_test1)\n",
    "\n",
    "rmse_loser_1 = np.sqrt(mean_squared_error(pred_loser_1, y_test1))\n",
    "rmse_loser_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.51985493  0.73903599  7.          0.71824677 17.          0.61827209]. \t  0.8899125543587183 \t 0.9907392220424587\n",
      "init   \t [ 8.4047092   4.35120551 12.          0.84231905 16.          0.80172484]. \t  0.9902255407751176 \t 0.9907392220424587\n",
      "init   \t [ 6.42749368  9.61026168 10.          0.73363448 15.          0.84932207]. \t  0.9904943853572861 \t 0.9907392220424587\n",
      "init   \t [ 2.66008363  8.36685389  6.          0.92082163 12.          0.69999264]. \t  0.9907392220424587 \t 0.9907392220424587\n",
      "init   \t [ 1.74779478  2.17691512 12.          0.71794562  2.          0.48990646]. \t  0.8940700725633864 \t 0.9907392220424587\n",
      "1      \t [ 3.43112436  8.55676877  6.00000044  0.80756459 11.59622208  0.77577814]. \t  0.9901679379187686 \t 0.9907392220424587\n",
      "2      \t [ 3.40451653  8.03172613  5.80879517  0.89095486 12.28462328  0.78873698]. \t  0.9864761193439872 \t 0.9907392220424587\n",
      "3      \t [ 3.29659784  8.42987735  6.5650638   0.99791771 12.20245508  0.61092166]. \t  0.8960528104491554 \t 0.9907392220424587\n",
      "4      \t [ 3.19144269  8.86220105  5.80241021  0.70717263 12.34562072  0.92554256]. \t  0.9849062529560961 \t 0.9907392220424587\n",
      "5      \t [ 3.20001864  8.47957282  5.83505107  0.50100561 12.10197879  0.1909809 ]. \t  0.894156569914102 \t 0.9907392220424587\n",
      "6      \t [ 3.10519607  8.27835422  6.15316376  0.5        12.03755606  1.        ]. \t  0.9885452595782814 \t 0.9907392220424587\n",
      "7      \t [ 6.37234395  8.83416908  9.99940826  0.72238621 15.03227565  0.39525125]. \t  0.8906902862621925 \t 0.9907392220424587\n",
      "8      \t [ 6.34996857  9.22675414  9.27947116  0.73091815 15.36173232  0.8799284 ]. \t  0.9904799827763435 \t 0.9907392220424587\n",
      "9      \t [ 6.16362128  9.25503281  9.41040873  0.7681615  14.50884419  0.79250868]. \t  \u001b[92m0.9913009246352145\u001b[0m \t 0.9913009246352145\n",
      "10     \t [ 5.64661118  9.31919634  9.74695769  0.74355482 15.13639524  0.79042692]. \t  0.9904847839132289 \t 0.9913009246352145\n",
      "11     \t [ 6.15322937  9.61813634  9.46431181  0.64460486 15.04693252  0.21439809]. \t  0.8955007346067171 \t 0.9913009246352145\n",
      "12     \t [ 6.16899876  9.1985769   9.6787108   0.5        14.97542339  0.91605708]. \t  0.9896158388443537 \t 0.9913009246352145\n",
      "13     \t [ 6.17940943  9.29440451  9.64346854  1.         15.02880934  0.73905619]. \t  \u001b[92m0.9916561880218918\u001b[0m \t 0.9916561880218918\n",
      "14     \t [ 5.84154668 10.          9.07170249  0.66457176 14.95440203  1.        ]. \t  \u001b[92m0.9937541371517818\u001b[0m \t 0.9937541371517818\n",
      "15     \t [ 5.48521651  9.19731339  8.59042285  0.65605412 14.99812391  0.81049509]. \t  0.9903503609307167 \t 0.9937541371517818\n",
      "16     \t [ 6.33751778  9.49416558  8.2904607   0.60962678 14.75951561  0.97770477]. \t  0.9902063356744341 \t 0.9937541371517818\n",
      "17     \t [ 5.96614507  9.63029545  8.32474808  0.5        15.64719442  1.        ]. \t  0.9917378175820747 \t 0.9937541371517818\n",
      "18     \t [ 5.9784213   9.52016013  8.52325086  1.         15.20663864  1.        ]. \t  \u001b[92m0.9937829410690965\u001b[0m \t 0.9937829410690965\n",
      "19     \t [ 6.23987814  9.00663027  8.27134693  0.5        15.32626137  0.40173873]. \t  0.8926345909911889 \t 0.9937829410690965\n",
      "20     \t [ 6.01382303  9.46946829  8.76523903  0.5        15.10761358  0.83023603]. \t  0.9894862129884453 \t 0.9937829410690965\n",
      "21     \t [ 5.79522831  9.13572389  7.4557735   0.59844503 15.22274543  1.        ]. \t  0.9915793752248642 \t 0.9937829410690965\n",
      "22     \t [ 6.12325574  8.48861278  8.07355009  0.78629008 14.90145107  1.        ]. \t  0.9937205323741528 \t 0.9937829410690965\n",
      "23     \t [ 5.74636038  8.6689599   8.11046968  0.67983995 15.8475481   1.        ]. \t  0.9936821249384971 \t 0.9937829410690965\n",
      "24     \t [ 6.61301332  8.98290891  7.789585    0.71329324 15.61669756  1.        ]. \t  0.991420954163632 \t 0.9937829410690965\n",
      "25     \t [ 6.04444154  9.00822173  8.05973497  0.62667491 15.32452165  1.        ]. \t  0.9925539396355078 \t 0.9937829410690965\n",
      "26     \t [ 5.89889938  9.31124754  7.50851675  0.93259734 16.24955018  0.6772544 ]. \t  0.9898174677866988 \t 0.9937829410690965\n",
      "27     \t [ 5.99477608  8.38950858  7.37165493  1.         15.55415939  0.52870289]. \t  0.8870752445301892 \t 0.9937829410690965\n",
      "28     \t [ 5.92339137  8.9890329   7.92528886  1.         14.0826683   0.62452383]. \t  0.8870752445301892 \t 0.9937829410690965\n",
      "29     \t [ 5.24680495  9.44534511  8.67673196  0.84503287 16.20581659  0.64752832]. \t  0.8967009239641422 \t 0.9937829410690965\n",
      "30     \t [ 6.78758004  8.94846481  7.39855582  1.         14.6337042   0.79700005]. \t  0.9910416808056753 \t 0.9937829410690965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.064326562111266"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\n",
    "\n",
    "np.random.seed(run_num_2)\n",
    "surrogate_loser_2 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=test_perc, random_state=run_num_2)\n",
    "\n",
    "def f_syn_polarity2(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_2, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train2, y=y_train2).mean())\n",
    "    return  score\n",
    "\n",
    "loser_2 = GPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity2, param, n_jobs = -1) # define BayesOpt\n",
    "loser_2.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_2 = loser_2.getResult()[0]\n",
    "params_loser_2['max_depth'] = int(params_loser_2['max_depth'])\n",
    "params_loser_2['min_child_weight'] = int(params_loser_2['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train2 = xgb.DMatrix(X_train2, y_train2)\n",
    "dX_loser_test2 = xgb.DMatrix(X_test2, y_test2)\n",
    "model_loser_2 = xgb.train(params_loser_2, dX_loser_train2)\n",
    "pred_loser_2 = model_loser_2.predict(dX_loser_test2)\n",
    "\n",
    "rmse_loser_2 = np.sqrt(mean_squared_error(pred_loser_2, y_test2))\n",
    "rmse_loser_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 7.51575607  1.09251275 13.          0.80377147 16.          0.31229624]. \t  0.8853133459581143 \t 0.9850406749037081\n",
      "init   \t [6.12794891 1.19652403 5.         0.74822174 8.         0.33695838]. \t  0.8834410179031171 \t 0.9850406749037081\n",
      "init   \t [ 6.21631223  4.4909976  10.          0.66764529  9.          0.77207323]. \t  0.9850406749037081 \t 0.9850406749037081\n",
      "init   \t [ 0.58127233  4.79918477 11.          0.64526552 15.          0.62431773]. \t  0.8862927047985528 \t 0.9850406749037081\n",
      "init   \t [ 5.76228258  0.67219724  7.          0.88228691 13.          0.29867219]. \t  0.8858222329336453 \t 0.9850406749037081\n",
      "1      \t [6.32090591 3.59919158 9.99999852 0.65981308 9.00000002 0.90744883]. \t  0.9845317837795499 \t 0.9850406749037081\n",
      "2      \t [6.69946221 4.09064519 9.42869281 0.7203414  8.71440443 0.80642204]. \t  0.9849254499001097 \t 0.9850406749037081\n",
      "3      \t [6.00755335 3.9957583  9.33568101 0.63043484 9.24569543 0.71071081]. \t  \u001b[92m0.9853287261076954\u001b[0m \t 0.9853287261076954\n",
      "4      \t [5.90920883 3.9844455  9.67028866 0.64609421 8.42900603 0.71312467]. \t  \u001b[92m0.985712790092764\u001b[0m \t 0.985712790092764\n",
      "5      \t [6.29727657 3.94982515 9.7813652  0.99999567 8.91610843 0.20566511]. \t  0.8861102862119461 \t 0.985712790092764\n",
      "6      \t [6.12898119 4.06686336 9.64303954 1.         8.88378491 1.        ]. \t  \u001b[92m0.9944886641480286\u001b[0m \t 0.9944886641480286\n",
      "7      \t [6.26497574 4.03331836 9.69716516 0.5        8.87562682 0.74685704]. \t  0.9846662080757781 \t 0.9944886641480286\n",
      "8      \t [6.25708839 3.08455862 9.06503165 0.890501   8.64101107 0.73604824]. \t  0.9856983884106908 \t 0.9944886641480286\n",
      "9      \t [6.64997505 3.24487842 9.76585727 1.         8.00282815 0.85661569]. \t  0.9656645765723401 \t 0.9944886641480286\n",
      "10     \t [5.74458699 2.86420751 9.89739534 0.96326888 8.37340525 0.74250783]. \t  0.9867497593261164 \t 0.9944886641480286\n",
      "11     \t [6.59423018 2.55896282 9.87892524 1.         8.78657673 0.87938698]. \t  0.9655445548570545 \t 0.9944886641480286\n",
      "12     \t [6.09590285 5.130412   9.19441776 0.82303052 8.9728823  0.48943618]. \t  0.884996493671725 \t 0.9944886641480286\n",
      "13     \t [6.66848778 4.8466534  9.49953518 0.90204648 9.79101967 0.72115811]. \t  0.9861688678814144 \t 0.9944886641480286\n",
      "14     \t [6.25401901 2.84684488 9.68630863 0.5        8.35069701 1.        ]. \t  0.9939941835563482 \t 0.9944886641480286\n",
      "15     \t [5.75125002 5.00782021 9.76103823 0.82787459 9.87762162 0.56768941]. \t  0.8849388835553863 \t 0.9944886641480286\n",
      "16     \t [6.27450549 3.19638114 9.69521254 0.91732425 8.56783968 0.78058685]. \t  0.98578960219834 \t 0.9944886641480286\n",
      "17     \t [6.25248412 2.16516116 9.2703076  1.         7.91689879 0.77319777]. \t  0.9657077836928737 \t 0.9944886641480286\n",
      "18     \t [5.77980831 3.1931188  9.07344269 0.86229048 7.58023316 0.82222929]. \t  0.9861016492683566 \t 0.9944886641480286\n",
      "19     \t [ 6.45471355  4.01190929 10.07996367  0.96161134 10.04081604  0.80375967]. \t  0.9862072698547192 \t 0.9944886641480286\n",
      "20     \t [6.28752167 3.99876768 8.66557372 0.98567459 7.97217243 0.50651456]. \t  0.8881506295811694 \t 0.9944886641480286\n",
      "21     \t [6.82139663 3.00777291 8.75728343 0.77016589 7.78750933 1.        ]. \t  0.9939413736088859 \t 0.9944886641480286\n",
      "22     \t [6.41670335 2.96405501 9.12819197 0.57476899 7.74218505 0.26545868]. \t  0.8849724752648224 \t 0.9944886641480286\n",
      "23     \t [6.52851414 3.88461239 9.27382889 0.54416096 7.38464055 1.        ]. \t  0.9939461746766313 \t 0.9944886641480286\n",
      "24     \t [5.73474869 2.05232422 9.55370788 0.85731857 9.03499025 1.        ]. \t  0.9944166523496166 \t 0.9944886641480286\n",
      "25     \t [ 5.82140843  2.74646025 10.26292808  0.78426435  9.680394    0.91545036]. \t  0.9859432210854204 \t 0.9944886641480286\n",
      "26     \t [ 5.94808489  2.16313354 10.0438057   0.54918429  9.02262557  0.35387222]. \t  0.8868592070070624 \t 0.9944886641480286\n",
      "27     \t [6.45816288 2.69395853 9.57406871 0.56279938 9.82626134 0.96045008]. \t  0.9834659981830197 \t 0.9944886641480286\n",
      "28     \t [6.57284517 2.95858122 9.37594437 1.         7.06813173 1.        ]. \t  \u001b[92m0.9946662939053393\u001b[0m \t 0.9946662939053393\n",
      "29     \t [6.41897089 3.34477457 9.19004037 0.94403194 7.66697527 0.96873718]. \t  0.986663347781657 \t 0.9946662939053393\n",
      "30     \t [7.37434914 2.94649168 9.55895514 0.5        7.35975894 1.        ]. \t  0.9941094015072807 \t 0.9946662939053393\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0625316655752366"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\n",
    "\n",
    "np.random.seed(run_num_3)\n",
    "surrogate_loser_3 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=test_perc, random_state=run_num_3)\n",
    "\n",
    "def f_syn_polarity3(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_3, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train3, y=y_train3).mean())\n",
    "    return  score\n",
    "\n",
    "loser_3 = GPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity3, param, n_jobs = -1) # define BayesOpt\n",
    "loser_3.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_3 = loser_3.getResult()[0]\n",
    "params_loser_3['max_depth'] = int(params_loser_3['max_depth'])\n",
    "params_loser_3['min_child_weight'] = int(params_loser_3['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train3 = xgb.DMatrix(X_train3, y_train3)\n",
    "dX_loser_test3 = xgb.DMatrix(X_test3, y_test3)\n",
    "model_loser_3 = xgb.train(params_loser_3, dX_loser_train3)\n",
    "pred_loser_3 = model_loser_3.predict(dX_loser_test3)\n",
    "\n",
    "rmse_loser_3 = np.sqrt(mean_squared_error(pred_loser_3, y_test3))\n",
    "rmse_loser_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.27423888  7.57027691 14.          0.71146658  9.          0.39774643]. \t  0.7961286970370575 \t 0.9680361805433857\n",
      "init   \t [ 2.32539597  3.72936196 10.          0.82732983  1.          0.94661358]. \t  0.9680361805433857 \t 0.9680361805433857\n",
      "init   \t [1.75719325 8.52964023 6.         0.7434712  8.         0.81674001]. \t  0.9546131048168407 \t 0.9680361805433857\n",
      "init   \t [ 6.71917565  6.77963281 14.          0.99125515  2.          0.14599602]. \t  0.7981786092152628 \t 0.9680361805433857\n",
      "init   \t [ 3.45494695  9.84067498 11.          0.7159999   7.          0.26316389]. \t  0.7957782443289703 \t 0.9680361805433857\n",
      "1      \t [ 2.54475597  3.9179364  10.87889719  0.88293341  1.          1.        ]. \t  \u001b[92m0.9959049123393965\u001b[0m \t 0.9959049123393965\n",
      "2      \t [ 2.38764344  4.57153636 10.32997261  0.88759426  1.00000007  0.82653912]. \t  0.9680217732607336 \t 0.9959049123393965\n",
      "3      \t [ 3.11678541  4.07118715 10.23789093  1.          1.          0.99680721]. \t  0.9679497663023008 \t 0.9959049123393965\n",
      "4      \t [ 2.7243506   3.97471786 10.42554934  0.5         1.00000001  0.38145872]. \t  0.7980297915340859 \t 0.9959049123393965\n",
      "5      \t [ 2.54828654  4.06014801 10.3798909   1.          1.69527379  1.        ]. \t  \u001b[92m0.9963897885831384\u001b[0m \t 0.9963897885831384\n",
      "6      \t [2.2896253  8.31159312 6.00014595 0.77113171 7.25251108 0.85623655]. \t  0.954829144913834 \t 0.9963897885831384\n",
      "7      \t [1.71581168 8.6695876  6.63134258 0.84496474 7.34222281 0.92415518]. \t  0.9563030102748539 \t 0.9963897885831384\n",
      "8      \t [1.89640048 7.84711509 6.53516331 0.8193611  7.70416467 0.81750725]. \t  0.9553380261505744 \t 0.9963897885831384\n",
      "9      \t [2.43239639 8.55425694 6.56807198 0.99719106 7.8893294  0.87528374]. \t  0.9555972720543973 \t 0.9963897885831384\n",
      "10     \t [2.07795261 8.4647232  6.45986151 0.52917411 7.61172063 0.2335379 ]. \t  0.7938723238036528 \t 0.9963897885831384\n",
      "11     \t [2.1049327  8.3846611  6.42658633 0.5        7.70242068 1.        ]. \t  0.9888189017019103 \t 0.9963897885831384\n",
      "12     \t [ 2.61393181  4.16894457 10.35111685  0.5         1.24903199  1.        ]. \t  0.9955736569880006 \t 0.9963897885831384\n",
      "13     \t [ 2.53429176  4.05784093 10.37681759  1.          1.13179058  1.        ]. \t  \u001b[92m0.9964281950507949\u001b[0m \t 0.9964281950507949\n",
      "14     \t [ 3.10004123  4.66473661 11.15626413  0.92955833  1.51312758  0.89792695]. \t  0.9687851130929691 \t 0.9964281950507949\n",
      "15     \t [ 3.3316612   3.64215351 11.08684505  0.85698841  1.72819368  1.        ]. \t  \u001b[92m0.9965386145614495\u001b[0m \t 0.9965386145614495\n",
      "16     \t [ 2.52387884  3.99941097 11.48596989  0.70837346  1.9592864   0.95423578]. \t  0.9691931770928255 \t 0.9965386145614495\n",
      "17     \t [ 2.48599807  3.02192324 10.71441911  0.60366588  1.73303028  1.        ]. \t  0.9960681338460861 \t 0.9965386145614495\n",
      "18     \t [3.09735065 3.02305155 9.93829377 0.76328827 1.44331708 1.        ]. \t  0.9951559792366709 \t 0.9965386145614495\n",
      "19     \t [ 3.01016872  3.47665542 10.42022085  0.5         2.32180944  1.        ]. \t  0.9954248307331182 \t 0.9965386145614495\n",
      "20     \t [ 2.8817079   3.57929123 10.60465224  0.68129168  1.67534409  1.        ]. \t  0.9960441285765161 \t 0.9965386145614495\n",
      "21     \t [2.35175129 2.93397324 9.67747801 0.64977223 2.10837159 0.9685506 ]. \t  0.9668359723791223 \t 0.9965386145614495\n",
      "22     \t [ 2.92984692  2.71955528 10.43968433  1.          2.42908263  0.95924703]. \t  0.9679497651960163 \t 0.9965386145614495\n",
      "23     \t [2.90451863 3.85585304 9.17653358 0.75039543 1.6306428  0.8863832 ]. \t  0.9665911332048095 \t 0.9965386145614495\n",
      "24     \t [3.10430227 3.31218927 9.73126678 1.         2.21052269 0.44765078]. \t  0.7933633230881755 \t 0.9965386145614495\n",
      "25     \t [ 3.05314432  3.9510391  11.97288149  1.          1.18821912  0.98881127]. \t  0.9693612078952493 \t 0.9965386145614495\n",
      "26     \t [ 2.86099981  3.00825258 11.73847965  1.          2.09860872  1.        ]. \t  \u001b[92m0.9966058250847066\u001b[0m \t 0.9966058250847066\n",
      "27     \t [ 2.34871659  3.24314765 10.98067354  0.92404792  2.73547279  1.        ]. \t  0.9962745698715962 \t 0.9966058250847066\n",
      "28     \t [ 2.91772302  3.41623438 11.30634467  1.          2.34465029  0.40782753]. \t  0.7930560590395063 \t 0.9966058250847066\n",
      "29     \t [ 2.82875695  2.77008088 11.23161855  0.5         2.63151189  1.        ]. \t  0.9960057237682864 \t 0.9966058250847066\n",
      "30     \t [2.68614284 3.05251769 9.12414145 0.5        1.07229479 0.70276104]. \t  0.9657990045283867 \t 0.9966058250847066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05498246135836864"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\n",
    "\n",
    "np.random.seed(run_num_4)\n",
    "surrogate_loser_4 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=test_perc, random_state=run_num_4)\n",
    "\n",
    "def f_syn_polarity4(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_4, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train4, y=y_train4).mean())\n",
    "    return  score\n",
    "\n",
    "loser_4 = GPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity4, param, n_jobs = -1) # define BayesOpt\n",
    "loser_4.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_4 = loser_4.getResult()[0]\n",
    "params_loser_4['max_depth'] = int(params_loser_4['max_depth'])\n",
    "params_loser_4['min_child_weight'] = int(params_loser_4['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train4 = xgb.DMatrix(X_train4, y_train4)\n",
    "dX_loser_test4 = xgb.DMatrix(X_test4, y_test4)\n",
    "model_loser_4 = xgb.train(params_loser_4, dX_loser_train4)\n",
    "pred_loser_4 = model_loser_4.predict(dX_loser_test4)\n",
    "\n",
    "rmse_loser_4 = np.sqrt(mean_squared_error(pred_loser_4, y_test4))\n",
    "rmse_loser_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.57353274  5.41760627  8.          0.53710825 19.          0.26034376]. \t  0.8874977192853896 \t 0.9875226863588296\n",
      "init   \t [5.17826437 9.78817462 5.         0.84048437 6.         0.79712615]. \t  0.9790156454078135 \t 0.9875226863588296\n",
      "init   \t [ 9.84476201  9.65484856  5.          0.59464714 19.          0.34528747]. \t  0.8875793339107304 \t 0.9875226863588296\n",
      "init   \t [ 9.17476842  4.53262239 13.          0.64368565 10.          0.515308  ]. \t  0.8874977192853896 \t 0.9875226863588296\n",
      "init   \t [ 8.66719465  7.99685372 12.          0.79833571 17.          0.72443466]. \t  0.9875226863588296 \t 0.9875226863588296\n",
      "1      \t [ 8.04875096  7.60250855 11.99995594  0.72320561 16.47912318  0.65900801]. \t  0.8875793339107304 \t 0.9875226863588296\n",
      "2      \t [ 8.28903908  7.4554031  11.46697547  0.7748613  17.2901625   0.64952432]. \t  0.8875025212520042 \t 0.9875226863588296\n",
      "3      \t [ 7.85136279  8.22426624 11.89960844  0.6635413  17.22136924  0.82816725]. \t  \u001b[92m0.9878395388526501\u001b[0m \t 0.9878395388526501\n",
      "4      \t [ 8.09536546  7.93327327 12.10255576  0.93982568 17.20358738  0.1       ]. \t  0.8875793339107304 \t 0.9878395388526501\n",
      "5      \t [ 8.1626096   7.57956794 12.3981503   0.55933326 17.34721432  0.88939182]. \t  0.9874458736309594 \t 0.9878395388526501\n",
      "6      \t [ 8.10070413  7.77681836 12.07091328  1.         17.13911401  1.        ]. \t  \u001b[92m0.9947959174795408\u001b[0m \t 0.9947959174795408\n",
      "7      \t [5.61712585 9.72734704 5.69140993 0.86093722 6.34570498 0.58828895]. \t  0.8875793339107304 \t 0.9947959174795408\n",
      "8      \t [5.17644416 9.73318178 5.         0.84883466 6.90840278 0.77729859]. \t  0.9790444501548358 \t 0.9947959174795408\n",
      "9      \t [ 4.67560522 10.          5.53455279  0.89449216  6.46508203  0.68748566]. \t  0.9790444501548358 \t 0.9947959174795408\n",
      "10     \t [4.89137041 9.14960679 5.36315533 0.93811449 6.41483077 0.74222106]. \t  0.9790492500471372 \t 0.9947959174795408\n",
      "11     \t [5.05493202 9.64602872 5.41596927 0.5        6.46009971 1.        ]. \t  0.9839844857037349 \t 0.9947959174795408\n",
      "12     \t [4.98737472 9.6268541  5.16752044 0.52075037 6.43480779 0.14944945]. \t  0.8874977192853896 \t 0.9947959174795408\n",
      "13     \t [ 8.20598801  7.84286812 11.97756688  0.5        17.10947064  0.67225417]. \t  0.9864041013938357 \t 0.9947959174795408\n",
      "14     \t [ 8.41221233  8.18596459 12.11118655  0.87063441 18.08961539  0.79993122]. \t  0.9878683429082346 \t 0.9947959174795408\n",
      "15     \t [ 8.23830617  8.53574013 12.8013474   0.81788945 17.50497668  0.83512324]. \t  0.9876139011836353 \t 0.9947959174795408\n",
      "16     \t [ 7.54649533  8.17208754 12.61263269  0.78342934 18.06355613  0.7765744 ]. \t  0.9877627253641985 \t 0.9947959174795408\n",
      "17     \t [ 8.3141782   7.80160135 12.99471925  1.         18.04885069  0.58944789]. \t  0.8945836932758181 \t 0.9947959174795408\n",
      "18     \t [ 8.08986078  8.09584604 12.39204231  0.8568551  17.64201577  0.72651336]. \t  0.9878539403964358 \t 0.9947959174795408\n",
      "19     \t [ 8.17415111  8.47032834 12.97863292  0.5        18.5147513   1.        ]. \t  0.9929860126396463 \t 0.9947959174795408\n",
      "20     \t [ 7.64330281  8.14321227 13.56221548  0.5        17.8280882   1.        ]. \t  0.9931252355835875 \t 0.9947959174795408\n",
      "21     \t [ 7.59311188  8.86711085 13.37231385  1.         18.21350073  1.        ]. \t  0.9947575111501776 \t 0.9947959174795408\n",
      "22     \t [ 7.35492066  8.94489978 12.93613165  0.5        17.7571301   1.        ]. \t  0.9931252355835875 \t 0.9947959174795408\n",
      "23     \t [ 7.90529317  9.19102763 12.18816419  0.83799048 18.30767667  1.        ]. \t  0.9944838660534657 \t 0.9947959174795408\n",
      "24     \t [ 7.78060497  8.86001329 13.08082579  0.56844864 18.22499937  0.29569584]. \t  0.8874977192853896 \t 0.9947959174795408\n",
      "25     \t [ 8.64365621  8.95260877 11.41777471  0.83481013 17.66664022  1.        ]. \t  0.9939845796922625 \t 0.9947959174795408\n",
      "26     \t [ 8.04298868  8.64782922 11.27040074  0.5        18.25729163  1.        ]. \t  0.9930052158043279 \t 0.9947959174795408\n",
      "27     \t [ 8.27816643  8.83138665 11.97293579  0.5        17.91272037  1.        ]. \t  0.9931252355835875 \t 0.9947959174795408\n",
      "28     \t [ 8.18671409  8.41278712 11.1035492   1.         17.61834781  1.        ]. \t  \u001b[92m0.9948103205444897\u001b[0m \t 0.9948103205444897\n",
      "29     \t [ 8.24735508  8.86561885 11.3002479   0.92872161 18.0206077   0.24051501]. \t  0.8874977192853896 \t 0.9948103205444897\n",
      "30     \t [ 8.67274964  8.83399663 11.39391238  1.         18.69526342  1.        ]. \t  0.9943926457663008 \t 0.9948103205444897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06481725208023831"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\n",
    "\n",
    "np.random.seed(run_num_5)\n",
    "surrogate_loser_5 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train5, X_test5, y_train5, y_test5 = train_test_split(X, y, test_size=test_perc, random_state=run_num_5)\n",
    "\n",
    "def f_syn_polarity5(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_5, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train5, y=y_train5).mean())\n",
    "    return  score\n",
    "\n",
    "loser_5 = GPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity5, param, n_jobs = -1) # define BayesOpt\n",
    "loser_5.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_5 = loser_5.getResult()[0]\n",
    "params_loser_5['max_depth'] = int(params_loser_5['max_depth'])\n",
    "params_loser_5['min_child_weight'] = int(params_loser_5['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train5 = xgb.DMatrix(X_train5, y_train5)\n",
    "dX_loser_test5 = xgb.DMatrix(X_test5, y_test5)\n",
    "model_loser_5 = xgb.train(params_loser_5, dX_loser_train5)\n",
    "pred_loser_5 = model_loser_5.predict(dX_loser_test5)\n",
    "\n",
    "rmse_loser_5 = np.sqrt(mean_squared_error(pred_loser_5, y_test5))\n",
    "rmse_loser_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [8.92860151 3.31979805 5.         0.99251441 2.         0.57683563]. \t  0.8573338214862317 \t 0.989121369524546\n",
      "init   \t [4.18807429 3.35407849 9.         0.87750649 3.         0.56623277]. \t  0.8635317250300133 \t 0.989121369524546\n",
      "init   \t [ 5.788586    6.45355096 14.          0.70660047 12.          0.82154882]. \t  0.989121369524546 \t 0.989121369524546\n",
      "init   \t [4.58184578 6.73834679 5.         0.90108528 3.         0.65482895]. \t  0.8589277181043972 \t 0.989121369524546\n",
      "init   \t [ 4.42510505  5.75952352 14.          0.97882365 15.          0.29525604]. \t  0.8624707456178685 \t 0.989121369524546\n",
      "1      \t [ 5.37727055  6.24418657 13.99999715  0.78872166 12.90500021  0.66278343]. \t  0.8568442162918638 \t 0.989121369524546\n",
      "2      \t [ 5.10602925  5.87911652 14.00000117  0.71578879 11.93143582  0.71227668]. \t  \u001b[92m0.9892269854782749\u001b[0m \t 0.9892269854782749\n",
      "3      \t [ 5.19776217  6.47317782 14.62821187  0.72116838 12.09222515  0.61050561]. \t  0.8615009913329534 \t 0.9892269854782749\n",
      "4      \t [ 5.8241017   5.69739345 14.40262522  0.69528095 12.21497675  0.69720949]. \t  \u001b[92m0.9892845963551952\u001b[0m \t 0.9892845963551952\n",
      "5      \t [ 5.62944616  6.06332352 14.02046351  0.99610243 12.07909324  0.1       ]. \t  0.8634213022696047 \t 0.9892845963551952\n",
      "6      \t [ 5.50521301  6.02021277 14.20263036  1.         12.17436658  1.        ]. \t  \u001b[92m0.9950743657874556\u001b[0m \t 0.9950743657874556\n",
      "7      \t [ 5.80357559  5.68199107 13.48719101  0.5        12.28008587  0.82225347]. \t  0.9884972596197299 \t 0.9950743657874556\n",
      "8      \t [ 5.54593366  6.0110179  14.02594079  0.5        12.23389356  0.72071637]. \t  0.9885452675314338 \t 0.9950743657874556\n",
      "9      \t [ 5.97508583  5.56207131 13.77619675  0.81681984 11.4111564   0.84032612]. \t  0.9893950145521141 \t 0.9950743657874556\n",
      "10     \t [ 6.60945944  5.75863514 13.70152317  1.         12.14594896  0.84374082]. \t  0.9911665015847775 \t 0.9950743657874556\n",
      "11     \t [ 5.98796585  5.00959098 13.71802995  1.         12.13771837  0.69315203]. \t  0.9912625167167474 \t 0.9950743657874556\n",
      "12     \t [ 5.89006459  5.72009371 13.69733226  1.         12.03367355  0.75993223]. \t  0.9912625167167474 \t 0.9950743657874556\n",
      "13     \t [ 6.57499771  5.09185858 13.85089965  0.5        11.96695526  1.        ]. \t  0.9934420905665841 \t 0.9950743657874556\n",
      "14     \t [ 6.53623801  5.27718011 13.59175375  0.5        11.93098895  0.21647234]. \t  0.8564985544875313 \t 0.9950743657874556\n",
      "15     \t [ 6.52244665  5.19438362 13.705371    0.65827657 13.04275181  0.78968468]. \t  0.9892173869382606 \t 0.9950743657874556\n",
      "16     \t [ 5.13291994  5.29038509 13.84240154  0.89587679 14.00641698  0.48919595]. \t  0.8636757437867724 \t 0.9950743657874556\n",
      "17     \t [ 5.42980256  6.23273396 14.48358106  0.8506491  11.04489029  0.83458604]. \t  0.9892077809998615 \t 0.9950743657874556\n",
      "18     \t [ 6.64880242  4.99206016 12.97125185  0.71330361 12.44800051  1.        ]. \t  0.9942006173000996 \t 0.9950743657874556\n",
      "19     \t [ 5.8346774   4.65880798 13.3477482   0.5        12.92551897  1.        ]. \t  0.9935429055217115 \t 0.9950743657874556\n",
      "20     \t [ 5.90403339  5.30557441 13.07596497  1.         13.30994781  1.        ]. \t  0.9949783494800414 \t 0.9950743657874556\n",
      "21     \t [ 5.95101308  5.03348371 13.08548371  0.70407175 13.07564093  0.28668367]. \t  0.8570026503520216 \t 0.9950743657874556\n",
      "22     \t [ 6.19731628  5.16229163 13.4670111   0.75119943 12.66198402  1.        ]. \t  0.9945366717140135 \t 0.9950743657874556\n",
      "23     \t [ 5.66630413  5.66567539 13.20952606  0.5        14.03233755  0.94808028]. \t  0.9885788727239077 \t 0.9950743657874556\n",
      "24     \t [ 4.9511722   5.38086547 13.0401164   0.5        13.4197869   1.        ]. \t  0.993466092655554 \t 0.9950743657874556\n",
      "25     \t [ 4.79182342  6.01141869 13.26507956  1.         14.20834328  0.82773999]. \t  0.9911617002404568 \t 0.9950743657874556\n",
      "26     \t [ 5.18838882  5.06107909 12.8969862   0.98352186 14.23071385  1.        ]. \t  0.9949351419446452 \t 0.9950743657874556\n",
      "27     \t [ 6.2044297   4.76287163 12.69542971  0.5        13.72274931  1.        ]. \t  0.9932404556779773 \t 0.9950743657874556\n",
      "28     \t [ 4.51703264  5.46584649 13.53214649  0.5        14.59490782  1.        ]. \t  0.9934948969877135 \t 0.9950743657874556\n",
      "29     \t [ 4.66293429  5.56138973 13.0881664   0.54015236 14.54028312  0.2820511 ]. \t  0.8563161235933313 \t 0.9950743657874556\n",
      "30     \t [ 6.35422647  6.0431156  14.65703883  1.         11.40862624  0.82674043]. \t  0.9911809034742821 \t 0.9950743657874556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0601842959615071"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\n",
    "\n",
    "np.random.seed(run_num_6)\n",
    "surrogate_loser_6 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train6, X_test6, y_train6, y_test6 = train_test_split(X, y, test_size=test_perc, random_state=run_num_6)\n",
    "\n",
    "def f_syn_polarity6(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_6, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train6, y=y_train6).mean())\n",
    "    return  score\n",
    "\n",
    "loser_6 = GPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity6, param, n_jobs = -1) # define BayesOpt\n",
    "loser_6.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_6 = loser_6.getResult()[0]\n",
    "params_loser_6['max_depth'] = int(params_loser_6['max_depth'])\n",
    "params_loser_6['min_child_weight'] = int(params_loser_6['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train6 = xgb.DMatrix(X_train6, y_train6)\n",
    "dX_loser_test6 = xgb.DMatrix(X_test6, y_test6)\n",
    "model_loser_6 = xgb.train(params_loser_6, dX_loser_train6)\n",
    "pred_loser_6 = model_loser_6.predict(dX_loser_test6)\n",
    "\n",
    "rmse_loser_6 = np.sqrt(mean_squared_error(pred_loser_6, y_test6))\n",
    "rmse_loser_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.11505126  6.92324143 13.          0.94046175 12.          0.3650384 ]. \t  0.8297634105475625 \t 0.9886700606517818\n",
      "init   \t [ 5.15357029  9.0066636   6.          0.74104227 15.          0.8966337 ]. \t  0.9814544396474091 \t 0.9886700606517818\n",
      "init   \t [ 2.63548913  9.6610934  14.          0.64712408  1.          0.80820948]. \t  0.9886700606517818 \t 0.9886700606517818\n",
      "init   \t [ 1.40821426  1.72758589 12.          0.70495069  5.          0.86632715]. \t  0.9884588284677487 \t 0.9886700606517818\n",
      "init   \t [2.85126987 2.73434054 7.         0.88394733 8.         0.65545939]. \t  0.8285920024977876 \t 0.9886700606517818\n",
      "1      \t [ 3.11101452  9.28260134 13.36659845  0.68217491  1.21135528  0.82304321]. \t  \u001b[92m0.9893085666380245\u001b[0m \t 0.9893085666380245\n",
      "2      \t [ 3.38436655  9.99954672 13.74124648  0.53563981  1.08616909  0.99570233]. \t  0.9882907964898706 \t 0.9893085666380245\n",
      "3      \t [ 2.67569874  9.97902812 13.20884853  0.5034907   1.05365897  0.99601565]. \t  0.9881707758808855 \t 0.9893085666380245\n",
      "4      \t [ 2.84932295  9.80353054 13.68015054  0.5314374   1.75891761  0.96223228]. \t  0.9881947805281754 \t 0.9893085666380245\n",
      "5      \t [ 2.96636065  9.98000797 13.51806424  0.99815195  1.2845649   0.44806549]. \t  0.8283183479283774 \t 0.9893085666380245\n",
      "6      \t [ 2.89003732  9.67536377 13.63848342  1.          1.20369776  1.        ]. \t  \u001b[92m0.9967066425294901\u001b[0m \t 0.9967066425294901\n",
      "7      \t [ 1.83630705  2.01820556 11.54187639  0.75214075  5.57916544  0.82258573]. \t  0.988996516663969 \t 0.9967066425294901\n",
      "8      \t [ 1.4879311   1.23986131 11.71351612  0.7669766   5.67065675  1.        ]. \t  0.99610653976114 \t 0.9967066425294901\n",
      "9      \t [ 1.01098539  1.92307129 11.78186543  0.63663746  5.72776664  0.77115151]. \t  0.9883916179445135 \t 0.9967066425294901\n",
      "10     \t [ 1.23161615  1.64636208 11.16532745  0.72116779  5.21449501  0.84453371]. \t  0.9886892666513584 \t 0.9967066425294901\n",
      "11     \t [ 1.49749641  1.52420269 11.65842362  0.50276129  5.44253668  0.25429818]. \t  0.827454201092506 \t 0.9967066425294901\n",
      "12     \t [ 1.33821897  1.75305931 11.66846998  1.          5.44270697  0.87376329]. \t  0.9934420855190881 \t 0.9967066425294901\n",
      "13     \t [ 1.43028531  1.74548795 11.62208843  0.5         5.43479995  1.        ]. \t  0.9954632306328383 \t 0.9967066425294901\n",
      "14     \t [ 0.49133395  1.03341781 11.85632184  0.61302333  5.15010685  0.87048011]. \t  0.9884828302801433 \t 0.9967066425294901\n",
      "15     \t [ 2.92772217  9.66793661 13.62499129  0.5         1.19442308  1.        ]. \t  0.9950311547257263 \t 0.9967066425294901\n",
      "16     \t [ 4.76454133  8.7376055   6.6488849   0.7555268  14.56707217  0.88665274]. \t  0.9816128657560318 \t 0.9967066425294901\n",
      "17     \t [ 5.47228689  8.80715208  6.23128228  0.78380235 14.22141815  1.        ]. \t  0.9887564943913957 \t 0.9967066425294901\n",
      "18     \t [ 4.80110493  9.36858132  6.09749927  0.74181728 14.27370191  0.99995317]. \t  0.9814544396474091 \t 0.9967066425294901\n",
      "19     \t [ 5.29489441  9.32684333  6.55577793  0.74179363 14.55462043  0.48740833]. \t  0.8280591088697972 \t 0.9967066425294901\n",
      "20     \t [ 4.85658628  8.67686277  5.88952484  0.93703033 14.35951327  0.5246929 ]. \t  0.8257931347459376 \t 0.9967066425294901\n",
      "21     \t [ 1.98350086  9.24092494 13.28396862  0.79450126  1.51462947  0.65050106]. \t  0.8289088531938699 \t 0.9967066425294901\n",
      "22     \t [ 3.61939887  9.95311725 12.75889868  0.59702365  1.6789081   1.        ]. \t  0.9955400405949568 \t 0.9967066425294901\n",
      "23     \t [ 2.80034782  9.48438307 12.53023209  0.7685322   1.89178553  1.        ]. \t  0.9963225766775393 \t 0.9967066425294901\n",
      "24     \t [ 3.24752382  9.54467393 12.29607139  0.88588405  1.00715847  1.        ]. \t  0.9964521992145841 \t 0.9967066425294901\n",
      "25     \t [ 3.66021094  9.04256209 12.57977751  1.          1.64972237  1.        ]. \t  0.9964810044456128 \t 0.9967066425294901\n",
      "26     \t [ 3.33385535  9.23273849 12.35514864  0.5         1.51177805  0.46090643]. \t  0.8263644026205758 \t 0.9967066425294901\n",
      "27     \t [ 3.23812387  9.55354904 12.81537227  0.91890231  1.46957176  1.        ]. \t  0.9965290129796106 \t 0.9967066425294901\n",
      "28     \t [ 4.25012939  9.35269357 12.68724589  0.51808934  1.          1.        ]. \t  0.9948151181550461 \t 0.9967066425294901\n",
      "29     \t [ 3.54556836  8.57534722 12.38393324  0.58542583  1.          1.        ]. \t  0.9955976501581453 \t 0.9967066425294901\n",
      "30     \t [ 3.94732444  9.21086704 11.82461945  0.5         1.35353805  1.        ]. \t  0.9946902970314658 \t 0.9967066425294901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06331398538451588"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\n",
    "\n",
    "np.random.seed(run_num_7)\n",
    "surrogate_loser_7 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train7, X_test7, y_train7, y_test7 = train_test_split(X, y, test_size=test_perc, random_state=run_num_7)\n",
    "\n",
    "def f_syn_polarity7(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_7, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train7, y=y_train7).mean())\n",
    "    return  score\n",
    "\n",
    "loser_7 = GPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity7, param, n_jobs = -1) # define BayesOpt\n",
    "loser_7.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_7 = loser_7.getResult()[0]\n",
    "params_loser_7['max_depth'] = int(params_loser_7['max_depth'])\n",
    "params_loser_7['min_child_weight'] = int(params_loser_7['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train7 = xgb.DMatrix(X_train7, y_train7)\n",
    "dX_loser_test7 = xgb.DMatrix(X_test7, y_test7)\n",
    "model_loser_7 = xgb.train(params_loser_7, dX_loser_train7)\n",
    "pred_loser_7 = model_loser_7.predict(dX_loser_test7)\n",
    "\n",
    "rmse_loser_7 = np.sqrt(mean_squared_error(pred_loser_7, y_test7))\n",
    "rmse_loser_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.09499875  0.42215015  7.          0.62339266 17.          0.52353668]. \t  0.8894852354753713 \t 0.8894852354753713\n",
      "init   \t [ 9.1272854   1.60640294  7.          0.68662086 17.          0.1484676 ]. \t  0.8891731849480878 \t 0.8894852354753713\n",
      "init   \t [ 6.02074372  4.29318577 11.          0.77432386  5.          0.51209718]. \t  0.8891155762837446 \t 0.8894852354753713\n",
      "init   \t [2.81633282 5.85246014 5.         0.54171534 6.         0.39755341]. \t  0.8890483697707694 \t 0.8894852354753713\n",
      "init   \t [ 6.23734577  3.09242107 14.          0.6186205  15.          0.51761313]. \t  0.8892067935977129 \t 0.8894852354753713\n",
      "1      \t [6.07385922e-01 1.77561699e-04 7.70123324e+00 5.30464199e-01\n",
      " 1.66493835e+01 5.76554988e-01]. \t  \u001b[92m0.8897972824072298\u001b[0m \t 0.8897972824072298\n",
      "2      \t [ 0.99314801  0.80566768  7.92019244  0.66762468 17.04262647  0.66074261]. \t  0.889475634031314 \t 0.8897972824072298\n",
      "3      \t [ 1.59221857  0.1126534   7.73119865  0.68373097 16.66426633  0.68053068]. \t  \u001b[92m0.983283548617921\u001b[0m \t 0.983283548617921\n",
      "4      \t [ 1.11327686  0.59178104  7.45267191  0.61730255 16.26258513  1.        ]. \t  \u001b[92m0.9921362769573058\u001b[0m \t 0.9921362769573058\n",
      "5      \t [ 1.21779383  0.57468836  7.60059072  0.5        16.33591533  0.14826368]. \t  0.888851540167597 \t 0.9921362769573058\n",
      "6      \t [ 1.06911769  0.36540255  7.55009648  1.         16.56228521  0.63046063]. \t  0.8956879064836031 \t 0.9921362769573058\n",
      "7      \t [ 1.19310028  0.3291751   7.56371177  0.5        16.83179701  1.        ]. \t  0.9917522113127405 \t 0.9921362769573058\n",
      "8      \t [ 1.23411236  0.38915981  8.44878108  0.5        16.14046449  0.91172016]. \t  0.983624418827075 \t 0.9921362769573058\n",
      "9      \t [ 1.89774287  0.96488757  8.05135654  0.5        16.32805035  0.94797657]. \t  0.9834227846298779 \t 0.9921362769573058\n",
      "10     \t [ 1.90561699  0.28671195  7.91267148  0.5        15.65382966  0.99513956]. \t  0.9829762935578142 \t 0.9921362769573058\n",
      "11     \t [ 1.49492301  0.47263532  7.88652126  0.5        16.25323975  0.8303152 ]. \t  0.9831923297828115 \t 0.9921362769573058\n",
      "12     \t [ 2.24257758  0.48645872  8.7964161   0.84128562 15.82648231  1.        ]. \t  \u001b[92m0.9938357585531158\u001b[0m \t 0.9938357585531158\n",
      "13     \t [ 1.70265413  1.05982465  8.48182676  0.7015384  15.33416696  1.        ]. \t  0.9937013428997391 \t 0.9938357585531158\n",
      "14     \t [ 2.50818034  0.94818337  7.93068881  1.         15.57268729  1.        ]. \t  0.9918962338033116 \t 0.9938357585531158\n",
      "15     \t [ 1.81969458  0.67597681  8.25733191  1.         15.79919657  1.        ]. \t  \u001b[92m0.9938933708128838\u001b[0m \t 0.9938933708128838\n",
      "16     \t [ 2.43714085  0.91888894  8.39632362  0.5        15.53837501  1.        ]. \t  0.9928467987523796 \t 0.9938933708128838\n",
      "17     \t [ 1.86445069  1.36350233  7.22911854  0.53302948 15.46424761  1.        ]. \t  0.991709008409908 \t 0.9938933708128838\n",
      "18     \t [ 2.19241953  0.81403473  6.86882369  0.7117335  16.22899196  1.        ]. \t  0.98862207735359 \t 0.9938933708128838\n",
      "19     \t [ 1.52908502  0.08265746  9.11666705  0.5        15.25110471  1.        ]. \t  0.9935333094698894 \t 0.9938933708128838\n",
      "20     \t [ 1.66303128  1.58301783  7.10383049  0.74710599 16.45781512  0.88375429]. \t  0.9835187828910374 \t 0.9938933708128838\n",
      "21     \t [ 2.31540576  1.26705811  7.36598751  0.69245213 16.00625379  0.3248611 ]. \t  0.8894228260889996 \t 0.9938933708128838\n",
      "22     \t [ 1.74636422  0.36551102  8.90785787  0.53757812 15.56763438  0.25656655]. \t  0.8893460123239728 \t 0.9938933708128838\n",
      "23     \t [ 2.53955575  1.66672727  7.40836404  0.56082018 16.09990808  1.        ]. \t  0.9915937807098038 \t 0.9938933708128838\n",
      "24     \t [ 1.44010773  0.73189808  9.50830775  0.5        15.82300269  1.        ]. \t  0.9933460752262091 \t 0.9938933708128838\n",
      "25     \t [ 1.34026745  0.          9.3893377   0.93084914 16.11942891  0.82302353]. \t  0.9859335980667959 \t 0.9938933708128838\n",
      "26     \t [ 2.06416324  1.17830047  7.44450596  0.70347499 15.98664987  1.        ]. \t  0.9916657937528018 \t 0.9938933708128838\n",
      "27     \t [ 1.31939991  1.10991149  6.35046764  0.5        15.99133334  0.77775808]. \t  0.9788283799777714 \t 0.9938933708128838\n",
      "28     \t [ 2.32409496  1.15602962  7.31999372  0.5        17.23293274  0.73993341]. \t  0.9828658769511565 \t 0.9938933708128838\n",
      "29     \t [ 1.03018407  0.48894325  9.18432386  1.         15.4805768   0.82930308]. \t  0.9929140031910713 \t 0.9938933708128838\n",
      "30     \t [ 2.19373381  0.41874891  8.14611235  0.93978675 14.69186342  0.94977098]. \t  0.9848390102123 \t 0.9938933708128838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0646401566773533"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\n",
    "\n",
    "np.random.seed(run_num_8)\n",
    "surrogate_loser_8 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train8, X_test8, y_train8, y_test8 = train_test_split(X, y, test_size=test_perc, random_state=run_num_8)\n",
    "\n",
    "def f_syn_polarity8(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_8, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train8, y=y_train8).mean())\n",
    "    return  score\n",
    "\n",
    "loser_8 = GPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity8, param, n_jobs = -1) # define BayesOpt\n",
    "loser_8.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_8 = loser_8.getResult()[0]\n",
    "params_loser_8['max_depth'] = int(params_loser_8['max_depth'])\n",
    "params_loser_8['min_child_weight'] = int(params_loser_8['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train8 = xgb.DMatrix(X_train8, y_train8)\n",
    "dX_loser_test8 = xgb.DMatrix(X_test8, y_test8)\n",
    "model_loser_8 = xgb.train(params_loser_8, dX_loser_train8)\n",
    "pred_loser_8 = model_loser_8.predict(dX_loser_test8)\n",
    "\n",
    "rmse_loser_8 = np.sqrt(mean_squared_error(pred_loser_8, y_test8))\n",
    "rmse_loser_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.72278559  4.88078399 14.          0.8670313   5.          0.82724497]. \t  0.968425042143454 \t 0.968425042143454\n",
      "init   \t [ 5.6561742   2.97622499 12.          0.75688314 17.          0.10614316]. \t  0.7923984116619751 \t 0.968425042143454\n",
      "init   \t [7.69793028 7.46767101 7.         0.74680784 9.         0.93605355]. \t  0.9563894026036909 \t 0.968425042143454\n",
      "init   \t [ 3.95454044  9.73956297 10.          0.66345176 13.          0.59891121]. \t  0.7929360981296009 \t 0.968425042143454\n",
      "init   \t [ 2.92269116  8.1614236  14.          0.61078869 19.          0.13894609]. \t  0.7908333515969722 \t 0.968425042143454\n",
      "1      \t [ 7.60863201  5.02369233 14.          0.86260025  4.76578023  0.79156586]. \t  0.9682042036061591 \t 0.968425042143454\n",
      "2      \t [ 7.01867964  5.61674133 14.30475738  0.94188818  4.70375305  0.99335795]. \t  \u001b[92m0.9686170722691062\u001b[0m \t 0.9686170722691062\n",
      "3      \t [ 6.94707024  5.15822773 13.77895865  0.85858015  4.19716459  0.71477874]. \t  0.9683578287853237 \t 0.9686170722691062\n",
      "4      \t [ 7.05664528  4.83312407 14.58751021  0.82670471  4.39436483  0.84261108]. \t  0.9681801987514379 \t 0.9686170722691062\n",
      "5      \t [ 7.05006332  5.22547049 14.26975394  0.99999212  4.7068809   0.17813064]. \t  0.7911454074482495 \t 0.9686170722691062\n",
      "6      \t [ 7.07089656  5.14575715 14.09874337  0.5         4.62795132  0.98076935]. \t  0.9642435257913982 \t 0.9686170722691062\n",
      "7      \t [ 7.08730422  4.99311405 14.07067438  1.          4.56062392  1.        ]. \t  \u001b[92m0.9961209418580762\u001b[0m \t 0.9961209418580762\n",
      "8      \t [ 5.97121737  5.28031426 14.41860967  0.84478762  4.30612081  0.84878932]. \t  0.9685162576596978 \t 0.9961209418580762\n",
      "9      \t [ 6.01321665  5.75236883 13.59193839  0.92303988  4.70439538  0.81645187]. \t  0.968837909630957 \t 0.9961209418580762\n",
      "10     \t [ 5.76573988  4.92507603 13.54967529  0.65775359  4.40803189  0.52013798]. \t  0.7917118958280024 \t 0.9961209418580762\n",
      "11     \t [ 6.14256639  5.96582493 13.91720365  0.53302315  3.93714356  0.59771276]. \t  0.7893307039606419 \t 0.9961209418580762\n",
      "12     \t [ 5.91852392  5.62650688 14.2540838   0.5         5.06704737  0.61341902]. \t  0.791956736177692 \t 0.9961209418580762\n",
      "13     \t [7.04763741 7.02413675 7.00030053 0.7202028  9.51741427 0.94726596]. \t  0.9568022707134878 \t 0.9961209418580762\n",
      "14     \t [7.2822724  7.68609197 6.38924343 0.71666196 9.52134286 1.        ]. \t  0.9885452594416112 \t 0.9961209418580762\n",
      "15     \t [7.23223866 7.88907959 7.24600091 0.69813547 9.63169467 0.96818666]. \t  0.9564854155230597 \t 0.9961209418580762\n",
      "16     \t [6.83413727 7.67502132 6.86038236 0.55554614 8.94007543 1.        ]. \t  0.988449242788478 \t 0.9961209418580762\n",
      "17     \t [7.12476556 7.60145877 6.83429492 0.90950607 9.27507859 0.32684107]. \t  0.7876840247068085 \t 0.9961209418580762\n",
      "18     \t [7.28830892 7.50923933 6.86751489 0.5        9.35381708 1.        ]. \t  0.9884684463680222 \t 0.9961209418580762\n",
      "19     \t [7.10675764 7.58379693 6.8920361  1.         9.23497782 1.        ]. \t  0.9883964336015972 \t 0.9961209418580762\n",
      "20     \t [6.24191802 7.73472495 6.61741674 0.59498053 9.91073175 1.        ]. \t  0.9885068527665292 \t 0.9961209418580762\n",
      "21     \t [6.59288259 8.60259075 6.45051916 0.55381321 9.52207719 1.        ]. \t  0.988410836459115 \t 0.9961209418580762\n",
      "22     \t [6.2805638  7.92502177 5.86092957 0.54259368 9.26211939 1.        ]. \t  0.9835908194448876 \t 0.9961209418580762\n",
      "23     \t [ 6.60648134  8.20026605  5.81629667  0.84802601 10.10425996  1.        ]. \t  0.98369643747293 \t 0.9961209418580762\n",
      "24     \t [6.47909555 8.08039825 6.26225433 1.         9.56603059 1.        ]. \t  0.9884300392089339 \t 0.9961209418580762\n",
      "25     \t [6.56975745 8.09908571 6.07736353 0.5        9.80060026 0.51435088]. \t  0.786968706545846 \t 0.9961209418580762\n",
      "26     \t [6.45519724 7.03490854 5.74873127 0.77705281 9.95815254 1.        ]. \t  0.9837348436640055 \t 0.9961209418580762\n",
      "27     \t [6.19674723 6.77585584 6.23614729 0.53231444 9.16797326 1.        ]. \t  0.9884588447165377 \t 0.9961209418580762\n",
      "28     \t [7.04145043 6.79632054 5.94986268 0.80290775 8.8987493  1.        ]. \t  0.9837588482421517 \t 0.9961209418580762\n",
      "29     \t [6.62484352 7.26305117 6.13759431 0.63283633 9.41632085 1.        ]. \t  0.9884924501164428 \t 0.9961209418580762\n",
      "30     \t [6.67509736 5.964078   6.1385742  1.         9.41854069 1.        ]. \t  0.9884300392089339 \t 0.9961209418580762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06109824360218985"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\n",
    "\n",
    "np.random.seed(run_num_9)\n",
    "surrogate_loser_9 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train9, X_test9, y_train9, y_test9 = train_test_split(X, y, test_size=test_perc, random_state=run_num_9)\n",
    "\n",
    "def f_syn_polarity9(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_9, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train9, y=y_train9).mean())\n",
    "    return  score\n",
    "\n",
    "loser_9 = GPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity9, param, n_jobs = -1) # define BayesOpt\n",
    "loser_9.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_9 = loser_9.getResult()[0]\n",
    "params_loser_9['max_depth'] = int(params_loser_9['max_depth'])\n",
    "params_loser_9['min_child_weight'] = int(params_loser_9['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train9 = xgb.DMatrix(X_train9, y_train9)\n",
    "dX_loser_test9 = xgb.DMatrix(X_test9, y_test9)\n",
    "model_loser_9 = xgb.train(params_loser_9, dX_loser_train9)\n",
    "pred_loser_9 = model_loser_9.predict(dX_loser_test9)\n",
    "\n",
    "rmse_loser_9 = np.sqrt(mean_squared_error(pred_loser_9, y_test9))\n",
    "rmse_loser_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.53589585  1.15006943  6.          0.93623727 14.          0.13663866]. \t  0.8925625895791941 \t 0.9854871661142187\n",
      "init   \t [ 3.97194461  2.33132197 14.          0.9533253  19.          0.26403087]. \t  0.8916888516708369 \t 0.9854871661142187\n",
      "init   \t [ 7.43539415  0.69582081  9.          0.9763222  11.          0.12608349]. \t  0.8916264438056544 \t 0.9854871661142187\n",
      "init   \t [ 9.82027485  3.39637684  9.          0.78522537 19.          0.87618843]. \t  0.9854871661142187 \t 0.9854871661142187\n",
      "init   \t [ 0.57576207  5.82646405 12.          0.77704362 18.          0.11865044]. \t  0.8923177518569682 \t 0.9854871661142187\n",
      "1      \t [ 9.99921704  3.04883105  8.50435002  0.87840248 19.64332185  0.82478253]. \t  \u001b[92m0.9856695950032489\u001b[0m \t 0.9856695950032489\n",
      "2      \t [ 9.23926256  3.00793564  8.47774568  0.86206135 19.17157042  0.78704598]. \t  \u001b[92m0.9858280233936165\u001b[0m \t 0.9858280233936165\n",
      "3      \t [ 9.81193127  3.58930446  8.14938051  0.94791713 19.06354926  0.81955086]. \t  0.9858088201597912 \t 0.9858280233936165\n",
      "4      \t [ 9.994002    2.85865856  8.3654222   0.59931602 18.83430448  0.80929993]. \t  0.9844453849575467 \t 0.9858280233936165\n",
      "5      \t [ 9.83332198  3.15855023  8.57283148  0.99831584 19.09233391  0.17516287]. \t  0.8899557709675366 \t 0.9858280233936165\n",
      "6      \t [ 9.67882081  3.36956462  8.46809066  0.5        19.31373852  0.67777536]. \t  0.9840613215256284 \t 0.9858280233936165\n",
      "7      \t [ 6.74858966  1.04266026  6.70937147  0.94571366 13.29062732  0.13414017]. \t  0.8925865933967584 \t 0.9858280233936165\n",
      "8      \t [ 6.22620065  1.23246257  6.95214656  0.9649824  14.10001513  0.15452516]. \t  0.8927978313197255 \t 0.9858280233936165\n",
      "9      \t [ 6.48926092  0.34281502  6.5940952   0.99516472 13.95599695  0.14673069]. \t  0.8928890457296689 \t 0.9858280233936165\n",
      "10     \t [ 5.8557686   0.87887135  6.38975097  0.93251579 13.47895957  0.35498523]. \t  0.8927738238375406 \t 0.9858280233936165\n",
      "11     \t [ 6.56200607  0.95583591  6.56125133  0.9954017  13.84937813  0.92667167]. \t  0.9828131237728633 \t 0.9858280233936165\n",
      "12     \t [ 6.45699477  0.90722154  6.56526173  0.5        13.82098306  0.44892558]. \t  0.888390716019174 \t 0.9858280233936165\n",
      "13     \t [ 7.14180678  0.75916542  8.31617604  1.         11.78610783  0.21337261]. \t  0.8873680894882475 \t 0.9858280233936165\n",
      "14     \t [ 7.81357606  1.11163773  8.16014779  0.96050286 11.11675339  0.1       ]. \t  0.8917032519008908 \t 0.9858280233936165\n",
      "15     \t [ 6.84693381  1.14036218  8.31720722  0.92771523 10.88769272  0.1       ]. \t  0.8920489103863217 \t 0.9858280233936165\n",
      "16     \t [ 7.29653909  0.27468277  8.09571317  1.         10.93037716  0.1       ]. \t  0.8873680894882475 \t 0.9858280233936165\n",
      "17     \t [ 7.32951866  0.81530133  8.37064997  1.         11.04752763  0.89540526]. \t  0.9832355567496521 \t 0.9858280233936165\n",
      "18     \t [ 7.33249277  0.75633699  8.3865702   0.5        11.14509249  0.40417605]. \t  0.8901190087229036 \t 0.9858280233936165\n",
      "19     \t [ 7.07931098  0.98654361  7.34989274  1.         11.38856241  0.65241097]. \t  0.8873680894882475 \t 0.9858280233936165\n",
      "20     \t [ 9.79611416  3.10954991  8.51303953  1.         19.09334607  1.        ]. \t  \u001b[92m0.9938453588917785\u001b[0m \t 0.9938453588917785\n",
      "21     \t [ 9.3109297   3.47418297  8.49243598  0.62574072 18.1532274   0.65290782]. \t  0.8918856839013913 \t 0.9938453588917785\n",
      "22     \t [ 6.31853166  0.62908772  7.37912811  1.         13.05933007  0.86102402]. \t  0.9819489393227738 \t 0.9938453588917785\n",
      "23     \t [ 6.67446926  0.32504749  6.62360805  1.         12.73986322  0.95306315]. \t  0.9777770211650133 \t 0.9938453588917785\n",
      "24     \t [ 7.29549281  0.60455057  7.39169872  1.         12.56646067  1.        ]. \t  0.9911808964907601 \t 0.9938453588917785\n",
      "25     \t [ 6.7177025   0.19063457  7.57486693  1.         11.94998833  1.        ]. \t  0.9920066439807904 \t 0.9938453588917785\n",
      "26     \t [ 6.595531    0.98761017  7.1511251   1.         12.39451089  1.        ]. \t  0.9919346319749472 \t 0.9938453588917785\n",
      "27     \t [ 6.77968524  0.39930482  7.22941571  1.         12.36838335  0.43269132]. \t  0.8873680894882475 \t 0.9938453588917785\n",
      "28     \t [ 9.27606876  3.02019827  7.49634273  0.70667719 18.57655027  0.46739614]. \t  0.889274054126996 \t 0.9938453588917785\n",
      "29     \t [ 7.74793554  0.25124411  7.73976221  1.         11.58598616  0.99347626]. \t  0.9819009309270635 \t 0.9938453588917785\n",
      "30     \t [ 8.67297315  3.81182241  7.90088619  1.         18.71468693  0.44675353]. \t  0.8873392847412253 \t 0.9938453588917785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06738310386109525"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\n",
    "\n",
    "np.random.seed(run_num_10)\n",
    "surrogate_loser_10 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train10, X_test10, y_train10, y_test10 = train_test_split(X, y, test_size=test_perc, random_state=run_num_10)\n",
    "\n",
    "def f_syn_polarity10(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_10, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train10, y=y_train10).mean())\n",
    "    return  score\n",
    "\n",
    "loser_10 = GPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity10, param, n_jobs = -1) # define BayesOpt\n",
    "loser_10.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_10 = loser_10.getResult()[0]\n",
    "params_loser_10['max_depth'] = int(params_loser_10['max_depth'])\n",
    "params_loser_10['min_child_weight'] = int(params_loser_10['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train10 = xgb.DMatrix(X_train10, y_train10)\n",
    "dX_loser_test10 = xgb.DMatrix(X_test10, y_test10)\n",
    "model_loser_10 = xgb.train(params_loser_10, dX_loser_train10)\n",
    "pred_loser_10 = model_loser_10.predict(dX_loser_test10)\n",
    "\n",
    "rmse_loser_10 = np.sqrt(mean_squared_error(pred_loser_10, y_test10))\n",
    "rmse_loser_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 9.81035663  8.2124785   9.          0.68487173 15.          0.28207988]. \t  0.8223029792710461 \t 0.9791740729684557\n",
      "init   \t [ 4.91906771  1.38558332  6.          0.618878   19.          0.10263204]. \t  0.8222694087887517 \t 0.9791740729684557\n",
      "init   \t [ 5.23426175  1.89971159 13.          0.76313779 17.          0.46597853]. \t  0.8261292537401452 \t 0.9791740729684557\n",
      "init   \t [ 4.0204056   3.75167577  9.          0.86204818 17.          0.26560056]. \t  0.8280591870022723 \t 0.9791740729684557\n",
      "init   \t [8.8704459  5.64917815 5.         0.51406171 1.         0.82826731]. \t  0.9791740729684557 \t 0.9791740729684557\n",
      "1      \t [8.02776449 5.47540798 5.27568131 0.55701687 1.00000002 0.68903624]. \t  \u001b[92m0.9810991916797995\u001b[0m \t 0.9810991916797995\n",
      "2      \t [8.67133264 5.02712129 5.49265605 0.55613132 1.3803911  0.77636521]. \t  \u001b[92m0.9810991916797995\u001b[0m \t 0.9810991916797995\n",
      "3      \t [8.58941355 5.13466314 5.00072391 0.500018   1.0000999  0.1466084 ]. \t  0.8133783707838496 \t 0.9810991916797995\n",
      "4      \t [8.67387658 5.72743749 5.82100424 0.53256176 1.         0.52979637]. \t  0.8069404171025498 \t 0.9810991916797995\n",
      "5      \t [8.41967062 5.67757242 5.08955131 0.50001164 1.75574822 0.67224153]. \t  0.980033410646759 \t 0.9810991916797995\n",
      "6      \t [8.38081825 5.26498487 5.         0.5        1.32805045 1.        ]. \t  \u001b[92m0.9838548567363183\u001b[0m \t 0.9838548567363183\n",
      "7      \t [8.52557286 5.51111179 5.20467464 1.         1.3105852  0.69829372]. \t  0.9812720215448092 \t 0.9838548567363183\n",
      "8      \t [7.7402235  5.05308436 5.69711818 0.63134108 1.87957187 0.35753008]. \t  0.8133207458015974 \t 0.9838548567363183\n",
      "9      \t [8.05007272 5.64269555 5.85898751 0.51794583 1.71880048 1.        ]. \t  0.9837684395912124 \t 0.9838548567363183\n",
      "10     \t [ 4.24607657  3.15825169  8.24876415  0.80095874 17.50138334  0.22439416]. \t  0.8267773655264996 \t 0.9838548567363183\n",
      "11     \t [ 4.46019619  2.88430144  9.2760866   0.7931472  17.39278585  0.27417854]. \t  0.8261772584712351 \t 0.9838548567363183\n",
      "12     \t [ 3.45107353  2.93892675  8.91675997  0.85860425 17.39567533  0.4218199 ]. \t  0.8289905408350459 \t 0.9838548567363183\n",
      "13     \t [ 4.18712237  3.04132678  8.71132361  0.78022707 16.91576228  0.99942588]. \t  \u001b[92m0.9853815255453027\u001b[0m \t 0.9853815255453027\n",
      "14     \t [ 4.15405658  3.416848    8.901019    0.97163227 17.68232549  0.99643371]. \t  0.98504547133882 \t 0.9853815255453027\n",
      "15     \t [ 4.0709942   3.29857218  8.86055872  0.5        17.39642505  0.7592885 ]. \t  0.9848246339078256 \t 0.9853815255453027\n",
      "16     \t [ 4.22026193  2.45931881  8.60751051  1.         17.69047689  1.        ]. \t  \u001b[92m0.9933748738895524\u001b[0m \t 0.9933748738895524\n",
      "17     \t [ 4.15254256  2.99505761  8.76643474  1.         17.36660297  0.74642947]. \t  0.9845989876649819 \t 0.9933748738895524\n",
      "18     \t [ 5.02285689  2.83547726  8.50014665  0.5735889  17.68070851  1.        ]. \t  0.9931444338390604 \t 0.9933748738895524\n",
      "19     \t [ 4.66437322  2.17312781  7.58215871  0.60092738 18.23946257  0.78689454]. \t  0.984411761580258 \t 0.9933748738895524\n",
      "20     \t [ 4.43305837  2.77385619  8.28663923  0.59933414 18.46077833  1.        ]. \t  0.9931924415433327 \t 0.9933748738895524\n",
      "21     \t [ 4.33324307  2.62712471  7.93506907  0.5        17.65220066  1.        ]. \t  0.9905904027722793 \t 0.9933748738895524\n",
      "22     \t [ 4.63170766  2.1276489   8.42669581  0.5        18.07248025  0.50924676]. \t  0.8236711984625216 \t 0.9933748738895524\n",
      "23     \t [ 4.81479246  2.71124281  7.93272333  1.         18.08542613  1.        ]. \t  0.9912049052866768 \t 0.9933748738895524\n",
      "24     \t [ 4.58902162  2.28069622  6.70925221  0.85809453 18.80011925  0.14418478]. \t  0.8240457068458653 \t 0.9933748738895524\n",
      "25     \t [ 4.85322951  1.56969196  6.74906953  1.         18.99686662  0.89049157]. \t  0.9838404603091725 \t 0.9933748738895524\n",
      "26     \t [ 5.41406922  1.95868731  6.61891958  0.52883063 18.75199207  0.66698502]. \t  0.9820593748056385 \t 0.9933748738895524\n",
      "27     \t [ 5.00796095  1.63735054  6.49023332  1.         18.21453177  0.573226  ]. \t  0.7870598907399559 \t 0.9933748738895524\n",
      "28     \t [ 5.18136766  1.49994282  7.08004649  0.70432498 19.0058663   0.20578982]. \t  0.8252267078350081 \t 0.9933748738895524\n",
      "29     \t [ 4.64019426  1.81057749  6.54208564  0.5        18.86828755  0.79662719]. \t  0.983874076426364 \t 0.9933748738895524\n",
      "30     \t [ 4.94843632  3.87647215  8.64219148  0.72645105 17.01511945  1.        ]. \t  \u001b[92m0.9934852898738701\u001b[0m \t 0.9934852898738701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06350779003544968"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\n",
    "\n",
    "np.random.seed(run_num_11)\n",
    "surrogate_loser_11 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train11, X_test11, y_train11, y_test11 = train_test_split(X, y, test_size=test_perc, random_state=run_num_11)\n",
    "\n",
    "def f_syn_polarity11(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train11, y=y_train11).mean())\n",
    "    return  score\n",
    "\n",
    "loser_11 = GPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity11, param, n_jobs = -1) # define BayesOpt\n",
    "loser_11.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_11 = loser_11.getResult()[0]\n",
    "params_loser_11['max_depth'] = int(params_loser_11['max_depth'])\n",
    "params_loser_11['min_child_weight'] = int(params_loser_11['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train11 = xgb.DMatrix(X_train11, y_train11)\n",
    "dX_loser_test11 = xgb.DMatrix(X_test11, y_test11)\n",
    "model_loser_11 = xgb.train(params_loser_11, dX_loser_train11)\n",
    "pred_loser_11 = model_loser_11.predict(dX_loser_test11)\n",
    "\n",
    "rmse_loser_11 = np.sqrt(mean_squared_error(pred_loser_11, y_test11))\n",
    "rmse_loser_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.9151945   6.22108771  9.          0.89267929 16.          0.87460279]. \t  0.9904271937791528 \t 0.9904271937791528\n",
      "init   \t [ 1.50636966  1.98518759 11.          0.67890863 17.          0.5381501 ]. \t  0.88993656488314 \t 0.9904271937791528\n",
      "init   \t [ 3.31015428  8.02639569  7.          0.75154158 13.          0.11992952]. \t  0.8906470902736388 \t 0.9904271937791528\n",
      "init   \t [ 2.90728553  2.46394443  5.          0.94461307 17.          0.43194161]. \t  0.8861198789973782 \t 0.9904271937791528\n",
      "init   \t [ 9.33140102  6.51378143  8.          0.76907392 18.          0.38515251]. \t  0.8871856699871236 \t 0.9904271937791528\n",
      "1      \t [ 2.20850053  6.60048665  8.57964171  0.86300391 15.36962087  0.71585139]. \t  0.9902207578919282 \t 0.9904271937791528\n",
      "2      \t [ 1.45617388  6.55019346  9.05143081  0.82970662 15.32482527  0.76226071]. \t  \u001b[92m0.9905136049779549\u001b[0m \t 0.9905136049779549\n",
      "3      \t [ 2.21722526  6.25514507  9.38183001  0.75613866 15.28000918  0.78017764]. \t  0.9903599839473237 \t 0.9905136049779549\n",
      "4      \t [ 1.82903751  5.82368155  8.72299959  0.83636053 15.2772349   0.77249248]. \t  0.9902015541741003 \t 0.9905136049779549\n",
      "5      \t [ 1.91662896  6.32723391  8.86663821  0.5        15.46596048  1.        ]. \t  \u001b[92m0.9933604722757284\u001b[0m \t 0.9933604722757284\n",
      "6      \t [ 1.90480431  6.25797608  8.96530636  0.63230995 15.57813388  0.15195825]. \t  0.8899173595058852 \t 0.9933604722757284\n",
      "7      \t [ 1.93576127  6.29430916  8.97306839  1.         15.42803472  0.78384771]. \t  0.9883196130589503 \t 0.9933604722757284\n",
      "8      \t [ 1.9748437   6.47343758  8.7752699   0.5        14.37973619  0.47155165]. \t  0.8906614928545813 \t 0.9933604722757284\n",
      "9      \t [ 1.3574762   5.66131954  9.66514247  0.5440455  15.76437399  0.82961716]. \t  0.9900239244167598 \t 0.9933604722757284\n",
      "10     \t [ 1.36574245  5.80085466  9.55839257  0.5        14.73208926  0.61558931]. \t  0.8901333982200228 \t 0.9933604722757284\n",
      "11     \t [ 2.83281059  7.40745109  7.68036306  0.77106742 14.02765493  0.36454908]. \t  0.8898357448805444 \t 0.9933604722757284\n",
      "12     \t [ 3.02020244  6.44941865  8.18746338  0.55329374 14.70192964  0.46959879]. \t  0.8903254326325895 \t 0.9933604722757284\n",
      "13     \t [ 2.14562021  6.63795377  7.61325748  0.63665844 14.75430338  0.41539621]. \t  0.8895909010045218 \t 0.9933604722757284\n",
      "14     \t [ 2.51935631  7.30108451  8.27631854  0.5        14.88935316  0.16602242]. \t  0.8897157233726901 \t 0.9933604722757284\n",
      "15     \t [ 2.51865364  7.1052863   8.09313268  0.5        14.60021308  1.        ]. \t  0.9933268654238159 \t 0.9933604722757284\n",
      "16     \t [ 1.54616138  6.55504856  9.99251779  0.53963151 15.9095262   0.74103612]. \t  0.9901967516543593 \t 0.9933604722757284\n",
      "17     \t [ 2.95161298  7.19195612  7.6092232   0.96292332 15.1194367   0.74317687]. \t  0.9897646769917956 \t 0.9933604722757284\n",
      "18     \t [ 2.59439506  6.95238592  8.08246698  1.         14.65841868  0.63994611]. \t  0.8979635018268172 \t 0.9933604722757284\n",
      "19     \t [ 3.37155149  7.77131022  6.90661374  0.5        14.11249519  0.65013402]. \t  0.8878385869898504 \t 0.9933604722757284\n",
      "20     \t [ 2.71435808  6.88720263  7.76900165  0.5        15.255063    0.72977448]. \t  0.9885740670910214 \t 0.9933604722757284\n",
      "21     \t [ 3.52318005  7.66450479  7.93135806  0.5        14.66980311  0.80119903]. \t  0.9885308582419091 \t 0.9933604722757284\n",
      "22     \t [ 2.78797078  8.07465983  7.53444617  0.5256517  14.77058978  0.81304094]. \t  0.988564463710966 \t 0.9933604722757284\n",
      "23     \t [ 3.3005073   8.39736106  7.62509535  0.5        13.64865606  0.602905  ]. \t  0.8873585009584336 \t 0.9933604722757284\n",
      "24     \t [ 3.45683117  8.20501051  7.34024398  1.         14.29287695  0.1       ]. \t  0.8964176405702106 \t 0.9933604722757284\n",
      "25     \t [ 2.68132353  8.35388709  6.9503852   0.5        13.73991772  0.1       ]. \t  0.8878337861295362 \t 0.9933604722757284\n",
      "26     \t [ 2.91162397  8.22530849  7.0410631   1.         13.68626889  0.78252065]. \t  0.9862360442460253 \t 0.9933604722757284\n",
      "27     \t [ 3.03421826  7.62818119  8.47415559  0.67602186 15.48639646  1.        ]. \t  0.9931300425966367 \t 0.9933604722757284\n",
      "28     \t [ 2.16682108  5.92029922 10.00857308  0.64343816 16.37746569  0.78967663]. \t  0.9902783693219828 \t 0.9933604722757284\n",
      "29     \t [ 1.7451752   5.91056577 10.1627631   1.         15.74630727  0.49501159]. \t  0.89778106941145 \t 0.9933604722757284\n",
      "30     \t [ 1.39861962  6.14490228  9.78164469  0.9522941  16.70344605  0.77017336]. \t  0.9906816350889508 \t 0.9933604722757284\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07131099845430462"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\n",
    "\n",
    "np.random.seed(run_num_12)\n",
    "surrogate_loser_12 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train12, X_test12, y_train12, y_test12 = train_test_split(X, y, test_size=test_perc, random_state=run_num_12)\n",
    "\n",
    "def f_syn_polarity12(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_12, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train12, y=y_train12).mean())\n",
    "    return  score\n",
    "\n",
    "loser_12 = GPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity12, param, n_jobs = -1) # define BayesOpt\n",
    "loser_12.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_12 = loser_12.getResult()[0]\n",
    "params_loser_12['max_depth'] = int(params_loser_12['max_depth'])\n",
    "params_loser_12['min_child_weight'] = int(params_loser_12['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train12 = xgb.DMatrix(X_train12, y_train12)\n",
    "dX_loser_test12 = xgb.DMatrix(X_test12, y_test12)\n",
    "model_loser_12 = xgb.train(params_loser_12, dX_loser_train12)\n",
    "pred_loser_12 = model_loser_12.predict(dX_loser_test12)\n",
    "\n",
    "rmse_loser_12 = np.sqrt(mean_squared_error(pred_loser_12, y_test12))\n",
    "rmse_loser_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.60644309  4.13600652  6.          0.61497171 13.          0.31340376]. \t  0.8166764439986761 \t 0.9879595510260936\n",
      "init   \t [ 4.54930866  5.99748524  8.          0.75090625 13.          0.20817186]. \t  0.8204018598829208 \t 0.9879595510260936\n",
      "init   \t [8.80885505 2.26218951 9.         0.95881521 6.         0.90500855]. \t  0.9879595510260936 \t 0.9879595510260936\n",
      "init   \t [ 6.39630194  9.30791981  8.          0.77550908 10.          0.88560697]. \t  0.9870906082391165 \t 0.9879595510260936\n",
      "init   \t [ 8.71308214  7.36202238 13.          0.60528576  7.          0.38868957]. \t  0.8188272073625565 \t 0.9879595510260936\n",
      "1      \t [8.75818261 1.82535264 9.73293248 0.96041632 6.24428973 0.73785956]. \t  0.9879355470702414 \t 0.9879595510260936\n",
      "2      \t [8.77662081 2.71131097 9.72080185 0.86534868 6.18008817 0.71654634]. \t  0.9877243135033327 \t 0.9879595510260936\n",
      "3      \t [9.49297492 2.25702224 9.53230378 0.95481676 6.19220421 0.87870772]. \t  0.9879211437287174 \t 0.9879595510260936\n",
      "4      \t [8.91513666 2.30534322 9.30625279 0.90492142 6.81868073 0.85152339]. \t  0.9874986743822985 \t 0.9879595510260936\n",
      "5      \t [9.04012806 2.2526465  9.30393708 0.95287273 6.28548835 0.16240568]. \t  0.8217076914683421 \t 0.9879595510260936\n",
      "6      \t [8.94372265 2.20926373 9.47662914 0.5        6.25009266 1.        ]. \t  \u001b[92m0.9938165506184232\u001b[0m \t 0.9938165506184232\n",
      "7      \t [8.90201187 2.30112375 9.52487054 1.         6.28882595 1.        ]. \t  \u001b[92m0.9944550614447311\u001b[0m \t 0.9944550614447311\n",
      "8      \t [9.31451304 1.35979826 8.90205507 0.85046776 6.48845926 0.92456239]. \t  0.9882091963847243 \t 0.9944550614447311\n",
      "9      \t [9.34723654 1.35433936 9.20567661 0.7903233  5.51916693 0.81782643]. \t  0.9869273803713104 \t 0.9944550614447311\n",
      "10     \t [9.66350276 1.14770844 9.73033441 0.61415147 6.24395377 0.69922864]. \t  0.9849542535408314 \t 0.9944550614447311\n",
      "11     \t [8.85033544 0.9115925  9.30908754 0.5        6.0892324  0.60486375]. \t  0.8205170599947572 \t 0.9944550614447311\n",
      "12     \t [9.50002077 1.63931128 9.18072501 0.5        6.10213434 0.67978912]. \t  0.9859240206173464 \t 0.9944550614447311\n",
      "13     \t [ 6.05240039  8.69308814  8.00064318  0.77169566 10.55730831  0.76051579]. \t  0.9870762058656052 \t 0.9944550614447311\n",
      "14     \t [ 6.91654097  8.8860984   8.11839324  0.77415879 10.57751065  0.81561968]. \t  0.9869513850877439 \t 0.9944550614447311\n",
      "15     \t [ 6.55131233  8.92006186  7.33024037  0.79014539 10.40942316  0.89940399]. \t  0.9870906039522018 \t 0.9944550614447311\n",
      "16     \t [6.58554241 8.4687358  7.89607332 0.60540021 9.93461474 0.91668569]. \t  0.986058439935222 \t 0.9944550614447311\n",
      "17     \t [ 6.54230977  8.89613996  7.79759337  0.58264369 10.21956127  0.20458139]. \t  0.815024949710053 \t 0.9944550614447311\n",
      "18     \t [ 6.4554968   8.94470063  7.88995692  0.5        10.42565666  1.        ]. \t  0.99179060485179 \t 0.9944550614447311\n",
      "19     \t [ 6.50432253  8.77955895  7.89829151  1.         10.24009991  0.95612361]. \t  0.9580216765799742 \t 0.9944550614447311\n",
      "20     \t [7.42852851 9.21693599 7.59076799 0.55537493 9.76700558 0.98786424]. \t  0.9856935757959334 \t 0.9944550614447311\n",
      "21     \t [6.62247874 9.16502527 7.14485628 0.5        9.28573958 1.        ]. \t  0.9918530154135804 \t 0.9944550614447311\n",
      "22     \t [6.88874753 9.92161794 7.10155611 0.7486901  9.81868854 0.95198805]. \t  0.9868313581866747 \t 0.9944550614447311\n",
      "23     \t [6.89779438 9.86313978 7.7808318  0.5        9.15862487 1.        ]. \t  0.9921602715108438 \t 0.9944550614447311\n",
      "24     \t [ 6.50714065  7.88885287  7.92570396  0.61451581 11.08609508  0.67579536]. \t  0.9864473040891671 \t 0.9944550614447311\n",
      "25     \t [6.8182911  9.44744255 7.54425923 0.5886058  9.66421462 0.95601685]. \t  0.9859096129889077 \t 0.9944550614447311\n",
      "26     \t [7.44730707 9.73967609 6.93537579 0.78584233 8.91725994 1.        ]. \t  0.9888909159910161 \t 0.9944550614447311\n",
      "27     \t [ 7.35987619  8.11411294  7.40200127  0.5        10.56106778  0.85646776]. \t  0.9858279977412728 \t 0.9944550614447311\n",
      "28     \t [7.40105221 9.00668037 6.46137388 0.64372513 9.76666005 1.        ]. \t  0.9887276888837917 \t 0.9944550614447311\n",
      "29     \t [7.48248385 8.39747636 7.05962598 0.52709241 9.17306991 1.        ]. \t  0.9913921387697856 \t 0.9944550614447311\n",
      "30     \t [6.70783771 8.0102005  6.78742041 0.5        9.90775417 0.88594122]. \t  0.9857559909212132 \t 0.9944550614447311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06176950917580233"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\n",
    "\n",
    "np.random.seed(run_num_13)\n",
    "surrogate_loser_13 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train13, X_test13, y_train13, y_test13 = train_test_split(X, y, test_size=test_perc, random_state=run_num_13)\n",
    "\n",
    "def f_syn_polarity13(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_13, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train13, y=y_train13).mean())\n",
    "    return  score\n",
    "\n",
    "loser_13 = GPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity13, param, n_jobs = -1) # define BayesOpt\n",
    "loser_13.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_13 = loser_13.getResult()[0]\n",
    "params_loser_13['max_depth'] = int(params_loser_13['max_depth'])\n",
    "params_loser_13['min_child_weight'] = int(params_loser_13['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train13 = xgb.DMatrix(X_train13, y_train13)\n",
    "dX_loser_test13 = xgb.DMatrix(X_test13, y_test13)\n",
    "model_loser_13 = xgb.train(params_loser_13, dX_loser_train13)\n",
    "pred_loser_13 = model_loser_13.predict(dX_loser_test13)\n",
    "\n",
    "rmse_loser_13 = np.sqrt(mean_squared_error(pred_loser_13, y_test13))\n",
    "rmse_loser_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [6.47551049 5.07149688 9.         0.9481426  2.         0.30826454]. \t  0.8841083588522155 \t 0.985981635228031\n",
      "init   \t [1.30128292 8.51503709 6.         0.58757726 8.         0.65878816]. \t  0.8847900532901706 \t 0.985981635228031\n",
      "init   \t [ 1.87389374  2.97833637 11.          0.92510013  4.          0.27446484]. \t  0.8861919064379334 \t 0.985981635228031\n",
      "init   \t [ 9.80899716  0.11774133 11.          0.53577929  6.          0.38130944]. \t  0.8891587951046561 \t 0.985981635228031\n",
      "init   \t [ 6.4601046   0.32698917 10.          0.67912907 15.          0.96967672]. \t  0.985981635228031 \t 0.985981635228031\n",
      "1      \t [ 6.04704707  0.         10.73515922  0.73996947 14.99999999  0.99943441]. \t  0.9855639564395027 \t 0.985981635228031\n",
      "2      \t [ 6.6277366   0.47581753 10.72604035  0.68437373 15.44088114  0.79160505]. \t  \u001b[92m0.9863032877524404\u001b[0m \t 0.9863032877524404\n",
      "3      \t [ 5.90276298  0.42258861 10.29514601  0.58639411 15.61451785  0.98908834]. \t  0.9855591651901744 \t 0.9863032877524404\n",
      "4      \t [ 6.06995672  0.87092503 10.57573309  0.73793331 14.93801457  0.99475319]. \t  0.9856743765033037 \t 0.9863032877524404\n",
      "5      \t [ 6.0693892   0.39411067 10.39546015  0.69220058 15.11343426  0.29380745]. \t  0.8886499012147467 \t 0.9863032877524404\n",
      "6      \t [ 6.2252745   0.41108148 10.41748693  1.         15.29648043  1.        ]. \t  \u001b[92m0.9948727297234042\u001b[0m \t 0.9948727297234042\n",
      "7      \t [ 6.253539    0.41412098 10.50647997  0.5        15.14774933  1.        ]. \t  0.9931732428038534 \t 0.9948727297234042\n",
      "8      \t [ 5.69299975  0.59731708 11.40418592  0.7941576  15.59623767  0.73297979]. \t  0.9858376038179596 \t 0.9948727297234042\n",
      "9      \t [ 4.97871401  0.4756941  10.86267748  0.82328015 15.12510208  0.97297419]. \t  0.9861208531936198 \t 0.9948727297234042\n",
      "10     \t [ 5.51122871  0.53786176 11.49045316  1.         14.6161267   0.77688299]. \t  0.9845989817186166 \t 0.9948727297234042\n",
      "11     \t [ 6.86797451  1.00905541  9.66837495  0.58271672 15.61658204  0.68425494]. \t  0.9853191198927211 \t 0.9948727297234042\n",
      "12     \t [ 5.33749608  0.         11.44996086  1.         15.18171846  1.        ]. \t  0.9948487254218334 \t 0.9948727297234042\n",
      "13     \t [ 5.59677534  0.40410316 11.05520964  0.85776361 15.10654779  0.83119654]. \t  0.9865145148198335 \t 0.9948727297234042\n",
      "14     \t [ 4.83739389  0.82748358 11.90819104  0.7745282  15.16598756  1.        ]. \t  0.9946951003809562 \t 0.9948727297234042\n",
      "15     \t [ 6.2689105   1.40916644 10.38344321  0.67035462 15.86075927  0.5487108 ]. \t  0.8875409226030149 \t 0.9948727297234042\n",
      "16     \t [ 4.78464237  0.15085382 11.57798353  0.5        14.48639852  1.        ]. \t  0.993984576649936 \t 0.9948727297234042\n",
      "17     \t [ 5.62614943  0.31648477 12.17356838  0.5        14.95378944  1.        ]. \t  0.9935477046534311 \t 0.9948727297234042\n",
      "18     \t [ 5.99098877  1.18753294  9.49830176  0.76627525 15.25621107  0.72811304]. \t  0.9847526173384926 \t 0.9948727297234042\n",
      "19     \t [ 6.74642449  1.28454952  9.95200057  0.99934377 14.9643768   0.46048519]. \t  0.8849821042972451 \t 0.9948727297234042\n",
      "20     \t [ 4.78461584  0.19679828 11.58595549  0.5        15.60975486  1.        ]. \t  0.9939125646440926 \t 0.9948727297234042\n",
      "21     \t [ 6.38856357  0.64841252  9.59722862  0.98824685 15.78771812  0.30794089]. \t  0.886657580553997 \t 0.9948727297234042\n",
      "22     \t [ 5.01440718  0.27283258 12.02445091  0.65527685 15.07016799  0.32875492]. \t  0.8883474569716591 \t 0.9948727297234042\n",
      "23     \t [ 5.18410418  0.47728125 11.5877712   0.5        15.02585859  1.        ]. \t  0.9936245171738696 \t 0.9948727297234042\n",
      "24     \t [ 4.03886861  0.22719434 11.4806231   1.         15.0140139   0.99789232]. \t  0.9849782512045989 \t 0.9948727297234042\n",
      "25     \t [ 6.54510122  0.         11.84313491  0.94007796 14.70302495  0.81428975]. \t  0.9858616126138763 \t 0.9948727297234042\n",
      "26     \t [ 6.40670635  0.87362957 12.13898914  1.         15.16169663  0.74848309]. \t  0.9852326929291982 \t 0.9948727297234042\n",
      "27     \t [ 4.63961893  0.06920731 12.28554724  1.         14.75033502  1.        ]. \t  \u001b[92m0.9949495428661369\u001b[0m \t 0.9949495428661369\n",
      "28     \t [ 6.41531923  0.06615884 12.14624897  0.82102899 15.69091191  0.81749668]. \t  0.9857463912057547 \t 0.9949495428661369\n",
      "29     \t [ 5.51995059  0.42773153 12.51247968  1.         15.65842752  0.89967119]. \t  0.9851510792718702 \t 0.9949495428661369\n",
      "30     \t [ 6.04408304  0.23789931 12.24148398  1.         15.13171123  0.47338286]. \t  0.8857310297249951 \t 0.9949495428661369\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.053341897983467085"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\n",
    "\n",
    "np.random.seed(run_num_14)\n",
    "surrogate_loser_14 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train14, X_test14, y_train14, y_test14 = train_test_split(X, y, test_size=test_perc, random_state=run_num_14)\n",
    "\n",
    "def f_syn_polarity14(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_14, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train14, y=y_train14).mean())\n",
    "    return  score\n",
    "\n",
    "loser_14 = GPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity14, param, n_jobs = -1) # define BayesOpt\n",
    "loser_14.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_14 = loser_14.getResult()[0]\n",
    "params_loser_14['max_depth'] = int(params_loser_14['max_depth'])\n",
    "params_loser_14['min_child_weight'] = int(params_loser_14['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train14 = xgb.DMatrix(X_train14, y_train14)\n",
    "dX_loser_test14 = xgb.DMatrix(X_test14, y_test14)\n",
    "model_loser_14 = xgb.train(params_loser_14, dX_loser_train14)\n",
    "pred_loser_14 = model_loser_14.predict(dX_loser_test14)\n",
    "\n",
    "rmse_loser_14 = np.sqrt(mean_squared_error(pred_loser_14, y_test14))\n",
    "rmse_loser_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.64315424  8.97297685  6.          0.81493635 11.          0.33008564]. \t  0.7975400806883499 \t 0.9868697857411816\n",
      "init   \t [9.51455383 9.60803032 5.         0.68485728 1.         0.30145081]. \t  0.7964118845990855 \t 0.9868697857411816\n",
      "init   \t [ 8.11809889  6.32508536  7.          0.61766329 19.          0.51383159]. \t  0.7990859351689633 \t 0.9868697857411816\n",
      "init   \t [ 7.28571251  5.36071389 10.          0.76240297 12.          0.12035683]. \t  0.7997340414931001 \t 0.9868697857411816\n",
      "init   \t [ 6.80847768  3.59767194 14.          0.51365585 16.          0.89730673]. \t  0.9868697857411816 \t 0.9868697857411816\n",
      "1      \t [ 6.86771762  3.93133782 13.40798203  0.50594722 15.40798203  0.82812472]. \t  \u001b[92m0.9871530246071581\u001b[0m \t 0.9871530246071581\n",
      "2      \t [ 7.48953172  3.50387631 13.8742387   0.5        15.44248454  0.94154453]. \t  0.9870234034529592 \t 0.9871530246071581\n",
      "3      \t [ 6.68745217  3.32291849 13.98016362  0.55847271 15.17302886  0.77467989]. \t  0.9870954217508091 \t 0.9871530246071581\n",
      "4      \t [ 6.98492492  3.17376629 13.40872774  0.85131981 15.68449637  0.79069191]. \t  \u001b[92m0.9895918344044974\u001b[0m \t 0.9895918344044974\n",
      "5      \t [ 7.07978305  3.58157915 13.8058408   0.58536986 15.6007595   0.18666014]. \t  0.7995612137023583 \t 0.9895918344044974\n",
      "6      \t [ 6.9592496   3.72044786 13.89418701  1.         15.46783035  1.        ]. \t  \u001b[92m0.9945558762608239\u001b[0m \t 0.9945558762608239\n",
      "7      \t [ 6.91800804  3.43580192 13.66774814  0.5        15.53542237  1.        ]. \t  0.9925155304021396 \t 0.9945558762608239\n",
      "8      \t [ 7.18314422  2.75771494 14.45144395  0.98604925 15.78700249  0.8286495 ]. \t  0.9897070527011825 \t 0.9945558762608239\n",
      "9      \t [ 7.67100482  3.18779462 13.98384035  1.         16.42276928  0.95761399]. \t  0.991377752090513 \t 0.9945558762608239\n",
      "10     \t [ 7.53231433  4.06397415 13.23396765  0.84980462 16.24077641  0.95152856]. \t  0.9886460759854522 \t 0.9945558762608239\n",
      "11     \t [ 8.1207078   3.38367733 13.17905632  1.         15.85685433  0.88221508]. \t  0.9914161566221718 \t 0.9945558762608239\n",
      "12     \t [ 7.5288928   3.48015359 13.65667295  1.         15.90891722  0.83759592]. \t  0.9912961374651722 \t 0.9945558762608239\n",
      "13     \t [ 7.90426128  3.22453803 13.02895366  0.5        16.52521117  1.        ]. \t  0.9923715072201404 \t 0.9945558762608239\n",
      "14     \t [ 7.79550776  3.8237931  12.497398    0.5        15.70908683  0.94099587]. \t  0.9871050212588682 \t 0.9945558762608239\n",
      "15     \t [ 8.41729823  3.92677982 13.19823234  0.5        16.13791246  1.        ]. \t  0.992385909801083 \t 0.9945558762608239\n",
      "16     \t [ 8.12279532  2.58594627 13.91959266  0.5        16.08355031  1.        ]. \t  0.9924099141026538 \t 0.9945558762608239\n",
      "17     \t [ 7.23675337  2.46420741 14.04368754  0.5        16.53697919  0.80875432]. \t  0.9867305542926488 \t 0.9945558762608239\n",
      "18     \t [ 7.58057274  2.91891452 14.73503536  0.5        16.36757922  1.        ]. \t  0.9923955115217113 \t 0.9945558762608239\n",
      "19     \t [ 7.97072348  2.99632591 13.96933633  0.5        16.49590523  0.45775839]. \t  0.799753248114925 \t 0.9945558762608239\n",
      "20     \t [ 7.86328538  4.51786596 13.04680045  0.77809328 15.28705769  1.        ]. \t  0.9939749855765103 \t 0.9945558762608239\n",
      "21     \t [ 7.89593461  4.33921192 12.88665313  0.5        15.73912818  0.38405006]. \t  0.8000893010077813 \t 0.9945558762608239\n",
      "22     \t [ 7.3266348   3.82792713 12.94926445  1.         14.69905421  0.79017114]. \t  0.9914305592031143 \t 0.9945558762608239\n",
      "23     \t [ 7.42067769  2.90493333 14.07858893  0.51395345 16.16901269  1.        ]. \t  0.9924723226593121 \t 0.9945558762608239\n",
      "24     \t [ 7.86281559  2.12416949 14.52595813  1.         16.61986948  1.        ]. \t  \u001b[92m0.9946134864463083\u001b[0m \t 0.9946134864463083\n",
      "25     \t [ 6.78223958  2.80775217 14.75979335  1.         16.75583058  0.8621299 ]. \t  0.9914737648716581 \t 0.9946134864463083\n",
      "26     \t [ 7.78117568  3.81825413 13.11083548  0.62365454 15.34843205  1.        ]. \t  0.9935909137090942 \t 0.9946134864463083\n",
      "27     \t [ 8.28455152  3.97991476 12.40902409  0.96861192 16.48721096  1.        ]. \t  0.9945414743713092 \t 0.9946134864463083\n",
      "28     \t [ 7.25985831  4.34764336 12.43285407  1.         15.42088048  0.98240523]. \t  0.9914161548244592 \t 0.9946134864463083\n",
      "29     \t [ 8.08491095  2.12812386 13.34643202  1.         16.61348369  0.98358027]. \t  0.9913249426270573 \t 0.9946134864463083\n",
      "30     \t [ 8.85057707  2.9154273  13.22688354  0.8574567  16.71953883  1.        ]. \t  0.9939557833798238 \t 0.9946134864463083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05849860225050029"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\n",
    "\n",
    "np.random.seed(run_num_15)\n",
    "surrogate_loser_15 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train15, X_test15, y_train15, y_test15 = train_test_split(X, y, test_size=test_perc, random_state=run_num_15)\n",
    "\n",
    "def f_syn_polarity15(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_15, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train15, y=y_train15).mean())\n",
    "    return  score\n",
    "\n",
    "loser_15 = GPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity15, param, n_jobs = -1) # define BayesOpt\n",
    "loser_15.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_15 = loser_15.getResult()[0]\n",
    "params_loser_15['max_depth'] = int(params_loser_15['max_depth'])\n",
    "params_loser_15['min_child_weight'] = int(params_loser_15['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train15 = xgb.DMatrix(X_train15, y_train15)\n",
    "dX_loser_test15 = xgb.DMatrix(X_test15, y_test15)\n",
    "model_loser_15 = xgb.train(params_loser_15, dX_loser_train15)\n",
    "pred_loser_15 = model_loser_15.predict(dX_loser_test15)\n",
    "\n",
    "rmse_loser_15 = np.sqrt(mean_squared_error(pred_loser_15, y_test15))\n",
    "rmse_loser_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [8.63948332 3.16366002 7.         0.75395583 1.         0.56999262]. \t  0.8862591155624773 \t 0.9857703856869544\n",
      "init   \t [ 3.3426621   9.77029134 14.          0.93751218 17.          0.80549151]. \t  0.9857703856869544 \t 0.9857703856869544\n",
      "init   \t [ 3.36871073  4.8308288  13.          0.91984982 14.          0.48829226]. \t  0.8862111023959116 \t 0.9857703856869544\n",
      "init   \t [ 8.84540736  1.59826081  8.          0.75772888 14.          0.29029227]. \t  0.8862879265323566 \t 0.9857703856869544\n",
      "init   \t [ 7.74981136  8.34377636 13.          0.83182409 15.          0.19378622]. \t  0.8859086345058471 \t 0.9857703856869544\n",
      "1      \t [ 8.48805787  2.49755915  8.31841837  0.67868543 14.00001893  0.36396769]. \t  0.8844684259178325 \t 0.9857703856869544\n",
      "2      \t [ 8.56157492  1.79574967  8.75341858  0.7138917  14.58784722  0.29422982]. \t  0.8850061096890321 \t 0.9857703856869544\n",
      "3      \t [ 7.88179626  1.76206626  8.08445235  0.69262908 14.24431966  0.27670481]. \t  0.8850109099962039 \t 0.9857703856869544\n",
      "4      \t [ 8.32811056  1.71246855  8.71851384  0.5        13.65064661  0.1       ]. \t  0.8876465830001101 \t 0.9857703856869544\n",
      "5      \t [ 8.34548221  1.7566088   8.52497564  0.93275604 13.91885967  0.94617298]. \t  0.9833363977693369 \t 0.9857703856869544\n",
      "6      \t [ 8.45909157  1.8191101   8.31056726  0.5        14.16777757  0.77350482]. \t  0.9806047515339601 \t 0.9857703856869544\n",
      "7      \t [ 9.22700761  1.97850489  8.936257    0.86861877 13.73937322  0.6666441 ]. \t  0.8870896252620106 \t 0.9857703856869544\n",
      "8      \t [ 4.11252287  9.52166156 13.82333521  0.91488212 16.64481753  0.69793394]. \t  \u001b[92m0.9862600680458621\u001b[0m \t 0.9862600680458621\n",
      "9      \t [ 3.64329013  9.38923152 13.33790416  0.95934252 17.17953337  0.49372859]. \t  0.8860046869749522 \t 0.9862600680458621\n",
      "10     \t [ 3.47752616  8.97199276 13.90018664  0.87685185 16.68706984  0.99176256]. \t  0.985876009868718 \t 0.9862600680458621\n",
      "11     \t [ 3.91574979  9.22165947 14.18599799  1.         17.34332374  0.88878905]. \t  0.9684202629205055 \t 0.9862600680458621\n",
      "12     \t [ 3.5849946   9.21224385 14.21235971  0.99999377 16.82106457  0.22462946]. \t  0.8869072041860605 \t 0.9862600680458621\n",
      "13     \t [ 8.53887108  1.88531434  8.53994174  1.         13.96189582  0.38278713]. \t  0.7898155662770684 \t 0.9862600680458621\n",
      "14     \t [ 8.3623585   2.13004095  9.3643752   0.5        13.82199236  1.        ]. \t  \u001b[92m0.9935237048452641\u001b[0m \t 0.9935237048452641\n",
      "15     \t [ 8.72975426  1.19935588  9.17119412  0.5        13.75384599  1.        ]. \t  \u001b[92m0.9935237048452641\u001b[0m \t 0.9935237048452641\n",
      "16     \t [ 8.65689004  1.84132373  8.74535759  0.5        13.12059615  1.        ]. \t  0.9933700774534961 \t 0.9935237048452641\n",
      "17     \t [ 7.68035554  1.43574811  8.92050482  0.5        13.58221249  1.        ]. \t  0.9935189039849499 \t 0.9935237048452641\n",
      "18     \t [ 3.71514261  9.37366726 13.94916291  0.5        17.00269197  0.76189732]. \t  0.9837444617691932 \t 0.9935237048452641\n",
      "19     \t [ 7.60575157  2.39453578  8.4812389   0.5        13.48069854  1.        ]. \t  \u001b[92m0.9935237048452641\u001b[0m \t 0.9935237048452641\n",
      "20     \t [ 7.44772726  2.24793077  8.90578399  0.5        14.34133103  0.67046836]. \t  0.9813824909048554 \t 0.9935237048452641\n",
      "21     \t [ 8.05260005  1.92531319  8.80362354  0.5        13.77476542  0.9440125 ]. \t  0.9810032230783218 \t 0.9935237048452641\n",
      "22     \t [ 6.79489621  1.9564714   8.49695115  0.94817811 13.69188244  0.88153516]. \t  0.9851318915931495 \t 0.9935237048452641\n",
      "23     \t [ 7.67695312  1.3863587   7.94116524  0.82338248 13.13005915  0.99533557]. \t  0.982304202568658 \t 0.9935237048452641\n",
      "24     \t [ 8.52874402  0.78955144  8.21144346  0.68748612 13.4432987   0.95696227]. \t  0.9822513958709137 \t 0.9935237048452641\n",
      "25     \t [ 7.03031     2.29692868  8.54087778  0.5        13.45673882  0.19882899]. \t  0.8887459513869406 \t 0.9935237048452641\n",
      "26     \t [ 7.42493209  1.81644609  8.89631229  1.         12.84839638  1.        ]. \t  \u001b[92m0.9936437255234049\u001b[0m \t 0.9936437255234049\n",
      "27     \t [ 7.16050817  2.61160042  9.36583883  0.91777669 13.6170323   0.88785922]. \t  0.9847526270854696 \t 0.9936437255234049\n",
      "28     \t [ 8.23978028  1.0375371   8.94031464  1.         12.97941985  1.        ]. \t  \u001b[92m0.9937013321134643\u001b[0m \t 0.9937013321134643\n",
      "29     \t [ 7.19620089  3.01285401  8.44504743  0.97057276 14.07060279  0.69859045]. \t  0.9849494577949266 \t 0.9937013321134643\n",
      "30     \t [ 7.90250757  3.22013602  9.21184851  0.5        14.06731531  0.50346184]. \t  0.8869840700847491 \t 0.9937013321134643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06523883728737642"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\n",
    "\n",
    "np.random.seed(run_num_16)\n",
    "surrogate_loser_16 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train16, X_test16, y_train16, y_test16 = train_test_split(X, y, test_size=test_perc, random_state=run_num_16)\n",
    "\n",
    "def f_syn_polarity16(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_16, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train16, y=y_train16).mean())\n",
    "    return  score\n",
    "\n",
    "loser_16 = GPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity16, param, n_jobs = -1) # define BayesOpt\n",
    "loser_16.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_16 = loser_16.getResult()[0]\n",
    "params_loser_16['max_depth'] = int(params_loser_16['max_depth'])\n",
    "params_loser_16['min_child_weight'] = int(params_loser_16['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train16 = xgb.DMatrix(X_train16, y_train16)\n",
    "dX_loser_test16 = xgb.DMatrix(X_test16, y_test16)\n",
    "model_loser_16 = xgb.train(params_loser_16, dX_loser_train16)\n",
    "pred_loser_16 = model_loser_16.predict(dX_loser_test16)\n",
    "\n",
    "rmse_loser_16 = np.sqrt(mean_squared_error(pred_loser_16, y_test16))\n",
    "rmse_loser_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.51824028  7.30519624 13.          0.53095783 18.          0.70874494]. \t  0.9897406652228038 \t 0.991204913651782\n",
      "init   \t [ 8.04444873  5.33512865 14.          0.77275393  6.          0.33037499]. \t  0.8877329011327202 \t 0.991204913651782\n",
      "init   \t [ 2.78665017  0.43321861  6.          0.9708069  14.          0.8542801 ]. \t  0.991204913651782 \t 0.991204913651782\n",
      "init   \t [7.75225299 4.53658749 6.         0.70978712 9.         0.19759133]. \t  0.8873200050200097 \t 0.991204913651782\n",
      "init   \t [ 9.75833611  5.75935719 10.          0.77333839 14.          0.97649057]. \t  0.9908496585622393 \t 0.991204913651782\n",
      "1      \t [ 2.61098819  0.          5.99999995  0.99341655 14.75556096  1.        ]. \t  0.9842341302995434 \t 0.991204913651782\n",
      "2      \t [ 3.03974411  0.60245649  5.55543211  0.80741988 14.68963656  0.84693465]. \t  0.9864568961278931 \t 0.991204913651782\n",
      "3      \t [ 2.45176142  0.15874403  5.28604039  0.92880775 14.28810841  0.88207516]. \t  0.9866969228259036 \t 0.991204913651782\n",
      "4      \t [ 2.22906307  0.75200914  5.83332335  0.93678201 14.57879917  0.86893081]. \t  0.9867689357306162 \t 0.991204913651782\n",
      "5      \t [ 9.36280535  5.26377785  9.72590595  0.81320221 14.54825659  0.79597671]. \t  0.9904799919031916 \t 0.991204913651782\n",
      "6      \t [ 2.61067822  0.29197881  5.78111242  1.         14.52607958  0.22482838]. \t  0.8873777037082623 \t 0.991204913651782\n",
      "7      \t [ 9.13647957  6.09242521  9.77046821  0.94454226 14.37774373  0.71140106]. \t  \u001b[92m0.991468955161067\u001b[0m \t 0.991468955161067\n",
      "8      \t [ 9.38387735  5.72020514 10.46602284  0.91535114 14.62972155  0.89391511]. \t  0.9913057242509584 \t 0.991468955161067\n",
      "9      \t [ 9.80038904  5.92063658  9.87247169  0.56180067 14.75383541  0.63117056]. \t  0.8913863378546775 \t 0.991468955161067\n",
      "10     \t [ 9.31420798  5.5831926  10.19019741  0.70617579 14.12950496  0.27609677]. \t  0.8883473719798305 \t 0.991468955161067\n",
      "11     \t [ 2.55265903  0.31093389  5.82194886  0.5        14.4145402   0.89563966]. \t  0.9847093887815301 \t 0.991468955161067\n",
      "12     \t [ 9.1221849   5.72154351 10.02833886  0.5        14.33512769  1.        ]. \t  \u001b[92m0.9930916281084167\u001b[0m \t 0.9930916281084167\n",
      "13     \t [ 6.33438621  7.41799776 12.18258806  0.64912586 17.72091333  0.62036436]. \t  0.8884433908455408 \t 0.9930916281084167\n",
      "14     \t [ 6.79208916  7.19384429 12.8479075   0.5400177  17.16767124  0.65811013]. \t  0.8913191158537178 \t 0.9930916281084167\n",
      "15     \t [ 7.08307783  6.88792699 12.45964948  0.61516953 17.97764039  0.79646419]. \t  0.9904415872332474 \t 0.9930916281084167\n",
      "16     \t [ 7.01105471  7.61015569 12.66588099  1.         17.81147333  1.        ]. \t  \u001b[92m0.9944166526945688\u001b[0m \t 0.9944166526945688\n",
      "17     \t [ 7.12555237  7.58355812 12.61020092  0.50098728 17.90033834  0.29501951]. \t  0.8951453583790104 \t 0.9944166526945688\n",
      "18     \t [ 6.7469555   7.06617267 12.74135771  1.         17.81605074  0.41034413]. \t  0.8873777037082623 \t 0.9944166526945688\n",
      "19     \t [ 6.83725518  7.36734365 12.57560251  0.5        17.77819923  1.        ]. \t  0.9930628232231028 \t 0.9944166526945688\n",
      "20     \t [ 6.82913702  7.45733976 12.38519154  0.89534265 18.80053192  0.85235205]. \t  0.9916273823068645 \t 0.9944166526945688\n",
      "21     \t [ 7.41311348  7.27118321 13.1076469   0.81755857 18.63437251  0.97841872]. \t  0.9906096237053797 \t 0.9944166526945688\n",
      "22     \t [ 6.8699726   8.08432741 13.16808843  0.97745919 18.72664321  0.84707087]. \t  0.9915937775292353 \t 0.9944166526945688\n",
      "23     \t [ 6.95421559  7.52885211 12.83242028  0.82559356 18.40127143  0.81848239]. \t  0.9905856101386754 \t 0.9944166526945688\n",
      "24     \t [ 7.36014337  7.87831606 12.91498794  1.         19.61494178  1.        ]. \t  0.9939173704127451 \t 0.9944166526945688\n",
      "25     \t [ 7.85065589  7.62324511 12.28754672  1.         18.90662706  1.        ]. \t  0.9939701798762011 \t 0.9944166526945688\n",
      "26     \t [ 7.128398    8.504108   12.3597365   1.         19.07543577  1.        ]. \t  0.9942390200332311 \t 0.9944166526945688\n",
      "27     \t [ 6.2508699   8.37502945 13.10653747  0.71742169 17.53390133  0.69114265]. \t  0.9912097093955299 \t 0.9944166526945688\n",
      "28     \t [ 7.8365926   8.35241484 13.16749236  1.         18.87073118  1.        ]. \t  0.9939701798762011 \t 0.9944166526945688\n",
      "29     \t [ 7.47330008  7.9778902  12.74109892  0.5        19.1142437   1.        ]. \t  0.9928035831272743 \t 0.9944166526945688\n",
      "30     \t [ 8.28791286  7.20188633 12.89176478  0.76916514 17.93821155  1.        ]. \t  0.993432489467294 \t 0.9944166526945688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06305140594321365"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\n",
    "\n",
    "np.random.seed(run_num_17)\n",
    "surrogate_loser_17 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train17, X_test17, y_train17, y_test17 = train_test_split(X, y, test_size=test_perc, random_state=run_num_17)\n",
    "\n",
    "def f_syn_polarity17(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_17, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train17, y=y_train17).mean())\n",
    "    return  score\n",
    "\n",
    "loser_17 = GPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity17, param, n_jobs = -1) # define BayesOpt\n",
    "loser_17.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_17 = loser_17.getResult()[0]\n",
    "params_loser_17['max_depth'] = int(params_loser_17['max_depth'])\n",
    "params_loser_17['min_child_weight'] = int(params_loser_17['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train17 = xgb.DMatrix(X_train17, y_train17)\n",
    "dX_loser_test17 = xgb.DMatrix(X_test17, y_test17)\n",
    "model_loser_17 = xgb.train(params_loser_17, dX_loser_train17)\n",
    "pred_loser_17 = model_loser_17.predict(dX_loser_test17)\n",
    "\n",
    "rmse_loser_17 = np.sqrt(mean_squared_error(pred_loser_17, y_test17))\n",
    "rmse_loser_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.73429403  9.68540663 10.          0.68875849  9.          0.48011572]. \t  0.8596968089427542 \t 0.9883196147183771\n",
      "init   \t [ 6.12033333  7.66062926  8.          0.76133734 13.          0.93379456]. \t  0.9878107251845023 \t 0.9883196147183771\n",
      "init   \t [ 1.46524679  7.01527914  7.          0.90913299 10.          0.36016753]. \t  0.8627837722196116 \t 0.9883196147183771\n",
      "init   \t [ 9.73855241  3.33774046 14.          0.53290419  7.          0.7088681 ]. \t  0.9856023554376728 \t 0.9883196147183771\n",
      "init   \t [ 3.00618018  1.82702795 11.          0.75681389 14.          0.98627449]. \t  0.9883196147183771 \t 0.9883196147183771\n",
      "1      \t [ 3.36304015  2.54200031 10.59349774  0.76424639 13.88376601  0.99173224]. \t  0.9882956156716581 \t 0.9883196147183771\n",
      "2      \t [ 3.72872634  1.74869111 10.57828719  0.77525317 13.69728711  0.99980498]. \t  \u001b[92m0.9883628279926278\u001b[0m \t 0.9883628279926278\n",
      "3      \t [ 3.4516628   2.03810652 10.74267026  0.75440905 14.01753987  0.29442374]. \t  0.8639983806139625 \t 0.9883628279926278\n",
      "4      \t [ 3.63164744  2.20995818 11.32533713  0.73058937 13.62104733  1.        ]. \t  \u001b[92m0.9947095060034464\u001b[0m \t 0.9947095060034464\n",
      "5      \t [ 3.13247284  2.08218497 10.77369717  0.63562099 13.21795148  0.83488729]. \t  0.9877099109899007 \t 0.9947095060034464\n",
      "6      \t [ 3.33812785  2.09159152 10.86960361  1.         13.61514576  0.956039  ]. \t  0.986908182805419 \t 0.9947095060034464\n",
      "7      \t [ 3.40855554  2.07722341 10.84778798  0.5        13.76245194  1.        ]. \t  0.9941430096029409 \t 0.9947095060034464\n",
      "8      \t [ 4.16485525  2.61125289 10.58159602  0.6670012  13.09855907  0.68334383]. \t  0.9875322867639832 \t 0.9947095060034464\n",
      "9      \t [ 3.45922496  3.1569097  11.11756165  0.57260863 13.19059045  0.53242551]. \t  0.8633646670524923 \t 0.9947095060034464\n",
      "10     \t [ 4.00071767  1.95642515 11.24242519  0.55000844 12.78946893  0.3985917 ]. \t  0.8626541375134261 \t 0.9947095060034464\n",
      "11     \t [ 2.46155812  2.61795016 11.44364045  0.64473299 13.96502858  0.67795254]. \t  0.9875850847497363 \t 0.9947095060034464\n",
      "12     \t [ 2.1126979   2.32251128 10.55213937  0.66993638 14.07932175  0.67769443]. \t  0.9890349406926191 \t 0.9947095060034464\n",
      "13     \t [ 2.42320668  2.4502324  10.98632029  0.87569266 14.81932064  0.87037673]. \t  0.9887084877226786 \t 0.9947095060034464\n",
      "14     \t [ 2.0083133   1.81749065 11.28831885  0.73667519 14.45107172  0.55814876]. \t  0.8631534343152353 \t 0.9947095060034464\n",
      "15     \t [ 4.76205859  1.86056563 10.92506904  0.80998002 13.41545654  0.92518772]. \t  0.98830041473426 \t 0.9947095060034464\n",
      "16     \t [ 4.33989449  1.63670175 10.43461646  0.67380329 12.81451582  1.        ]. \t  0.9945798816686793 \t 0.9947095060034464\n",
      "17     \t [ 4.48089283  1.66867065 10.30060073  0.70775112 13.31622757  0.3735714 ]. \t  0.8624332993218302 \t 0.9947095060034464\n",
      "18     \t [ 2.55609676  2.5262809  10.90737194  1.         14.20946713  0.44061691]. \t  0.7918366936213684 \t 0.9947095060034464\n",
      "19     \t [ 2.30468733  2.30795093 11.01249687  0.5        14.31051252  1.        ]. \t  0.9942102209559113 \t 0.9947095060034464\n",
      "20     \t [ 2.39995182  1.74009869 10.17839199  0.87390519 14.82218602  0.97644738]. \t  0.9886172711001383 \t 0.9947095060034464\n",
      "21     \t [ 3.07570002  1.74241354 10.87393068  0.87195293 15.2947021   1.        ]. \t  \u001b[92m0.9949015361291602\u001b[0m \t 0.9949015361291602\n",
      "22     \t [ 2.62269703  1.93071315 10.59396151  0.5        15.02611886  0.54967814]. \t  0.8630190051098722 \t 0.9949015361291602\n",
      "23     \t [ 2.99602875  2.18433634 11.91056629  0.82193572 14.9266354   1.        ]. \t  0.9947335082307337 \t 0.9949015361291602\n",
      "24     \t [ 3.7044705   2.451783   11.24861499  0.91167609 14.89788117  1.        ]. \t  \u001b[92m0.9949447438719878\u001b[0m \t 0.9949447438719878\n",
      "25     \t [ 3.25217982  3.11925483 11.81114833  0.5        14.397379    0.9765806 ]. \t  0.9875082743035635 \t 0.9949447438719878\n",
      "26     \t [ 2.10976328  1.48769969 10.00058942  0.70107753 13.73035636  0.84294058]. \t  0.9883340210330304 \t 0.9949447438719878\n",
      "27     \t [ 2.57355382  2.10369785  9.61896457  0.64980729 13.82217701  1.        ]. \t  0.9936581263066347 \t 0.9949447438719878\n",
      "28     \t [ 3.36178321  2.56145079 11.8706288   0.5        14.32246218  0.3746966 ]. \t  0.8634030739350056 \t 0.9949447438719878\n",
      "29     \t [ 3.08834687  2.66270245 11.41107983  0.5        15.40042012  1.        ]. \t  0.9941478161329637 \t 0.9949447438719878\n",
      "30     \t [ 3.0621332   2.4080114  11.3399099   0.66175872 14.74235709  1.        ]. \t  0.9944166547688523 \t 0.9949447438719878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.055682260635581474"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\n",
    "\n",
    "np.random.seed(run_num_18)\n",
    "surrogate_loser_18 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train18, X_test18, y_train18, y_test18 = train_test_split(X, y, test_size=test_perc, random_state=run_num_18)\n",
    "\n",
    "def f_syn_polarity18(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train18, y=y_train18).mean())\n",
    "    return  score\n",
    "\n",
    "loser_18 = GPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity18, param, n_jobs = -1) # define BayesOpt\n",
    "loser_18.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_18 = loser_18.getResult()[0]\n",
    "params_loser_18['max_depth'] = int(params_loser_18['max_depth'])\n",
    "params_loser_18['min_child_weight'] = int(params_loser_18['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train18 = xgb.DMatrix(X_train18, y_train18)\n",
    "dX_loser_test18 = xgb.DMatrix(X_test18, y_test18)\n",
    "model_loser_18 = xgb.train(params_loser_18, dX_loser_train18)\n",
    "pred_loser_18 = model_loser_18.predict(dX_loser_test18)\n",
    "\n",
    "rmse_loser_18 = np.sqrt(mean_squared_error(pred_loser_18, y_test18))\n",
    "rmse_loser_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.59445577  7.61491991 14.          0.8201684   7.          0.41902487]. \t  0.8889427962173378 \t 0.9874362876751138\n",
      "init   \t [ 7.966974    3.46000022 11.          0.65165179  6.          0.61588843]. \t  0.8888371874545623 \t 0.9874362876751138\n",
      "init   \t [ 0.65664016  0.24088885 10.          0.76028365  4.          0.74988462]. \t  0.9874362876751138 \t 0.9874362876751138\n",
      "init   \t [ 4.0108054   5.55029685 12.          0.98976746 13.          0.16498374]. \t  0.8919048872735041 \t 0.9874362876751138\n",
      "init   \t [ 6.78611641  2.0276123   5.          0.8840465  10.          0.9586535 ]. \t  0.9715264078631116 \t 0.9874362876751138\n",
      "1      \t [ 0.48286332  0.44976506 10.81683489  0.82470355  4.27227938  0.75785374]. \t  0.9873018619268663 \t 0.9874362876751138\n",
      "2      \t [ 1.27876751  0.62889541 10.50308127  0.74925252  4.09690348  0.75233918]. \t  0.9872970641780224 \t 0.9874362876751138\n",
      "3      \t [ 0.64610537  0.97350837 10.17345905  0.91184411  4.37191674  0.99887846]. \t  0.9873930794482798 \t 0.9874362876751138\n",
      "4      \t [ 0.93239818  0.28828379 10.27713922  1.          4.75597017  0.74057459]. \t  \u001b[92m0.9905904062986123\u001b[0m \t 0.9905904062986123\n",
      "5      \t [ 0.71888918  0.7023983  10.27234863  0.5         4.44812472  0.28060729]. \t  0.8908535177409543 \t 0.9905904062986123\n",
      "6      \t [ 0.81835872  0.40498455 10.37295151  0.5         4.44031975  1.        ]. \t  \u001b[92m0.9954152241040101\u001b[0m \t 0.9954152241040101\n",
      "7      \t [ 0.80231678  0.51556509 10.36365048  1.          4.23032642  0.76272639]. \t  0.9906912236046282 \t 0.9954152241040101\n",
      "8      \t [1.42686097 0.66780067 9.39634647 0.68648541 4.42660603 0.84688987]. \t  0.986077652226883 \t 0.9954152241040101\n",
      "9      \t [0.49862677 0.4452121  9.20142951 0.81801462 4.8048822  0.89975646]. \t  0.9864761195534758 \t 0.9954152241040101\n",
      "10     \t [0.60787974 0.90994641 9.11647698 0.5        3.98514243 1.        ]. \t  0.994834324085479 \t 0.9954152241040101\n",
      "11     \t [ 1.06404297  0.88955211 11.41683419  0.80640527  4.82635066  0.75553143]. \t  0.9872730566958375 \t 0.9954152241040101\n",
      "12     \t [0.84910787 0.06025809 8.95167559 0.5        4.1085203  0.68403868]. \t  0.9846950108867745 \t 0.9954152241040101\n",
      "13     \t [0.80400154 0.50312979 9.44300706 0.63457863 4.29807283 0.83559347]. \t  0.9862504831981932 \t 0.9954152241040101\n",
      "14     \t [0.95700125 0.68614281 8.33903783 1.         4.33600641 1.        ]. \t  0.9949063359530347 \t 0.9954152241040101\n",
      "15     \t [ 1.48627182  1.16718961 10.4866141   0.74629322  5.09625638  0.89588624]. \t  0.9867977774019564 \t 0.9954152241040101\n",
      "16     \t [0.05858551 0.24664536 8.70718306 0.98703291 3.89817033 1.        ]. \t  0.9951271728308789 \t 0.9954152241040101\n",
      "17     \t [1.00029361 0.41032731 8.81441728 0.90169123 3.33156809 1.        ]. \t  0.9950071546419262 \t 0.9954152241040101\n",
      "18     \t [0.58363828 0.65661452 8.63359696 0.92549362 3.76127783 0.34763552]. \t  0.8907862976758771 \t 0.9954152241040101\n",
      "19     \t [0.57322617 0.47051812 8.1682425  0.5        3.65237901 1.        ]. \t  0.9945990859404241 \t 0.9954152241040101\n",
      "20     \t [0.20536104 0.1228765  9.21687953 0.5        3.09174332 0.82189927]. \t  0.9863032907256231 \t 0.9954152241040101\n",
      "21     \t [ 0.5255674   0.95228859 10.7463946   0.75600143  5.32909215  0.87269521]. \t  0.9871194344207117 \t 0.9954152241040101\n",
      "22     \t [1.04440085 1.26982112 9.28262805 1.         5.28488747 1.        ]. \t  \u001b[92m0.9956216586083432\u001b[0m \t 0.9956216586083432\n",
      "23     \t [1.46109095 0.41589746 9.12856088 1.         5.4611314  1.        ]. \t  0.9955112404114242 \t 0.9956216586083432\n",
      "24     \t [1.31104618 0.75912566 9.04303732 1.         5.26535928 0.27575252]. \t  0.8897829077060798 \t 0.9956216586083432\n",
      "25     \t [1.20933534 0.81837101 8.583406   0.5        5.19946127 1.        ]. \t  0.993802150526657 \t 0.9956216586083432\n",
      "26     \t [1.0876281  0.69972075 9.71101515 0.50933651 5.64476533 0.80657776]. \t  0.9860296439694602 \t 0.9956216586083432\n",
      "27     \t [ 0.80415864  1.36652409 11.00476881  0.5         4.53522553  0.89480667]. \t  0.9869129965974771 \t 0.9956216586083432\n",
      "28     \t [ 0.96552744  0.8387879  10.73390774  0.68845447  4.80598973  0.78751794]. \t  0.9871338370707979 \t 0.9956216586083432\n",
      "29     \t [ 1.58955563  1.64441538 11.50321443  1.          4.45164343  1.        ]. \t  \u001b[92m0.9967978582531654\u001b[0m \t 0.9967978582531654\n",
      "30     \t [ 0.97564956  1.78465176 11.5025357   1.          5.24658185  1.        ]. \t  \u001b[92m0.9969802886633587\u001b[0m \t 0.9969802886633587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05273921155674804"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\n",
    "\n",
    "np.random.seed(run_num_19)\n",
    "surrogate_loser_19 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train19, X_test19, y_train19, y_test19 = train_test_split(X, y, test_size=test_perc, random_state=run_num_19)\n",
    "\n",
    "def f_syn_polarity19(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_19, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train19, y=y_train19).mean())\n",
    "    return  score\n",
    "\n",
    "loser_19 = GPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity19, param, n_jobs = -1) # define BayesOpt\n",
    "loser_19.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_19 = loser_19.getResult()[0]\n",
    "params_loser_19['max_depth'] = int(params_loser_19['max_depth'])\n",
    "params_loser_19['min_child_weight'] = int(params_loser_19['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train19 = xgb.DMatrix(X_train19, y_train19)\n",
    "dX_loser_test19 = xgb.DMatrix(X_test19, y_test19)\n",
    "model_loser_19 = xgb.train(params_loser_19, dX_loser_train19)\n",
    "pred_loser_19 = model_loser_19.predict(dX_loser_test19)\n",
    "\n",
    "rmse_loser_19 = np.sqrt(mean_squared_error(pred_loser_19, y_test19))\n",
    "rmse_loser_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.70517285  5.64528755 11.          0.51213999 19.          0.57883228]. \t  0.8868016210751127 \t 0.9913825280594238\n",
      "init   \t [0.68106288 5.8452906  6.         0.58037829 8.         0.87974899]. \t  0.991036867637945 \t 0.9913825280594238\n",
      "init   \t [ 4.04121426  7.17971417 11.          0.57603584 10.          0.19954225]. \t  0.8886163118407602 \t 0.9913825280594238\n",
      "init   \t [ 5.27556761  8.67655329 13.          0.54252273  4.          0.48570796]. \t  0.8864991504193368 \t 0.9913825280594238\n",
      "init   \t [ 5.30003889  5.73946822 12.          0.52666698 16.          0.77467019]. \t  0.9913825280594238 \t 0.9913825280594238\n",
      "1      \t [ 5.44423671  5.70594793 11.64407075  0.52149642 17.06778273  0.7049659 ]. \t  0.9912625141572762 \t 0.9913825280594238\n",
      "2      \t [ 5.45180339  5.13144524 12.22660653  0.5        16.60706141  0.73874764]. \t  0.9912241031261956 \t 0.9913825280594238\n",
      "3      \t [ 6.01079364  5.74447328 12.0577313   0.5        16.50953391  0.96839561]. \t  0.9908448505110751 \t 0.9913825280594238\n",
      "4      \t [ 5.25977667  5.91628491 12.41554483  0.5        16.73767644  1.        ]. \t  \u001b[92m0.9933268609986774\u001b[0m \t 0.9933268609986774\n",
      "5      \t [ 5.59300638  5.82061959 12.24771225  0.70431615 16.58618292  0.2564824 ]. \t  0.8891443864239102 \t 0.9933268609986774\n",
      "6      \t [ 5.42829202  5.58745507 11.95941969  1.         16.56064108  1.        ]. \t  \u001b[92m0.9949495376797377\u001b[0m \t 0.9949495376797377\n",
      "7      \t [ 5.84950782  5.50056017 11.85651343  0.60889182 18.20042994  0.87363636]. \t  0.9912289068905068 \t 0.9949495376797377\n",
      "8      \t [ 6.06367807  5.15979945 11.07060611  0.5        18.10490765  0.64063878]. \t  0.888083413303605 \t 0.9949495376797377\n",
      "9      \t [ 5.14916918  5.16990369 11.39746218  0.5        18.35194028  0.79809715]. \t  0.991094483493138 \t 0.9949495376797377\n",
      "10     \t [ 5.53005193  5.42870016 11.59641828  0.5        18.34020036  0.1       ]. \t  0.8879537900751223 \t 0.9949495376797377\n",
      "11     \t [ 5.4751131   5.9099154  11.20540982  0.5        18.21498234  0.87837036]. \t  0.9911760964590518 \t 0.9949495376797377\n",
      "12     \t [ 5.53460754  5.45842381 11.2923197   1.         18.30620791  0.70635438]. \t  0.991334532455126 \t 0.9949495376797377\n",
      "13     \t [1.02906448 5.6058813  6.24956611 0.6214823  8.73387925 0.70493476]. \t  0.9910608800983648 \t 0.9949495376797377\n",
      "14     \t [1.28177756 6.05793241 6.59379038 0.66860407 8.10094577 0.75521313]. \t  0.9916273835514345 \t 0.9949495376797377\n",
      "15     \t [0.59535767 6.20175622 6.57065721 0.50168502 8.54884102 1.        ]. \t  0.9889245184162528 \t 0.9949495376797377\n",
      "16     \t [1.19299975 6.31381048 5.93054323 0.50153685 8.51917704 0.99778065]. \t  0.9867353778317972 \t 0.9949495376797377\n",
      "17     \t [0.79861129 6.26604949 6.17341589 0.81516283 8.4292869  0.32118032]. \t  0.8894612408537906 \t 0.9949495376797377\n",
      "18     \t [0.94755813 5.97209365 6.25913847 1.         8.41413552 1.        ]. \t  0.9895486263850987 \t 0.9949495376797377\n",
      "19     \t [ 5.64593757  5.56870065 12.81850904  0.7556384  15.91817392  1.        ]. \t  0.9945318632472618 \t 0.9949495376797377\n",
      "20     \t [0.97297134 5.96623663 6.28591547 0.5        8.3592232  0.85265389]. \t  0.9906768031143823 \t 0.9949495376797377\n",
      "21     \t [0.78697614 6.8519695  6.30673556 0.74360914 7.60741911 1.        ]. \t  0.9891165514459638 \t 0.9949495376797377\n",
      "22     \t [1.31450896 7.08276955 6.82220894 0.78209076 8.41241127 1.        ]. \t  0.9891213527211348 \t 0.9949495376797377\n",
      "23     \t [1.47397377 6.25951    6.76485251 0.7877452  9.31012248 0.73987021]. \t  0.9911857024665331 \t 0.9949495376797377\n",
      "24     \t [0.78679491 5.99084115 6.06002527 0.68702297 9.70159299 0.88784922]. \t  0.991896219421612 \t 0.9949495376797377\n",
      "25     \t [0.85106254 6.95680459 6.33281744 0.69690279 9.29228726 1.        ]. \t  0.9890733441179931 \t 0.9949495376797377\n",
      "26     \t [1.22093193 6.13021911 5.77625663 0.91084253 7.1348939  0.78894543]. \t  0.9868697954211719 \t 0.9949495376797377\n",
      "27     \t [0.57544985 5.94296308 6.44369894 0.94603585 7.03605727 0.76216087]. \t  0.9921746629586972 \t 0.9949495376797377\n",
      "28     \t [0.27871041 6.16670801 5.58941187 0.83584382 7.10486616 1.        ]. \t  0.9844789661550025 \t 0.9949495376797377\n",
      "29     \t [0.60164951 6.21567783 5.90989848 0.5        7.07461521 0.38121233]. \t  0.887924968733825 \t 0.9949495376797377\n",
      "30     \t [0.56294287 6.05297742 6.89253476 0.80479187 9.49265778 0.64798313]. \t  0.8901189490368416 \t 0.9949495376797377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06258804691391658"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\n",
    "\n",
    "np.random.seed(run_num_20)\n",
    "surrogate_loser_20 = GaussianProcess(cov_func)\n",
    "\n",
    "X_train20, X_test20, y_train20, y_test20 = train_test_split(X, y, test_size=test_perc, random_state=run_num_20)\n",
    "\n",
    "def f_syn_polarity20(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_20, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train20, y=y_train20).mean())\n",
    "    return  score\n",
    "\n",
    "loser_20 = GPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity20, param, n_jobs = -1) # define BayesOpt\n",
    "loser_20.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_loser_20 = loser_20.getResult()[0]\n",
    "params_loser_20['max_depth'] = int(params_loser_20['max_depth'])\n",
    "params_loser_20['min_child_weight'] = int(params_loser_20['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_loser_train20 = xgb.DMatrix(X_train20, y_train20)\n",
    "dX_loser_test20 = xgb.DMatrix(X_test20, y_test20)\n",
    "model_loser_20 = xgb.train(params_loser_20, dX_loser_train20)\n",
    "pred_loser_20 = model_loser_20.predict(dX_loser_test20)\n",
    "\n",
    "rmse_loser_20 = np.sqrt(mean_squared_error(pred_loser_20, y_test20))\n",
    "rmse_loser_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.12170176  1.69069754 11.          0.92695323  3.          0.36579277]. \t  0.8890243937641641 \t 0.9846806025668978\n",
      "init   \t [ 1.49162957  0.22478325  6.          0.85613654 15.          0.19363322]. \t  0.8875985382508563 \t 0.9846806025668978\n",
      "init   \t [ 5.27957805  2.81819356 11.          0.81062146 14.          0.74826845]. \t  0.9846806025668978 \t 0.9846806025668978\n",
      "init   \t [3.81060005 6.24236569 7.         0.95038709 5.         0.9861745 ]. \t  0.9822369877610099 \t 0.9846806025668978\n",
      "init   \t [4.77531134 7.87990376 6.         0.78863691 6.         0.47918609]. \t  0.8869792272703285 \t 0.9846806025668978\n",
      "1      \t [4.2715594  7.02481577 6.5221798  0.87309966 5.47782122 0.74392509]. \t  0.9779162613257566 \t 0.9846806025668978\n",
      "2      \t [3.92707445 6.00212986 6.00421818 0.82927105 5.386277   0.8741341 ]. \t  0.9779354646287257 \t 0.9846806025668978\n",
      "3      \t [4.24022397 6.34720662 6.29733253 0.80391203 4.60571059 0.4749495 ]. \t  0.887190465124152 \t 0.9846806025668978\n",
      "4      \t [3.33596136 6.67854385 6.23124525 0.91093507 5.02915095 0.75580158]. \t  0.9779402656964713 \t 0.9846806025668978\n",
      "5      \t [3.70282842 6.28384514 6.61148396 0.54777949 5.40078676 0.2779573 ]. \t  0.8870608418956692 \t 0.9846806025668978\n",
      "6      \t [3.87816312 6.4997114  6.36911973 0.5        5.05939776 1.        ]. \t  \u001b[92m0.9887564933542389\u001b[0m \t 0.9887564933542389\n",
      "7      \t [4.17682292 7.06244906 5.54918828 0.88801785 5.57938158 0.35522972]. \t  0.8871184522194394 \t 0.9887564933542389\n",
      "8      \t [3.88963334 6.43666963 6.35751867 1.         5.20572468 0.72015359]. \t  0.9836292254285318 \t 0.9887564933542389\n",
      "9      \t [3.28234719 5.62882549 6.43318244 0.68263791 4.66694645 0.84621679]. \t  0.9778730544817983 \t 0.9887564933542389\n",
      "10     \t [4.06567922 7.80514744 6.25512892 0.52614742 5.26062642 0.26448714]. \t  0.8867151807827747 \t 0.9887564933542389\n",
      "11     \t [3.82361231 7.50470494 6.098      0.53551388 6.12407689 0.7587214 ]. \t  0.9768360803392374 \t 0.9887564933542389\n",
      "12     \t [4.37430188 7.27263097 6.29283874 0.5        6.07121726 0.1374475 ]. \t  0.88573100441837 \t 0.9887564933542389\n",
      "13     \t [4.1802494  5.32583449 6.72311323 0.59055189 4.9075214  0.8953555 ]. \t  0.9770617199442778 \t 0.9887564933542389\n",
      "14     \t [4.36932515 7.41620906 5.95584127 0.5        5.65881174 0.81651133]. \t  0.9686218808735241 \t 0.9887564933542389\n",
      "15     \t [3.47378906 5.57428917 6.69926362 0.60602601 5.41248286 1.        ]. \t  \u001b[92m0.9889341225583994\u001b[0m \t 0.9889341225583994\n",
      "16     \t [3.74878782 5.82735815 6.53171373 0.65370424 4.96493978 0.86306941]. \t  0.977949866933093 \t 0.9889341225583994\n",
      "17     \t [3.4692717  5.00210131 7.2680425  1.         4.75995577 1.        ]. \t  \u001b[92m0.9930292151275463\u001b[0m \t 0.9930292151275463\n",
      "18     \t [3.58285039 4.70932473 6.36869049 1.         5.06054341 1.        ]. \t  0.9893805983498463 \t 0.9930292151275463\n",
      "19     \t [4.15056204 4.96580717 7.09675556 1.         5.5136522  1.        ]. \t  0.9929524025379638 \t 0.9930292151275463\n",
      "20     \t [3.65413622 4.83728282 6.87873477 0.80990248 5.13544283 0.38725145]. \t  0.8871472573813244 \t 0.9930292151275463\n",
      "21     \t [3.70437766 6.98590561 6.13125494 0.5        5.55895499 0.62716498]. \t  0.8864031248623537 \t 0.9930292151275463\n",
      "22     \t [3.58077351 7.09481068 6.83377683 0.72268676 4.34391015 0.85555644]. \t  0.9777962405784839 \t 0.9930292151275463\n",
      "23     \t [2.70966179 6.30284118 7.01884481 0.86520671 4.62364319 0.94273032]. \t  0.9821793775063837 \t 0.9930292151275463\n",
      "24     \t [4.46946287 5.06614774 6.1989247  0.63004136 5.78022229 0.97111583]. \t  0.977791441239333 \t 0.9930292151275463\n",
      "25     \t [3.96984507 4.55501985 6.8506035  0.5        5.22714857 1.        ]. \t  0.9887516925630685 \t 0.9930292151275463\n",
      "26     \t [4.37272103 6.55043709 5.91956391 0.86005632 6.47018445 1.        ]. \t  0.9842197230190642 \t 0.9930292151275463\n",
      "27     \t [4.55870023 5.95300912 6.51083584 0.58694043 5.90687064 0.96620658]. \t  0.9769753043203352 \t 0.9930292151275463\n",
      "28     \t [3.14314406 6.44535918 6.46053483 1.         4.19821891 1.        ]. \t  0.9894334070527204 \t 0.9930292151275463\n",
      "29     \t [3.18788675 6.51022541 6.81025164 0.8249316  4.45428126 0.35471269]. \t  0.8871760625432095 \t 0.9930292151275463\n",
      "30     \t [4.22210297 7.49099441 5.86653718 1.         6.40295104 0.7471972 ]. \t  0.9811712065205381 \t 0.9930292151275463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07076761765922786"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \n",
    "\n",
    "np.random.seed(run_num_1)\n",
    "surrogate_winner_1 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=test_perc, random_state=run_num_1)\n",
    "\n",
    "def f_syn_polarity1(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_1, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train1, y=y_train1).mean())\n",
    "    return  score\n",
    "\n",
    "winner_1 = GPGO(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity1, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_1.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_1 = winner_1.getResult()[0]\n",
    "params_winner_1['max_depth'] = int(params_winner_1['max_depth'])\n",
    "params_winner_1['min_child_weight'] = int(params_winner_1['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train1 = xgb.DMatrix(X_train1, y_train1)\n",
    "dX_winner_test1 = xgb.DMatrix(X_test1, y_test1)\n",
    "model_winner_1 = xgb.train(params_winner_1, dX_winner_train1)\n",
    "pred_winner_1 = model_winner_1.predict(dX_winner_test1)\n",
    "\n",
    "rmse_winner_1 = np.sqrt(mean_squared_error(pred_winner_1, y_test1))\n",
    "rmse_winner_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.51985493  0.73903599  7.          0.71824677 17.          0.61827209]. \t  0.8899125543587183 \t 0.9907392220424587\n",
      "init   \t [ 8.4047092   4.35120551 12.          0.84231905 16.          0.80172484]. \t  0.9902255407751176 \t 0.9907392220424587\n",
      "init   \t [ 6.42749368  9.61026168 10.          0.73363448 15.          0.84932207]. \t  0.9904943853572861 \t 0.9907392220424587\n",
      "init   \t [ 2.66008363  8.36685389  6.          0.92082163 12.          0.69999264]. \t  0.9907392220424587 \t 0.9907392220424587\n",
      "init   \t [ 1.74779478  2.17691512 12.          0.71794562  2.          0.48990646]. \t  0.8940700725633864 \t 0.9907392220424587\n",
      "1      \t [ 3.544957    8.41694683  6.00000297  0.95097258 12.69535016  0.7970868 ]. \t  0.9907200188777733 \t 0.9907392220424587\n",
      "2      \t [ 3.30723254  8.52799374  6.81316137  0.90322348 12.05864334  0.89278881]. \t  \u001b[92m0.990873645301542\u001b[0m \t 0.990873645301542\n",
      "3      \t [ 2.94610158  9.17546493  6.24221332  0.95199704 12.47160548  0.87694691]. \t  0.9899278765110787 \t 0.990873645301542\n",
      "4      \t [ 2.80757371  8.39250381  6.56902649  0.80997453 12.76071101  0.51709791]. \t  0.8958943776336494 \t 0.990873645301542\n",
      "5      \t [ 3.3357556   8.76474409  6.20009742  0.9638614  12.09874323  0.20453283]. \t  0.8951502664106453 \t 0.990873645301542\n",
      "6      \t [ 3.16737576  8.60252488  6.16101173  0.5        12.2421863   0.96727517]. \t  0.9878347423466316 \t 0.990873645301542\n",
      "7      \t [ 3.08762277  8.53758974  6.2630659   1.         12.32157762  0.9148537 ]. \t  \u001b[92m0.9910800922516126\u001b[0m \t 0.9910800922516126\n",
      "8      \t [ 3.86375985  9.04971136  6.76822113  0.64202835 12.86997992  0.73435484]. \t  0.9887516830198043 \t 0.9910800922516126\n",
      "9      \t [ 2.33103242  8.93583156  6.71183475  0.56543708 11.69565384  0.54215928]. \t  0.8898837735350945 \t 0.9910800922516126\n",
      "10     \t [ 3.1708135   9.35312794  7.20775672  0.5        12.28486755  0.64118773]. \t  0.8925049885055426 \t 0.9910800922516126\n",
      "11     \t [ 3.4903087   9.09667155  5.87529734  0.5        13.2505788   0.41436386]. \t  0.8954623658910226 \t 0.9910800922516126\n",
      "12     \t [ 2.32270388  8.91332213  5.61868709  0.5        12.42482024  0.2458989 ]. \t  0.8955679820522228 \t 0.9910800922516126\n",
      "13     \t [ 3.25056342  8.87212558  6.4346395   0.6079646  12.53340714  0.57803621]. \t  0.8902198301616613 \t 0.9910800922516126\n",
      "14     \t [ 4.21687693  9.30974189  5.93290346  1.         13.08785467  1.        ]. \t  0.9839316796951758 \t 0.9910800922516126\n",
      "15     \t [ 4.01696178  9.49636194  6.94350842  1.         12.10313419  1.        ]. \t  0.9886604789812514 \t 0.9910800922516126\n",
      "16     \t [ 3.28395173  9.00143761  5.09393843  0.81354283 12.59753047  0.80108721]. \t  0.9862072703366801 \t 0.9910800922516126\n",
      "17     \t [ 3.71793416  9.11083903  7.66400214  1.         12.58032951  1.        ]. \t  \u001b[92m0.9925107284355407\u001b[0m \t 0.9925107284355407\n",
      "18     \t [ 2.76528793  8.6056978   5.45779301  0.69628473 13.20735148  0.97936896]. \t  0.9853671343016862 \t 0.9925107284355407\n",
      "19     \t [ 3.78614848  9.73926813  6.94291396  1.         13.0706047   1.        ]. \t  0.9886364749562517 \t 0.9925107284355407\n",
      "20     \t [ 3.59153711  8.94696362  6.15373512  1.         13.62721639  1.        ]. \t  0.988598068488595 \t 0.9925107284355407\n",
      "21     \t [ 2.97802838  8.58949201  5.39362347  1.         12.98165108  0.27602107]. \t  0.8870752445301892 \t 0.9925107284355407\n",
      "22     \t [ 3.81520818  9.17235     6.87039786  1.         12.69996073  1.        ]. \t  0.9886508775371942 \t 0.9925107284355407\n",
      "23     \t [ 3.3448952   9.15543688  7.49261201  1.         11.52320642  0.77797028]. \t  0.991257717168958 \t 0.9925107284355407\n",
      "24     \t [ 4.06541518  9.52042459  7.50577856  0.97600825 12.43045516  0.3672547 ]. \t  0.8941517076549924 \t 0.9925107284355407\n",
      "25     \t [ 3.47857414  8.99905318  5.63951177  0.76834039 13.06947923  1.        ]. \t  0.9839700871308313 \t 0.9925107284355407\n",
      "26     \t [ 4.32366132  9.28524786  6.54821121  0.89085498 13.62558949  0.57054183]. \t  0.8944013391159139 \t 0.9925107284355407\n",
      "27     \t [ 2.92290143  9.18727366  5.75561582  0.80402305 11.59479306  0.85860997]. \t  0.9850934847106362 \t 0.9925107284355407\n",
      "28     \t [ 2.58295239  8.8642281   5.49711567  0.69716878 12.32334281  0.97090927]. \t  0.9849206542924684 \t 0.9925107284355407\n",
      "29     \t [ 3.04693038  9.23307616  6.74381511  0.95877352 11.57943133  1.        ]. \t  0.9887852996899928 \t 0.9925107284355407\n",
      "30     \t [ 3.90855405  9.08629954  7.57649725  0.5        11.9472639   1.        ]. \t  0.9901151254130302 \t 0.9925107284355407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.069722307025005"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \n",
    "\n",
    "np.random.seed(run_num_2)\n",
    "surrogate_winner_2 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=test_perc, random_state=run_num_2)\n",
    "\n",
    "def f_syn_polarity2(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_2, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train2, y=y_train2).mean())\n",
    "    return  score\n",
    "\n",
    "winner_2 = GPGO(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity2, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_2.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_2 = winner_2.getResult()[0]\n",
    "params_winner_2['max_depth'] = int(params_winner_2['max_depth'])\n",
    "params_winner_2['min_child_weight'] = int(params_winner_2['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train2 = xgb.DMatrix(X_train2, y_train2)\n",
    "dX_winner_test2 = xgb.DMatrix(X_test2, y_test2)\n",
    "model_winner_2 = xgb.train(params_winner_2, dX_winner_train2)\n",
    "pred_winner_2 = model_winner_2.predict(dX_winner_test2)\n",
    "\n",
    "rmse_winner_2 = np.sqrt(mean_squared_error(pred_winner_2, y_test2))\n",
    "rmse_winner_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 7.51575607  1.09251275 13.          0.80377147 16.          0.31229624]. \t  0.8853133459581143 \t 0.9850406749037081\n",
      "init   \t [6.12794891 1.19652403 5.         0.74822174 8.         0.33695838]. \t  0.8834410179031171 \t 0.9850406749037081\n",
      "init   \t [ 6.21631223  4.4909976  10.          0.66764529  9.          0.77207323]. \t  0.9850406749037081 \t 0.9850406749037081\n",
      "init   \t [ 0.58127233  4.79918477 11.          0.64526552 15.          0.62431773]. \t  0.8862927047985528 \t 0.9850406749037081\n",
      "init   \t [ 5.76228258  0.67219724  7.          0.88228691 13.          0.29867219]. \t  0.8858222329336453 \t 0.9850406749037081\n",
      "1      \t [6.22390694 4.87103853 9.09040166 0.71624391 9.4547901  0.48906535]. \t  0.8852893469114709 \t 0.9850406749037081\n",
      "2      \t [6.91613504 4.48438682 9.24981297 0.55970862 8.79852219 1.        ]. \t  \u001b[92m0.9936821314388352\u001b[0m \t 0.9936821314388352\n",
      "3      \t [6.19244525 4.06774048 9.19676841 0.52130854 8.70588524 0.43602618]. \t  0.8861822976646393 \t 0.9936821314388352\n",
      "4      \t [6.53032545 4.01929408 9.41743907 0.88126635 9.48215419 0.98009253]. \t  0.9862360757080421 \t 0.9936821314388352\n",
      "5      \t [6.88417413 4.34879947 9.64909697 0.5        9.29291413 0.31789009]. \t  0.8865759659978 \t 0.9936821314388352\n",
      "6      \t [6.53474735 4.49158851 9.43935097 1.         8.93581721 0.69560397]. \t  0.9655397540658841 \t 0.9936821314388352\n",
      "7      \t [6.35558123 4.43778455 9.40789497 0.5        9.19932664 1.        ]. \t  \u001b[92m0.9939749807373855\u001b[0m \t 0.9939749807373855\n",
      "8      \t [6.82475502 3.67321531 9.94230399 0.55070537 8.67270912 1.        ]. \t  0.9933508768480787 \t 0.9939749807373855\n",
      "9      \t [7.22323616 3.43015567 8.97364876 0.5143249  8.91387724 0.95688857]. \t  0.9836820504493389 \t 0.9939749807373855\n",
      "10     \t [6.81212095 3.89692136 9.41040782 0.57271862 8.95613755 0.87512084]. \t  0.9847238166018096 \t 0.9939749807373855\n",
      "11     \t [7.64922455 3.52003246 9.30928035 0.5        8.00368882 1.        ]. \t  \u001b[92m0.9940325898857112\u001b[0m \t 0.9940325898857112\n",
      "12     \t [7.94207095 3.4264057  9.62020876 0.7832925  8.82374603 1.        ]. \t  \u001b[92m0.9943110349438681\u001b[0m \t 0.9943110349438681\n",
      "13     \t [7.41747271 2.76035044 9.50180451 0.84778555 8.43115041 1.        ]. \t  \u001b[92m0.9943638435775987\u001b[0m \t 0.9943638435775987\n",
      "14     \t [7.71365832 3.37981551 8.880525   1.         8.50203061 1.        ]. \t  0.9939845790008247 \t 0.9943638435775987\n",
      "15     \t [7.98489928 2.8931989  9.04092909 0.5        8.58208523 1.        ]. \t  0.9935285068128207 \t 0.9943638435775987\n",
      "16     \t [7.63198057 3.19645033 9.32871176 0.59792894 8.47152078 0.36333563]. \t  0.8851981184653397 \t 0.9943638435775987\n",
      "17     \t [6.82096947 4.81486139 9.92214826 0.93148323 9.81130034 1.        ]. \t  \u001b[92m0.9945510755395442\u001b[0m \t 0.9945510755395442\n",
      "18     \t [7.54746757 3.29464075 9.35143839 0.61292011 8.52698544 1.        ]. \t  0.9941862140968629 \t 0.9945510755395442\n",
      "19     \t [ 7.18743252  4.1655309  10.50706042  0.96844822  9.13272952  1.        ]. \t  \u001b[92m0.9954488284667584\u001b[0m \t 0.9954488284667584\n",
      "20     \t [7.66408423 2.71034582 8.97474245 1.         9.41325989 0.86443055]. \t  0.9611038032493265 \t 0.9954488284667584\n",
      "21     \t [ 7.30900175  4.23741289 10.23448915  0.70885025  7.98431976  0.97358042]. \t  0.9850838793276339 \t 0.9954488284667584\n",
      "22     \t [7.65322927 4.00023183 8.85605544 0.86538282 9.73687883 1.        ]. \t  0.9937829453568058 \t 0.9954488284667584\n",
      "23     \t [7.08438063 3.75769894 8.23016921 0.77098138 9.42181587 0.63126456]. \t  0.8854045672824359 \t 0.9954488284667584\n",
      "24     \t [ 7.42173367  3.29084933 10.37019299  1.          7.97805552  0.90229137]. \t  0.9666631454226948 \t 0.9954488284667584\n",
      "25     \t [8.03387993 3.40634174 8.52860694 0.5995881  9.24962423 0.81964544]. \t  0.9842917541777356 \t 0.9954488284667584\n",
      "26     \t [6.75981418 3.63957464 9.7671644  0.68726576 7.5623209  0.81474401]. \t  0.9849734668005053 \t 0.9954488284667584\n",
      "27     \t [7.13076005 4.4519448  9.74242326 0.8232519  9.3505216  1.        ]. \t  0.994387847395163 \t 0.9954488284667584\n",
      "28     \t [ 6.68460881  3.87725939 10.41302439  0.99130262  8.33878725  0.85518457]. \t  0.9866297423126081 \t 0.9954488284667584\n",
      "29     \t [7.08119692 4.30799276 9.21457696 0.5        7.69193503 0.61990015]. \t  0.8856926025142092 \t 0.9954488284667584\n",
      "30     \t [ 7.02267941  3.82450235 10.17397496  0.5         8.06568884  0.5826582 ]. \t  0.8858702317875135 \t 0.9954488284667584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06284954684387484"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \n",
    "\n",
    "np.random.seed(run_num_3)\n",
    "surrogate_winner_3 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=test_perc, random_state=run_num_3)\n",
    "\n",
    "def f_syn_polarity3(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_3, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train3, y=y_train3).mean())\n",
    "    return  score\n",
    "\n",
    "winner_3 = GPGO(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity3, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_3.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_3 = winner_3.getResult()[0]\n",
    "params_winner_3['max_depth'] = int(params_winner_3['max_depth'])\n",
    "params_winner_3['min_child_weight'] = int(params_winner_3['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train3 = xgb.DMatrix(X_train3, y_train3)\n",
    "dX_winner_test3 = xgb.DMatrix(X_test3, y_test3)\n",
    "model_winner_3 = xgb.train(params_winner_3, dX_winner_train3)\n",
    "pred_winner_3 = model_winner_3.predict(dX_winner_test3)\n",
    "\n",
    "rmse_winner_3 = np.sqrt(mean_squared_error(pred_winner_3, y_test3))\n",
    "rmse_winner_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.27423888  7.57027691 14.          0.71146658  9.          0.39774643]. \t  0.7961286970370575 \t 0.9680361805433857\n",
      "init   \t [ 2.32539597  3.72936196 10.          0.82732983  1.          0.94661358]. \t  0.9680361805433857 \t 0.9680361805433857\n",
      "init   \t [1.75719325 8.52964023 6.         0.7434712  8.         0.81674001]. \t  0.9546131048168407 \t 0.9680361805433857\n",
      "init   \t [ 6.71917565  6.77963281 14.          0.99125515  2.          0.14599602]. \t  0.7981786092152628 \t 0.9680361805433857\n",
      "init   \t [ 3.45494695  9.84067498 11.          0.7159999   7.          0.26316389]. \t  0.7957782443289703 \t 0.9680361805433857\n",
      "1      \t [ 2.42762908  3.53114883 10.78284318  0.83361834  1.7817006   0.8868016 ]. \t  \u001b[92m0.9680553837080712\u001b[0m \t 0.9680553837080712\n",
      "2      \t [ 3.00648037  4.16346124 10.1535505   0.83064638  1.68618922  0.9726306 ]. \t  0.967877752844446 \t 0.9680553837080712\n",
      "3      \t [ 2.07711757  4.39227784 10.42719751  0.82417189  1.57474604  0.75038322]. \t  \u001b[92m0.968199406336928\u001b[0m \t 0.968199406336928\n",
      "4      \t [ 2.69845578  4.11009411 10.82997114  0.82856808  1.03718398  0.99407089]. \t  0.9679593669166445 \t 0.968199406336928\n",
      "5      \t [ 2.65483426  3.87172848 10.39926313  0.81781753  1.35626342  0.24105086]. \t  0.7993931900907844 \t 0.968199406336928\n",
      "6      \t [ 2.44930998  3.94914097 10.39399767  0.5         1.47900071  1.        ]. \t  \u001b[92m0.9959529140282596\u001b[0m \t 0.9959529140282596\n",
      "7      \t [ 2.45859156  3.96170649 10.40232842  1.          1.46248746  1.        ]. \t  \u001b[92m0.9964281950507949\u001b[0m \t 0.9964281950507949\n",
      "8      \t [ 2.83461139  4.44227995 11.20113673  0.71893403  2.14719103  0.7925998 ]. \t  0.968866724122885 \t 0.9964281950507949\n",
      "9      \t [ 3.53523239  3.67597671 11.14089428  0.67921882  1.87467571  1.        ]. \t  0.9962985723754546 \t 0.9964281950507949\n",
      "10     \t [ 2.8196478   3.68066504 11.82652171  0.56706744  1.72940056  0.88644629]. \t  0.9679065497091965 \t 0.9964281950507949\n",
      "11     \t [ 2.93072032  3.92663681 11.10675368  0.67327742  1.74367272  0.88968079]. \t  0.9688523187762309 \t 0.9964281950507949\n",
      "12     \t [ 3.14254723  3.49875541 11.68113571  0.87281313  2.63214774  0.99214269]. \t  0.9685258710611424 \t 0.9964281950507949\n",
      "13     \t [ 3.25205839  2.75405002 11.61878163  0.88374511  1.8185556   1.        ]. \t  0.9964233966796211 \t 0.9964281950507949\n",
      "14     \t [ 1.94279038  4.1350712  11.75891954  0.98088714  2.40294369  0.74484322]. \t  0.9698508899775858 \t 0.9964281950507949\n",
      "15     \t [ 3.45573206  3.39369735 11.82450896  1.          2.02033239  0.45232803]. \t  0.7938337988252583 \t 0.9964281950507949\n",
      "16     \t [ 2.28696053  3.12795671 11.91237326  1.          2.10712153  1.        ]. \t  \u001b[92m0.9967642514007906\u001b[0m \t 0.9967642514007906\n",
      "17     \t [2.6383027  4.66734296 9.99057621 0.64704027 1.         0.95330862]. \t  0.9664327110372928 \t 0.9967642514007906\n",
      "18     \t [ 2.36154089  3.44092131 11.63651562  0.5         2.46288808  0.66053528]. \t  0.7980874018578561 \t 0.9967642514007906\n",
      "19     \t [ 2.7073562   3.65991644 11.70179482  1.          2.08172037  1.        ]. \t  0.9966394311069058 \t 0.9967642514007906\n",
      "20     \t [ 1.63450733  3.7170815  11.58558733  0.8288787   1.43364868  0.89500838]. \t  0.9699132993639575 \t 0.9967642514007906\n",
      "21     \t [ 2.33209846  2.88264804 11.48298304  0.76114742  1.13375421  1.        ]. \t  0.9965434147303359 \t 0.9967642514007906\n",
      "22     \t [ 1.91632086  4.89876318 11.53192587  0.64461415  1.87860183  0.99063365]. \t  0.9691259633890001 \t 0.9967642514007906\n",
      "23     \t [ 3.20847814  2.99184714 10.54921984  0.70601174  1.16318502  1.        ]. \t  0.9961065404520282 \t 0.9967642514007906\n",
      "24     \t [ 3.58163566  2.96775481 10.74428959  0.8084487   2.27527537  1.        ]. \t  0.996116142034371 \t 0.9967642514007906\n",
      "25     \t [ 2.05935717  4.58764127 11.64748436  0.90837197  1.72861578  0.36862327]. \t  0.7982506208754053 \t 0.9967642514007906\n",
      "26     \t [ 3.11731924  2.99842081 11.14188726  0.59293448  1.80457523  1.        ]. \t  0.996250565708311 \t 0.9967642514007906\n",
      "27     \t [ 2.00230382  4.22631835 11.42662077  0.65203348  1.98445323  1.        ]. \t  0.9962697679049975 \t 0.9967642514007906\n",
      "28     \t [ 2.55541028  5.21433961 10.65103013  0.54004456  1.70750241  1.        ]. \t  0.9959001035968047 \t 0.9967642514007906\n",
      "29     \t [ 2.18242087  3.25073667 12.32461949  0.88870446  1.33925348  0.87092818]. \t  0.9695436363003348 \t 0.9967642514007906\n",
      "30     \t [ 3.37492611  3.14900705 10.86216176  1.          1.72361783  1.        ]. \t  0.9963033732357687 \t 0.9967642514007906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.054568189625080324"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \n",
    "\n",
    "np.random.seed(run_num_4)\n",
    "surrogate_winner_4 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=test_perc, random_state=run_num_4)\n",
    "\n",
    "def f_syn_polarity4(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_4, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train4, y=y_train4).mean())\n",
    "    return  score\n",
    "\n",
    "winner_4 = GPGO(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity4, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_4.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_4 = winner_4.getResult()[0]\n",
    "params_winner_4['max_depth'] = int(params_winner_4['max_depth'])\n",
    "params_winner_4['min_child_weight'] = int(params_winner_4['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train4 = xgb.DMatrix(X_train4, y_train4)\n",
    "dX_winner_test4 = xgb.DMatrix(X_test4, y_test4)\n",
    "model_winner_4 = xgb.train(params_winner_4, dX_winner_train4)\n",
    "pred_winner_4 = model_winner_4.predict(dX_winner_test4)\n",
    "\n",
    "rmse_winner_4 = np.sqrt(mean_squared_error(pred_winner_4, y_test4))\n",
    "rmse_winner_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.57353274  5.41760627  8.          0.53710825 19.          0.26034376]. \t  0.8874977192853896 \t 0.9875226863588296\n",
      "init   \t [5.17826437 9.78817462 5.         0.84048437 6.         0.79712615]. \t  0.9790156454078135 \t 0.9875226863588296\n",
      "init   \t [ 9.84476201  9.65484856  5.          0.59464714 19.          0.34528747]. \t  0.8875793339107304 \t 0.9875226863588296\n",
      "init   \t [ 9.17476842  4.53262239 13.          0.64368565 10.          0.515308  ]. \t  0.8874977192853896 \t 0.9875226863588296\n",
      "init   \t [ 8.66719465  7.99685372 12.          0.79833571 17.          0.72443466]. \t  0.9875226863588296 \t 0.9875226863588296\n",
      "1      \t [ 7.89730815  7.50595604 11.99994802  0.70483931 16.35152917  0.64297451]. \t  0.8875793339107304 \t 0.9875226863588296\n",
      "2      \t [ 8.30490576  7.15954105 11.49624212  0.78610346 17.26339316  0.62895121]. \t  0.8875025212520042 \t 0.9875226863588296\n",
      "3      \t [ 8.37813546  7.97059687 11.11320935  0.62575193 16.58570019  0.62840385]. \t  0.8875793339107304 \t 0.9875226863588296\n",
      "4      \t [ 7.74856348  8.08055149 11.7296037   0.67293281 17.24491333  0.61345525]. \t  0.8875793339107304 \t 0.9875226863588296\n",
      "5      \t [ 8.30131696  7.77172326 11.73965616  1.         16.83990556  0.1       ]. \t  0.8945836932758181 \t 0.9875226863588296\n",
      "6      \t [ 8.1786203   7.75469753 11.63583407  1.         16.83522308  1.        ]. \t  \u001b[92m0.994762311941348\u001b[0m \t 0.994762311941348\n",
      "7      \t [5.66386443 9.72087038 5.7650444  0.86311506 6.38252306 0.56604913]. \t  0.8875793339107304 \t 0.994762311941348\n",
      "8      \t [5.46705992 9.09322404 5.46824095 0.84639349 5.60938178 0.71520075]. \t  0.9790204461989841 \t 0.994762311941348\n",
      "9      \t [5.07520642 9.04176773 5.28291264 0.93407463 6.41624172 0.63742871]. \t  0.8874977192853896 \t 0.994762311941348\n",
      "10     \t [4.85768192 9.59946817 5.77940986 0.65310305 5.84381317 0.68652118]. \t  0.9789820392473269 \t 0.994762311941348\n",
      "11     \t [5.28759096 9.49348556 5.30362249 0.51754116 5.89667682 0.11988506]. \t  0.8874977192853896 \t 0.994762311941348\n",
      "12     \t [5.36769545 9.42157641 5.44354242 0.5        6.06092283 1.        ]. \t  0.9839844857037349 \t 0.994762311941348\n",
      "13     \t [ 8.30013638  7.69621739 11.76093549  0.5        16.8877871   0.69038806]. \t  0.9864041013938357 \t 0.994762311941348\n",
      "14     \t [5.24855854 9.5311274  5.4970063  1.         5.91414789 0.69993727]. \t  0.9788764192141146 \t 0.994762311941348\n",
      "15     \t [4.66024099 9.10788083 5.         0.5        5.48038974 0.83224502]. \t  0.9797597669329227 \t 0.994762311941348\n",
      "16     \t [ 8.66826285  8.11182411 11.23568849  0.99477922 17.60948188  0.67206944]. \t  0.9876955134580877 \t 0.994762311941348\n",
      "17     \t [5.11237858 9.7497457  5.22331625 0.5        4.96702844 0.88069052]. \t  0.9798029740534563 \t 0.994762311941348\n",
      "18     \t [ 8.41810198  7.91792756 11.53578997  0.93031571 17.16631956  0.64938783]. \t  0.8874977192853896 \t 0.994762311941348\n",
      "19     \t [ 9.12521444  8.34204616 10.89062992  0.84921116 18.23197314  0.84781391]. \t  0.9878395383686436 \t 0.994762311941348\n",
      "20     \t [ 8.41241452  8.03882805 10.45909782  0.98931268 18.10432998  0.6556695 ]. \t  0.8874977192853896 \t 0.994762311941348\n",
      "21     \t [ 8.89488667  8.61581226 11.04114032  1.         18.0707735   0.1       ]. \t  0.8948333348315408 \t 0.994762311941348\n",
      "22     \t [ 9.3420429   7.82290959 10.74904231  1.         17.73649516  0.41564117]. \t  0.894746920590474 \t 0.994762311941348\n",
      "23     \t [ 8.30567994  8.44602721 12.19622193  0.65722137 16.33210765  1.        ]. \t  0.9935285048076512 \t 0.994762311941348\n",
      "24     \t [ 8.91569096  7.73173464 12.04957854  0.8518495  16.16215843  1.        ]. \t  0.9942630253727135 \t 0.994762311941348\n",
      "25     \t [ 8.81545855  7.80156458 11.30654859  1.         18.42249398  0.53201358]. \t  0.8951549878399566 \t 0.994762311941348\n",
      "26     \t [5.06933612 9.45783597 5.25283988 0.51724683 5.56872055 0.75862088]. \t  0.9789196273026609 \t 0.994762311941348\n",
      "27     \t [4.6429807  9.16954243 5.23500583 0.92682572 4.58305943 1.        ]. \t  0.9840901028329082 \t 0.994762311941348\n",
      "28     \t [ 8.80500924  8.10273806 12.52441601  0.70975255 16.31954601  0.13395911]. \t  0.8875793339107304 \t 0.994762311941348\n",
      "29     \t [ 8.460521    7.80647676 12.67544667  0.88867572 16.65612983  1.        ]. \t  0.9944598579489868 \t 0.994762311941348\n",
      "30     \t [ 8.79521764  8.15665531 10.97005109  0.5        17.91329334  0.60215882]. \t  0.8874977192853896 \t 0.994762311941348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0653432316547455"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \n",
    "\n",
    "np.random.seed(run_num_5)\n",
    "surrogate_winner_5 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train5, X_test5, y_train5, y_test5 = train_test_split(X, y, test_size=test_perc, random_state=run_num_5)\n",
    "\n",
    "def f_syn_polarity5(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_5, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train5, y=y_train5).mean())\n",
    "    return  score\n",
    "\n",
    "winner_5 = GPGO(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity5, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_5.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_5 = winner_5.getResult()[0]\n",
    "params_winner_5['max_depth'] = int(params_winner_5['max_depth'])\n",
    "params_winner_5['min_child_weight'] = int(params_winner_5['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train5 = xgb.DMatrix(X_train5, y_train5)\n",
    "dX_winner_test5 = xgb.DMatrix(X_test5, y_test5)\n",
    "model_winner_5 = xgb.train(params_winner_5, dX_winner_train5)\n",
    "pred_winner_5 = model_winner_5.predict(dX_winner_test5)\n",
    "\n",
    "rmse_winner_5 = np.sqrt(mean_squared_error(pred_winner_5, y_test5))\n",
    "rmse_winner_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [8.92860151 3.31979805 5.         0.99251441 2.         0.57683563]. \t  0.8573338214862317 \t 0.989121369524546\n",
      "init   \t [4.18807429 3.35407849 9.         0.87750649 3.         0.56623277]. \t  0.8635317250300133 \t 0.989121369524546\n",
      "init   \t [ 5.788586    6.45355096 14.          0.70660047 12.          0.82154882]. \t  0.989121369524546 \t 0.989121369524546\n",
      "init   \t [4.58184578 6.73834679 5.         0.90108528 3.         0.65482895]. \t  0.8589277181043972 \t 0.989121369524546\n",
      "init   \t [ 4.42510505  5.75952352 14.          0.97882365 15.          0.29525604]. \t  0.8624707456178685 \t 0.989121369524546\n",
      "1      \t [ 5.25534162  6.18212313 14.          0.81306121 13.17325644  0.61572641]. \t  0.8598398931802448 \t 0.989121369524546\n",
      "2      \t [ 6.19892181  6.91778712 13.99999985  0.76853886 12.86965461  0.8027176 ]. \t  \u001b[92m0.9894574269116426\u001b[0m \t 0.9894574269116426\n",
      "3      \t [ 6.28224554  6.05807666 13.99999898  0.93044049 12.61817388  0.37088279]. \t  0.8639685928778914 \t 0.9894574269116426\n",
      "4      \t [ 5.61736793  6.90653853 13.99999918  0.99813987 12.46961883  0.16269012]. \t  0.8634213022004609 \t 0.9894574269116426\n",
      "5      \t [ 5.81676932  6.5310984  14.73173432  0.82905555 12.60195373  0.80813221]. \t  0.9893085982367337 \t 0.9894574269116426\n",
      "6      \t [ 5.76892211  6.50789215 14.03248672  1.         12.61578452  1.        ]. \t  \u001b[92m0.9951031705344778\u001b[0m \t 0.9951031705344778\n",
      "7      \t [ 5.82703824  6.533045   14.16687756  0.5        12.62380887  0.61226735]. \t  0.8562729139836217 \t 0.9951031705344778\n",
      "8      \t [ 6.43345906  6.88345343 14.36222891  1.         12.04480407  0.76609149]. \t  0.991104090815556 \t 0.9951031705344778\n",
      "9      \t [ 5.31206142  6.31357206 14.313443    1.         14.33205014  0.41800287]. \t  0.7963206569418789 \t 0.9951031705344778\n",
      "10     \t [ 4.46974873  5.57038455 14.39195262  1.         13.96464156  0.3191202 ]. \t  0.7963206569418789 \t 0.9951031705344778\n",
      "11     \t [ 4.79803422  5.89722    13.37350885  1.         14.18562507  0.1994755 ]. \t  0.7963206569418789 \t 0.9951031705344778\n",
      "12     \t [ 4.3623847   6.28793571 13.93320613  0.98943126 14.26556962  0.97675518]. \t  0.9890397508888656 \t 0.9951031705344778\n",
      "13     \t [ 4.84763598  5.63979929 13.90190232  0.72796051 14.40739975  1.        ]. \t  0.9948151214048039 \t 0.9951031705344778\n",
      "14     \t [ 4.53192362  6.08304877 13.98074854  0.5        14.34950922  0.58770935]. \t  0.8562633157893264 \t 0.9951031705344778\n",
      "15     \t [ 6.05097119  6.6544562  14.28146684  1.         12.38260699  0.67187198]. \t  0.9911376961463173 \t 0.9951031705344778\n",
      "16     \t [ 6.48549706  6.92341749 13.32298608  0.84125261 11.98973908  0.79816054]. \t  0.9891933778657687 \t 0.9951031705344778\n",
      "17     \t [ 6.81007562  6.39625581 13.91637842  0.61800288 12.02314153  1.        ]. \t  0.9940757965222381 \t 0.9951031705344778\n",
      "18     \t [ 6.29496937  7.20761079 13.96305616  0.5189135  11.89304232  1.        ]. \t  0.9939749803916665 \t 0.9951031705344778\n",
      "19     \t [ 5.07866999  6.79617808 14.84405851  1.         13.33977268  1.        ]. \t  \u001b[92m0.9951127720476748\u001b[0m \t 0.9951127720476748\n",
      "20     \t [ 5.44407747  6.01959893 14.88505899  1.         13.48332858  1.        ]. \t  0.9950311585977781 \t 0.9951127720476748\n",
      "21     \t [ 4.87449572  6.04735368 14.23612192  1.         14.01438007  1.        ]. \t  0.9950551627610614 \t 0.9951127720476748\n",
      "22     \t [ 5.95773216  6.77082499 14.98785638  1.         13.38754194  1.        ]. \t  0.9950311585977781 \t 0.9951127720476748\n",
      "23     \t [ 6.3828103   6.65053858 13.83839454  0.88137025 11.38064069  1.        ]. \t  0.9950023546113376 \t 0.9951127720476748\n",
      "24     \t [ 6.35804502  6.74155617 13.85865651  0.82608558 12.02471641  1.        ]. \t  0.9950743669628999 \t 0.9951127720476748\n",
      "25     \t [ 6.65424493  6.79360607 13.83402583  0.5485771  11.55626682  0.40613056]. \t  0.8559656592692337 \t 0.9951127720476748\n",
      "26     \t [ 5.62495062  7.59056378 14.41552903  0.9901848  12.6572189   0.96548015]. \t  0.9890061438295096 \t 0.9951127720476748\n",
      "27     \t [ 5.51987728  7.34510685 13.73185136  0.93765479 11.55878995  0.72563552]. \t  0.9889389339285687 \t 0.9951127720476748\n",
      "28     \t [ 5.59267419  6.7318757  14.50003456  1.         13.20986049  1.        ]. \t  0.9950311585977781 \t 0.9951127720476748\n",
      "29     \t [ 5.98423991  7.74934584 13.52173063  0.8707052  12.3595578   0.59710961]. \t  0.8596478716284217 \t 0.9951127720476748\n",
      "30     \t [ 5.02481638  6.95152372 14.37806359  1.         12.08684082  1.        ]. \t  \u001b[92m0.9951367760726705\u001b[0m \t 0.9951367760726705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.061174892521172426"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \n",
    "\n",
    "np.random.seed(run_num_6)\n",
    "surrogate_winner_6 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train6, X_test6, y_train6, y_test6 = train_test_split(X, y, test_size=test_perc, random_state=run_num_6)\n",
    "\n",
    "def f_syn_polarity6(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=int(min_child_weight),\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_6, objective = obj_classifier, booster='gbtree', silent=None, eval_metric = 'rmse')\n",
    "    score = np.array(cross_val_score(reg, X=X_train6, y=y_train6).mean())\n",
    "    return  score\n",
    "\n",
    "winner_6 = GPGO(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity6, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_6.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_6 = winner_6.getResult()[0]\n",
    "params_winner_6['max_depth'] = int(params_winner_6['max_depth'])\n",
    "params_winner_6['min_child_weight'] = int(params_winner_6['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train6 = xgb.DMatrix(X_train6, y_train6)\n",
    "dX_winner_test6 = xgb.DMatrix(X_test6, y_test6)\n",
    "model_winner_6 = xgb.train(params_winner_6, dX_winner_train6)\n",
    "pred_winner_6 = model_winner_6.predict(dX_winner_test6)\n",
    "\n",
    "rmse_winner_6 = np.sqrt(mean_squared_error(pred_winner_6, y_test6))\n",
    "rmse_winner_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.11505126  6.92324143 13.          0.94046175 12.          0.3650384 ]. \t  0.8297634105475625 \t 0.9886700606517818\n",
      "init   \t [ 5.15357029  9.0066636   6.          0.74104227 15.          0.8966337 ]. \t  0.9814544396474091 \t 0.9886700606517818\n",
      "init   \t [ 2.63548913  9.6610934  14.          0.64712408  1.          0.80820948]. \t  0.9886700606517818 \t 0.9886700606517818\n",
      "init   \t [ 1.40821426  1.72758589 12.          0.70495069  5.          0.86632715]. \t  0.9884588284677487 \t 0.9886700606517818\n",
      "init   \t [2.85126987 2.73434054 7.         0.88394733 8.         0.65545939]. \t  0.8285920024977876 \t 0.9886700606517818\n",
      "1      \t [ 3.2268738   9.18895008 13.21108839  0.69063345  1.26293521  0.8262318 ]. \t  \u001b[92m0.9893325709395954\u001b[0m \t 0.9893325709395954\n",
      "2      \t [ 2.59391944  8.62753105 13.80604876  0.71303656  1.06458348  0.67507108]. \t  0.9889917149047857 \t 0.9893325709395954\n",
      "3      \t [ 3.41155166  9.07000048 14.1930547   0.78208964  1.16409925  0.75921604]. \t  0.9890781276938331 \t 0.9893325709395954\n",
      "4      \t [ 2.78602065  9.19155251 13.87131358  0.73017871  1.83120362  0.77761907]. \t  0.9892173510526371 \t 0.9893325709395954\n",
      "5      \t [ 2.93042026  9.26997172 13.7235076   0.99904183  1.20126003  0.16645829]. \t  0.8289616632104759 \t 0.9893325709395954\n",
      "6      \t [ 1.54639345  2.1672354  11.58903096  0.70185415  5.81420455  0.91781178]. \t  0.9887804805081514 \t 0.9893325709395954\n",
      "7      \t [ 2.87624015  9.1353188  13.7787815   1.          1.17535988  1.        ]. \t  \u001b[92m0.9967066425294901\u001b[0m \t 0.9967066425294901\n",
      "8      \t [ 2.95983828  9.12882182 13.81528579  0.5         1.2321007   0.77445493]. \t  0.9881755766720559 \t 0.9967066425294901\n",
      "9      \t [ 3.43742974  8.26004329 13.48412133  0.99052141  1.71083865  0.67610196]. \t  0.989265361799236 \t 0.9967066425294901\n",
      "10     \t [ 1.60105252  2.13227218 11.17866058  0.70624677  4.97506748  0.90432119]. \t  0.9886460587010996 \t 0.9967066425294901\n",
      "11     \t [ 1.82178223  1.48050814 11.41756258  0.790085    5.42714357  0.64023145]. \t  0.8289232590245702 \t 0.9967066425294901\n",
      "12     \t [ 0.89351029  1.86152452 11.40539911  0.77134723  5.35117963  0.98106837]. \t  0.9888908993965081 \t 0.9967066425294901\n",
      "13     \t [ 1.24283576  2.20836048 11.63289974  0.51155325  5.2565502   0.37844816]. \t  0.8272477665190291 \t 0.9967066425294901\n",
      "14     \t [ 1.42705701  2.0939911  11.65443615  1.          5.25708381  1.        ]. \t  \u001b[92m0.9968602675012234\u001b[0m \t 0.9968602675012234\n",
      "15     \t [ 1.42654128  1.88247764 11.54641482  0.5         5.3061634   1.        ]. \t  0.9954632306328383 \t 0.9968602675012234\n",
      "16     \t [ 1.33639642  2.30973147 10.56566321  0.93870787  5.86260115  0.89592097]. \t  0.989001319183734 \t 0.9968602675012234\n",
      "17     \t [ 3.86729398  9.14096511 13.53275082  1.          1.91458167  0.82941082]. \t  0.9923475022973451 \t 0.9968602675012234\n",
      "18     \t [ 2.29038037  9.18036139 14.77254726  0.8453823   1.13800993  0.66129546]. \t  0.828534388855116 \t 0.9968602675012234\n",
      "19     \t [ 3.20364085  8.84833917 12.90944768  0.98676144  2.07880149  0.78559035]. \t  0.989265361799236 \t 0.9968602675012234\n",
      "20     \t [ 3.29814452  8.85992261 13.49198519  0.93862077  1.6701264   0.74002708]. \t  0.9893421748728248 \t 0.9968602675012234\n",
      "21     \t [ 4.03400868  8.5634879  12.7115194   0.77147043  1.80158572  0.966818  ]. \t  0.9889053006637187 \t 0.9968602675012234\n",
      "22     \t [ 3.75637881  8.43595478 13.24637755  0.57546646  2.48503543  1.        ]. \t  0.9954824307551934 \t 0.9968602675012234\n",
      "23     \t [ 3.13178228  8.01246571 12.80466272  0.51861737  1.81477522  1.        ]. \t  0.995146374197822 \t 0.9968602675012234\n",
      "24     \t [ 3.6753121   9.35327265 12.88663279  0.5         2.12378243  1.        ]. \t  0.9948295197679756 \t 0.9968602675012234\n",
      "25     \t [ 3.60099018  8.39078503 12.85611566  0.5         2.06338488  0.46054841]. \t  0.8263500010767899 \t 0.9968602675012234\n",
      "26     \t [ 3.55181283  8.52058782 13.0611868   0.77273765  1.95599031  1.        ]. \t  0.9962793693495745 \t 0.9968602675012234\n",
      "27     \t [ 2.55827659  7.74454097 13.45509124  0.83500815  1.98130438  0.7506447 ]. \t  0.9897118363460948 \t 0.9968602675012234\n",
      "28     \t [ 2.74090402  7.77450946 12.94067204  1.          1.25941849  0.68387846]. \t  0.9926403511810326 \t 0.9968602675012234\n",
      "29     \t [ 4.35161981  9.01969003 12.86596336  0.93705826  2.64879035  0.97416708]. \t  0.9889197026223672 \t 0.9968602675012234\n",
      "30     \t [ 2.23875946  8.48119526 12.86436362  0.73054545  1.6343963   0.83026462]. \t  0.98931816994896 \t 0.9968602675012234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05616713203326943"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \n",
    "\n",
    "np.random.seed(run_num_7)\n",
    "surrogate_winner_7 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train7, X_test7, y_train7, y_test7 = train_test_split(X, y, test_size=test_perc, random_state=run_num_7)\n",
    "\n",
    "def f_syn_polarity7(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_7, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train7, y=y_train7).mean())\n",
    "    return  score\n",
    "\n",
    "winner_7 = GPGO(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity7, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_7.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_7 = winner_7.getResult()[0]\n",
    "params_winner_7['max_depth'] = int(params_winner_7['max_depth'])\n",
    "params_winner_7['min_child_weight'] = int(params_winner_7['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train7 = xgb.DMatrix(X_train7, y_train7)\n",
    "dX_winner_test7 = xgb.DMatrix(X_test7, y_test7)\n",
    "model_winner_7 = xgb.train(params_winner_7, dX_winner_train7)\n",
    "pred_winner_7 = model_winner_7.predict(dX_winner_test7)\n",
    "\n",
    "rmse_winner_7 = np.sqrt(mean_squared_error(pred_winner_7, y_test7))\n",
    "rmse_winner_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.09499875  0.42215015  7.          0.62339266 17.          0.52353668]. \t  0.8894852354753713 \t 0.8894852354753713\n",
      "init   \t [ 9.1272854   1.60640294  7.          0.68662086 17.          0.1484676 ]. \t  0.8891731849480878 \t 0.8894852354753713\n",
      "init   \t [ 6.02074372  4.29318577 11.          0.77432386  5.          0.51209718]. \t  0.8891155762837446 \t 0.8894852354753713\n",
      "init   \t [2.81633282 5.85246014 5.         0.54171534 6.         0.39755341]. \t  0.8890483697707694 \t 0.8894852354753713\n",
      "init   \t [ 6.23734577  3.09242107 14.          0.6186205  15.          0.51761313]. \t  0.8892067935977129 \t 0.8894852354753713\n",
      "1      \t [ 0.97539034  0.53243412  7.          0.65184655 15.78653686  0.52922148]. \t  \u001b[92m0.8896004528040574\u001b[0m \t 0.8896004528040574\n",
      "2      \t [ 1.65243082  1.23002417  6.76901767  0.70499089 16.40232461  0.52950671]. \t  0.8894420289771139 \t 0.8896004528040574\n",
      "3      \t [ 1.00841357  0.63182868  6.05866303  0.60828245 16.40886489  0.46379431]. \t  0.8894420289771139 \t 0.8896004528040574\n",
      "4      \t [ 1.80751887  0.16108908  6.64141049  0.55937238 16.28634041  0.50010199]. \t  0.889571648471886 \t 0.8896004528040574\n",
      "5      \t [ 1.28230211  0.57250685  6.74809399  1.         16.38615638  0.1       ]. \t  \u001b[92m0.8963888304300448\u001b[0m \t 0.8963888304300448\n",
      "6      \t [ 1.27584526  0.55756354  6.66717539  1.         16.38896246  1.        ]. \t  \u001b[92m0.9888621198161559\u001b[0m \t 0.9888621198161559\n",
      "7      \t [ 1.24520358  0.66086062  6.74672972  0.5        16.38448571  0.70301296]. \t  0.9788523842793423 \t 0.9888621198161559\n",
      "8      \t [ 1.64887335  0.82347987  6.17271075  1.         15.44791693  0.68328643]. \t  \u001b[92m0.9913921451297508\u001b[0m \t 0.9913921451297508\n",
      "9      \t [ 2.07234136  0.84748725  5.77354033  1.         16.32017643  0.78095373]. \t  0.9833027689300177 \t 0.9913921451297508\n",
      "10     \t [ 2.40450923  0.84939487  6.54033293  1.         15.82554933  1.        ]. \t  0.9887613017495585 \t 0.9913921451297508\n",
      "11     \t [ 1.79052472  0.73999542  6.36258974  0.98860399 16.04885608  0.72396134]. \t  0.9797309259522794 \t 0.9913921451297508\n",
      "12     \t [ 2.39446099  1.21834678  5.79514378  0.57082232 15.55155445  0.99179432]. \t  0.9722896316001365 \t 0.9913921451297508\n",
      "13     \t [ 2.50578713  0.40862293  5.67097844  1.         15.41590038  1.        ]. \t  0.9840949104669988 \t 0.9913921451297508\n",
      "14     \t [ 2.69995348  0.97279377  5.90313291  1.         15.52814261  0.35762652]. \t  0.8991973342669756 \t 0.9913921451297508\n",
      "15     \t [ 2.93886106  0.60771243  5.92029628  0.53782789 16.20309698  0.99722934]. \t  0.9721696136877073 \t 0.9913921451297508\n",
      "16     \t [ 2.3877648   0.54571058  6.32939468  0.5        15.18692337  0.80879851]. \t  0.9787227610508596 \t 0.9913921451297508\n",
      "17     \t [ 2.19164132  0.56989135  5.6469649   0.5        15.78543868  0.66905738]. \t  0.9722464223361675 \t 0.9913921451297508\n",
      "18     \t [ 2.4722003   0.75953098  5.99700165  0.79683802 15.72218546  0.98276977]. \t  0.9723040339045076 \t 0.9913921451297508\n",
      "19     \t [ 2.64215399  0.78847557  6.5409679   0.86928481 17.06003125  0.94440644]. \t  0.9794044680040583 \t 0.9913921451297508\n",
      "20     \t [ 2.10698821  0.7635287   5.49970097  0.76784627 14.66432696  0.67334157]. \t  0.9722752290191939 \t 0.9913921451297508\n",
      "21     \t [ 2.70679231  1.1213075   5.66363558  0.5        16.90063104  0.6026504 ]. \t  0.887540943053791 \t 0.9913921451297508\n",
      "22     \t [ 2.17277749  1.24485008  6.6446268   0.9526982  14.74723845  0.84267386]. \t  0.9795917015563109 \t 0.9913921451297508\n",
      "23     \t [ 2.64658502  0.14366261  5.66668849  0.89564511 16.9426053   0.74094824]. \t  0.972342439819022 \t 0.9913921451297508\n",
      "24     \t [ 1.84674572  0.53472986  6.04935978  0.67512428 17.18117122  0.87702906]. \t  0.9802254202343473 \t 0.9913921451297508\n",
      "25     \t [ 1.9927397   0.75080704  7.43080233  0.95810291 15.46218313  0.87612835]. \t  0.9836003986226629 \t 0.9913921451297508\n",
      "26     \t [ 2.80670765  0.64701497  5.          0.82676646 16.0799139   0.79009206]. \t  0.9722272206926235 \t 0.9913921451297508\n",
      "27     \t [ 2.03334865  0.57962542  7.34386707  1.         16.54178265  0.96306616]. \t  \u001b[92m0.9923234993084266\u001b[0m \t 0.9923234993084266\n",
      "28     \t [ 1.9905425   0.42915435  6.52533685  1.         14.74125004  0.71128712]. \t  0.991099295139727 \t 0.9923234993084266\n",
      "29     \t [ 1.63221651  0.9394482   6.51116327  0.5        14.87364688  1.        ]. \t  0.9886940895668745 \t 0.9923234993084266\n",
      "30     \t [ 1.92392558  1.01049881  6.42758236  0.59038838 14.96899794  0.42001921]. \t  0.888664312008482 \t 0.9923234993084266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07028391118388262"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \n",
    "\n",
    "np.random.seed(run_num_8)\n",
    "surrogate_winner_8 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train8, X_test8, y_train8, y_test8 = train_test_split(X, y, test_size=test_perc, random_state=run_num_8)\n",
    "\n",
    "def f_syn_polarity8(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_8, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train8, y=y_train8).mean())\n",
    "    return  score\n",
    "\n",
    "winner_8 = GPGO(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity8, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_8.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_8 = winner_8.getResult()[0]\n",
    "params_winner_8['max_depth'] = int(params_winner_8['max_depth'])\n",
    "params_winner_8['min_child_weight'] = int(params_winner_8['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train8 = xgb.DMatrix(X_train8, y_train8)\n",
    "dX_winner_test8 = xgb.DMatrix(X_test8, y_test8)\n",
    "model_winner_8 = xgb.train(params_winner_8, dX_winner_train8)\n",
    "pred_winner_8 = model_winner_8.predict(dX_winner_test8)\n",
    "\n",
    "rmse_winner_8 = np.sqrt(mean_squared_error(pred_winner_8, y_test8))\n",
    "rmse_winner_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.72278559  4.88078399 14.          0.8670313   5.          0.82724497]. \t  0.968425042143454 \t 0.968425042143454\n",
      "init   \t [ 5.6561742   2.97622499 12.          0.75688314 17.          0.10614316]. \t  0.7923984116619751 \t 0.968425042143454\n",
      "init   \t [7.69793028 7.46767101 7.         0.74680784 9.         0.93605355]. \t  0.9563894026036909 \t 0.968425042143454\n",
      "init   \t [ 3.95454044  9.73956297 10.          0.66345176 13.          0.59891121]. \t  0.7929360981296009 \t 0.968425042143454\n",
      "init   \t [ 2.92269116  8.1614236  14.          0.61078869 19.          0.13894609]. \t  0.7908333515969722 \t 0.968425042143454\n",
      "1      \t [ 6.91154884  5.47393265 13.3349357   0.90002352  5.6649031   0.78439419]. \t  \u001b[92m0.9685786689129259\u001b[0m \t 0.9685786689129259\n",
      "2      \t [ 7.12205577  5.85460003 14.00827689  0.9556396   4.9916681   0.9979753 ]. \t  \u001b[92m0.9689243253240433\u001b[0m \t 0.9689243253240433\n",
      "3      \t [ 7.42669062  5.24665437 13.44938641  0.69283307  4.87338501  0.56103769]. \t  0.7946163747626604 \t 0.9689243253240433\n",
      "4      \t [ 6.36252418  5.68295618 13.66252157  0.73630197  4.9955781   0.53418329]. \t  0.7949908411757277 \t 0.9689243253240433\n",
      "5      \t [ 6.79416022  5.30311139 13.37735095  1.          4.95462316  1.        ]. \t  \u001b[92m0.9961353441624435\u001b[0m \t 0.9961353441624435\n",
      "6      \t [ 7.01122072  5.29589179 13.94127027  1.          5.4530399   1.        ]. \t  0.9961209416506448 \t 0.9961353441624435\n",
      "7      \t [7.31468318 7.68977607 7.86787268 0.80815657 8.71080605 0.77996251]. \t  0.9568214682775227 \t 0.9961353441624435\n",
      "8      \t [7.03391404 7.88966593 7.36895933 0.67646277 9.42561372 1.        ]. \t  0.9920210459394389 \t 0.9961353441624435\n",
      "9      \t [7.78442931 7.66585806 7.75113574 0.74997086 9.49519418 0.86359976]. \t  0.9566774509036393 \t 0.9961353441624435\n",
      "10     \t [7.1608591  7.08794906 7.55057515 0.79670328 9.27348819 0.88642928]. \t  0.9569654928423597 \t 0.9961353441624435\n",
      "11     \t [7.33010765 7.68981304 7.34501552 0.97998915 9.23256801 0.33219052]. \t  0.7893259008185828 \t 0.9961353441624435\n",
      "12     \t [7.40410609 7.58604577 7.52492603 0.5        9.08998521 1.        ]. \t  0.9911809100429415 \t 0.9961353441624435\n",
      "13     \t [7.39277067 7.63739095 7.51541138 1.         9.15708754 1.        ]. \t  0.9918290136011857 \t 0.9961353441624435\n",
      "14     \t [ 6.88508599  5.40316118 13.71037731  0.5         5.20559347  1.        ]. \t  0.9942918309494612 \t 0.9961353441624435\n",
      "15     \t [ 7.51237925  7.37328646  6.8443499   0.60527118 10.06180788  1.        ]. \t  0.9884540441327987 \t 0.9961353441624435\n",
      "16     \t [ 7.02885962  7.50897599  7.65542033  0.58723797 10.41957929  1.        ]. \t  0.9919106297476897 \t 0.9961353441624435\n",
      "17     \t [7.28964183 7.49738145 7.37403605 0.65173113 9.86958257 1.        ]. \t  0.9918242124642963 \t 0.9961353441624435\n",
      "18     \t [6.7531115  7.79165193 8.43736379 0.70154748 9.68517376 0.86703424]. \t  0.9605565013014594 \t 0.9961353441624435\n",
      "19     \t [ 6.93899818  8.42336097  7.97499947  0.5        10.43042283  0.84450369]. \t  0.9553476119051769 \t 0.9961353441624435\n",
      "20     \t [ 6.20043022  7.94404481  7.76038001  0.5        10.29985248  0.82132865]. \t  0.9563221837832017 \t 0.9961353441624435\n",
      "21     \t [ 6.90681534  8.04056015  6.889534    0.5        10.70469096  0.86120317]. \t  0.9537441217944934 \t 0.9961353441624435\n",
      "22     \t [ 6.67886229  8.05687451  7.67908225  1.         10.44824954  1.        ]. \t  0.9918866236483805 \t 0.9961353441624435\n",
      "23     \t [ 6.82898904  7.86962934  7.71331006  0.66183491 10.4615746   0.42895166]. \t  0.7902764592680573 \t 0.9961353441624435\n",
      "24     \t [ 6.72724719  8.04176743  7.7151273   0.50458152 10.21095574  1.        ]. \t  0.9911136986208372 \t 0.9961353441624435\n",
      "25     \t [ 6.51826786  7.21706574  6.76381285  0.76092832 10.51091544  1.        ]. \t  0.9885500594722 \t 0.9961353441624435\n",
      "26     \t [ 6.3728772   7.70560038  7.42640325  0.60433335 11.21873793  1.        ]. \t  0.9917089964493898 \t 0.9961353441624435\n",
      "27     \t [ 7.10890724  7.39579762  6.92677838  0.88425831 11.07979881  1.        ]. \t  0.9885212547943215 \t 0.9961353441624435\n",
      "28     \t [ 6.43456534  8.05151174  8.54953318  0.64174429 10.84423795  1.        ]. \t  0.9933604703405968 \t 0.9961353441624435\n",
      "29     \t [ 6.75721451  7.5691949   7.13002748  0.6962093  10.7401851   1.        ]. \t  0.9919634362379629 \t 0.9961353441624435\n",
      "30     \t [ 7.03648228  7.51994651  6.01605914  0.89898167 10.45654022  1.        ]. \t  0.9883532268267827 \t 0.9961353441624435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.061034664767576144"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \n",
    "\n",
    "np.random.seed(run_num_9)\n",
    "surrogate_winner_9 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train9, X_test9, y_train9, y_test9 = train_test_split(X, y, test_size=test_perc, random_state=run_num_9)\n",
    "\n",
    "def f_syn_polarity9(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_9, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train9, y=y_train9).mean())\n",
    "    return  score\n",
    "\n",
    "winner_9 = GPGO(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity9, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_9.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_9 = winner_9.getResult()[0]\n",
    "params_winner_9['max_depth'] = int(params_winner_9['max_depth'])\n",
    "params_winner_9['min_child_weight'] = int(params_winner_9['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train9 = xgb.DMatrix(X_train9, y_train9)\n",
    "dX_winner_test9 = xgb.DMatrix(X_test9, y_test9)\n",
    "model_winner_9 = xgb.train(params_winner_9, dX_winner_train9)\n",
    "pred_winner_9 = model_winner_9.predict(dX_winner_test9)\n",
    "\n",
    "rmse_winner_9 = np.sqrt(mean_squared_error(pred_winner_9, y_test9))\n",
    "rmse_winner_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.53589585  1.15006943  6.          0.93623727 14.          0.13663866]. \t  0.8925625895791941 \t 0.9854871661142187\n",
      "init   \t [ 3.97194461  2.33132197 14.          0.9533253  19.          0.26403087]. \t  0.8916888516708369 \t 0.9854871661142187\n",
      "init   \t [ 7.43539415  0.69582081  9.          0.9763222  11.          0.12608349]. \t  0.8916264438056544 \t 0.9854871661142187\n",
      "init   \t [ 9.82027485  3.39637684  9.          0.78522537 19.          0.87618843]. \t  0.9854871661142187 \t 0.9854871661142187\n",
      "init   \t [ 0.57576207  5.82646405 12.          0.77704362 18.          0.11865044]. \t  0.8923177518569682 \t 0.9854871661142187\n",
      "1      \t [ 9.03957372  3.64859645  9.55298615  0.71238322 18.47170424  0.89932455]. \t  0.9848150471222544 \t 0.9854871661142187\n",
      "2      \t [ 9.20317504  3.07150146  9.72773352  0.8513456  19.30902243  0.7487266 ]. \t  \u001b[92m0.9856263897495975\u001b[0m \t 0.9856263897495975\n",
      "3      \t [ 9.12211328  2.81140889  9.06218511  0.5900701  18.64134658  0.68951528]. \t  0.9845942095529724 \t 0.9856263897495975\n",
      "4      \t [ 8.91273818  3.57047623  8.97513825  0.63867482 19.21536133  0.68848038]. \t  0.9844693885676797 \t 0.9856263897495975\n",
      "5      \t [ 9.35304025  3.39991889  9.33346695  0.9570781  18.79199925  0.17611479]. \t  0.8904646623682698 \t 0.9856263897495975\n",
      "6      \t [ 9.14787599  3.24454999  9.18843522  1.         18.88748745  1.        ]. \t  \u001b[92m0.9938309565182674\u001b[0m \t 0.9938309565182674\n",
      "7      \t [ 9.34707572  3.33210388  9.395488    0.5        18.94240507  0.78544715]. \t  0.9840565208036018 \t 0.9938309565182674\n",
      "8      \t [ 8.19625915  3.12793448  9.71773855  0.71419841 18.78287149  0.49056831]. \t  0.890368638869902 \t 0.9938309565182674\n",
      "9      \t [ 9.58223428  2.71919512  8.72463114  0.85626183 19.67683951  0.46604611]. \t  0.8891252313293087 \t 0.9938309565182674\n",
      "10     \t [ 8.54459143  2.50606382  9.23265436  0.77975127 19.57114492  0.24181735]. \t  0.8913576021275765 \t 0.9938309565182674\n",
      "11     \t [ 9.36849988  3.84160308  8.44523533  0.58210316 18.1265317   0.77973581]. \t  0.984632616366342 \t 0.9938309565182674\n",
      "12     \t [ 8.47350101  3.66109309  8.8598401   0.56987413 18.05784179  0.55143109]. \t  0.8905654695792933 \t 0.9938309565182674\n",
      "13     \t [ 9.42328711  3.27808351  8.14269863  0.72670356 18.79426047  0.46342325]. \t  0.8903686363807259 \t 0.9938309565182674\n",
      "14     \t [ 9.17982123  3.52078772  8.87519639  0.69071611 18.54562754  0.68459409]. \t  0.9849398674852532 \t 0.9938309565182674\n",
      "15     \t [ 9.00872623  4.45757077  9.05643994  0.5        17.45304972  1.        ]. \t  0.9925587433317826 \t 0.9938309565182674\n",
      "16     \t [ 9.17650791  3.61108381  8.97394822  0.5        17.26588332  1.        ]. \t  0.9922418911145371 \t 0.9938309565182674\n",
      "17     \t [ 9.00246331  4.0488315   8.62736989  1.         17.43496828  1.        ]. \t  \u001b[92m0.9938981675946529\u001b[0m \t 0.9938981675946529\n",
      "18     \t [ 9.33191669  4.09331165  8.90560123  0.70864161 17.37023261  0.38751677]. \t  0.8899989720034172 \t 0.9938981675946529\n",
      "19     \t [ 8.56528286  3.85679289  9.60579784  0.86858682 17.43277863  0.94164544]. \t  0.9857944173022736 \t 0.9938981675946529\n",
      "20     \t [ 8.95529585  3.93334352  9.10636408  0.66933678 17.64998029  1.        ]. \t  0.9932308521596105 \t 0.9938981675946529\n",
      "21     \t [ 9.01496726  4.14225507  8.25707373  0.5        17.09153133  0.85187951]. \t  0.9839845077606016 \t 0.9938981675946529\n",
      "22     \t [ 9.85037834  4.22620541  8.3947644   0.60340711 17.24945505  1.        ]. \t  0.9927123667132088 \t 0.9938981675946529\n",
      "23     \t [ 9.114408    4.25536156  9.06957999  0.78388602 16.55369732  1.        ]. \t  0.9938117519015665 \t 0.9938981675946529\n",
      "24     \t [ 9.00941788  3.01098805  9.16862734  0.82274679 19.18500913  0.52800146]. \t  0.8888035835758205 \t 0.9938981675946529\n",
      "25     \t [ 9.11633584  2.68257232  9.97522788  0.9509001  18.13635766  0.84161456]. \t  0.9857992199603262 \t 0.9938981675946529\n",
      "26     \t [ 8.64114995  1.95591412  9.72742776  0.5        19.08741159  0.85939143]. \t  0.9840565208036018 \t 0.9938981675946529\n",
      "27     \t [ 8.4387049   2.55426439  9.49645677  0.5        18.03247379  1.        ]. \t  0.9925395400979572 \t 0.9938981675946529\n",
      "28     \t [ 9.38390834  4.27631171  8.55126109  0.84796785 19.34963992  0.82422836]. \t  0.985419951580644 \t 0.9938981675946529\n",
      "29     \t [ 9.33343159  4.10821977  9.4993589   0.97993748 19.52585965  0.91645161]. \t  0.9857128032992267 \t 0.9938981675946529\n",
      "30     \t [ 8.62403484  2.09567233  9.66492773  0.5        18.51339553  0.14810153]. \t  0.891986521743111 \t 0.9938981675946529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06801560732823773"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \n",
    "\n",
    "np.random.seed(run_num_10)\n",
    "surrogate_winner_10 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train10, X_test10, y_train10, y_test10 = train_test_split(X, y, test_size=test_perc, random_state=run_num_10)\n",
    "\n",
    "def f_syn_polarity10(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_10, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train10, y=y_train10).mean())\n",
    "    return  score\n",
    "\n",
    "winner_10 = GPGO(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity10, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_10.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_10 = winner_10.getResult()[0]\n",
    "params_winner_10['max_depth'] = int(params_winner_10['max_depth'])\n",
    "params_winner_10['min_child_weight'] = int(params_winner_10['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train10 = xgb.DMatrix(X_train10, y_train10)\n",
    "dX_winner_test10 = xgb.DMatrix(X_test10, y_test10)\n",
    "model_winner_10 = xgb.train(params_winner_10, dX_winner_train10)\n",
    "pred_winner_10 = model_winner_10.predict(dX_winner_test10)\n",
    "\n",
    "rmse_winner_10 = np.sqrt(mean_squared_error(pred_winner_10, y_test10))\n",
    "rmse_winner_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 9.81035663  8.2124785   9.          0.68487173 15.          0.28207988]. \t  0.8223029792710461 \t 0.9791740729684557\n",
      "init   \t [ 4.91906771  1.38558332  6.          0.618878   19.          0.10263204]. \t  0.8222694087887517 \t 0.9791740729684557\n",
      "init   \t [ 5.23426175  1.89971159 13.          0.76313779 17.          0.46597853]. \t  0.8261292537401452 \t 0.9791740729684557\n",
      "init   \t [ 4.0204056   3.75167577  9.          0.86204818 17.          0.26560056]. \t  0.8280591870022723 \t 0.9791740729684557\n",
      "init   \t [8.8704459  5.64917815 5.         0.51406171 1.         0.82826731]. \t  0.9791740729684557 \t 0.9791740729684557\n",
      "1      \t [8.83840156 5.9769512  5.72897031 0.61754657 1.72897031 0.58215982]. \t  0.8081454929199224 \t 0.9791740729684557\n",
      "2      \t [9.55646661 5.11780714 5.54017745 0.5        1.24451833 0.60523386]. \t  0.8085919130506237 \t 0.9791740729684557\n",
      "3      \t [9.67905856 6.24388058 5.13416611 0.57622666 1.15834351 0.66325915]. \t  0.8073100761557872 \t 0.9791740729684557\n",
      "4      \t [ 4.27589846  3.07926844  8.14784489  0.79294845 17.56827982  0.21930823]. \t  0.8273246503267084 \t 0.9791740729684557\n",
      "5      \t [9.07633036 5.6101762  5.         1.         1.31354016 0.1       ]. \t  0.7770405763699446 \t 0.9791740729684557\n",
      "6      \t [ 4.49490422  2.81186763  9.26459642  0.78612157 17.43669327  0.27534034]. \t  0.8259900263018581 \t 0.9791740729684557\n",
      "7      \t [9.2526068  5.56966602 5.         0.61526767 1.82621363 1.        ]. \t  \u001b[92m0.9838980629579824\u001b[0m \t 0.9838980629579824\n",
      "8      \t [9.19420362 5.67203296 5.37143719 1.         1.25099297 1.        ]. \t  0.983874058932987 \t 0.9838980629579824\n",
      "9      \t [ 4.52807498  2.97325143  8.57436395  0.72197628 16.57551187  0.31258931]. \t  0.8263980987371248 \t 0.9838980629579824\n",
      "10     \t [ 3.75258237  2.8970149   8.74933781  0.72630776 17.13762294  0.81232176]. \t  \u001b[92m0.985832800191894\u001b[0m \t 0.985832800191894\n",
      "11     \t [ 4.45752992  3.27734117  8.7088525   0.99002501 17.25658861  0.99117324]. \t  0.9855063486049089 \t 0.985832800191894\n",
      "12     \t [ 4.25577946  3.23759323  8.74745348  0.5        17.25255793  0.67603557]. \t  0.9850694742575152 \t 0.985832800191894\n",
      "13     \t [ 3.8127772   3.28072524  8.92006972  1.         17.87208408  0.71137156]. \t  0.9843157409167854 \t 0.985832800191894\n",
      "14     \t [ 4.07663892  3.11195574  8.76693812  1.         17.29810106  0.55417231]. \t  0.7915054358787571 \t 0.985832800191894\n",
      "15     \t [ 3.80498847  2.6855004   8.65158873  0.5        18.07505314  1.        ]. \t  \u001b[92m0.9925443339747492\u001b[0m \t 0.9925443339747492\n",
      "16     \t [ 3.47862353  3.20740422  9.42713471  0.50094849 17.6188249   1.        ]. \t  \u001b[92m0.9931060269565469\u001b[0m \t 0.9931060269565469\n",
      "17     \t [ 3.32046894  3.46630093  8.46968784  0.55856597 17.70218397  1.        ]. \t  \u001b[92m0.9931060273022659\u001b[0m \t 0.9931060273022659\n",
      "18     \t [ 4.32066783  3.67264511  8.89425022  0.72939573 18.22012137  1.        ]. \t  \u001b[92m0.993446883544507\u001b[0m \t 0.993446883544507\n",
      "19     \t [ 4.27987486  3.25353744  9.48428451  0.60650133 16.44976364  1.        ]. \t  \u001b[92m0.9934564860948608\u001b[0m \t 0.9934564860948608\n",
      "20     \t [ 4.45899983  3.79338156  9.6210013   0.85951827 17.349883    1.        ]. \t  \u001b[92m0.9938213499575742\u001b[0m \t 0.9938213499575742\n",
      "21     \t [ 3.74449757  3.86120161  9.13789469  0.66759318 16.90373899  1.        ]. \t  0.9934948927007988 \t 0.9938213499575742\n",
      "22     \t [ 3.91430899  3.31854383  8.98332568  0.67941032 17.69821849  1.        ]. \t  0.9933652702328981 \t 0.9938213499575742\n",
      "23     \t [ 5.01447484  3.84084632  8.95634401  0.77753984 16.80105454  0.98800711]. \t  0.9843925616653344 \t 0.9938213499575742\n",
      "24     \t [ 2.89571795  2.69334187  8.81372794  0.5        17.61934684  0.80394073]. \t  0.9849830609844613 \t 0.9938213499575742\n",
      "25     \t [ 3.57676906  3.29614536  8.61482597  0.67139077 18.62538588  0.44146117]. \t  0.8273198599071057 \t 0.9938213499575742\n",
      "26     \t [ 5.2132459   3.48340469  8.81356543  0.95485231 17.88749637  0.73019129]. \t  0.9847046188303433 \t 0.9938213499575742\n",
      "27     \t [ 3.08972865  3.09247079  9.07686906  0.5        16.7166554   1.        ]. \t  0.992765170990881 \t 0.9938213499575742\n",
      "28     \t [ 4.47123611  3.27169907  8.02752629  0.99785631 18.28618231  1.        ]. \t  0.9934036797428751 \t 0.9938213499575742\n",
      "29     \t [ 5.00367515  2.77068783  8.15498072  0.54732906 17.4106553   1.        ]. \t  0.9931588360742839 \t 0.9938213499575742\n",
      "30     \t [ 3.82284219  2.64496943  7.77332373  0.5        17.47655214  1.        ]. \t  0.990648012266324 \t 0.9938213499575742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.060909432683181663"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \n",
    "\n",
    "np.random.seed(run_num_11)\n",
    "surrogate_winner_11 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train11, X_test11, y_train11, y_test11 = train_test_split(X, y, test_size=test_perc, random_state=run_num_11)\n",
    "\n",
    "def f_syn_polarity11(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train11, y=y_train11).mean())\n",
    "    return  score\n",
    "\n",
    "winner_11 = GPGO(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity11, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_11.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_11 = winner_11.getResult()[0]\n",
    "params_winner_11['max_depth'] = int(params_winner_11['max_depth'])\n",
    "params_winner_11['min_child_weight'] = int(params_winner_11['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train11 = xgb.DMatrix(X_train11, y_train11)\n",
    "dX_winner_test11 = xgb.DMatrix(X_test11, y_test11)\n",
    "model_winner_11 = xgb.train(params_winner_11, dX_winner_train11)\n",
    "pred_winner_11 = model_winner_11.predict(dX_winner_test11)\n",
    "\n",
    "rmse_winner_11 = np.sqrt(mean_squared_error(pred_winner_11, y_test11))\n",
    "rmse_winner_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.9151945   6.22108771  9.          0.89267929 16.          0.87460279]. \t  0.9904271937791528 \t 0.9904271937791528\n",
      "init   \t [ 1.50636966  1.98518759 11.          0.67890863 17.          0.5381501 ]. \t  0.88993656488314 \t 0.9904271937791528\n",
      "init   \t [ 3.31015428  8.02639569  7.          0.75154158 13.          0.11992952]. \t  0.8906470902736388 \t 0.9904271937791528\n",
      "init   \t [ 2.90728553  2.46394443  5.          0.94461307 17.          0.43194161]. \t  0.8861198789973782 \t 0.9904271937791528\n",
      "init   \t [ 9.33140102  6.51378143  8.          0.76907392 18.          0.38515251]. \t  0.8871856699871236 \t 0.9904271937791528\n",
      "1      \t [ 2.28135108  6.69485473  8.47500647  0.85562094 15.21257328  0.6765053 ]. \t  0.9901871520080147 \t 0.9904271937791528\n",
      "2      \t [ 1.72585577  6.50896568  8.01181632  0.85660103 15.90038238  0.98095056]. \t  0.9903743855602672 \t 0.9904271937791528\n",
      "3      \t [ 2.23878875  5.77900952  8.28809381  0.92866961 15.53886318  0.87470847]. \t  \u001b[92m0.9905040083738929\u001b[0m \t 0.9905040083738929\n",
      "4      \t [ 1.47234493  6.17314038  8.55731786  0.76774276 15.30002301  0.66657078]. \t  0.888966791099678 \t 0.9905040083738929\n",
      "5      \t [ 2.17968675  6.31195349  8.37682197  0.50275069 15.92123546  0.36472096]. \t  0.8907143023180373 \t 0.9905040083738929\n",
      "6      \t [ 2.106658    6.35314393  8.53910879  0.5        15.58474885  1.        ]. \t  \u001b[92m0.9932980628893567\u001b[0m \t 0.9932980628893567\n",
      "7      \t [ 2.05253765  6.36918074  8.46359839  1.         15.69766049  0.80525969]. \t  0.9881947942862069 \t 0.9932980628893567\n",
      "8      \t [ 2.17097498  6.46024469  7.48250585  0.61643125 14.93897827  0.68913766]. \t  0.9886124728672501 \t 0.9932980628893567\n",
      "9      \t [ 2.80151224  7.35508589  7.68416816  0.7608643  14.04628584  0.38331791]. \t  0.88937006143007 \t 0.9932980628893567\n",
      "10     \t [ 1.51569315  5.59257492  8.49582674  0.63587248 16.37367206  1.        ]. \t  \u001b[92m0.9936725241858677\u001b[0m \t 0.9936725241858677\n",
      "11     \t [ 2.66680509  6.27909953  8.13618418  0.60203157 14.43731194  0.40384882]. \t  0.8901814068231645 \t 0.9936725241858677\n",
      "12     \t [ 3.03301308  6.83011648  7.73913055  0.71345524 14.99875602  0.81116307]. \t  0.9889533321518434 \t 0.9936725241858677\n",
      "13     \t [ 2.54435323  7.02450406  7.82549054  0.5        14.97747871  0.25801092]. \t  0.8866335710509942 \t 0.9936725241858677\n",
      "14     \t [ 2.44396102  6.81864636  7.9322204   0.76825879 14.62858486  1.        ]. \t  0.9923427046856813 \t 0.9936725241858677\n",
      "15     \t [ 1.14926716  6.39823151  8.82173893  0.5        16.49388433  0.94390011]. \t  0.9894190143577477 \t 0.9936725241858677\n",
      "16     \t [ 2.59695002  6.51994143  7.81494948  1.         14.90374328  0.60100002]. \t  0.896681675441788 \t 0.9936725241858677\n",
      "17     \t [ 3.47681701  7.43299204  6.98922277  0.5        13.78476581  0.73881241]. \t  0.9870089987977332 \t 0.9936725241858677\n",
      "18     \t [ 3.72805529  7.73501742  7.68058661  0.5        13.56597338  0.45582284]. \t  0.8873008906346636 \t 0.9936725241858677\n",
      "19     \t [ 3.25314419  7.1321427   7.28990321  0.5        13.11702573  0.4305118 ]. \t  0.8872960897743494 \t 0.9936725241858677\n",
      "20     \t [ 3.03020103  7.89051285  7.30943015  0.5        13.34365245  0.91708917]. \t  0.9885740659847366 \t 0.9936725241858677\n",
      "21     \t [ 2.48519667  6.47004487  7.95838399  0.5        15.04884094  0.8242395 ]. \t  0.9885116559069371 \t 0.9936725241858677\n",
      "22     \t [ 3.23609587  7.19860494  7.6773747   0.5        14.1494089   1.        ]. \t  0.9902399385160651 \t 0.9936725241858677\n",
      "23     \t [ 1.62393441  6.08498373  8.48801615  0.62080971 16.09158011  0.91856254]. \t  0.9902015530678158 \t 0.9936725241858677\n",
      "24     \t [ 2.21221784  7.2747224   7.36503472  0.71192542 15.39717856  1.        ]. \t  0.9921602740680266 \t 0.9936725241858677\n",
      "25     \t [ 1.36929857  5.75021101  9.33770644  0.53340261 16.89039425  1.        ]. \t  0.9934468873465269 \t 0.9936725241858677\n",
      "26     \t [ 2.06653692  5.16193402  9.0798229   0.625605   16.02851835  0.79035677]. \t  0.9901343411617031 \t 0.9936725241858677\n",
      "27     \t [ 2.42743852  5.73207601  9.13161446  0.6514569  15.17129022  0.55911626]. \t  0.8900037749915403 \t 0.9936725241858677\n",
      "28     \t [ 1.48154205  7.05616896  7.73127868  0.67399718 15.18363205  0.93208531]. \t  0.989553438861401 \t 0.9936725241858677\n",
      "29     \t [ 1.10720738  5.53979388  9.23536408  0.72836319 16.27799841  1.        ]. \t  \u001b[92m0.9940373833468937\u001b[0m \t 0.9940373833468937\n",
      "30     \t [ 3.39788166  7.64055147  7.25783906  1.         13.38614564  0.80329531]. \t  0.9862648485781969 \t 0.9940373833468937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.062092382613259334"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\n",
    "\n",
    "np.random.seed(run_num_12)\n",
    "surrogate_winner_12 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train12, X_test12, y_train12, y_test12 = train_test_split(X, y, test_size=test_perc, random_state=run_num_12)\n",
    "\n",
    "def f_syn_polarity12(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_12, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train12, y=y_train12).mean())\n",
    "    return  score\n",
    "\n",
    "winner_12 = GPGO(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity12, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_12.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_12 = winner_12.getResult()[0]\n",
    "params_winner_12['max_depth'] = int(params_winner_12['max_depth'])\n",
    "params_winner_12['min_child_weight'] = int(params_winner_12['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train12 = xgb.DMatrix(X_train12, y_train12)\n",
    "dX_winner_test12 = xgb.DMatrix(X_test12, y_test12)\n",
    "model_winner_12 = xgb.train(params_winner_12, dX_winner_train12)\n",
    "pred_winner_12 = model_winner_12.predict(dX_winner_test12)\n",
    "\n",
    "rmse_winner_12 = np.sqrt(mean_squared_error(pred_winner_12, y_test12))\n",
    "rmse_winner_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.60644309  4.13600652  6.          0.61497171 13.          0.31340376]. \t  0.8166764439986761 \t 0.9879595510260936\n",
      "init   \t [ 4.54930866  5.99748524  8.          0.75090625 13.          0.20817186]. \t  0.8204018598829208 \t 0.9879595510260936\n",
      "init   \t [8.80885505 2.26218951 9.         0.95881521 6.         0.90500855]. \t  0.9879595510260936 \t 0.9879595510260936\n",
      "init   \t [ 6.39630194  9.30791981  8.          0.77550908 10.          0.88560697]. \t  0.9870906082391165 \t 0.9879595510260936\n",
      "init   \t [ 8.71308214  7.36202238 13.          0.60528576  7.          0.38868957]. \t  0.8188272073625565 \t 0.9879595510260936\n",
      "1      \t [ 5.57305583  5.07112075  7.0046914   0.68325843 12.99999251  0.26054225]. \t  0.8181934899981783 \t 0.9879595510260936\n",
      "2      \t [ 5.99858602  8.55252692  7.98460722  0.76899537 10.67161273  0.73475108]. \t  0.9872202275264034 \t 0.9879595510260936\n",
      "3      \t [ 6.34847752  8.69491176  7.18501155  0.77252057 10.16256329  0.90240461]. \t  0.9870521993514334 \t 0.9879595510260936\n",
      "4      \t [ 6.93254301  8.60497118  7.92909263  0.74822698 10.43329419  0.94352157]. \t  0.9872922417448478 \t 0.9879595510260936\n",
      "5      \t [6.36090624 8.46304812 7.94163234 0.64437585 9.83600201 0.50533776]. \t  0.8190144344844373 \t 0.9879595510260936\n",
      "6      \t [ 6.53298942  9.10726924  7.60360347  0.76745695 10.63380273  0.40681992]. \t  0.8183231175135756 \t 0.9879595510260936\n",
      "7      \t [ 6.30627253  8.83593249  7.78754883  0.5        10.36875877  1.        ]. \t  \u001b[92m0.99179060485179\u001b[0m \t 0.99179060485179\n",
      "8      \t [ 6.36281544  8.74845327  7.81598122  1.         10.27742481  1.        ]. \t  \u001b[92m0.9921410669632866\u001b[0m \t 0.9921410669632866\n",
      "9      \t [ 6.46078856  7.77198349  7.3361564   0.68994479 10.90373977  0.74419035]. \t  0.9867737516658127 \t 0.9921410669632866\n",
      "10     \t [ 5.45390971  6.22810319  7.48548703  0.7149515  12.3711598   0.38196317]. \t  0.8192304619972819 \t 0.9921410669632866\n",
      "11     \t [ 6.34122823  7.63882103  8.27755529  0.68203489 11.41830728  0.63124457]. \t  0.8209251448759045 \t 0.9921410669632866\n",
      "12     \t [ 5.42518412  7.63005255  7.53139498  0.74583751 11.30678674  0.53769036]. \t  0.8201138080566394 \t 0.9921410669632866\n",
      "13     \t [7.1505803  9.32662017 7.44800461 0.67122422 9.57741603 1.        ]. \t  0.9920738567857704 \t 0.9921410669632866\n",
      "14     \t [ 6.16750023  8.22055376  7.53531496  0.89743432 11.49880439  1.        ]. \t  0.992112262285408 \t 0.9921410669632866\n",
      "15     \t [ 6.2216374   8.08858497  7.65218291  1.         11.02391631  0.63111589]. \t  0.7993259683941852 \t 0.9921410669632866\n",
      "16     \t [ 6.04472061  7.91527209  7.63800699  0.5        11.25574453  1.        ]. \t  0.9916033737195701 \t 0.9921410669632866\n",
      "17     \t [ 7.16590311  8.22382648  6.86980684  0.5        10.6932593   1.        ]. \t  0.9885788635969281 \t 0.9921410669632866\n",
      "18     \t [ 6.74958091  7.64743602  7.04486957  0.50594632 11.76448058  0.97662275]. \t  0.9862648716738037 \t 0.9921410669632866\n",
      "19     \t [ 6.30175202  8.04467609  6.5578303   0.54429003 11.22562922  1.        ]. \t  0.9886316730603841 \t 0.9921410669632866\n",
      "20     \t [ 6.70191468  8.20937753  7.19954097  0.5        11.25461341  1.        ]. \t  0.9916465808401035 \t 0.9921410669632866\n",
      "21     \t [ 5.80837603  7.65019497  7.29919334  0.6747914  12.31203937  0.81750919]. \t  0.9859768199858099 \t 0.9921410669632866\n",
      "22     \t [7.13952382 8.5806231  7.24475543 0.74457561 9.87434466 1.        ]. \t  0.9910512879899033 \t 0.9921410669632866\n",
      "23     \t [ 6.07046638  7.32613537  7.0241575   0.82347782 11.68892576  1.        ]. \t  0.9910896937661161 \t 0.9921410669632866\n",
      "24     \t [ 5.51665716  8.54456443  8.3922743   0.67232332 11.75530993  0.85494128]. \t  0.985707976786569 \t 0.9921410669632866\n",
      "25     \t [ 5.49168094  7.53486576  8.14445401  0.91512649 12.18985463  1.        ]. \t  \u001b[92m0.9937925490826038\u001b[0m \t 0.9937925490826038\n",
      "26     \t [ 6.71624646  7.525001    6.73557201  0.5        10.89993018  1.        ]. \t  0.9885740628057578 \t 0.9937925490826038\n",
      "27     \t [ 6.04657435  8.86537755  8.808453    0.82359449 10.96309261  1.        ]. \t  0.9934612946992786 \t 0.9937925490826038\n",
      "28     \t [ 5.40643932  8.26182407  8.65099042  0.84195363 11.07776031  1.        ]. \t  0.9934997017200796 \t 0.9937925490826038\n",
      "29     \t [ 5.42422503  9.03094462  8.31637242  0.9775483  10.99256924  1.        ]. \t  \u001b[92m0.9938357578625882\u001b[0m \t 0.9938357578625882\n",
      "30     \t [ 6.91354989  9.0199063   7.48358761  0.71128336 10.0624981   0.89127773]. \t  0.9863752877272653 \t 0.9938357578625882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06669500284642339"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \n",
    "\n",
    "np.random.seed(run_num_13)\n",
    "surrogate_winner_13 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train13, X_test13, y_train13, y_test13 = train_test_split(X, y, test_size=test_perc, random_state=run_num_13)\n",
    "\n",
    "def f_syn_polarity13(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_13, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train13, y=y_train13).mean())\n",
    "    return  score\n",
    "\n",
    "winner_13 = GPGO(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity13, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_13.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_13 = winner_13.getResult()[0]\n",
    "params_winner_13['max_depth'] = int(params_winner_13['max_depth'])\n",
    "params_winner_13['min_child_weight'] = int(params_winner_13['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train13 = xgb.DMatrix(X_train13, y_train13)\n",
    "dX_winner_test13 = xgb.DMatrix(X_test13, y_test13)\n",
    "model_winner_13 = xgb.train(params_winner_13, dX_winner_train13)\n",
    "pred_winner_13 = model_winner_13.predict(dX_winner_test13)\n",
    "\n",
    "rmse_winner_13 = np.sqrt(mean_squared_error(pred_winner_13, y_test13))\n",
    "rmse_winner_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [6.47551049 5.07149688 9.         0.9481426  2.         0.30826454]. \t  0.8841083588522155 \t 0.985981635228031\n",
      "init   \t [1.30128292 8.51503709 6.         0.58757726 8.         0.65878816]. \t  0.8847900532901706 \t 0.985981635228031\n",
      "init   \t [ 1.87389374  2.97833637 11.          0.92510013  4.          0.27446484]. \t  0.8861919064379334 \t 0.985981635228031\n",
      "init   \t [ 9.80899716  0.11774133 11.          0.53577929  6.          0.38130944]. \t  0.8891587951046561 \t 0.985981635228031\n",
      "init   \t [ 6.4601046   0.32698917 10.          0.67912907 15.          0.96967672]. \t  0.985981635228031 \t 0.985981635228031\n",
      "1      \t [ 6.76656743  1.25597983  9.45401532  0.63423848 15.          0.92155766]. \t  0.9854103378289975 \t 0.985981635228031\n",
      "2      \t [ 6.36098715  1.23747718 10.36288751  0.66133384 14.67661228  0.74148979]. \t  \u001b[92m0.9864905199218174\u001b[0m \t 0.9864905199218174\n",
      "3      \t [ 6.6695189   0.75569892  9.76646208  0.68089002 14.71116768  0.15435228]. \t  0.885822239087442 \t 0.9864905199218174\n",
      "4      \t [ 7.21307579  0.86816962 10.20398241  0.53121033 14.78997424  0.96279918]. \t  0.9854535475078509 \t 0.9864905199218174\n",
      "5      \t [ 6.71510125  1.00822766 10.17078516  0.74916157 15.49525011  0.66866543]. \t  0.9848486393848409 \t 0.9864905199218174\n",
      "6      \t [ 6.71625236  0.92500599  9.97687458  1.         14.8474194   0.92435543]. \t  0.9842677313456307 \t 0.9864905199218174\n",
      "7      \t [ 6.60088306  0.92161538  9.98217524  0.5        14.99085454  0.81901729]. \t  0.9847814330102326 \t 0.9864905199218174\n",
      "8      \t [ 7.16185536  1.91995803 10.03492135  0.734912   14.88355646  0.41701702]. \t  0.8874592929043293 \t 0.9864905199218174\n",
      "9      \t [ 6.64762715  0.29356349 10.9302611   0.82886368 14.89312649  0.50442969]. \t  0.8871280501371596 \t 0.9864905199218174\n",
      "10     \t [ 7.29332527  0.          9.90917421  0.83765104 15.41540985  0.65255502]. \t  0.8880258059684131 \t 0.9864905199218174\n",
      "11     \t [ 6.90307548  1.46376724 11.10837599  0.79379757 14.71329102  0.23823013]. \t  0.8875505282648389 \t 0.9864905199218174\n",
      "12     \t [ 7.60074252  1.09888993  9.28467616  0.76749819 15.37884124  0.60239028]. \t  0.8874929160050429 \t 0.9864905199218174\n",
      "13     \t [ 6.70997594  0.36183409  8.922778    0.90229887 15.58715065  0.85841937]. \t  0.9850646790669914 \t 0.9864905199218174\n",
      "14     \t [ 7.16198022  0.26564114  9.02415182  0.61664699 14.88174599  1.        ]. \t  \u001b[92m0.9932068460603012\u001b[0m \t 0.9932068460603012\n",
      "15     \t [ 7.04011082  0.5410614   9.4177266   0.73799661 15.33162132  1.        ]. \t  \u001b[92m0.9933988792974237\u001b[0m \t 0.9933988792974237\n",
      "16     \t [ 6.91067164  0.87746873  8.54302344  0.91163203 14.97988198  0.5353044 ]. \t  0.885212542480855 \t 0.9933988792974237\n",
      "17     \t [ 6.97785706  1.06706293 10.44476901  0.79243328 14.95744187  0.47603881]. \t  0.8875649308457815 \t 0.9933988792974237\n",
      "18     \t [ 6.67343553  1.89091267 11.01774103  0.62014031 15.02703609  1.        ]. \t  \u001b[92m0.9935044953894403\u001b[0m \t 0.9935044953894403\n",
      "19     \t [ 6.70685573  0.0363075   9.18132001  0.94155718 15.0694273   0.3853879 ]. \t  0.8848284775277735 \t 0.9935044953894403\n",
      "20     \t [ 6.73303314  1.26320028 11.33331234  0.54076626 14.2867161   1.        ]. \t  0.9933076642651862 \t 0.9935044953894403\n",
      "21     \t [ 6.29274604  1.06517189 11.29690982  0.80898751 15.09486704  1.        ]. \t  \u001b[92m0.994296630357756\u001b[0m \t 0.994296630357756\n",
      "22     \t [ 7.45104231  1.09438884  9.2999654   0.52649977 14.17823942  0.7294987 ]. \t  0.9843493505344614 \t 0.994296630357756\n",
      "23     \t [ 7.13037875  0.17789447  9.8991627   0.55188156 14.05980367  0.90807479]. \t  0.98493025580792 \t 0.994296630357756\n",
      "24     \t [ 6.18530686  1.67055731 11.43972024  0.5        14.6241887   0.53990612]. \t  0.8864847387962568 \t 0.994296630357756\n",
      "25     \t [ 6.48398029  1.06647497  9.24786641  1.         15.87448691  0.30742241]. \t  0.8853757696572232 \t 0.994296630357756\n",
      "26     \t [ 7.08335373  1.7374146  10.3958634   0.54965978 14.03608999  1.        ]. \t  0.9932980626828455 \t 0.994296630357756\n",
      "27     \t [ 6.61110538  1.69875662 11.03517814  1.         14.44669099  0.90904926]. \t  0.9840324889828053 \t 0.994296630357756\n",
      "28     \t [ 6.93167948  0.88001267  9.74636077  0.55001258 14.1459515   1.        ]. \t  0.993278859518164 \t 0.994296630357756\n",
      "29     \t [ 7.25066698  0.5230584   9.43822497  0.62780852 14.59639078  0.67245235]. \t  0.9851462925168878 \t 0.994296630357756\n",
      "30     \t [ 6.71977226  1.33672158 10.9067775   0.60329895 14.70910597  1.        ]. \t  0.9933844771313439 \t 0.994296630357756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0579300612981913"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \n",
    "\n",
    "np.random.seed(run_num_14)\n",
    "surrogate_winner_14 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train14, X_test14, y_train14, y_test14 = train_test_split(X, y, test_size=test_perc, random_state=run_num_14)\n",
    "\n",
    "def f_syn_polarity14(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_14, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train14, y=y_train14).mean())\n",
    "    return  score\n",
    "\n",
    "winner_14 = GPGO(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity14, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_14.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_14 = winner_14.getResult()[0]\n",
    "params_winner_14['max_depth'] = int(params_winner_14['max_depth'])\n",
    "params_winner_14['min_child_weight'] = int(params_winner_14['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train14 = xgb.DMatrix(X_train14, y_train14)\n",
    "dX_winner_test14 = xgb.DMatrix(X_test14, y_test14)\n",
    "model_winner_14 = xgb.train(params_winner_14, dX_winner_train14)\n",
    "pred_winner_14 = model_winner_14.predict(dX_winner_test14)\n",
    "\n",
    "rmse_winner_14 = np.sqrt(mean_squared_error(pred_winner_14, y_test14))\n",
    "rmse_winner_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.64315424  8.97297685  6.          0.81493635 11.          0.33008564]. \t  0.7975400806883499 \t 0.9868697857411816\n",
      "init   \t [9.51455383 9.60803032 5.         0.68485728 1.         0.30145081]. \t  0.7964118845990855 \t 0.9868697857411816\n",
      "init   \t [ 8.11809889  6.32508536  7.          0.61766329 19.          0.51383159]. \t  0.7990859351689633 \t 0.9868697857411816\n",
      "init   \t [ 7.28571251  5.36071389 10.          0.76240297 12.          0.12035683]. \t  0.7997340414931001 \t 0.9868697857411816\n",
      "init   \t [ 6.80847768  3.59767194 14.          0.51365585 16.          0.89730673]. \t  0.9868697857411816 \t 0.9868697857411816\n",
      "1      \t [ 6.88039155  4.00287278 13.28116377  0.50425224 15.28116377  0.81333916]. \t  \u001b[92m0.9871722280484146\u001b[0m \t 0.9871722280484146\n",
      "2      \t [ 6.71394342  3.83190036 13.64157995  0.93131716 15.72665651  0.1       ]. \t  0.7997724461630442 \t 0.9871722280484146\n",
      "3      \t [ 6.7155076   3.04575218 13.41001531  0.66739113 15.4279927   0.87619628]. \t  \u001b[92m0.988074774299494\u001b[0m \t 0.988074774299494\n",
      "4      \t [ 7.48759362  3.52445619 13.56914585  0.86289658 15.61139784  0.89331286]. \t  \u001b[92m0.9882140031206009\u001b[0m \t 0.9882140031206009\n",
      "5      \t [ 7.00651345  3.52240284 14.03838312  0.5048939  15.11773924  0.72050805]. \t  0.9870378061721873 \t 0.9882140031206009\n",
      "6      \t [ 6.73831395  3.69165986 13.74100297  1.         15.43326592  1.        ]. \t  \u001b[92m0.9945558762608239\u001b[0m \t 0.9945558762608239\n",
      "7      \t [ 6.97657382  3.54201474 13.58517552  0.5        15.57200257  0.78130158]. \t  0.9870042005648448 \t 0.9945558762608239\n",
      "8      \t [ 7.04465365  2.790624   14.34834346  1.         15.83544479  0.8174206 ]. \t  0.9914977712475125 \t 0.9945558762608239\n",
      "9      \t [ 7.15550073  3.57655962 13.07902174  1.         14.60620819  0.59173997]. \t  0.8878337729924066 \t 0.9945558762608239\n",
      "10     \t [ 7.36025593  2.62593253 13.80614315  1.         14.93619488  1.        ]. \t  \u001b[92m0.9946278894421074\u001b[0m \t 0.9946278894421074\n",
      "11     \t [ 7.09415534  3.07036761 13.83027042  1.         15.30374683  0.69707974]. \t  0.9914929703871985 \t 0.9946278894421074\n",
      "12     \t [ 6.64300408  2.47893953 14.32056857  0.5        15.28343504  1.        ]. \t  0.9925299329830821 \t 0.9946278894421074\n",
      "13     \t [ 7.2094683   2.88002061 14.16754568  0.57159378 15.50765177  1.        ]. \t  0.993379680003838 \t 0.9946278894421074\n",
      "14     \t [ 6.67736027  2.86857501 13.58081018  0.5253671  14.39573582  1.        ]. \t  0.9933412706321842 \t 0.9946278894421074\n",
      "15     \t [ 7.18002415  3.36785574 13.45435465  0.64583126 14.78887848  1.        ]. \t  0.9936869332662323 \t 0.9946278894421074\n",
      "16     \t [ 6.64772099  2.05644793 13.52733092  0.84663345 14.90918081  1.        ]. \t  \u001b[92m0.9946951019013626\u001b[0m \t 0.9946951019013626\n",
      "17     \t [ 7.35247205  4.50004661 13.94956915  0.80295614 15.65500089  0.72435857]. \t  0.9876619054290418 \t 0.9946951019013626\n",
      "18     \t [ 6.71568078  2.64000704 13.81676801  0.83023054 15.00324578  1.        ]. \t  0.9946470930216499 \t 0.9946951019013626\n",
      "19     \t [ 7.11951722  2.29898479 13.37352337  0.5        14.68840204  0.7413333 ]. \t  0.9865865304192217 \t 0.9946951019013626\n",
      "20     \t [ 7.06185073  1.93925997 13.80741403  0.71704057 15.71565987  0.95576857]. \t  0.9879787554337837 \t 0.9946951019013626\n",
      "21     \t [ 7.23597568  4.30206348 13.65789289  0.98725753 15.01545231  0.71031618]. \t  0.9896542393657306 \t 0.9946951019013626\n",
      "22     \t [ 7.21884736  3.86733412 14.35601595  1.         15.81216284  0.7802718 ]. \t  0.9914977712475125 \t 0.9946951019013626\n",
      "23     \t [ 7.17687097  1.82876653 14.27143698  0.67926008 14.98439029  0.74055182]. \t  0.9874218559139113 \t 0.9946951019013626\n",
      "24     \t [ 6.96736727  2.12972984 13.83756093  0.50468544 15.03385971  1.        ]. \t  0.992717166120478 \t 0.9946951019013626\n",
      "25     \t [ 6.69675533  2.10629305 14.19307531  0.59535812 15.62260293  0.38326431]. \t  0.7999068683158429 \t 0.9946951019013626\n",
      "26     \t [ 6.65551527  2.70218395 12.6957601   0.71084876 14.55340953  1.        ]. \t  0.9940950086055058 \t 0.9946951019013626\n",
      "27     \t [ 7.12832615  1.95977956 14.58339897  0.91222601 15.53638405  1.        ]. \t  0.9946710970466496 \t 0.9946951019013626\n",
      "28     \t [ 7.10248448  4.01415325 13.81240239  0.81931272 15.50466176  0.79960454]. \t  0.9890685436725356 \t 0.9946951019013626\n",
      "29     \t [ 6.22997606  3.71741648 13.30971338  0.50637399 14.6320964   0.84875544]. \t  0.9865481198029977 \t 0.9946951019013626\n",
      "30     \t [ 8.15734004  4.43961207 14.05749976  1.         15.45381558  0.43522762]. \t  0.8878433798296012 \t 0.9946951019013626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.058520186594154804"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \n",
    "\n",
    "np.random.seed(run_num_15)\n",
    "surrogate_winner_15 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train15, X_test15, y_train15, y_test15 = train_test_split(X, y, test_size=test_perc, random_state=run_num_15)\n",
    "\n",
    "def f_syn_polarity15(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_15, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train15, y=y_train15).mean())\n",
    "    return  score\n",
    "\n",
    "winner_15 = GPGO(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity15, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_15.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_15 = winner_15.getResult()[0]\n",
    "params_winner_15['max_depth'] = int(params_winner_15['max_depth'])\n",
    "params_winner_15['min_child_weight'] = int(params_winner_15['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train15 = xgb.DMatrix(X_train15, y_train15)\n",
    "dX_winner_test15 = xgb.DMatrix(X_test15, y_test15)\n",
    "model_winner_15 = xgb.train(params_winner_15, dX_winner_train15)\n",
    "pred_winner_15 = model_winner_15.predict(dX_winner_test15)\n",
    "\n",
    "rmse_winner_15 = np.sqrt(mean_squared_error(pred_winner_15, y_test15))\n",
    "rmse_winner_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [8.63948332 3.16366002 7.         0.75395583 1.         0.56999262]. \t  0.8862591155624773 \t 0.9857703856869544\n",
      "init   \t [ 3.3426621   9.77029134 14.          0.93751218 17.          0.80549151]. \t  0.9857703856869544 \t 0.9857703856869544\n",
      "init   \t [ 3.36871073  4.8308288  13.          0.91984982 14.          0.48829226]. \t  0.8862111023959116 \t 0.9857703856869544\n",
      "init   \t [ 8.84540736  1.59826081  8.          0.75772888 14.          0.29029227]. \t  0.8862879265323566 \t 0.9857703856869544\n",
      "init   \t [ 7.74981136  8.34377636 13.          0.83182409 15.          0.19378622]. \t  0.8859086345058471 \t 0.9857703856869544\n",
      "1      \t [ 6.6977968   8.68197124 13.23890485  0.85614693 15.4758894   0.33917052]. \t  0.8871664589401602 \t 0.9857703856869544\n",
      "2      \t [ 6.94914704  8.47418031 12.16578415  0.89930391 15.10326776  0.37547194]. \t  0.886263931219348 \t 0.9857703856869544\n",
      "3      \t [ 6.81698538  7.71317263 12.99247204  0.84784001 14.93391107  0.39443839]. \t  0.886888009180224 \t 0.9857703856869544\n",
      "4      \t [ 6.85372751  8.66002584 12.98471728  0.90518524 14.43144678  0.10503795]. \t  0.8868544071683063 \t 0.9857703856869544\n",
      "5      \t [ 7.10193463  8.51453164 12.93481506  0.51637848 14.86327747  0.9730768 ]. \t  0.9809744027050233 \t 0.9857703856869544\n",
      "6      \t [ 4.20503911  9.47309325 13.77481955  0.90867858 16.57028372  0.70460553]. \t  \u001b[92m0.9857943919245233\u001b[0m \t 0.9857943919245233\n",
      "7      \t [ 7.01403508  8.389307   12.8395848   0.5        15.03818805  0.21729313]. \t  0.8879970162099314 \t 0.9857943919245233\n",
      "8      \t [ 7.09856123  8.42648999 12.93375932  1.         14.95777781  0.67601503]. \t  0.9674024783213326 \t 0.9857943919245233\n",
      "9      \t [ 4.11861017  9.79953485 14.31255706  1.         17.21493874  0.41604617]. \t  0.7896091274858469 \t 0.9857943919245233\n",
      "10     \t [ 3.62489482  8.92307563 13.83692744  1.         17.03711949  0.60899453]. \t  0.7895659197430193 \t 0.9857943919245233\n",
      "11     \t [ 3.71630638  9.48507705 14.47331107  0.99998593 16.40752732  0.99661251]. \t  0.9857319871015756 \t 0.9857943919245233\n",
      "12     \t [ 5.96147284  8.53508741 12.76435279  0.703247   14.88937707  0.88351267]. \t  0.9836292241125274 \t 0.9857943919245233\n",
      "13     \t [ 3.54845207  9.67051524 14.04236604  0.98556127 16.33884789  0.29031044]. \t  0.8851021255124597 \t 0.9857943919245233\n",
      "14     \t [ 3.75137392  9.57299525 14.10900312  0.5        16.68724578  0.84655858]. \t  0.9837492626295073 \t 0.9857943919245233\n",
      "15     \t [ 6.28782728  8.42057942 13.67845862  0.54492446 14.719522    0.87477376]. \t  0.9803070869933501 \t 0.9857943919245233\n",
      "16     \t [ 6.33787985  9.26177528 13.2154222   0.54362738 14.83375859  0.93928306]. \t  0.9805279231106625 \t 0.9857943919245233\n",
      "17     \t [ 5.52834864  8.95506773 13.64531562  0.77387914 15.47138451  0.77814138]. \t  0.9846469951597139 \t 0.9857943919245233\n",
      "18     \t [ 6.17671122  8.6874467  13.29781074  0.5        15.32208333  1.        ]. \t  \u001b[92m0.9935285050141504\u001b[0m \t 0.9935285050141504\n",
      "19     \t [ 7.57545416  7.95436552 12.33152683  0.70497063 14.2676345   0.57047199]. \t  0.8832970449028608 \t 0.9935285050141504\n",
      "20     \t [ 6.04453116  8.79743277 13.33964113  0.84386197 14.95936448  0.57882559]. \t  0.8869504193963093 \t 0.9935285050141504\n",
      "21     \t [ 6.55756324  8.38686712 12.7757161   0.5        14.18256567  1.        ]. \t  \u001b[92m0.9935573101760354\u001b[0m \t 0.9935573101760354\n",
      "22     \t [ 4.79531906  9.02197331 14.35744489  0.67646011 16.02583255  0.80725731]. \t  0.9837972608612309 \t 0.9935573101760354\n",
      "23     \t [ 7.32998287  8.07947801 13.49042671  0.57023099 14.14242287  0.60867724]. \t  0.8838779387677423 \t 0.9935573101760354\n",
      "24     \t [ 4.47586808  9.12678438 13.79328911  0.61253203 15.57348922  1.        ]. \t  \u001b[92m0.9943350394520837\u001b[0m \t 0.9943350394520837\n",
      "25     \t [ 6.55067996  9.11179681 12.13076927  0.57239675 14.57848862  0.99923089]. \t  0.9822466019248783 \t 0.9943350394520837\n",
      "26     \t [ 4.90203196  9.67910416 14.06681075  0.7036606  15.78343834  1.        ]. \t  \u001b[92m0.9946710969083639\u001b[0m \t 0.9946710969083639\n",
      "27     \t [ 4.70114195  9.21105342 13.98872936  1.         15.89885678  1.        ]. \t  \u001b[92m0.9948439284328162\u001b[0m \t 0.9948439284328162\n",
      "28     \t [ 5.01292777  9.19998389 13.76753713  0.5        15.93783099  1.        ]. \t  0.9935909142622364 \t 0.9948439284328162\n",
      "29     \t [ 5.25290646  9.08822898 14.1620928   0.5        15.26186817  1.        ]. \t  0.9936773273970366 \t 0.9948439284328162\n",
      "30     \t [ 6.59138005  8.69827269 12.68003263  0.57809158 14.65618853  0.8805118 ]. \t  0.9817185077051765 \t 0.9948439284328162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.062106740271556486"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \n",
    "\n",
    "np.random.seed(run_num_16)\n",
    "surrogate_winner_16 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train16, X_test16, y_train16, y_test16 = train_test_split(X, y, test_size=test_perc, random_state=run_num_16)\n",
    "\n",
    "def f_syn_polarity16(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_16, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train16, y=y_train16).mean())\n",
    "    return  score\n",
    "\n",
    "winner_16 = GPGO(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity16, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_16.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_16 = winner_16.getResult()[0]\n",
    "params_winner_16['max_depth'] = int(params_winner_16['max_depth'])\n",
    "params_winner_16['min_child_weight'] = int(params_winner_16['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train16 = xgb.DMatrix(X_train16, y_train16)\n",
    "dX_winner_test16 = xgb.DMatrix(X_test16, y_test16)\n",
    "model_winner_16 = xgb.train(params_winner_16, dX_winner_train16)\n",
    "pred_winner_16 = model_winner_16.predict(dX_winner_test16)\n",
    "\n",
    "rmse_winner_16 = np.sqrt(mean_squared_error(pred_winner_16, y_test16))\n",
    "rmse_winner_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 6.51824028  7.30519624 13.          0.53095783 18.          0.70874494]. \t  0.9897406652228038 \t 0.991204913651782\n",
      "init   \t [ 8.04444873  5.33512865 14.          0.77275393  6.          0.33037499]. \t  0.8877329011327202 \t 0.991204913651782\n",
      "init   \t [ 2.78665017  0.43321861  6.          0.9708069  14.          0.8542801 ]. \t  0.991204913651782 \t 0.991204913651782\n",
      "init   \t [7.75225299 4.53658749 6.         0.70978712 9.         0.19759133]. \t  0.8873200050200097 \t 0.991204913651782\n",
      "init   \t [ 9.75833611  5.75935719 10.          0.77333839 14.          0.97649057]. \t  0.9908496585622393 \t 0.991204913651782\n",
      "1      \t [ 2.98837154  1.0910144   6.          0.94484529 13.13237279  0.63022869]. \t  0.8920248356821929 \t 0.991204913651782\n",
      "2      \t [ 2.84753522  0.92172553  6.929807    0.92569675 13.81470973  0.68673987]. \t  0.9911185031444076 \t 0.991204913651782\n",
      "3      \t [ 3.39541972  0.29790639  6.55732255  0.98401935 13.40320648  0.93530653]. \t  0.9910560914071812 \t 0.991204913651782\n",
      "4      \t [ 2.43559829  0.36484413  6.53205902  0.83068513 13.2595907   0.84158612]. \t  0.9906576251176715 \t 0.991204913651782\n",
      "5      \t [ 2.87091755  0.76545656  6.42057864  1.         13.56385855  1.        ]. \t  0.9887036847880809 \t 0.991204913651782\n",
      "6      \t [ 2.96702961  0.42875179  6.41914116  0.78456273 13.59633751  0.23887809]. \t  0.8917511959094188 \t 0.991204913651782\n",
      "7      \t [ 2.79372144  0.          6.97078606  0.63527586 14.13446116  1.        ]. \t  0.9888333095377048 \t 0.991204913651782\n",
      "8      \t [ 3.014027    0.3620603   7.55848951  0.5        13.26013013  0.88439831]. \t  0.9902687461670903 \t 0.991204913651782\n",
      "9      \t [ 2.97754388  0.4652242   6.6877473   0.5        13.59642792  0.90528385]. \t  0.9897406637016624 \t 0.991204913651782\n",
      "10     \t [ 2.83768383  0.17429001  7.1954716   1.         13.59002715  0.77896986]. \t  0.987829930561757 \t 0.991204913651782\n",
      "11     \t [ 3.11309618  0.95188491  7.12911417  0.93993273 12.59029983  0.56307614]. \t  0.8928841976989924 \t 0.991204913651782\n",
      "12     \t [ 9.02664208  5.98870972 10.20442457  0.79973787 14.54242958  0.9934393 ]. \t  0.9910608871509291 \t 0.991204913651782\n",
      "13     \t [ 9.38042931  5.86595299 10.13504312  0.78375939 14.27018068  0.22578637]. \t  0.8868735573696167 \t 0.991204913651782\n",
      "14     \t [ 9.79407381  6.40390499 10.27987049  0.79319778 14.52469996  0.89828063]. \t  \u001b[92m0.9913153345452925\u001b[0m \t 0.9913153345452925\n",
      "15     \t [ 9.68514055  5.62399142 10.57784214  0.72360591 14.59789787  0.92135213]. \t  0.9908400549056128 \t 0.9913153345452925\n",
      "16     \t [ 9.40747538  6.09703836 10.66405926  0.76378098 13.98878695  0.92514304]. \t  0.9912193140201553 \t 0.9913153345452925\n",
      "17     \t [ 9.50637254  5.99243819 10.25178609  0.5        14.33962987  0.91050788]. \t  0.9885644551372604 \t 0.9913153345452925\n",
      "18     \t [ 9.53642953  5.94222572 10.31526728  1.         14.34179995  0.90396525]. \t  0.9880843756052656 \t 0.9913153345452925\n",
      "19     \t [ 9.04583646  6.39751594 10.99259682  0.72338039 15.00327488  0.67575026]. \t  0.9911665041418427 \t 0.9913153345452925\n",
      "20     \t [ 8.56617776  5.5758808  11.00921795  0.67026247 14.63262502  0.67059496]. \t  0.9907488380065014 \t 0.9913153345452925\n",
      "21     \t [ 8.32557453  6.40019293 10.70276423  0.7766179  14.42801831  0.54458409]. \t  0.887358435411071 \t 0.9913153345452925\n",
      "22     \t [ 8.59179634  5.83975105 10.4659995   0.73643608 15.27215295  0.45192508]. \t  0.8878384906048046 \t 0.9913153345452925\n",
      "23     \t [ 8.99338102  5.98053477 10.74708322  0.71642362 14.6534678   0.68685105]. \t  0.9907488353790753 \t 0.9913153345452925\n",
      "24     \t [ 8.21342756  6.08491123 11.03695438  0.82000469 15.15323414  1.        ]. \t  \u001b[92m0.994243827807824\u001b[0m \t 0.994243827807824\n",
      "25     \t [ 9.08117396  4.95393197 10.42655544  0.67059123 13.81567624  0.88800064]. \t  0.990835253907013 \t 0.994243827807824\n",
      "26     \t [ 8.07051672  5.31847138 10.36190278  0.80058477 14.42550343  1.        ]. \t  0.9941718162859673 \t 0.994243827807824\n",
      "27     \t [ 8.79601156  4.81538878 10.3847167   0.70606633 14.74510252  1.        ]. \t  0.9936725299938619 \t 0.994243827807824\n",
      "28     \t [ 8.81063567  7.07240739 10.55670319  0.87879838 15.23249701  1.        ]. \t  0.9942294259183093 \t 0.994243827807824\n",
      "29     \t [ 9.38762968  7.2246873  10.93617814  0.80389341 14.61697246  1.        ]. \t  0.9939461780637705 \t 0.994243827807824\n",
      "30     \t [ 8.75625482  5.29028814 10.51334805  0.7214273  14.37741432  1.        ]. \t  0.993643726076547 \t 0.994243827807824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06419524453559945"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \n",
    "\n",
    "np.random.seed(run_num_17)\n",
    "surrogate_winner_17 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train17, X_test17, y_train17, y_test17 = train_test_split(X, y, test_size=test_perc, random_state=run_num_17)\n",
    "\n",
    "def f_syn_polarity17(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_17, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train17, y=y_train17).mean())\n",
    "    return  score\n",
    "\n",
    "winner_17 = GPGO(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity17, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_17.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_17 = winner_17.getResult()[0]\n",
    "params_winner_17['max_depth'] = int(params_winner_17['max_depth'])\n",
    "params_winner_17['min_child_weight'] = int(params_winner_17['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train17 = xgb.DMatrix(X_train17, y_train17)\n",
    "dX_winner_test17 = xgb.DMatrix(X_test17, y_test17)\n",
    "model_winner_17 = xgb.train(params_winner_17, dX_winner_train17)\n",
    "pred_winner_17 = model_winner_17.predict(dX_winner_test17)\n",
    "\n",
    "rmse_winner_17 = np.sqrt(mean_squared_error(pred_winner_17, y_test17))\n",
    "rmse_winner_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 8.73429403  9.68540663 10.          0.68875849  9.          0.48011572]. \t  0.8299977850025021 \t 0.9872154265261087\n",
      "init   \t [ 6.12033333  7.66062926  8.          0.76133734 13.          0.93379456]. \t  0.984824605557074 \t 0.9872154265261087\n",
      "init   \t [ 1.46524679  7.01527914  7.          0.90913299 10.          0.36016753]. \t  0.8300890052204674 \t 0.9872154265261087\n",
      "init   \t [ 9.73855241  3.33774046 14.          0.53290419  7.          0.7088681 ]. \t  0.9802782361972252 \t 0.9872154265261087\n",
      "init   \t [ 3.00618018  1.82702795 11.          0.75681389 14.          0.98627449]. \t  0.9872154265261087 \t 0.9872154265261087\n",
      "1      \t [ 3.45252332  2.71791287 10.49443694  0.76563699 13.85599884  0.99309531]. \t  0.9871098052483424 \t 0.9872154265261087\n",
      "2      \t [ 3.89702539  2.0830713  10.93481047  0.73902711 14.1425567   0.48309102]. \t  0.8296761366265865 \t 0.9872154265261087\n",
      "3      \t [ 3.25303234  2.02236445 10.16041312  0.71817649 14.5125232   0.99349097]. \t  0.9871050038348859 \t 0.9872154265261087\n",
      "4      \t [ 2.89473324  2.29590606 10.45478521  0.64278501 14.02446533  0.32460221]. \t  0.8282743282833777 \t 0.9872154265261087\n",
      "5      \t [ 3.45662558  1.83855587 10.26919923  1.         13.64168383  0.98964126]. \t  0.9869273862466758 \t 0.9872154265261087\n",
      "6      \t [ 3.41036537  2.05594754 10.51686633  0.5        13.93763948  1.        ]. \t  \u001b[92m0.9936677227724114\u001b[0m \t 0.9936677227724114\n",
      "7      \t [ 3.28818938  2.20625107 10.62084167  1.         14.16970126  0.9849539 ]. \t  0.986951389441962 \t 0.9936677227724114\n",
      "8      \t [ 3.89793318  2.56009565  9.5955996   0.88967649 14.06073179  0.60656871]. \t  0.8293304925228054 \t 0.9936677227724114\n",
      "9      \t [ 3.1281847   1.00205883 10.54141117  0.87297236 14.29776417  0.56346338]. \t  0.8288600172005793 \t 0.9936677227724114\n",
      "10     \t [ 6.71778636  7.45275162  8.33486784  0.67082841 12.34557854  0.85328383]. \t  0.9849686248670769 \t 0.9936677227724114\n",
      "11     \t [ 6.53092512  8.2609251   8.44905925  0.74097906 12.67851795  0.99993449]. \t  0.984872611117933 \t 0.9936677227724114\n",
      "12     \t [ 5.92755022  7.79365576  8.38248713  0.67899837 12.25061855  0.99999171]. \t  0.9848966184617866 \t 0.9936677227724114\n",
      "13     \t [ 6.41308008  7.99927808  7.76357329  0.6977131  12.33909083  0.75769926]. \t  0.9832931287659995 \t 0.9936677227724114\n",
      "14     \t [ 6.27434256  7.82217654  8.37958137  0.92744587 12.59894833  0.37742871]. \t  0.8300265979083793 \t 0.9936677227724114\n",
      "15     \t [ 6.3786098   7.80278115  8.17291689  0.5        12.5804488   1.        ]. \t  0.9920498578762257 \t 0.9936677227724114\n",
      "16     \t [ 6.39834753  7.78071458  8.12062693  1.         12.4418892   1.        ]. \t  0.9936053147688954 \t 0.9936677227724114\n",
      "17     \t [ 5.60092513  8.5825199   7.90028242  0.85755218 12.87122493  1.        ]. \t  0.9918818323286538 \t 0.9936677227724114\n",
      "18     \t [ 5.24425623  7.8200324   7.42826838  0.78944628 12.62286555  0.84777942]. \t  0.9848342072777023 \t 0.9936677227724114\n",
      "19     \t [ 5.72734642  8.05839042  7.8474156   0.7930893  12.66033545  0.90067982]. \t  0.9840708718706054 \t 0.9936677227724114\n",
      "20     \t [ 5.22929394  8.133298    7.25408224  0.88288473 13.54901534  1.        ]. \t  0.991910636799111 \t 0.9936677227724114\n",
      "21     \t [ 4.75907301  7.98399478  7.93780077  0.82327954 13.25681405  1.        ]. \t  0.9917282046603147 \t 0.9936677227724114\n",
      "22     \t [ 6.71097144  8.13896373  8.46988404  0.63506292 11.58832727  0.85433065]. \t  0.9842628977094664 \t 0.9936677227724114\n",
      "23     \t [ 5.12594813  7.20575426  7.4665648   0.79826584 13.29623089  1.        ]. \t  0.9917378073489421 \t 0.9936677227724114\n",
      "24     \t [ 5.13937964  7.88088457  7.53351004  0.5        13.21553061  1.        ]. \t  0.9900671246921662 \t 0.9936677227724114\n",
      "25     \t [ 5.18602248  7.83763674  7.59950067  1.         13.19199212  1.        ]. \t  0.9919442430978812 \t 0.9936677227724114\n",
      "26     \t [ 4.81230548  7.73753162  7.34454887  0.76251537 13.32186083  0.29822185]. \t  0.8294697058559279 \t 0.9936677227724114\n",
      "27     \t [ 5.76586115  6.80472485  7.55191394  0.63001684 12.50559879  0.70248935]. \t  0.9838308278868979 \t 0.9936677227724114\n",
      "28     \t [ 5.30251348  8.73084544  8.03986822  0.78700777 13.82219519  0.88145704]. \t  0.9856119462337408 \t 0.9936677227724114\n",
      "29     \t [ 4.76189593  8.89872492  7.59319787  0.81371543 13.36662245  0.8520317 ]. \t  0.9839892554475519 \t 0.9936677227724114\n",
      "30     \t [ 5.84228258  7.17239217  6.94321709  0.73004235 13.05952663  0.71361709]. \t  0.9851126454216502 \t 0.9936677227724114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.061592132248393286"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \n",
    "\n",
    "np.random.seed(run_num_18)\n",
    "surrogate_winner_18 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train18, X_test18, y_train18, y_test18 = train_test_split(X, y, test_size=test_perc, random_state=run_num_18)\n",
    "\n",
    "def f_syn_polarity18(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_18, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train18, y=y_train18).mean())\n",
    "    return  score\n",
    "\n",
    "winner_18 = GPGO(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity18, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_18.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_18 = winner_18.getResult()[0]\n",
    "params_winner_18['max_depth'] = int(params_winner_18['max_depth'])\n",
    "params_winner_18['min_child_weight'] = int(params_winner_18['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train18 = xgb.DMatrix(X_train18, y_train18)\n",
    "dX_winner_test18 = xgb.DMatrix(X_test18, y_test18)\n",
    "model_winner_18 = xgb.train(params_winner_18, dX_winner_train18)\n",
    "pred_winner_18 = model_winner_18.predict(dX_winner_test18)\n",
    "\n",
    "rmse_winner_18 = np.sqrt(mean_squared_error(pred_winner_18, y_test18))\n",
    "rmse_winner_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.59445577  7.61491991 14.          0.8201684   7.          0.41902487]. \t  0.8889427962173378 \t 0.9874362876751138\n",
      "init   \t [ 7.966974    3.46000022 11.          0.65165179  6.          0.61588843]. \t  0.8888371874545623 \t 0.9874362876751138\n",
      "init   \t [ 0.65664016  0.24088885 10.          0.76028365  4.          0.74988462]. \t  0.9874362876751138 \t 0.9874362876751138\n",
      "init   \t [ 4.0108054   5.55029685 12.          0.98976746 13.          0.16498374]. \t  0.8919048872735041 \t 0.9874362876751138\n",
      "init   \t [ 6.78611641  2.0276123   5.          0.8840465  10.          0.9586535 ]. \t  0.9715264078631116 \t 0.9874362876751138\n",
      "1      \t [ 0.44034589  0.50091276 11.01678533  0.84048688  4.33893516  0.7598332 ]. \t  \u001b[92m0.9876859258427911\u001b[0m \t 0.9876859258427911\n",
      "2      \t [ 0.          0.30981758 10.18823845  0.99995918  4.78001037  0.76360414]. \t  0.9873450705685629 \t 0.9876859258427911\n",
      "3      \t [ 0.9805254   0.36655449 10.36501778  0.92285632  4.85544208  0.58239802]. \t  0.8917464596437182 \t 0.9876859258427911\n",
      "4      \t [ 0.2719756   0.02594769 10.49221798  0.50749918  4.39301532  0.18271014]. \t  0.8902342147811054 \t 0.9876859258427911\n",
      "5      \t [ 0.33483034  0.98315702 10.25799008  0.7846352   4.32801721  0.44906628]. \t  0.8927402329578301 \t 0.9876859258427911\n",
      "6      \t [ 0.41327634  0.38085336 10.38832324  0.5         4.52266037  1.        ]. \t  \u001b[92m0.9953960209393286\u001b[0m \t 0.9953960209393286\n",
      "7      \t [ 0.39815836  0.25876075 10.44026615  1.          4.30113491  0.71143821]. \t  0.9908400468863222 \t 0.9953960209393286\n",
      "8      \t [0.41940339 0.22095251 9.25721365 0.56815264 4.7132275  0.44756024]. \t  0.891031146530252 \t 0.9953960209393286\n",
      "9      \t [ 0.07372769  0.71779842 11.14655606  0.66994221  5.30843615  0.44003583]. \t  0.8920441121534713 \t 0.9953960209393286\n",
      "10     \t [ 6.30815367  1.48318044  5.30322675  0.84691616 10.60307297  0.8655379 ]. \t  0.9714880011188858 \t 0.9953960209393286\n",
      "11     \t [6.10005567 1.93747836 5.34220617 0.65600469 9.93692994 0.48555108]. \t  0.8905078641647318 \t 0.9953960209393286\n",
      "12     \t [ 6.71066879  1.89915446  5.87731385  0.84469487 10.20476688  0.97733943]. \t  0.9714880011188858 \t 0.9953960209393286\n",
      "13     \t [ 6.56805799  2.3008496   5.33843462  0.73984829 10.67688837  0.70542976]. \t  0.9714495947895228 \t 0.9953960209393286\n",
      "14     \t [ 6.91718946  1.68101394  5.36692705  0.76469735 10.32976206  0.37214309]. \t  0.8909351350629028 \t 0.9953960209393286\n",
      "15     \t [ 6.56225929  1.86999617  5.33501405  0.5        10.31253825  1.        ]. \t  0.9839412763014566 \t 0.9953960209393286\n",
      "16     \t [ 6.46395041  1.95688727  5.37871086  1.         10.30382962  0.8291095 ]. \t  0.9810367869260873 \t 0.9953960209393286\n",
      "17     \t [7.07190209 2.75408488 5.55987789 0.55769286 9.74310873 0.63488767]. \t  0.8890580457669794 \t 0.9953960209393286\n",
      "18     \t [ 0.3463613   0.47554539 10.40948538  0.67503439  4.74154042  0.52881474]. \t  0.8925481957103684 \t 0.9953960209393286\n",
      "19     \t [0.         0.55930429 9.39228154 0.93574103 4.25732123 1.        ]. \t  \u001b[92m0.9957464794553483\u001b[0m \t 0.9957464794553483\n",
      "20     \t [0.84330912 0.64606584 9.48228271 1.         4.59774626 1.        ]. \t  0.995616857748029 \t 0.9957464794553483\n",
      "21     \t [ 0.          0.08534236 11.52148228  0.84248909  5.08566594  1.        ]. \t  \u001b[92m0.9971099091952338\u001b[0m \t 0.9971099091952338\n",
      "22     \t [ 0.          0.94704532 11.50465388  1.          4.9890187   1.        ]. \t  \u001b[92m0.9971147113692798\u001b[0m \t 0.9971147113692798\n",
      "23     \t [ 0.73158702  0.55233893 11.48968154  1.          5.28934284  1.        ]. \t  0.9969850894545291 \t 0.9971147113692798\n",
      "24     \t [ 1.0790006   1.32932263 10.72230533  1.          4.60948833  1.        ]. \t  0.9967162440426871 \t 0.9971147113692798\n",
      "25     \t [0.31809454 0.         9.59659431 0.90712546 4.52464001 1.        ]. \t  0.9956936703376114 \t 0.9971147113692798\n",
      "26     \t [ 0.38157354  0.67325682 11.00271164  1.          4.96999848  1.        ]. \t  \u001b[92m0.9971339145339613\u001b[0m \t 0.9971339145339613\n",
      "27     \t [ 0.32997861  0.63201594 11.87964612  0.5         5.02145884  0.72109873]. \t  0.9869081949765812 \t 0.9971339145339613\n",
      "28     \t [ 0.9622861   1.05129041 10.14071102  0.8777444   4.11583009  1.        ]. \t  0.996533812180474 \t 0.9971339145339613\n",
      "29     \t [ 0.13671881  0.46154304 11.78378756  1.          5.1249392   0.42875915]. \t  0.8889139599407504 \t 0.9971339145339613\n",
      "30     \t [ 1.4602967   0.7359444  11.08783972  0.5804964   4.55431437  1.        ]. \t  0.9958664983357387 \t 0.9971339145339613\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.050575015587794314"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \n",
    "\n",
    "np.random.seed(run_num_19)\n",
    "surrogate_winner_19 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train19, X_test19, y_train19, y_test19 = train_test_split(X, y, test_size=test_perc, random_state=run_num_19)\n",
    "\n",
    "def f_syn_polarity19(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_19, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train19, y=y_train19).mean())\n",
    "    return  score\n",
    "\n",
    "winner_19 = GPGO(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity19, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_19.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_19 = winner_19.getResult()[0]\n",
    "params_winner_19['max_depth'] = int(params_winner_19['max_depth'])\n",
    "params_winner_19['min_child_weight'] = int(params_winner_19['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train19 = xgb.DMatrix(X_train19, y_train19)\n",
    "dX_winner_test19 = xgb.DMatrix(X_test19, y_test19)\n",
    "model_winner_19 = xgb.train(params_winner_19, dX_winner_train19)\n",
    "pred_winner_19 = model_winner_19.predict(dX_winner_test19)\n",
    "\n",
    "rmse_winner_19 = np.sqrt(mean_squared_error(pred_winner_19, y_test19))\n",
    "rmse_winner_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 5.70517285  5.64528755 11.          0.51213999 19.          0.57883228]. \t  0.8868016210751127 \t 0.9913825280594238\n",
      "init   \t [0.68106288 5.8452906  6.         0.58037829 8.         0.87974899]. \t  0.991036867637945 \t 0.9913825280594238\n",
      "init   \t [ 4.04121426  7.17971417 11.          0.57603584 10.          0.19954225]. \t  0.8886163118407602 \t 0.9913825280594238\n",
      "init   \t [ 5.27556761  8.67655329 13.          0.54252273  4.          0.48570796]. \t  0.8864991504193368 \t 0.9913825280594238\n",
      "init   \t [ 5.30003889  5.73946822 12.          0.52666698 16.          0.77467019]. \t  0.9913825280594238 \t 0.9913825280594238\n",
      "1      \t [ 5.48098603  5.69740503 11.55336536  0.5201778  17.33990449  0.68720215]. \t  0.9911328909287936 \t 0.9913825280594238\n",
      "2      \t [ 4.96485783  4.9892144  11.70674381  0.60980202 16.61587918  0.64862816]. \t  0.8867392148693094 \t 0.9913825280594238\n",
      "3      \t [ 5.37126185  5.59756538 12.48267681  0.85952055 16.84807009  0.77366056]. \t  \u001b[92m0.9916513795558709\u001b[0m \t 0.9916513795558709\n",
      "4      \t [ 5.97513197  5.33021892 11.87348233  0.70897836 16.54172902  0.58044046]. \t  0.8886883071832047 \t 0.9916513795558709\n",
      "5      \t [ 5.17709838  5.86719546 11.80377558  1.         16.6236766   0.26090133]. \t  0.8983859576213412 \t 0.9916513795558709\n",
      "6      \t [ 5.33174941  5.65926357 11.74984254  0.92422324 16.63993033  1.        ]. \t  \u001b[92m0.9947863089821981\u001b[0m \t 0.9947863089821981\n",
      "7      \t [ 5.29983421  5.65468047 12.02446411  0.5        16.70071082  0.66884694]. \t  0.9913825293039938 \t 0.9947863089821981\n",
      "8      \t [ 5.53501426  5.2531179  11.66087138  1.         18.13950853  0.56597631]. \t  0.898150714774519 \t 0.9947863089821981\n",
      "9      \t [ 5.54617478  5.38457378 10.64719758  0.67650346 18.00889032  0.46996475]. \t  0.8873441048769131 \t 0.9947863089821981\n",
      "10     \t [ 6.01016997  6.07162042 11.20661117  0.91582922 18.16397712  0.57425441]. \t  0.8897973099260592 \t 0.9947863089821981\n",
      "11     \t [ 5.24065322  5.24081318 12.36277588  1.         16.03636638  0.66764346]. \t  0.991478556051982 \t 0.9947863089821981\n",
      "12     \t [ 5.07054874  5.94008057 11.18665279  0.73054186 18.30443176  0.75503702]. \t  0.9913057196875344 \t 0.9947863089821981\n",
      "13     \t [ 5.41252901  5.80794251 11.31571039  0.5        18.26734407  0.1       ]. \t  0.887228880633948 \t 0.9947863089821981\n",
      "14     \t [ 5.60958692  5.61613833 11.24560156  0.5        18.2457213   0.95942165]. \t  0.9911760964590518 \t 0.9947863089821981\n",
      "15     \t [0.73983484 5.98567412 6.39279078 0.55972753 8.78550621 0.75139478]. \t  0.9911280921427631 \t 0.9947863089821981\n",
      "16     \t [1.34521173 5.6297642  6.197133   0.65329335 8.39422636 0.7909002 ]. \t  0.9916945860541283 \t 0.9947863089821981\n",
      "17     \t [1.12524662 6.33576199 6.39289026 0.58014528 8.17973382 0.76055192]. \t  0.9911424872562845 \t 0.9947863089821981\n",
      "18     \t [0.85105088 5.7348601  6.70739488 0.67633596 8.19195724 0.99950602]. \t  0.9921602639731796 \t 0.9947863089821981\n",
      "19     \t [0.85438103 5.79853856 6.40261654 0.66794872 8.22012983 0.31186788]. \t  0.8908438749339993 \t 0.9947863089821981\n",
      "20     \t [0.92258731 5.9971075  6.2018094  1.         8.40159915 0.99844818]. \t  0.9915169658384922 \t 0.9947863089821981\n",
      "21     \t [0.95814015 5.91377868 6.27515895 0.5        8.35672025 0.96437326]. \t  0.9906768031143823 \t 0.9947863089821981\n",
      "22     \t [1.33299934 5.76679236 6.20439    0.96511215 7.38729556 0.92545612]. \t  0.9920642464903251 \t 0.9947863089821981\n",
      "23     \t [ 5.36316719  5.45475305 12.07383244  0.90287418 16.47179446  0.68228183]. \t  0.9917041873598998 \t 0.9947863089821981\n",
      "24     \t [ 4.57743479  5.6258747  12.60736791  0.78265218 16.11706847  1.        ]. \t  0.9947142952477722 \t 0.9947863089821981\n",
      "25     \t [ 5.2599012   5.29939512 12.88778158  0.5        15.99451029  1.        ]. \t  0.9937733410078952 \t 0.9947863089821981\n",
      "26     \t [ 4.79420813  5.01277538 12.25833867  0.5        15.65736099  1.        ]. \t  0.9938021454783522 \t 0.9947863089821981\n",
      "27     \t [ 4.78967358  5.34050687 12.6490289   0.5        15.75097013  0.37644328]. \t  0.8894612602137713 \t 0.9947863089821981\n",
      "28     \t [ 4.83509818  6.03924151 11.96817564  1.         17.43214563  0.90290134]. \t  0.9916321853797477 \t 0.9947863089821981\n",
      "29     \t [ 5.32227874  5.79525403 11.3481948   1.         17.72818456  0.73898847]. \t  0.9913393329005835 \t 0.9947863089821981\n",
      "30     \t [ 5.02356971  5.38214638 12.363082    0.6772844  16.07207922  1.        ]. \t  0.9943302319540619 \t 0.9947863089821981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06267706497245876"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \n",
    "\n",
    "np.random.seed(run_num_20)\n",
    "surrogate_winner_20 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "X_train20, X_test20, y_train20, y_test20 = train_test_split(X, y, test_size=test_perc, random_state=run_num_20)\n",
    "\n",
    "def f_syn_polarity20(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
    "    reg = XGBClassifier(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
    "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_20, objective = obj_classifier, booster='gbtree', silent=None)\n",
    "    score = np.array(cross_val_score(reg, X=X_train20, y=y_train20).mean())\n",
    "    return  score\n",
    "\n",
    "winner_20 = GPGO(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity20, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_20.run(max_iter = max_iter, init_evals = n_init) # run\n",
    "\n",
    "### Return optimal parameters' set:\n",
    "params_winner_20 = winner_20.getResult()[0]\n",
    "params_winner_20['max_depth'] = int(params_winner_20['max_depth'])\n",
    "params_winner_20['min_child_weight'] = int(params_winner_20['min_child_weight'])\n",
    "\n",
    "### Re-train with optimal parameters, run predictons:\n",
    "dX_winner_train20 = xgb.DMatrix(X_train20, y_train20)\n",
    "dX_winner_test20 = xgb.DMatrix(X_test20, y_test20)\n",
    "model_winner_20 = xgb.train(params_winner_20, dX_winner_train20)\n",
    "pred_winner_20 = model_winner_20.predict(dX_winner_test20)\n",
    "\n",
    "rmse_winner_20 = np.sqrt(mean_squared_error(pred_winner_20, y_test20))\n",
    "rmse_winner_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.291132357728754, -4.966027453310725)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 1\n",
    "\n",
    "loser_output_1 = np.append(np.max(loser_1.GP.y[0:n_init]),loser_1.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_1 = np.append(np.max(winner_1.GP.y[0:n_init]),winner_1.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_1 = np.log(y_global_orig - loser_output_1)\n",
    "regret_winner_1 = np.log(y_global_orig - winner_output_1)\n",
    "\n",
    "train_regret_loser_1 = min_max_array(regret_loser_1)\n",
    "train_regret_winner_1 = min_max_array(regret_winner_1)\n",
    "\n",
    "min_train_regret_loser_1 = min(train_regret_loser_1)\n",
    "min_train_regret_winner_1 = min(train_regret_winner_1)\n",
    "\n",
    "min_train_regret_loser_1, min_train_regret_winner_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.080458324746212, -4.8942837405938295)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 2\n",
    "\n",
    "loser_output_2 = np.append(np.max(loser_2.GP.y[0:n_init]),loser_2.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_2 = np.append(np.max(winner_2.GP.y[0:n_init]),winner_2.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_2 = np.log(y_global_orig - loser_output_2)\n",
    "regret_winner_2 = np.log(y_global_orig - winner_output_2)\n",
    "\n",
    "train_regret_loser_2 = min_max_array(regret_loser_2)\n",
    "train_regret_winner_2 = min_max_array(regret_winner_2)\n",
    "\n",
    "min_train_regret_loser_2 = min(train_regret_loser_2)\n",
    "min_train_regret_winner_2 = min(train_regret_winner_2)\n",
    "\n",
    "min_train_regret_loser_2, min_train_regret_winner_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.2337089551039755, -5.392370599328206)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 3\n",
    "\n",
    "loser_output_3 = np.append(np.max(loser_3.GP.y[0:n_init]),loser_3.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_3 = np.append(np.max(winner_3.GP.y[0:n_init]),winner_3.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_3 = np.log(y_global_orig - loser_output_3)\n",
    "regret_winner_3 = np.log(y_global_orig - winner_output_3)\n",
    "\n",
    "train_regret_loser_3 = min_max_array(regret_loser_3)\n",
    "train_regret_winner_3 = min_max_array(regret_winner_3)\n",
    "\n",
    "min_train_regret_loser_3 = min(train_regret_loser_3)\n",
    "min_train_regret_winner_3 = min(train_regret_winner_3)\n",
    "\n",
    "min_train_regret_loser_3, min_train_regret_winner_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.685694576876569, -5.733494971552463)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 4\n",
    "\n",
    "loser_output_4 = np.append(np.max(loser_4.GP.y[0:n_init]),loser_4.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_4 = np.append(np.max(winner_4.GP.y[0:n_init]),winner_4.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_4 = np.log(y_global_orig - loser_output_4)\n",
    "regret_winner_4 = np.log(y_global_orig - winner_output_4)\n",
    "\n",
    "train_regret_loser_4 = min_max_array(regret_loser_4)\n",
    "train_regret_winner_4 = min_max_array(regret_winner_4)\n",
    "\n",
    "min_train_regret_loser_4 = min(train_regret_loser_4)\n",
    "min_train_regret_winner_4 = min(train_regret_winner_4)\n",
    "\n",
    "min_train_regret_loser_4, min_train_regret_winner_4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.261083345655753, -5.251875088191955)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 5\n",
    "\n",
    "loser_output_5 = np.append(np.max(loser_5.GP.y[0:n_init]),loser_5.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_5 = np.append(np.max(winner_5.GP.y[0:n_init]),winner_5.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_5 = np.log(y_global_orig - loser_output_5)\n",
    "regret_winner_5 = np.log(y_global_orig - winner_output_5)\n",
    "\n",
    "train_regret_loser_5 = min_max_array(regret_loser_5)\n",
    "train_regret_winner_5 = min_max_array(regret_winner_5)\n",
    "\n",
    "min_train_regret_loser_5 = min(train_regret_loser_5)\n",
    "min_train_regret_winner_5 = min(train_regret_winner_5)\n",
    "\n",
    "min_train_regret_loser_5, min_train_regret_winner_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.313302238528062, -5.326053701462592)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 6\n",
    "\n",
    "loser_output_6 = np.append(np.max(loser_6.GP.y[0:n_init]),loser_6.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_6 = np.append(np.max(winner_6.GP.y[0:n_init]),winner_6.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_6 = np.log(y_global_orig - loser_output_6)\n",
    "regret_winner_6 = np.log(y_global_orig - winner_output_6)\n",
    "\n",
    "train_regret_loser_6 = min_max_array(regret_loser_6)\n",
    "train_regret_winner_6 = min_max_array(regret_winner_6)\n",
    "\n",
    "min_train_regret_loser_6 = min(train_regret_loser_6)\n",
    "min_train_regret_winner_6 = min(train_regret_winner_6)\n",
    "\n",
    "min_train_regret_loser_6, min_train_regret_winner_6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.715847726815147, -5.763617674163383)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 7\n",
    "\n",
    "loser_output_7 = np.append(np.max(loser_7.GP.y[0:n_init]),loser_7.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_7 = np.append(np.max(winner_7.GP.y[0:n_init]),winner_7.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_7 = np.log(y_global_orig - loser_output_7)\n",
    "regret_winner_7 = np.log(y_global_orig - winner_output_7)\n",
    "\n",
    "train_regret_loser_7 = min_max_array(regret_loser_7)\n",
    "train_regret_winner_7 = min_max_array(regret_winner_7)\n",
    "\n",
    "min_train_regret_loser_7 = min(train_regret_loser_7)\n",
    "min_train_regret_winner_7 = min(train_regret_winner_7)\n",
    "\n",
    "min_train_regret_loser_7, min_train_regret_winner_7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.0983803459040615, -4.869591474775643)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 8\n",
    "\n",
    "loser_output_8 = np.append(np.max(loser_8.GP.y[0:n_init]),loser_8.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_8 = np.append(np.max(winner_8.GP.y[0:n_init]),winner_8.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_8 = np.log(y_global_orig - loser_output_8)\n",
    "regret_winner_8 = np.log(y_global_orig - winner_output_8)\n",
    "\n",
    "train_regret_loser_8 = min_max_array(regret_loser_8)\n",
    "train_regret_winner_8 = min_max_array(regret_winner_8)\n",
    "\n",
    "min_train_regret_loser_8 = min(train_regret_loser_8)\n",
    "min_train_regret_winner_8 = min(train_regret_winner_8)\n",
    "\n",
    "min_train_regret_loser_8, min_train_regret_winner_8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.552162901741616, -5.555882646817933)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 9\n",
    "\n",
    "loser_output_9 = np.append(np.max(loser_9.GP.y[0:n_init]),loser_9.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_9 = np.append(np.max(winner_9.GP.y[0:n_init]),winner_9.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_9 = np.log(y_global_orig - loser_output_9)\n",
    "regret_winner_9 = np.log(y_global_orig - winner_output_9)\n",
    "\n",
    "train_regret_loser_9 = min_max_array(regret_loser_9)\n",
    "train_regret_winner_9 = min_max_array(regret_winner_9)\n",
    "\n",
    "min_train_regret_loser_9 = min(train_regret_loser_9)\n",
    "min_train_regret_winner_9 = min(train_regret_winner_9)\n",
    "\n",
    "min_train_regret_loser_9, min_train_regret_winner_9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.090548830026827, -5.099166158593012)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 10\n",
    "\n",
    "loser_output_10 = np.append(np.max(loser_10.GP.y[0:n_init]),loser_10.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_10 = np.append(np.max(winner_10.GP.y[0:n_init]),winner_10.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_10 = np.log(y_global_orig - loser_output_10)\n",
    "regret_winner_10 = np.log(y_global_orig - winner_output_10)\n",
    "\n",
    "train_regret_loser_10 = min_max_array(regret_loser_10)\n",
    "train_regret_winner_10 = min_max_array(regret_winner_10)\n",
    "\n",
    "min_train_regret_loser_10 = min(train_regret_loser_10)\n",
    "min_train_regret_winner_10 = min(train_regret_winner_10)\n",
    "\n",
    "min_train_regret_loser_10, min_train_regret_winner_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.033692562698344, -5.086655471110898)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 11\n",
    "\n",
    "loser_output_11 = np.append(np.max(loser_11.GP.y[0:n_init]),loser_11.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_11 = np.append(np.max(winner_11.GP.y[0:n_init]),winner_11.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_11 = np.log(y_global_orig - loser_output_11)\n",
    "regret_winner_11 = np.log(y_global_orig - winner_output_11)\n",
    "\n",
    "train_regret_loser_11 = min_max_array(regret_loser_11)\n",
    "train_regret_winner_11 = min_max_array(regret_winner_11)\n",
    "\n",
    "min_train_regret_loser_11 = min(train_regret_loser_11)\n",
    "min_train_regret_winner_11 = min(train_regret_winner_11)\n",
    "\n",
    "min_train_regret_loser_11, min_train_regret_winner_11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.014714443886075, -5.122245858496792)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 12\n",
    "\n",
    "loser_output_12 = np.append(np.max(loser_12.GP.y[0:n_init]),loser_12.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_12 = np.append(np.max(winner_12.GP.y[0:n_init]),winner_12.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_12 = np.log(y_global_orig - loser_output_12)\n",
    "regret_winner_12 = np.log(y_global_orig - winner_output_12)\n",
    "\n",
    "train_regret_loser_12 = min_max_array(regret_loser_12)\n",
    "train_regret_winner_12 = min_max_array(regret_winner_12)\n",
    "\n",
    "min_train_regret_loser_12 = min(train_regret_loser_12)\n",
    "min_train_regret_winner_12 = min(train_regret_winner_12)\n",
    "\n",
    "min_train_regret_loser_12, min_train_regret_winner_12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.194869739346894, -5.088990079782527)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 13\n",
    "\n",
    "loser_output_13 = np.append(np.max(loser_13.GP.y[0:n_init]),loser_13.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_13 = np.append(np.max(winner_13.GP.y[0:n_init]),winner_13.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_13 = np.log(y_global_orig - loser_output_13)\n",
    "regret_winner_13 = np.log(y_global_orig - winner_output_13)\n",
    "\n",
    "train_regret_loser_13 = min_max_array(regret_loser_13)\n",
    "train_regret_winner_13 = min_max_array(regret_winner_13)\n",
    "\n",
    "min_train_regret_loser_13 = min(train_regret_loser_13)\n",
    "min_train_regret_winner_13 = min(train_regret_winner_13)\n",
    "\n",
    "min_train_regret_loser_13, min_train_regret_winner_13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.288276518234649, -5.166698113505033)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 14\n",
    "\n",
    "loser_output_14 = np.append(np.max(loser_14.GP.y[0:n_init]),loser_14.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_14 = np.append(np.max(winner_14.GP.y[0:n_init]),winner_14.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_14 = np.log(y_global_orig - loser_output_14)\n",
    "regret_winner_14 = np.log(y_global_orig - winner_output_14)\n",
    "\n",
    "train_regret_loser_14 = min_max_array(regret_loser_14)\n",
    "train_regret_winner_14 = min_max_array(regret_winner_14)\n",
    "\n",
    "min_train_regret_loser_14 = min(train_regret_loser_14)\n",
    "min_train_regret_winner_14 = min(train_regret_winner_14)\n",
    "\n",
    "min_train_regret_loser_14, min_train_regret_winner_14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.223856939399704, -5.239124715651585)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 15\n",
    "\n",
    "loser_output_15 = np.append(np.max(loser_15.GP.y[0:n_init]),loser_15.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_15 = np.append(np.max(winner_15.GP.y[0:n_init]),winner_15.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_15 = np.log(y_global_orig - loser_output_15)\n",
    "regret_winner_15 = np.log(y_global_orig - winner_output_15)\n",
    "\n",
    "train_regret_loser_15 = min_max_array(regret_loser_15)\n",
    "train_regret_winner_15 = min_max_array(regret_winner_15)\n",
    "\n",
    "min_train_regret_loser_15 = min(train_regret_loser_15)\n",
    "min_train_regret_winner_15 = min(train_regret_winner_15)\n",
    "\n",
    "min_train_regret_loser_15, min_train_regret_winner_15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.067417114524272, -5.2675803136322905)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 16\n",
    "\n",
    "loser_output_16 = np.append(np.max(loser_16.GP.y[0:n_init]),loser_16.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_16 = np.append(np.max(winner_16.GP.y[0:n_init]),winner_16.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_16 = np.log(y_global_orig - loser_output_16)\n",
    "regret_winner_16 = np.log(y_global_orig - winner_output_16)\n",
    "\n",
    "train_regret_loser_16 = min_max_array(regret_loser_16)\n",
    "train_regret_winner_16 = min_max_array(regret_winner_16)\n",
    "\n",
    "min_train_regret_loser_16 = min(train_regret_loser_16)\n",
    "min_train_regret_winner_16 = min(train_regret_winner_16)\n",
    "\n",
    "min_train_regret_loser_16, min_train_regret_winner_16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.1879668069170135, -5.157482575154998)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 17\n",
    "\n",
    "loser_output_17 = np.append(np.max(loser_17.GP.y[0:n_init]),loser_17.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_17 = np.append(np.max(winner_17.GP.y[0:n_init]),winner_17.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_17 = np.log(y_global_orig - loser_output_17)\n",
    "regret_winner_17 = np.log(y_global_orig - winner_output_17)\n",
    "\n",
    "train_regret_loser_17 = min_max_array(regret_loser_17)\n",
    "train_regret_winner_17 = min_max_array(regret_winner_17)\n",
    "\n",
    "min_train_regret_loser_17 = min(train_regret_loser_17)\n",
    "min_train_regret_winner_17 = min(train_regret_winner_17)\n",
    "\n",
    "min_train_regret_loser_17, min_train_regret_winner_17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.287326759541631, -5.062095355927372)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 18\n",
    "\n",
    "loser_output_18 = np.append(np.max(loser_18.GP.y[0:n_init]),loser_18.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_18 = np.append(np.max(winner_18.GP.y[0:n_init]),winner_18.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_18 = np.log(y_global_orig - loser_output_18)\n",
    "regret_winner_18 = np.log(y_global_orig - winner_output_18)\n",
    "\n",
    "train_regret_loser_18 = min_max_array(regret_loser_18)\n",
    "train_regret_winner_18 = min_max_array(regret_winner_18)\n",
    "\n",
    "min_train_regret_loser_18 = min(train_regret_loser_18)\n",
    "min_train_regret_winner_18 = min(train_regret_winner_18)\n",
    "\n",
    "min_train_regret_loser_18, min_train_regret_winner_18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.802594036057416, -5.854808129351306)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 19\n",
    "\n",
    "loser_output_19 = np.append(np.max(loser_19.GP.y[0:n_init]),loser_19.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_19 = np.append(np.max(winner_19.GP.y[0:n_init]),winner_19.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_19 = np.log(y_global_orig - loser_output_19)\n",
    "regret_winner_19 = np.log(y_global_orig - winner_output_19)\n",
    "\n",
    "train_regret_loser_19 = min_max_array(regret_loser_19)\n",
    "train_regret_winner_19 = min_max_array(regret_winner_19)\n",
    "\n",
    "min_train_regret_loser_19 = min(train_regret_loser_19)\n",
    "min_train_regret_winner_19 = min(train_regret_winner_19)\n",
    "\n",
    "min_train_regret_loser_19, min_train_regret_winner_19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.288275491318392, -5.256467225333817)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 20\n",
    "\n",
    "loser_output_20 = np.append(np.max(loser_20.GP.y[0:n_init]),loser_20.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_20 = np.append(np.max(winner_20.GP.y[0:n_init]),winner_20.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_20 = np.log(y_global_orig - loser_output_20)\n",
    "regret_winner_20 = np.log(y_global_orig - winner_output_20)\n",
    "\n",
    "train_regret_loser_20 = min_max_array(regret_loser_20)\n",
    "train_regret_winner_20 = min_max_array(regret_winner_20)\n",
    "\n",
    "min_train_regret_loser_20 = min(train_regret_loser_20)\n",
    "min_train_regret_winner_20 = min(train_regret_winner_20)\n",
    "\n",
    "min_train_regret_loser_20, min_train_regret_winner_20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration1 :\n",
    "\n",
    "slice1 = 0\n",
    "\n",
    "loser1 = [train_regret_loser_1[slice1],\n",
    "       train_regret_loser_2[slice1],\n",
    "       train_regret_loser_3[slice1],\n",
    "       train_regret_loser_4[slice1],\n",
    "       train_regret_loser_5[slice1],\n",
    "       train_regret_loser_6[slice1],\n",
    "       train_regret_loser_7[slice1],\n",
    "       train_regret_loser_8[slice1],\n",
    "       train_regret_loser_9[slice1],\n",
    "       train_regret_loser_10[slice1],\n",
    "       train_regret_loser_11[slice1],\n",
    "       train_regret_loser_12[slice1],\n",
    "       train_regret_loser_13[slice1],\n",
    "       train_regret_loser_14[slice1],\n",
    "       train_regret_loser_15[slice1],\n",
    "       train_regret_loser_16[slice1],\n",
    "       train_regret_loser_17[slice1],\n",
    "       train_regret_loser_18[slice1],\n",
    "       train_regret_loser_19[slice1],\n",
    "       train_regret_loser_20[slice1]]\n",
    "\n",
    "winner1 = [train_regret_winner_1[slice1],\n",
    "       train_regret_winner_2[slice1],\n",
    "       train_regret_winner_3[slice1],\n",
    "       train_regret_winner_4[slice1],\n",
    "       train_regret_winner_5[slice1],\n",
    "       train_regret_winner_6[slice1],\n",
    "       train_regret_winner_7[slice1],\n",
    "       train_regret_winner_8[slice1],\n",
    "       train_regret_winner_9[slice1],\n",
    "       train_regret_winner_10[slice1],\n",
    "       train_regret_winner_11[slice1],\n",
    "       train_regret_winner_12[slice1],\n",
    "       train_regret_winner_13[slice1],\n",
    "       train_regret_winner_14[slice1],\n",
    "       train_regret_winner_15[slice1],\n",
    "       train_regret_winner_16[slice1],\n",
    "       train_regret_winner_17[slice1],\n",
    "       train_regret_winner_18[slice1],\n",
    "       train_regret_winner_19[slice1],\n",
    "       train_regret_winner_20[slice1]]\n",
    "\n",
    "loser1_results = pd.DataFrame(loser1).sort_values(by=[0], ascending=False)\n",
    "winner1_results = pd.DataFrame(winner1).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser1 = np.asarray(loser1_results[4:5][0])[0]\n",
    "median_loser1 = np.asarray(loser1_results[9:10][0])[0]\n",
    "upper_loser1 = np.asarray(loser1_results[14:15][0])[0]\n",
    "\n",
    "lower_winner1 = np.asarray(winner1_results[4:5][0])[0]\n",
    "median_winner1 = np.asarray(winner1_results[9:10][0])[0]\n",
    "upper_winner1 = np.asarray(winner1_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration11 :\n",
    "\n",
    "slice11 = 10\n",
    "\n",
    "loser11 = [train_regret_loser_1[slice11],\n",
    "       train_regret_loser_2[slice11],\n",
    "       train_regret_loser_3[slice11],\n",
    "       train_regret_loser_4[slice11],\n",
    "       train_regret_loser_5[slice11],\n",
    "       train_regret_loser_6[slice11],\n",
    "       train_regret_loser_7[slice11],\n",
    "       train_regret_loser_8[slice11],\n",
    "       train_regret_loser_9[slice11],\n",
    "       train_regret_loser_10[slice11],\n",
    "       train_regret_loser_11[slice11],\n",
    "       train_regret_loser_12[slice11],\n",
    "       train_regret_loser_13[slice11],\n",
    "       train_regret_loser_14[slice11],\n",
    "       train_regret_loser_15[slice11],\n",
    "       train_regret_loser_16[slice11],\n",
    "       train_regret_loser_17[slice11],\n",
    "       train_regret_loser_18[slice11],\n",
    "       train_regret_loser_19[slice11],\n",
    "       train_regret_loser_20[slice11]]\n",
    "\n",
    "winner11 = [train_regret_winner_1[slice11],\n",
    "       train_regret_winner_2[slice11],\n",
    "       train_regret_winner_3[slice11],\n",
    "       train_regret_winner_4[slice11],\n",
    "       train_regret_winner_5[slice11],\n",
    "       train_regret_winner_6[slice11],\n",
    "       train_regret_winner_7[slice11],\n",
    "       train_regret_winner_8[slice11],\n",
    "       train_regret_winner_9[slice11],\n",
    "       train_regret_winner_10[slice11],\n",
    "       train_regret_winner_11[slice11],\n",
    "       train_regret_winner_12[slice11],\n",
    "       train_regret_winner_13[slice11],\n",
    "       train_regret_winner_14[slice11],\n",
    "       train_regret_winner_15[slice11],\n",
    "       train_regret_winner_16[slice11],\n",
    "       train_regret_winner_17[slice11],\n",
    "       train_regret_winner_18[slice11],\n",
    "       train_regret_winner_19[slice11],\n",
    "       train_regret_winner_20[slice11]]\n",
    "\n",
    "loser11_results = pd.DataFrame(loser11).sort_values(by=[0], ascending=False)\n",
    "winner11_results = pd.DataFrame(winner11).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser11 = np.asarray(loser11_results[4:5][0])[0]\n",
    "median_loser11 = np.asarray(loser11_results[9:10][0])[0]\n",
    "upper_loser11 = np.asarray(loser11_results[14:15][0])[0]\n",
    "\n",
    "lower_winner11 = np.asarray(winner11_results[4:5][0])[0]\n",
    "median_winner11 = np.asarray(winner11_results[9:10][0])[0]\n",
    "upper_winner11 = np.asarray(winner11_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration21 :\n",
    "\n",
    "slice21 = 20\n",
    "\n",
    "loser21 = [train_regret_loser_1[slice21],\n",
    "       train_regret_loser_2[slice21],\n",
    "       train_regret_loser_3[slice21],\n",
    "       train_regret_loser_4[slice21],\n",
    "       train_regret_loser_5[slice21],\n",
    "       train_regret_loser_6[slice21],\n",
    "       train_regret_loser_7[slice21],\n",
    "       train_regret_loser_8[slice21],\n",
    "       train_regret_loser_9[slice21],\n",
    "       train_regret_loser_10[slice21],\n",
    "       train_regret_loser_11[slice21],\n",
    "       train_regret_loser_12[slice21],\n",
    "       train_regret_loser_13[slice21],\n",
    "       train_regret_loser_14[slice21],\n",
    "       train_regret_loser_15[slice21],\n",
    "       train_regret_loser_16[slice21],\n",
    "       train_regret_loser_17[slice21],\n",
    "       train_regret_loser_18[slice21],\n",
    "       train_regret_loser_19[slice21],\n",
    "       train_regret_loser_20[slice21]]\n",
    "\n",
    "winner21 = [train_regret_winner_1[slice21],\n",
    "       train_regret_winner_2[slice21],\n",
    "       train_regret_winner_3[slice21],\n",
    "       train_regret_winner_4[slice21],\n",
    "       train_regret_winner_5[slice21],\n",
    "       train_regret_winner_6[slice21],\n",
    "       train_regret_winner_7[slice21],\n",
    "       train_regret_winner_8[slice21],\n",
    "       train_regret_winner_9[slice21],\n",
    "       train_regret_winner_10[slice21],\n",
    "       train_regret_winner_11[slice21],\n",
    "       train_regret_winner_12[slice21],\n",
    "       train_regret_winner_13[slice21],\n",
    "       train_regret_winner_14[slice21],\n",
    "       train_regret_winner_15[slice21],\n",
    "       train_regret_winner_16[slice21],\n",
    "       train_regret_winner_17[slice21],\n",
    "       train_regret_winner_18[slice21],\n",
    "       train_regret_winner_19[slice21],\n",
    "       train_regret_winner_20[slice21]]\n",
    "\n",
    "loser21_results = pd.DataFrame(loser21).sort_values(by=[0], ascending=False)\n",
    "winner21_results = pd.DataFrame(winner21).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser21 = np.asarray(loser21_results[4:5][0])[0]\n",
    "median_loser21 = np.asarray(loser21_results[9:10][0])[0]\n",
    "upper_loser21 = np.asarray(loser21_results[14:15][0])[0]\n",
    "\n",
    "lower_winner21 = np.asarray(winner21_results[4:5][0])[0]\n",
    "median_winner21 = np.asarray(winner21_results[9:10][0])[0]\n",
    "upper_winner21 = np.asarray(winner21_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration31 :\n",
    "\n",
    "slice31 = 30\n",
    "\n",
    "loser31 = [train_regret_loser_1[slice31],\n",
    "       train_regret_loser_2[slice31],\n",
    "       train_regret_loser_3[slice31],\n",
    "       train_regret_loser_4[slice31],\n",
    "       train_regret_loser_5[slice31],\n",
    "       train_regret_loser_6[slice31],\n",
    "       train_regret_loser_7[slice31],\n",
    "       train_regret_loser_8[slice31],\n",
    "       train_regret_loser_9[slice31],\n",
    "       train_regret_loser_10[slice31],\n",
    "       train_regret_loser_11[slice31],\n",
    "       train_regret_loser_12[slice31],\n",
    "       train_regret_loser_13[slice31],\n",
    "       train_regret_loser_14[slice31],\n",
    "       train_regret_loser_15[slice31],\n",
    "       train_regret_loser_16[slice31],\n",
    "       train_regret_loser_17[slice31],\n",
    "       train_regret_loser_18[slice31],\n",
    "       train_regret_loser_19[slice31],\n",
    "       train_regret_loser_20[slice31]]\n",
    "\n",
    "winner31 = [train_regret_winner_1[slice31],\n",
    "       train_regret_winner_2[slice31],\n",
    "       train_regret_winner_3[slice31],\n",
    "       train_regret_winner_4[slice31],\n",
    "       train_regret_winner_5[slice31],\n",
    "       train_regret_winner_6[slice31],\n",
    "       train_regret_winner_7[slice31],\n",
    "       train_regret_winner_8[slice31],\n",
    "       train_regret_winner_9[slice31],\n",
    "       train_regret_winner_10[slice31],\n",
    "       train_regret_winner_11[slice31],\n",
    "       train_regret_winner_12[slice31],\n",
    "       train_regret_winner_13[slice31],\n",
    "       train_regret_winner_14[slice31],\n",
    "       train_regret_winner_15[slice31],\n",
    "       train_regret_winner_16[slice31],\n",
    "       train_regret_winner_17[slice31],\n",
    "       train_regret_winner_18[slice31],\n",
    "       train_regret_winner_19[slice31],\n",
    "       train_regret_winner_20[slice31]]\n",
    "\n",
    "loser31_results = pd.DataFrame(loser31).sort_values(by=[0], ascending=False)\n",
    "winner31_results = pd.DataFrame(winner31).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser31 = np.asarray(loser31_results[4:5][0])[0]\n",
    "median_loser31 = np.asarray(loser31_results[9:10][0])[0]\n",
    "upper_loser31 = np.asarray(loser31_results[14:15][0])[0]\n",
    "\n",
    "lower_winner31 = np.asarray(winner31_results[4:5][0])[0]\n",
    "median_winner31 = np.asarray(winner31_results[9:10][0])[0]\n",
    "upper_winner31 = np.asarray(winner31_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration2 :\n",
    "\n",
    "slice2 = 1\n",
    "\n",
    "loser2 = [train_regret_loser_1[slice2],\n",
    "       train_regret_loser_2[slice2],\n",
    "       train_regret_loser_3[slice2],\n",
    "       train_regret_loser_4[slice2],\n",
    "       train_regret_loser_5[slice2],\n",
    "       train_regret_loser_6[slice2],\n",
    "       train_regret_loser_7[slice2],\n",
    "       train_regret_loser_8[slice2],\n",
    "       train_regret_loser_9[slice2],\n",
    "       train_regret_loser_10[slice2],\n",
    "       train_regret_loser_11[slice2],\n",
    "       train_regret_loser_12[slice2],\n",
    "       train_regret_loser_13[slice2],\n",
    "       train_regret_loser_14[slice2],\n",
    "       train_regret_loser_15[slice2],\n",
    "       train_regret_loser_16[slice2],\n",
    "       train_regret_loser_17[slice2],\n",
    "       train_regret_loser_18[slice2],\n",
    "       train_regret_loser_19[slice2],\n",
    "       train_regret_loser_20[slice2]]\n",
    "\n",
    "winner2 = [train_regret_winner_1[slice2],\n",
    "       train_regret_winner_2[slice2],\n",
    "       train_regret_winner_3[slice2],\n",
    "       train_regret_winner_4[slice2],\n",
    "       train_regret_winner_5[slice2],\n",
    "       train_regret_winner_6[slice2],\n",
    "       train_regret_winner_7[slice2],\n",
    "       train_regret_winner_8[slice2],\n",
    "       train_regret_winner_9[slice2],\n",
    "       train_regret_winner_10[slice2],\n",
    "       train_regret_winner_11[slice2],\n",
    "       train_regret_winner_12[slice2],\n",
    "       train_regret_winner_13[slice2],\n",
    "       train_regret_winner_14[slice2],\n",
    "       train_regret_winner_15[slice2],\n",
    "       train_regret_winner_16[slice2],\n",
    "       train_regret_winner_17[slice2],\n",
    "       train_regret_winner_18[slice2],\n",
    "       train_regret_winner_19[slice2],\n",
    "       train_regret_winner_20[slice2]]\n",
    "\n",
    "loser2_results = pd.DataFrame(loser2).sort_values(by=[0], ascending=False)\n",
    "winner2_results = pd.DataFrame(winner2).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser2 = np.asarray(loser2_results[4:5][0])[0]\n",
    "median_loser2 = np.asarray(loser2_results[9:10][0])[0]\n",
    "upper_loser2 = np.asarray(loser2_results[14:15][0])[0]\n",
    "\n",
    "lower_winner2 = np.asarray(winner2_results[4:5][0])[0]\n",
    "median_winner2 = np.asarray(winner2_results[9:10][0])[0]\n",
    "upper_winner2 = np.asarray(winner2_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration12 :\n",
    "\n",
    "slice12 = 11\n",
    "\n",
    "loser12 = [train_regret_loser_1[slice12],\n",
    "       train_regret_loser_2[slice12],\n",
    "       train_regret_loser_3[slice12],\n",
    "       train_regret_loser_4[slice12],\n",
    "       train_regret_loser_5[slice12],\n",
    "       train_regret_loser_6[slice12],\n",
    "       train_regret_loser_7[slice12],\n",
    "       train_regret_loser_8[slice12],\n",
    "       train_regret_loser_9[slice12],\n",
    "       train_regret_loser_10[slice12],\n",
    "       train_regret_loser_11[slice12],\n",
    "       train_regret_loser_12[slice12],\n",
    "       train_regret_loser_13[slice12],\n",
    "       train_regret_loser_14[slice12],\n",
    "       train_regret_loser_15[slice12],\n",
    "       train_regret_loser_16[slice12],\n",
    "       train_regret_loser_17[slice12],\n",
    "       train_regret_loser_18[slice12],\n",
    "       train_regret_loser_19[slice12],\n",
    "       train_regret_loser_20[slice12]]\n",
    "\n",
    "winner12 = [train_regret_winner_1[slice12],\n",
    "       train_regret_winner_2[slice12],\n",
    "       train_regret_winner_3[slice12],\n",
    "       train_regret_winner_4[slice12],\n",
    "       train_regret_winner_5[slice12],\n",
    "       train_regret_winner_6[slice12],\n",
    "       train_regret_winner_7[slice12],\n",
    "       train_regret_winner_8[slice12],\n",
    "       train_regret_winner_9[slice12],\n",
    "       train_regret_winner_10[slice12],\n",
    "       train_regret_winner_11[slice12],\n",
    "       train_regret_winner_12[slice12],\n",
    "       train_regret_winner_13[slice12],\n",
    "       train_regret_winner_14[slice12],\n",
    "       train_regret_winner_15[slice12],\n",
    "       train_regret_winner_16[slice12],\n",
    "       train_regret_winner_17[slice12],\n",
    "       train_regret_winner_18[slice12],\n",
    "       train_regret_winner_19[slice12],\n",
    "       train_regret_winner_20[slice12]]\n",
    "\n",
    "loser12_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\n",
    "winner12_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser12 = np.asarray(loser12_results[4:5][0])[0]\n",
    "median_loser12 = np.asarray(loser12_results[9:10][0])[0]\n",
    "upper_loser12 = np.asarray(loser12_results[14:15][0])[0]\n",
    "\n",
    "lower_winner12 = np.asarray(winner12_results[4:5][0])[0]\n",
    "median_winner12 = np.asarray(winner12_results[9:10][0])[0]\n",
    "upper_winner12 = np.asarray(winner12_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration22 :\n",
    "\n",
    "slice22 = 21\n",
    "\n",
    "loser22 = [train_regret_loser_1[slice22],\n",
    "       train_regret_loser_2[slice22],\n",
    "       train_regret_loser_3[slice22],\n",
    "       train_regret_loser_4[slice22],\n",
    "       train_regret_loser_5[slice22],\n",
    "       train_regret_loser_6[slice22],\n",
    "       train_regret_loser_7[slice22],\n",
    "       train_regret_loser_8[slice22],\n",
    "       train_regret_loser_9[slice22],\n",
    "       train_regret_loser_10[slice22],\n",
    "       train_regret_loser_11[slice22],\n",
    "       train_regret_loser_12[slice22],\n",
    "       train_regret_loser_13[slice22],\n",
    "       train_regret_loser_14[slice22],\n",
    "       train_regret_loser_15[slice22],\n",
    "       train_regret_loser_16[slice22],\n",
    "       train_regret_loser_17[slice22],\n",
    "       train_regret_loser_18[slice22],\n",
    "       train_regret_loser_19[slice22],\n",
    "       train_regret_loser_20[slice22]]\n",
    "\n",
    "winner22 = [train_regret_winner_1[slice22],\n",
    "       train_regret_winner_2[slice22],\n",
    "       train_regret_winner_3[slice22],\n",
    "       train_regret_winner_4[slice22],\n",
    "       train_regret_winner_5[slice22],\n",
    "       train_regret_winner_6[slice22],\n",
    "       train_regret_winner_7[slice22],\n",
    "       train_regret_winner_8[slice22],\n",
    "       train_regret_winner_9[slice22],\n",
    "       train_regret_winner_10[slice22],\n",
    "       train_regret_winner_11[slice22],\n",
    "       train_regret_winner_12[slice22],\n",
    "       train_regret_winner_13[slice22],\n",
    "       train_regret_winner_14[slice22],\n",
    "       train_regret_winner_15[slice22],\n",
    "       train_regret_winner_16[slice22],\n",
    "       train_regret_winner_17[slice22],\n",
    "       train_regret_winner_18[slice22],\n",
    "       train_regret_winner_19[slice22],\n",
    "       train_regret_winner_20[slice22]]\n",
    "\n",
    "loser22_results = pd.DataFrame(loser22).sort_values(by=[0], ascending=False)\n",
    "winner22_results = pd.DataFrame(winner22).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser22 = np.asarray(loser22_results[4:5][0])[0]\n",
    "median_loser22 = np.asarray(loser22_results[9:10][0])[0]\n",
    "upper_loser22 = np.asarray(loser22_results[14:15][0])[0]\n",
    "\n",
    "lower_winner22 = np.asarray(winner22_results[4:5][0])[0]\n",
    "median_winner22 = np.asarray(winner22_results[9:10][0])[0]\n",
    "upper_winner22 = np.asarray(winner22_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration3 :\n",
    "\n",
    "slice3 = 2\n",
    "\n",
    "loser3 = [train_regret_loser_1[slice3],\n",
    "       train_regret_loser_2[slice3],\n",
    "       train_regret_loser_3[slice3],\n",
    "       train_regret_loser_4[slice3],\n",
    "       train_regret_loser_5[slice3],\n",
    "       train_regret_loser_6[slice3],\n",
    "       train_regret_loser_7[slice3],\n",
    "       train_regret_loser_8[slice3],\n",
    "       train_regret_loser_9[slice3],\n",
    "       train_regret_loser_10[slice3],\n",
    "       train_regret_loser_11[slice3],\n",
    "       train_regret_loser_12[slice3],\n",
    "       train_regret_loser_13[slice3],\n",
    "       train_regret_loser_14[slice3],\n",
    "       train_regret_loser_15[slice3],\n",
    "       train_regret_loser_16[slice3],\n",
    "       train_regret_loser_17[slice3],\n",
    "       train_regret_loser_18[slice3],\n",
    "       train_regret_loser_19[slice3],\n",
    "       train_regret_loser_20[slice3]]\n",
    "\n",
    "winner3 = [train_regret_winner_1[slice3],\n",
    "       train_regret_winner_2[slice3],\n",
    "       train_regret_winner_3[slice3],\n",
    "       train_regret_winner_4[slice3],\n",
    "       train_regret_winner_5[slice3],\n",
    "       train_regret_winner_6[slice3],\n",
    "       train_regret_winner_7[slice3],\n",
    "       train_regret_winner_8[slice3],\n",
    "       train_regret_winner_9[slice3],\n",
    "       train_regret_winner_10[slice3],\n",
    "       train_regret_winner_11[slice3],\n",
    "       train_regret_winner_12[slice3],\n",
    "       train_regret_winner_13[slice3],\n",
    "       train_regret_winner_14[slice3],\n",
    "       train_regret_winner_15[slice3],\n",
    "       train_regret_winner_16[slice3],\n",
    "       train_regret_winner_17[slice3],\n",
    "       train_regret_winner_18[slice3],\n",
    "       train_regret_winner_19[slice3],\n",
    "       train_regret_winner_20[slice3]]\n",
    "\n",
    "loser3_results = pd.DataFrame(loser3).sort_values(by=[0], ascending=False)\n",
    "winner3_results = pd.DataFrame(winner3).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser3 = np.asarray(loser3_results[4:5][0])[0]\n",
    "median_loser3 = np.asarray(loser3_results[9:10][0])[0]\n",
    "upper_loser3 = np.asarray(loser3_results[14:15][0])[0]\n",
    "\n",
    "lower_winner3 = np.asarray(winner3_results[4:5][0])[0]\n",
    "median_winner3 = np.asarray(winner3_results[9:10][0])[0]\n",
    "upper_winner3 = np.asarray(winner3_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration13 :\n",
    "\n",
    "slice13 = 12\n",
    "\n",
    "loser13 = [train_regret_loser_1[slice13],\n",
    "       train_regret_loser_2[slice13],\n",
    "       train_regret_loser_3[slice13],\n",
    "       train_regret_loser_4[slice13],\n",
    "       train_regret_loser_5[slice13],\n",
    "       train_regret_loser_6[slice13],\n",
    "       train_regret_loser_7[slice13],\n",
    "       train_regret_loser_8[slice13],\n",
    "       train_regret_loser_9[slice13],\n",
    "       train_regret_loser_10[slice13],\n",
    "       train_regret_loser_11[slice13],\n",
    "       train_regret_loser_12[slice13],\n",
    "       train_regret_loser_13[slice13],\n",
    "       train_regret_loser_14[slice13],\n",
    "       train_regret_loser_15[slice13],\n",
    "       train_regret_loser_16[slice13],\n",
    "       train_regret_loser_17[slice13],\n",
    "       train_regret_loser_18[slice13],\n",
    "       train_regret_loser_19[slice13],\n",
    "       train_regret_loser_20[slice13]]\n",
    "\n",
    "winner13 = [train_regret_winner_1[slice13],\n",
    "       train_regret_winner_2[slice13],\n",
    "       train_regret_winner_3[slice13],\n",
    "       train_regret_winner_4[slice13],\n",
    "       train_regret_winner_5[slice13],\n",
    "       train_regret_winner_6[slice13],\n",
    "       train_regret_winner_7[slice13],\n",
    "       train_regret_winner_8[slice13],\n",
    "       train_regret_winner_9[slice13],\n",
    "       train_regret_winner_10[slice13],\n",
    "       train_regret_winner_11[slice13],\n",
    "       train_regret_winner_12[slice13],\n",
    "       train_regret_winner_13[slice13],\n",
    "       train_regret_winner_14[slice13],\n",
    "       train_regret_winner_15[slice13],\n",
    "       train_regret_winner_16[slice13],\n",
    "       train_regret_winner_17[slice13],\n",
    "       train_regret_winner_18[slice13],\n",
    "       train_regret_winner_19[slice13],\n",
    "       train_regret_winner_20[slice13]]\n",
    "\n",
    "loser13_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\n",
    "winner13_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser13 = np.asarray(loser13_results[4:5][0])[0]\n",
    "median_loser13 = np.asarray(loser13_results[9:10][0])[0]\n",
    "upper_loser13 = np.asarray(loser13_results[14:15][0])[0]\n",
    "\n",
    "lower_winner13 = np.asarray(winner13_results[4:5][0])[0]\n",
    "median_winner13 = np.asarray(winner13_results[9:10][0])[0]\n",
    "upper_winner13 = np.asarray(winner13_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration23 :\n",
    "\n",
    "slice23 = 22\n",
    "\n",
    "loser23 = [train_regret_loser_1[slice23],\n",
    "       train_regret_loser_2[slice23],\n",
    "       train_regret_loser_3[slice23],\n",
    "       train_regret_loser_4[slice23],\n",
    "       train_regret_loser_5[slice23],\n",
    "       train_regret_loser_6[slice23],\n",
    "       train_regret_loser_7[slice23],\n",
    "       train_regret_loser_8[slice23],\n",
    "       train_regret_loser_9[slice23],\n",
    "       train_regret_loser_10[slice23],\n",
    "       train_regret_loser_11[slice23],\n",
    "       train_regret_loser_12[slice23],\n",
    "       train_regret_loser_13[slice23],\n",
    "       train_regret_loser_14[slice23],\n",
    "       train_regret_loser_15[slice23],\n",
    "       train_regret_loser_16[slice23],\n",
    "       train_regret_loser_17[slice23],\n",
    "       train_regret_loser_18[slice23],\n",
    "       train_regret_loser_19[slice23],\n",
    "       train_regret_loser_20[slice23]]\n",
    "\n",
    "winner23 = [train_regret_winner_1[slice23],\n",
    "       train_regret_winner_2[slice23],\n",
    "       train_regret_winner_3[slice23],\n",
    "       train_regret_winner_4[slice23],\n",
    "       train_regret_winner_5[slice23],\n",
    "       train_regret_winner_6[slice23],\n",
    "       train_regret_winner_7[slice23],\n",
    "       train_regret_winner_8[slice23],\n",
    "       train_regret_winner_9[slice23],\n",
    "       train_regret_winner_10[slice23],\n",
    "       train_regret_winner_11[slice23],\n",
    "       train_regret_winner_12[slice23],\n",
    "       train_regret_winner_13[slice23],\n",
    "       train_regret_winner_14[slice23],\n",
    "       train_regret_winner_15[slice23],\n",
    "       train_regret_winner_16[slice23],\n",
    "       train_regret_winner_17[slice23],\n",
    "       train_regret_winner_18[slice23],\n",
    "       train_regret_winner_19[slice23],\n",
    "       train_regret_winner_20[slice23]]\n",
    "\n",
    "loser23_results = pd.DataFrame(loser23).sort_values(by=[0], ascending=False)\n",
    "winner23_results = pd.DataFrame(winner23).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser23 = np.asarray(loser23_results[4:5][0])[0]\n",
    "median_loser23 = np.asarray(loser23_results[9:10][0])[0]\n",
    "upper_loser23 = np.asarray(loser23_results[14:15][0])[0]\n",
    "\n",
    "lower_winner23 = np.asarray(winner23_results[4:5][0])[0]\n",
    "median_winner23 = np.asarray(winner23_results[9:10][0])[0]\n",
    "upper_winner23 = np.asarray(winner23_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration4 :\n",
    "\n",
    "slice4 = 3\n",
    "\n",
    "loser4 = [train_regret_loser_1[slice4],\n",
    "       train_regret_loser_2[slice4],\n",
    "       train_regret_loser_3[slice4],\n",
    "       train_regret_loser_4[slice4],\n",
    "       train_regret_loser_5[slice4],\n",
    "       train_regret_loser_6[slice4],\n",
    "       train_regret_loser_7[slice4],\n",
    "       train_regret_loser_8[slice4],\n",
    "       train_regret_loser_9[slice4],\n",
    "       train_regret_loser_10[slice4],\n",
    "       train_regret_loser_11[slice4],\n",
    "       train_regret_loser_12[slice4],\n",
    "       train_regret_loser_13[slice4],\n",
    "       train_regret_loser_14[slice4],\n",
    "       train_regret_loser_15[slice4],\n",
    "       train_regret_loser_16[slice4],\n",
    "       train_regret_loser_17[slice4],\n",
    "       train_regret_loser_18[slice4],\n",
    "       train_regret_loser_19[slice4],\n",
    "       train_regret_loser_20[slice4]]\n",
    "\n",
    "winner4 = [train_regret_winner_1[slice4],\n",
    "       train_regret_winner_2[slice4],\n",
    "       train_regret_winner_3[slice4],\n",
    "       train_regret_winner_4[slice4],\n",
    "       train_regret_winner_5[slice4],\n",
    "       train_regret_winner_6[slice4],\n",
    "       train_regret_winner_7[slice4],\n",
    "       train_regret_winner_8[slice4],\n",
    "       train_regret_winner_9[slice4],\n",
    "       train_regret_winner_10[slice4],\n",
    "       train_regret_winner_11[slice4],\n",
    "       train_regret_winner_12[slice4],\n",
    "       train_regret_winner_13[slice4],\n",
    "       train_regret_winner_14[slice4],\n",
    "       train_regret_winner_15[slice4],\n",
    "       train_regret_winner_16[slice4],\n",
    "       train_regret_winner_17[slice4],\n",
    "       train_regret_winner_18[slice4],\n",
    "       train_regret_winner_19[slice4],\n",
    "       train_regret_winner_20[slice4]]\n",
    "\n",
    "loser4_results = pd.DataFrame(loser4).sort_values(by=[0], ascending=False)\n",
    "winner4_results = pd.DataFrame(winner4).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser4 = np.asarray(loser4_results[4:5][0])[0]\n",
    "median_loser4 = np.asarray(loser4_results[9:10][0])[0]\n",
    "upper_loser4 = np.asarray(loser4_results[14:15][0])[0]\n",
    "\n",
    "lower_winner4 = np.asarray(winner4_results[4:5][0])[0]\n",
    "median_winner4 = np.asarray(winner4_results[9:10][0])[0]\n",
    "upper_winner4 = np.asarray(winner4_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration14 :\n",
    "\n",
    "slice14 = 13\n",
    "\n",
    "loser14 = [train_regret_loser_1[slice14],\n",
    "       train_regret_loser_2[slice14],\n",
    "       train_regret_loser_3[slice14],\n",
    "       train_regret_loser_4[slice14],\n",
    "       train_regret_loser_5[slice14],\n",
    "       train_regret_loser_6[slice14],\n",
    "       train_regret_loser_7[slice14],\n",
    "       train_regret_loser_8[slice14],\n",
    "       train_regret_loser_9[slice14],\n",
    "       train_regret_loser_10[slice14],\n",
    "       train_regret_loser_11[slice14],\n",
    "       train_regret_loser_12[slice14],\n",
    "       train_regret_loser_13[slice14],\n",
    "       train_regret_loser_14[slice14],\n",
    "       train_regret_loser_15[slice14],\n",
    "       train_regret_loser_16[slice14],\n",
    "       train_regret_loser_17[slice14],\n",
    "       train_regret_loser_18[slice14],\n",
    "       train_regret_loser_19[slice14],\n",
    "       train_regret_loser_20[slice14]]\n",
    "\n",
    "winner14 = [train_regret_winner_1[slice14],\n",
    "       train_regret_winner_2[slice14],\n",
    "       train_regret_winner_3[slice14],\n",
    "       train_regret_winner_4[slice14],\n",
    "       train_regret_winner_5[slice14],\n",
    "       train_regret_winner_6[slice14],\n",
    "       train_regret_winner_7[slice14],\n",
    "       train_regret_winner_8[slice14],\n",
    "       train_regret_winner_9[slice14],\n",
    "       train_regret_winner_10[slice14],\n",
    "       train_regret_winner_11[slice14],\n",
    "       train_regret_winner_12[slice14],\n",
    "       train_regret_winner_13[slice14],\n",
    "       train_regret_winner_14[slice14],\n",
    "       train_regret_winner_15[slice14],\n",
    "       train_regret_winner_16[slice14],\n",
    "       train_regret_winner_17[slice14],\n",
    "       train_regret_winner_18[slice14],\n",
    "       train_regret_winner_19[slice14],\n",
    "       train_regret_winner_20[slice14]]\n",
    "\n",
    "loser14_results = pd.DataFrame(loser14).sort_values(by=[0], ascending=False)\n",
    "winner14_results = pd.DataFrame(winner14).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser14 = np.asarray(loser14_results[4:5][0])[0]\n",
    "median_loser14 = np.asarray(loser14_results[9:10][0])[0]\n",
    "upper_loser14 = np.asarray(loser14_results[14:15][0])[0]\n",
    "\n",
    "lower_winner14 = np.asarray(winner14_results[4:5][0])[0]\n",
    "median_winner14 = np.asarray(winner14_results[9:10][0])[0]\n",
    "upper_winner14 = np.asarray(winner14_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration24 :\n",
    "\n",
    "slice24 = 23\n",
    "\n",
    "loser24 = [train_regret_loser_1[slice24],\n",
    "       train_regret_loser_2[slice24],\n",
    "       train_regret_loser_3[slice24],\n",
    "       train_regret_loser_4[slice24],\n",
    "       train_regret_loser_5[slice24],\n",
    "       train_regret_loser_6[slice24],\n",
    "       train_regret_loser_7[slice24],\n",
    "       train_regret_loser_8[slice24],\n",
    "       train_regret_loser_9[slice24],\n",
    "       train_regret_loser_10[slice24],\n",
    "       train_regret_loser_11[slice24],\n",
    "       train_regret_loser_12[slice24],\n",
    "       train_regret_loser_13[slice24],\n",
    "       train_regret_loser_14[slice24],\n",
    "       train_regret_loser_15[slice24],\n",
    "       train_regret_loser_16[slice24],\n",
    "       train_regret_loser_17[slice24],\n",
    "       train_regret_loser_18[slice24],\n",
    "       train_regret_loser_19[slice24],\n",
    "       train_regret_loser_20[slice24]]\n",
    "\n",
    "winner24 = [train_regret_winner_1[slice24],\n",
    "       train_regret_winner_2[slice24],\n",
    "       train_regret_winner_3[slice24],\n",
    "       train_regret_winner_4[slice24],\n",
    "       train_regret_winner_5[slice24],\n",
    "       train_regret_winner_6[slice24],\n",
    "       train_regret_winner_7[slice24],\n",
    "       train_regret_winner_8[slice24],\n",
    "       train_regret_winner_9[slice24],\n",
    "       train_regret_winner_10[slice24],\n",
    "       train_regret_winner_11[slice24],\n",
    "       train_regret_winner_12[slice24],\n",
    "       train_regret_winner_13[slice24],\n",
    "       train_regret_winner_14[slice24],\n",
    "       train_regret_winner_15[slice24],\n",
    "       train_regret_winner_16[slice24],\n",
    "       train_regret_winner_17[slice24],\n",
    "       train_regret_winner_18[slice24],\n",
    "       train_regret_winner_19[slice24],\n",
    "       train_regret_winner_20[slice24]]\n",
    "\n",
    "loser24_results = pd.DataFrame(loser24).sort_values(by=[0], ascending=False)\n",
    "winner24_results = pd.DataFrame(winner24).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser24 = np.asarray(loser24_results[4:5][0])[0]\n",
    "median_loser24 = np.asarray(loser24_results[9:10][0])[0]\n",
    "upper_loser24 = np.asarray(loser24_results[14:15][0])[0]\n",
    "\n",
    "lower_winner24 = np.asarray(winner24_results[4:5][0])[0]\n",
    "median_winner24 = np.asarray(winner24_results[9:10][0])[0]\n",
    "upper_winner24 = np.asarray(winner24_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration5 :\n",
    "\n",
    "slice5 = 4\n",
    "\n",
    "loser5 = [train_regret_loser_1[slice5],\n",
    "       train_regret_loser_2[slice5],\n",
    "       train_regret_loser_3[slice5],\n",
    "       train_regret_loser_4[slice5],\n",
    "       train_regret_loser_5[slice5],\n",
    "       train_regret_loser_6[slice5],\n",
    "       train_regret_loser_7[slice5],\n",
    "       train_regret_loser_8[slice5],\n",
    "       train_regret_loser_9[slice5],\n",
    "       train_regret_loser_10[slice5],\n",
    "       train_regret_loser_11[slice5],\n",
    "       train_regret_loser_12[slice5],\n",
    "       train_regret_loser_13[slice5],\n",
    "       train_regret_loser_14[slice5],\n",
    "       train_regret_loser_15[slice5],\n",
    "       train_regret_loser_16[slice5],\n",
    "       train_regret_loser_17[slice5],\n",
    "       train_regret_loser_18[slice5],\n",
    "       train_regret_loser_19[slice5],\n",
    "       train_regret_loser_20[slice5]]\n",
    "\n",
    "winner5 = [train_regret_winner_1[slice5],\n",
    "       train_regret_winner_2[slice5],\n",
    "       train_regret_winner_3[slice5],\n",
    "       train_regret_winner_4[slice5],\n",
    "       train_regret_winner_5[slice5],\n",
    "       train_regret_winner_6[slice5],\n",
    "       train_regret_winner_7[slice5],\n",
    "       train_regret_winner_8[slice5],\n",
    "       train_regret_winner_9[slice5],\n",
    "       train_regret_winner_10[slice5],\n",
    "       train_regret_winner_11[slice5],\n",
    "       train_regret_winner_12[slice5],\n",
    "       train_regret_winner_13[slice5],\n",
    "       train_regret_winner_14[slice5],\n",
    "       train_regret_winner_15[slice5],\n",
    "       train_regret_winner_16[slice5],\n",
    "       train_regret_winner_17[slice5],\n",
    "       train_regret_winner_18[slice5],\n",
    "       train_regret_winner_19[slice5],\n",
    "       train_regret_winner_20[slice5]]\n",
    "\n",
    "loser5_results = pd.DataFrame(loser5).sort_values(by=[0], ascending=False)\n",
    "winner5_results = pd.DataFrame(winner5).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser5 = np.asarray(loser5_results[4:5][0])[0]\n",
    "median_loser5 = np.asarray(loser5_results[9:10][0])[0]\n",
    "upper_loser5 = np.asarray(loser5_results[14:15][0])[0]\n",
    "\n",
    "lower_winner5 = np.asarray(winner5_results[4:5][0])[0]\n",
    "median_winner5 = np.asarray(winner5_results[9:10][0])[0]\n",
    "upper_winner5 = np.asarray(winner5_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration15 :\n",
    "\n",
    "slice15 = 14\n",
    "\n",
    "loser15 = [train_regret_loser_1[slice15],\n",
    "       train_regret_loser_2[slice15],\n",
    "       train_regret_loser_3[slice15],\n",
    "       train_regret_loser_4[slice15],\n",
    "       train_regret_loser_5[slice15],\n",
    "       train_regret_loser_6[slice15],\n",
    "       train_regret_loser_7[slice15],\n",
    "       train_regret_loser_8[slice15],\n",
    "       train_regret_loser_9[slice15],\n",
    "       train_regret_loser_10[slice15],\n",
    "       train_regret_loser_11[slice15],\n",
    "       train_regret_loser_12[slice15],\n",
    "       train_regret_loser_13[slice15],\n",
    "       train_regret_loser_14[slice15],\n",
    "       train_regret_loser_15[slice15],\n",
    "       train_regret_loser_16[slice15],\n",
    "       train_regret_loser_17[slice15],\n",
    "       train_regret_loser_18[slice15],\n",
    "       train_regret_loser_19[slice15],\n",
    "       train_regret_loser_20[slice15]]\n",
    "\n",
    "winner15 = [train_regret_winner_1[slice15],\n",
    "       train_regret_winner_2[slice15],\n",
    "       train_regret_winner_3[slice15],\n",
    "       train_regret_winner_4[slice15],\n",
    "       train_regret_winner_5[slice15],\n",
    "       train_regret_winner_6[slice15],\n",
    "       train_regret_winner_7[slice15],\n",
    "       train_regret_winner_8[slice15],\n",
    "       train_regret_winner_9[slice15],\n",
    "       train_regret_winner_10[slice15],\n",
    "       train_regret_winner_11[slice15],\n",
    "       train_regret_winner_12[slice15],\n",
    "       train_regret_winner_13[slice15],\n",
    "       train_regret_winner_14[slice15],\n",
    "       train_regret_winner_15[slice15],\n",
    "       train_regret_winner_16[slice15],\n",
    "       train_regret_winner_17[slice15],\n",
    "       train_regret_winner_18[slice15],\n",
    "       train_regret_winner_19[slice15],\n",
    "       train_regret_winner_20[slice15]]\n",
    "\n",
    "loser15_results = pd.DataFrame(loser15).sort_values(by=[0], ascending=False)\n",
    "winner15_results = pd.DataFrame(winner15).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser15 = np.asarray(loser15_results[4:5][0])[0]\n",
    "median_loser15 = np.asarray(loser15_results[9:10][0])[0]\n",
    "upper_loser15 = np.asarray(loser15_results[14:15][0])[0]\n",
    "\n",
    "lower_winner15 = np.asarray(winner15_results[4:5][0])[0]\n",
    "median_winner15 = np.asarray(winner15_results[9:10][0])[0]\n",
    "upper_winner15 = np.asarray(winner15_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration25 :\n",
    "\n",
    "slice25 = 24\n",
    "\n",
    "loser25 = [train_regret_loser_1[slice25],\n",
    "       train_regret_loser_2[slice25],\n",
    "       train_regret_loser_3[slice25],\n",
    "       train_regret_loser_4[slice25],\n",
    "       train_regret_loser_5[slice25],\n",
    "       train_regret_loser_6[slice25],\n",
    "       train_regret_loser_7[slice25],\n",
    "       train_regret_loser_8[slice25],\n",
    "       train_regret_loser_9[slice25],\n",
    "       train_regret_loser_10[slice25],\n",
    "       train_regret_loser_11[slice25],\n",
    "       train_regret_loser_12[slice25],\n",
    "       train_regret_loser_13[slice25],\n",
    "       train_regret_loser_14[slice25],\n",
    "       train_regret_loser_15[slice25],\n",
    "       train_regret_loser_16[slice25],\n",
    "       train_regret_loser_17[slice25],\n",
    "       train_regret_loser_18[slice25],\n",
    "       train_regret_loser_19[slice25],\n",
    "       train_regret_loser_20[slice25]]\n",
    "\n",
    "winner25 = [train_regret_winner_1[slice25],\n",
    "       train_regret_winner_2[slice25],\n",
    "       train_regret_winner_3[slice25],\n",
    "       train_regret_winner_4[slice25],\n",
    "       train_regret_winner_5[slice25],\n",
    "       train_regret_winner_6[slice25],\n",
    "       train_regret_winner_7[slice25],\n",
    "       train_regret_winner_8[slice25],\n",
    "       train_regret_winner_9[slice25],\n",
    "       train_regret_winner_10[slice25],\n",
    "       train_regret_winner_11[slice25],\n",
    "       train_regret_winner_12[slice25],\n",
    "       train_regret_winner_13[slice25],\n",
    "       train_regret_winner_14[slice25],\n",
    "       train_regret_winner_15[slice25],\n",
    "       train_regret_winner_16[slice25],\n",
    "       train_regret_winner_17[slice25],\n",
    "       train_regret_winner_18[slice25],\n",
    "       train_regret_winner_19[slice25],\n",
    "       train_regret_winner_20[slice25]]\n",
    "\n",
    "loser25_results = pd.DataFrame(loser25).sort_values(by=[0], ascending=False)\n",
    "winner25_results = pd.DataFrame(winner25).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser25 = np.asarray(loser25_results[4:5][0])[0]\n",
    "median_loser25 = np.asarray(loser25_results[9:10][0])[0]\n",
    "upper_loser25 = np.asarray(loser25_results[14:15][0])[0]\n",
    "\n",
    "lower_winner25 = np.asarray(winner25_results[4:5][0])[0]\n",
    "median_winner25 = np.asarray(winner25_results[9:10][0])[0]\n",
    "upper_winner25= np.asarray(winner25_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration6 :\n",
    "\n",
    "slice6 = 5\n",
    "\n",
    "loser6 = [train_regret_loser_1[slice6],\n",
    "       train_regret_loser_2[slice6],\n",
    "       train_regret_loser_3[slice6],\n",
    "       train_regret_loser_4[slice6],\n",
    "       train_regret_loser_5[slice6],\n",
    "       train_regret_loser_6[slice6],\n",
    "       train_regret_loser_7[slice6],\n",
    "       train_regret_loser_8[slice6],\n",
    "       train_regret_loser_9[slice6],\n",
    "       train_regret_loser_10[slice6],\n",
    "       train_regret_loser_11[slice6],\n",
    "       train_regret_loser_12[slice6],\n",
    "       train_regret_loser_13[slice6],\n",
    "       train_regret_loser_14[slice6],\n",
    "       train_regret_loser_15[slice6],\n",
    "       train_regret_loser_16[slice6],\n",
    "       train_regret_loser_17[slice6],\n",
    "       train_regret_loser_18[slice6],\n",
    "       train_regret_loser_19[slice6],\n",
    "       train_regret_loser_20[slice6]]\n",
    "\n",
    "winner6 = [train_regret_winner_1[slice6],\n",
    "       train_regret_winner_2[slice6],\n",
    "       train_regret_winner_3[slice6],\n",
    "       train_regret_winner_4[slice6],\n",
    "       train_regret_winner_5[slice6],\n",
    "       train_regret_winner_6[slice6],\n",
    "       train_regret_winner_7[slice6],\n",
    "       train_regret_winner_8[slice6],\n",
    "       train_regret_winner_9[slice6],\n",
    "       train_regret_winner_10[slice6],\n",
    "       train_regret_winner_11[slice6],\n",
    "       train_regret_winner_12[slice6],\n",
    "       train_regret_winner_13[slice6],\n",
    "       train_regret_winner_14[slice6],\n",
    "       train_regret_winner_15[slice6],\n",
    "       train_regret_winner_16[slice6],\n",
    "       train_regret_winner_17[slice6],\n",
    "       train_regret_winner_18[slice6],\n",
    "       train_regret_winner_19[slice6],\n",
    "       train_regret_winner_20[slice6]]\n",
    "\n",
    "loser6_results = pd.DataFrame(loser6).sort_values(by=[0], ascending=False)\n",
    "winner6_results = pd.DataFrame(winner6).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser6 = np.asarray(loser6_results[4:5][0])[0]\n",
    "median_loser6 = np.asarray(loser6_results[9:10][0])[0]\n",
    "upper_loser6 = np.asarray(loser6_results[14:15][0])[0]\n",
    "\n",
    "lower_winner6 = np.asarray(winner6_results[4:5][0])[0]\n",
    "median_winner6 = np.asarray(winner6_results[9:10][0])[0]\n",
    "upper_winner6 = np.asarray(winner6_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration16 :\n",
    "\n",
    "slice16 = 15\n",
    "\n",
    "loser16 = [train_regret_loser_1[slice16],\n",
    "       train_regret_loser_2[slice16],\n",
    "       train_regret_loser_3[slice16],\n",
    "       train_regret_loser_4[slice16],\n",
    "       train_regret_loser_5[slice16],\n",
    "       train_regret_loser_6[slice16],\n",
    "       train_regret_loser_7[slice16],\n",
    "       train_regret_loser_8[slice16],\n",
    "       train_regret_loser_9[slice16],\n",
    "       train_regret_loser_10[slice16],\n",
    "       train_regret_loser_11[slice16],\n",
    "       train_regret_loser_12[slice16],\n",
    "       train_regret_loser_13[slice16],\n",
    "       train_regret_loser_14[slice16],\n",
    "       train_regret_loser_15[slice16],\n",
    "       train_regret_loser_16[slice16],\n",
    "       train_regret_loser_17[slice16],\n",
    "       train_regret_loser_18[slice16],\n",
    "       train_regret_loser_19[slice16],\n",
    "       train_regret_loser_20[slice16]]\n",
    "\n",
    "winner16 = [train_regret_winner_1[slice16],\n",
    "       train_regret_winner_2[slice16],\n",
    "       train_regret_winner_3[slice16],\n",
    "       train_regret_winner_4[slice16],\n",
    "       train_regret_winner_5[slice16],\n",
    "       train_regret_winner_6[slice16],\n",
    "       train_regret_winner_7[slice16],\n",
    "       train_regret_winner_8[slice16],\n",
    "       train_regret_winner_9[slice16],\n",
    "       train_regret_winner_10[slice16],\n",
    "       train_regret_winner_11[slice16],\n",
    "       train_regret_winner_12[slice16],\n",
    "       train_regret_winner_13[slice16],\n",
    "       train_regret_winner_14[slice16],\n",
    "       train_regret_winner_15[slice16],\n",
    "       train_regret_winner_16[slice16],\n",
    "       train_regret_winner_17[slice16],\n",
    "       train_regret_winner_18[slice16],\n",
    "       train_regret_winner_19[slice16],\n",
    "       train_regret_winner_20[slice16]]\n",
    "\n",
    "loser16_results = pd.DataFrame(loser16).sort_values(by=[0], ascending=False)\n",
    "winner16_results = pd.DataFrame(winner16).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser16 = np.asarray(loser16_results[4:5][0])[0]\n",
    "median_loser16 = np.asarray(loser16_results[9:10][0])[0]\n",
    "upper_loser16 = np.asarray(loser16_results[14:15][0])[0]\n",
    "\n",
    "lower_winner16 = np.asarray(winner16_results[4:5][0])[0]\n",
    "median_winner16 = np.asarray(winner16_results[9:10][0])[0]\n",
    "upper_winner16 = np.asarray(winner16_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration26 :\n",
    "\n",
    "slice26 = 25\n",
    "\n",
    "loser26 = [train_regret_loser_1[slice26],\n",
    "       train_regret_loser_2[slice26],\n",
    "       train_regret_loser_3[slice26],\n",
    "       train_regret_loser_4[slice26],\n",
    "       train_regret_loser_5[slice26],\n",
    "       train_regret_loser_6[slice26],\n",
    "       train_regret_loser_7[slice26],\n",
    "       train_regret_loser_8[slice26],\n",
    "       train_regret_loser_9[slice26],\n",
    "       train_regret_loser_10[slice26],\n",
    "       train_regret_loser_11[slice26],\n",
    "       train_regret_loser_12[slice26],\n",
    "       train_regret_loser_13[slice26],\n",
    "       train_regret_loser_14[slice26],\n",
    "       train_regret_loser_15[slice26],\n",
    "       train_regret_loser_16[slice26],\n",
    "       train_regret_loser_17[slice26],\n",
    "       train_regret_loser_18[slice26],\n",
    "       train_regret_loser_19[slice26],\n",
    "       train_regret_loser_20[slice26]]\n",
    "\n",
    "winner26 = [train_regret_winner_1[slice26],\n",
    "       train_regret_winner_2[slice26],\n",
    "       train_regret_winner_3[slice26],\n",
    "       train_regret_winner_4[slice26],\n",
    "       train_regret_winner_5[slice26],\n",
    "       train_regret_winner_6[slice26],\n",
    "       train_regret_winner_7[slice26],\n",
    "       train_regret_winner_8[slice26],\n",
    "       train_regret_winner_9[slice26],\n",
    "       train_regret_winner_10[slice26],\n",
    "       train_regret_winner_11[slice26],\n",
    "       train_regret_winner_12[slice26],\n",
    "       train_regret_winner_13[slice26],\n",
    "       train_regret_winner_14[slice26],\n",
    "       train_regret_winner_15[slice26],\n",
    "       train_regret_winner_16[slice26],\n",
    "       train_regret_winner_17[slice26],\n",
    "       train_regret_winner_18[slice26],\n",
    "       train_regret_winner_19[slice26],\n",
    "       train_regret_winner_20[slice26]]\n",
    "\n",
    "loser26_results = pd.DataFrame(loser26).sort_values(by=[0], ascending=False)\n",
    "winner26_results = pd.DataFrame(winner26).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser26 = np.asarray(loser26_results[4:5][0])[0]\n",
    "median_loser26 = np.asarray(loser26_results[9:10][0])[0]\n",
    "upper_loser26 = np.asarray(loser26_results[14:15][0])[0]\n",
    "\n",
    "lower_winner26 = np.asarray(winner26_results[4:5][0])[0]\n",
    "median_winner26 = np.asarray(winner26_results[9:10][0])[0]\n",
    "upper_winner26 = np.asarray(winner26_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration7 :\n",
    "\n",
    "slice7 = 6\n",
    "\n",
    "loser7 = [train_regret_loser_1[slice7],\n",
    "       train_regret_loser_2[slice7],\n",
    "       train_regret_loser_3[slice7],\n",
    "       train_regret_loser_4[slice7],\n",
    "       train_regret_loser_5[slice7],\n",
    "       train_regret_loser_6[slice7],\n",
    "       train_regret_loser_7[slice7],\n",
    "       train_regret_loser_8[slice7],\n",
    "       train_regret_loser_9[slice7],\n",
    "       train_regret_loser_10[slice7],\n",
    "       train_regret_loser_11[slice7],\n",
    "       train_regret_loser_12[slice7],\n",
    "       train_regret_loser_13[slice7],\n",
    "       train_regret_loser_14[slice7],\n",
    "       train_regret_loser_15[slice7],\n",
    "       train_regret_loser_16[slice7],\n",
    "       train_regret_loser_17[slice7],\n",
    "       train_regret_loser_18[slice7],\n",
    "       train_regret_loser_19[slice7],\n",
    "       train_regret_loser_20[slice7]]\n",
    "\n",
    "winner7 = [train_regret_winner_1[slice7],\n",
    "       train_regret_winner_2[slice7],\n",
    "       train_regret_winner_3[slice7],\n",
    "       train_regret_winner_4[slice7],\n",
    "       train_regret_winner_5[slice7],\n",
    "       train_regret_winner_6[slice7],\n",
    "       train_regret_winner_7[slice7],\n",
    "       train_regret_winner_8[slice7],\n",
    "       train_regret_winner_9[slice7],\n",
    "       train_regret_winner_10[slice7],\n",
    "       train_regret_winner_11[slice7],\n",
    "       train_regret_winner_12[slice7],\n",
    "       train_regret_winner_13[slice7],\n",
    "       train_regret_winner_14[slice7],\n",
    "       train_regret_winner_15[slice7],\n",
    "       train_regret_winner_16[slice7],\n",
    "       train_regret_winner_17[slice7],\n",
    "       train_regret_winner_18[slice7],\n",
    "       train_regret_winner_19[slice7],\n",
    "       train_regret_winner_20[slice7]]\n",
    "\n",
    "loser7_results = pd.DataFrame(loser7).sort_values(by=[0], ascending=False)\n",
    "winner7_results = pd.DataFrame(winner7).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser7 = np.asarray(loser7_results[4:5][0])[0]\n",
    "median_loser7 = np.asarray(loser7_results[9:10][0])[0]\n",
    "upper_loser7 = np.asarray(loser7_results[14:15][0])[0]\n",
    "\n",
    "lower_winner7 = np.asarray(winner7_results[4:5][0])[0]\n",
    "median_winner7 = np.asarray(winner7_results[9:10][0])[0]\n",
    "upper_winner7 = np.asarray(winner7_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration17 :\n",
    "\n",
    "slice17 = 16\n",
    "\n",
    "loser17 = [train_regret_loser_1[slice17],\n",
    "       train_regret_loser_2[slice17],\n",
    "       train_regret_loser_3[slice17],\n",
    "       train_regret_loser_4[slice17],\n",
    "       train_regret_loser_5[slice17],\n",
    "       train_regret_loser_6[slice17],\n",
    "       train_regret_loser_7[slice17],\n",
    "       train_regret_loser_8[slice17],\n",
    "       train_regret_loser_9[slice17],\n",
    "       train_regret_loser_10[slice17],\n",
    "       train_regret_loser_11[slice17],\n",
    "       train_regret_loser_12[slice17],\n",
    "       train_regret_loser_13[slice17],\n",
    "       train_regret_loser_14[slice17],\n",
    "       train_regret_loser_15[slice17],\n",
    "       train_regret_loser_16[slice17],\n",
    "       train_regret_loser_17[slice17],\n",
    "       train_regret_loser_18[slice17],\n",
    "       train_regret_loser_19[slice17],\n",
    "       train_regret_loser_20[slice17]]\n",
    "\n",
    "winner17 = [train_regret_winner_1[slice17],\n",
    "       train_regret_winner_2[slice17],\n",
    "       train_regret_winner_3[slice17],\n",
    "       train_regret_winner_4[slice17],\n",
    "       train_regret_winner_5[slice17],\n",
    "       train_regret_winner_6[slice17],\n",
    "       train_regret_winner_7[slice17],\n",
    "       train_regret_winner_8[slice17],\n",
    "       train_regret_winner_9[slice17],\n",
    "       train_regret_winner_10[slice17],\n",
    "       train_regret_winner_11[slice17],\n",
    "       train_regret_winner_12[slice17],\n",
    "       train_regret_winner_13[slice17],\n",
    "       train_regret_winner_14[slice17],\n",
    "       train_regret_winner_15[slice17],\n",
    "       train_regret_winner_16[slice17],\n",
    "       train_regret_winner_17[slice17],\n",
    "       train_regret_winner_18[slice17],\n",
    "       train_regret_winner_19[slice17],\n",
    "       train_regret_winner_20[slice17]]\n",
    "\n",
    "loser17_results = pd.DataFrame(loser17).sort_values(by=[0], ascending=False)\n",
    "winner17_results = pd.DataFrame(winner17).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser17 = np.asarray(loser17_results[4:5][0])[0]\n",
    "median_loser17 = np.asarray(loser17_results[9:10][0])[0]\n",
    "upper_loser17 = np.asarray(loser17_results[14:15][0])[0]\n",
    "\n",
    "lower_winner17 = np.asarray(winner17_results[4:5][0])[0]\n",
    "median_winner17 = np.asarray(winner17_results[9:10][0])[0]\n",
    "upper_winner17 = np.asarray(winner17_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration27 :\n",
    "\n",
    "slice27 = 26\n",
    "\n",
    "loser27 = [train_regret_loser_1[slice27],\n",
    "       train_regret_loser_2[slice27],\n",
    "       train_regret_loser_3[slice27],\n",
    "       train_regret_loser_4[slice27],\n",
    "       train_regret_loser_5[slice27],\n",
    "       train_regret_loser_6[slice27],\n",
    "       train_regret_loser_7[slice27],\n",
    "       train_regret_loser_8[slice27],\n",
    "       train_regret_loser_9[slice27],\n",
    "       train_regret_loser_10[slice27],\n",
    "       train_regret_loser_11[slice27],\n",
    "       train_regret_loser_12[slice27],\n",
    "       train_regret_loser_13[slice27],\n",
    "       train_regret_loser_14[slice27],\n",
    "       train_regret_loser_15[slice27],\n",
    "       train_regret_loser_16[slice27],\n",
    "       train_regret_loser_17[slice27],\n",
    "       train_regret_loser_18[slice27],\n",
    "       train_regret_loser_19[slice27],\n",
    "       train_regret_loser_20[slice27]]\n",
    "\n",
    "winner27 = [train_regret_winner_1[slice27],\n",
    "       train_regret_winner_2[slice27],\n",
    "       train_regret_winner_3[slice27],\n",
    "       train_regret_winner_4[slice27],\n",
    "       train_regret_winner_5[slice27],\n",
    "       train_regret_winner_6[slice27],\n",
    "       train_regret_winner_7[slice27],\n",
    "       train_regret_winner_8[slice27],\n",
    "       train_regret_winner_9[slice27],\n",
    "       train_regret_winner_10[slice27],\n",
    "       train_regret_winner_11[slice27],\n",
    "       train_regret_winner_12[slice27],\n",
    "       train_regret_winner_13[slice27],\n",
    "       train_regret_winner_14[slice27],\n",
    "       train_regret_winner_15[slice27],\n",
    "       train_regret_winner_16[slice27],\n",
    "       train_regret_winner_17[slice27],\n",
    "       train_regret_winner_18[slice27],\n",
    "       train_regret_winner_19[slice27],\n",
    "       train_regret_winner_20[slice27]]\n",
    "\n",
    "loser27_results = pd.DataFrame(loser27).sort_values(by=[0], ascending=False)\n",
    "winner27_results = pd.DataFrame(winner27).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser27 = np.asarray(loser27_results[4:5][0])[0]\n",
    "median_loser27 = np.asarray(loser27_results[9:10][0])[0]\n",
    "upper_loser27 = np.asarray(loser27_results[14:15][0])[0]\n",
    "\n",
    "lower_winner27 = np.asarray(winner27_results[4:5][0])[0]\n",
    "median_winner27 = np.asarray(winner27_results[9:10][0])[0]\n",
    "upper_winner27 = np.asarray(winner27_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration8 :\n",
    "\n",
    "slice8 = 7\n",
    "\n",
    "loser8 = [train_regret_loser_1[slice8],\n",
    "       train_regret_loser_2[slice8],\n",
    "       train_regret_loser_3[slice8],\n",
    "       train_regret_loser_4[slice8],\n",
    "       train_regret_loser_5[slice8],\n",
    "       train_regret_loser_6[slice8],\n",
    "       train_regret_loser_7[slice8],\n",
    "       train_regret_loser_8[slice8],\n",
    "       train_regret_loser_9[slice8],\n",
    "       train_regret_loser_10[slice8],\n",
    "       train_regret_loser_11[slice8],\n",
    "       train_regret_loser_12[slice8],\n",
    "       train_regret_loser_13[slice8],\n",
    "       train_regret_loser_14[slice8],\n",
    "       train_regret_loser_15[slice8],\n",
    "       train_regret_loser_16[slice8],\n",
    "       train_regret_loser_17[slice8],\n",
    "       train_regret_loser_18[slice8],\n",
    "       train_regret_loser_19[slice8],\n",
    "       train_regret_loser_20[slice8]]\n",
    "\n",
    "winner8 = [train_regret_winner_1[slice8],\n",
    "       train_regret_winner_2[slice8],\n",
    "       train_regret_winner_3[slice8],\n",
    "       train_regret_winner_4[slice8],\n",
    "       train_regret_winner_5[slice8],\n",
    "       train_regret_winner_6[slice8],\n",
    "       train_regret_winner_7[slice8],\n",
    "       train_regret_winner_8[slice8],\n",
    "       train_regret_winner_9[slice8],\n",
    "       train_regret_winner_10[slice8],\n",
    "       train_regret_winner_11[slice8],\n",
    "       train_regret_winner_12[slice8],\n",
    "       train_regret_winner_13[slice8],\n",
    "       train_regret_winner_14[slice8],\n",
    "       train_regret_winner_15[slice8],\n",
    "       train_regret_winner_16[slice8],\n",
    "       train_regret_winner_17[slice8],\n",
    "       train_regret_winner_18[slice8],\n",
    "       train_regret_winner_19[slice8],\n",
    "       train_regret_winner_20[slice8]]\n",
    "\n",
    "loser8_results = pd.DataFrame(loser8).sort_values(by=[0], ascending=False)\n",
    "winner8_results = pd.DataFrame(winner8).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser8 = np.asarray(loser8_results[4:5][0])[0]\n",
    "median_loser8 = np.asarray(loser8_results[9:10][0])[0]\n",
    "upper_loser8 = np.asarray(loser8_results[14:15][0])[0]\n",
    "\n",
    "lower_winner8 = np.asarray(winner8_results[4:5][0])[0]\n",
    "median_winner8 = np.asarray(winner8_results[9:10][0])[0]\n",
    "upper_winner8 = np.asarray(winner8_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration18 :\n",
    "\n",
    "slice18 = 17\n",
    "\n",
    "loser18 = [train_regret_loser_1[slice18],\n",
    "       train_regret_loser_2[slice18],\n",
    "       train_regret_loser_3[slice18],\n",
    "       train_regret_loser_4[slice18],\n",
    "       train_regret_loser_5[slice18],\n",
    "       train_regret_loser_6[slice18],\n",
    "       train_regret_loser_7[slice18],\n",
    "       train_regret_loser_8[slice18],\n",
    "       train_regret_loser_9[slice18],\n",
    "       train_regret_loser_10[slice18],\n",
    "       train_regret_loser_11[slice18],\n",
    "       train_regret_loser_12[slice18],\n",
    "       train_regret_loser_13[slice18],\n",
    "       train_regret_loser_14[slice18],\n",
    "       train_regret_loser_15[slice18],\n",
    "       train_regret_loser_16[slice18],\n",
    "       train_regret_loser_17[slice18],\n",
    "       train_regret_loser_18[slice18],\n",
    "       train_regret_loser_19[slice18],\n",
    "       train_regret_loser_20[slice18]]\n",
    "\n",
    "winner18 = [train_regret_winner_1[slice18],\n",
    "       train_regret_winner_2[slice18],\n",
    "       train_regret_winner_3[slice18],\n",
    "       train_regret_winner_4[slice18],\n",
    "       train_regret_winner_5[slice18],\n",
    "       train_regret_winner_6[slice18],\n",
    "       train_regret_winner_7[slice18],\n",
    "       train_regret_winner_8[slice18],\n",
    "       train_regret_winner_9[slice18],\n",
    "       train_regret_winner_10[slice18],\n",
    "       train_regret_winner_11[slice18],\n",
    "       train_regret_winner_12[slice18],\n",
    "       train_regret_winner_13[slice18],\n",
    "       train_regret_winner_14[slice18],\n",
    "       train_regret_winner_15[slice18],\n",
    "       train_regret_winner_16[slice18],\n",
    "       train_regret_winner_17[slice18],\n",
    "       train_regret_winner_18[slice18],\n",
    "       train_regret_winner_19[slice18],\n",
    "       train_regret_winner_20[slice18]]\n",
    "\n",
    "loser18_results = pd.DataFrame(loser18).sort_values(by=[0], ascending=False)\n",
    "winner18_results = pd.DataFrame(winner18).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser18 = np.asarray(loser18_results[4:5][0])[0]\n",
    "median_loser18 = np.asarray(loser18_results[9:10][0])[0]\n",
    "upper_loser18 = np.asarray(loser18_results[14:15][0])[0]\n",
    "\n",
    "lower_winner18 = np.asarray(winner18_results[4:5][0])[0]\n",
    "median_winner18 = np.asarray(winner18_results[9:10][0])[0]\n",
    "upper_winner18 = np.asarray(winner18_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration28 :\n",
    "\n",
    "slice28 = 27\n",
    "\n",
    "loser28 = [train_regret_loser_1[slice28],\n",
    "       train_regret_loser_2[slice28],\n",
    "       train_regret_loser_3[slice28],\n",
    "       train_regret_loser_4[slice28],\n",
    "       train_regret_loser_5[slice28],\n",
    "       train_regret_loser_6[slice28],\n",
    "       train_regret_loser_7[slice28],\n",
    "       train_regret_loser_8[slice28],\n",
    "       train_regret_loser_9[slice28],\n",
    "       train_regret_loser_10[slice28],\n",
    "       train_regret_loser_11[slice28],\n",
    "       train_regret_loser_12[slice28],\n",
    "       train_regret_loser_13[slice28],\n",
    "       train_regret_loser_14[slice28],\n",
    "       train_regret_loser_15[slice28],\n",
    "       train_regret_loser_16[slice28],\n",
    "       train_regret_loser_17[slice28],\n",
    "       train_regret_loser_18[slice28],\n",
    "       train_regret_loser_19[slice28],\n",
    "       train_regret_loser_20[slice28]]\n",
    "\n",
    "winner28 = [train_regret_winner_1[slice28],\n",
    "       train_regret_winner_2[slice28],\n",
    "       train_regret_winner_3[slice28],\n",
    "       train_regret_winner_4[slice28],\n",
    "       train_regret_winner_5[slice28],\n",
    "       train_regret_winner_6[slice28],\n",
    "       train_regret_winner_7[slice28],\n",
    "       train_regret_winner_8[slice28],\n",
    "       train_regret_winner_9[slice28],\n",
    "       train_regret_winner_10[slice28],\n",
    "       train_regret_winner_11[slice28],\n",
    "       train_regret_winner_12[slice28],\n",
    "       train_regret_winner_13[slice28],\n",
    "       train_regret_winner_14[slice28],\n",
    "       train_regret_winner_15[slice28],\n",
    "       train_regret_winner_16[slice28],\n",
    "       train_regret_winner_17[slice28],\n",
    "       train_regret_winner_18[slice28],\n",
    "       train_regret_winner_19[slice28],\n",
    "       train_regret_winner_20[slice28]]\n",
    "\n",
    "loser28_results = pd.DataFrame(loser28).sort_values(by=[0], ascending=False)\n",
    "winner28_results = pd.DataFrame(winner28).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser28 = np.asarray(loser28_results[4:5][0])[0]\n",
    "median_loser28 = np.asarray(loser28_results[9:10][0])[0]\n",
    "upper_loser28 = np.asarray(loser28_results[14:15][0])[0]\n",
    "\n",
    "lower_winner28 = np.asarray(winner28_results[4:5][0])[0]\n",
    "median_winner28 = np.asarray(winner28_results[9:10][0])[0]\n",
    "upper_winner28 = np.asarray(winner28_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration9 :\n",
    "\n",
    "slice9 = 8\n",
    "\n",
    "loser9 = [train_regret_loser_1[slice9],\n",
    "       train_regret_loser_2[slice9],\n",
    "       train_regret_loser_3[slice9],\n",
    "       train_regret_loser_4[slice9],\n",
    "       train_regret_loser_5[slice9],\n",
    "       train_regret_loser_6[slice9],\n",
    "       train_regret_loser_7[slice9],\n",
    "       train_regret_loser_8[slice9],\n",
    "       train_regret_loser_9[slice9],\n",
    "       train_regret_loser_10[slice9],\n",
    "       train_regret_loser_11[slice9],\n",
    "       train_regret_loser_12[slice9],\n",
    "       train_regret_loser_13[slice9],\n",
    "       train_regret_loser_14[slice9],\n",
    "       train_regret_loser_15[slice9],\n",
    "       train_regret_loser_16[slice9],\n",
    "       train_regret_loser_17[slice9],\n",
    "       train_regret_loser_18[slice9],\n",
    "       train_regret_loser_19[slice9],\n",
    "       train_regret_loser_20[slice9]]\n",
    "\n",
    "winner9 = [train_regret_winner_1[slice9],\n",
    "       train_regret_winner_2[slice9],\n",
    "       train_regret_winner_3[slice9],\n",
    "       train_regret_winner_4[slice9],\n",
    "       train_regret_winner_5[slice9],\n",
    "       train_regret_winner_6[slice9],\n",
    "       train_regret_winner_7[slice9],\n",
    "       train_regret_winner_8[slice9],\n",
    "       train_regret_winner_9[slice9],\n",
    "       train_regret_winner_10[slice9],\n",
    "       train_regret_winner_11[slice9],\n",
    "       train_regret_winner_12[slice9],\n",
    "       train_regret_winner_13[slice9],\n",
    "       train_regret_winner_14[slice9],\n",
    "       train_regret_winner_15[slice9],\n",
    "       train_regret_winner_16[slice9],\n",
    "       train_regret_winner_17[slice9],\n",
    "       train_regret_winner_18[slice9],\n",
    "       train_regret_winner_19[slice9],\n",
    "       train_regret_winner_20[slice9]]\n",
    "\n",
    "loser9_results = pd.DataFrame(loser9).sort_values(by=[0], ascending=False)\n",
    "winner9_results = pd.DataFrame(winner9).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser9 = np.asarray(loser9_results[4:5][0])[0]\n",
    "median_loser9 = np.asarray(loser9_results[9:10][0])[0]\n",
    "upper_loser9 = np.asarray(loser9_results[14:15][0])[0]\n",
    "\n",
    "lower_winner9 = np.asarray(winner9_results[4:5][0])[0]\n",
    "median_winner9 = np.asarray(winner9_results[9:10][0])[0]\n",
    "upper_winner9 = np.asarray(winner9_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration19 :\n",
    "\n",
    "slice19 = 18\n",
    "\n",
    "loser19 = [train_regret_loser_1[slice19],\n",
    "       train_regret_loser_2[slice19],\n",
    "       train_regret_loser_3[slice19],\n",
    "       train_regret_loser_4[slice19],\n",
    "       train_regret_loser_5[slice19],\n",
    "       train_regret_loser_6[slice19],\n",
    "       train_regret_loser_7[slice19],\n",
    "       train_regret_loser_8[slice19],\n",
    "       train_regret_loser_9[slice19],\n",
    "       train_regret_loser_10[slice19],\n",
    "       train_regret_loser_11[slice19],\n",
    "       train_regret_loser_12[slice19],\n",
    "       train_regret_loser_13[slice19],\n",
    "       train_regret_loser_14[slice19],\n",
    "       train_regret_loser_15[slice19],\n",
    "       train_regret_loser_16[slice19],\n",
    "       train_regret_loser_17[slice19],\n",
    "       train_regret_loser_18[slice19],\n",
    "       train_regret_loser_19[slice19],\n",
    "       train_regret_loser_20[slice19]]\n",
    "\n",
    "winner19 = [train_regret_winner_1[slice19],\n",
    "       train_regret_winner_2[slice19],\n",
    "       train_regret_winner_3[slice19],\n",
    "       train_regret_winner_4[slice19],\n",
    "       train_regret_winner_5[slice19],\n",
    "       train_regret_winner_6[slice19],\n",
    "       train_regret_winner_7[slice19],\n",
    "       train_regret_winner_8[slice19],\n",
    "       train_regret_winner_9[slice19],\n",
    "       train_regret_winner_10[slice19],\n",
    "       train_regret_winner_11[slice19],\n",
    "       train_regret_winner_12[slice19],\n",
    "       train_regret_winner_13[slice19],\n",
    "       train_regret_winner_14[slice19],\n",
    "       train_regret_winner_15[slice19],\n",
    "       train_regret_winner_16[slice19],\n",
    "       train_regret_winner_17[slice19],\n",
    "       train_regret_winner_18[slice19],\n",
    "       train_regret_winner_19[slice19],\n",
    "       train_regret_winner_20[slice19]]\n",
    "\n",
    "loser19_results = pd.DataFrame(loser19).sort_values(by=[0], ascending=False)\n",
    "winner19_results = pd.DataFrame(winner19).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser19 = np.asarray(loser19_results[4:5][0])[0]\n",
    "median_loser19 = np.asarray(loser19_results[9:10][0])[0]\n",
    "upper_loser19 = np.asarray(loser19_results[14:15][0])[0]\n",
    "\n",
    "lower_winner19 = np.asarray(winner19_results[4:5][0])[0]\n",
    "median_winner19 = np.asarray(winner19_results[9:10][0])[0]\n",
    "upper_winner19 = np.asarray(winner19_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration29 :\n",
    "\n",
    "slice29 = 28\n",
    "\n",
    "loser29 = [train_regret_loser_1[slice29],\n",
    "       train_regret_loser_2[slice29],\n",
    "       train_regret_loser_3[slice29],\n",
    "       train_regret_loser_4[slice29],\n",
    "       train_regret_loser_5[slice29],\n",
    "       train_regret_loser_6[slice29],\n",
    "       train_regret_loser_7[slice29],\n",
    "       train_regret_loser_8[slice29],\n",
    "       train_regret_loser_9[slice29],\n",
    "       train_regret_loser_10[slice29],\n",
    "       train_regret_loser_11[slice29],\n",
    "       train_regret_loser_12[slice29],\n",
    "       train_regret_loser_13[slice29],\n",
    "       train_regret_loser_14[slice29],\n",
    "       train_regret_loser_15[slice29],\n",
    "       train_regret_loser_16[slice29],\n",
    "       train_regret_loser_17[slice29],\n",
    "       train_regret_loser_18[slice29],\n",
    "       train_regret_loser_19[slice29],\n",
    "       train_regret_loser_20[slice29]]\n",
    "\n",
    "winner29 = [train_regret_winner_1[slice29],\n",
    "       train_regret_winner_2[slice29],\n",
    "       train_regret_winner_3[slice29],\n",
    "       train_regret_winner_4[slice29],\n",
    "       train_regret_winner_5[slice29],\n",
    "       train_regret_winner_6[slice29],\n",
    "       train_regret_winner_7[slice29],\n",
    "       train_regret_winner_8[slice29],\n",
    "       train_regret_winner_9[slice29],\n",
    "       train_regret_winner_10[slice29],\n",
    "       train_regret_winner_11[slice29],\n",
    "       train_regret_winner_12[slice29],\n",
    "       train_regret_winner_13[slice29],\n",
    "       train_regret_winner_14[slice29],\n",
    "       train_regret_winner_15[slice29],\n",
    "       train_regret_winner_16[slice29],\n",
    "       train_regret_winner_17[slice29],\n",
    "       train_regret_winner_18[slice29],\n",
    "       train_regret_winner_19[slice29],\n",
    "       train_regret_winner_20[slice29]]\n",
    "\n",
    "loser29_results = pd.DataFrame(loser29).sort_values(by=[0], ascending=False)\n",
    "winner29_results = pd.DataFrame(winner29).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser29 = np.asarray(loser29_results[4:5][0])[0]\n",
    "median_loser29 = np.asarray(loser29_results[9:10][0])[0]\n",
    "upper_loser29 = np.asarray(loser29_results[14:15][0])[0]\n",
    "\n",
    "lower_winner29 = np.asarray(winner29_results[4:5][0])[0]\n",
    "median_winner29 = np.asarray(winner29_results[9:10][0])[0]\n",
    "upper_winner29 = np.asarray(winner29_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration10 :\n",
    "\n",
    "slice10 = 9\n",
    "\n",
    "loser10 = [train_regret_loser_1[slice10],\n",
    "       train_regret_loser_2[slice10],\n",
    "       train_regret_loser_3[slice10],\n",
    "       train_regret_loser_4[slice10],\n",
    "       train_regret_loser_5[slice10],\n",
    "       train_regret_loser_6[slice10],\n",
    "       train_regret_loser_7[slice10],\n",
    "       train_regret_loser_8[slice10],\n",
    "       train_regret_loser_9[slice10],\n",
    "       train_regret_loser_10[slice10],\n",
    "       train_regret_loser_11[slice10],\n",
    "       train_regret_loser_12[slice10],\n",
    "       train_regret_loser_13[slice10],\n",
    "       train_regret_loser_14[slice10],\n",
    "       train_regret_loser_15[slice10],\n",
    "       train_regret_loser_16[slice10],\n",
    "       train_regret_loser_17[slice10],\n",
    "       train_regret_loser_18[slice10],\n",
    "       train_regret_loser_19[slice10],\n",
    "       train_regret_loser_20[slice10]]\n",
    "\n",
    "winner10 = [train_regret_winner_1[slice10],\n",
    "       train_regret_winner_2[slice10],\n",
    "       train_regret_winner_3[slice10],\n",
    "       train_regret_winner_4[slice10],\n",
    "       train_regret_winner_5[slice10],\n",
    "       train_regret_winner_6[slice10],\n",
    "       train_regret_winner_7[slice10],\n",
    "       train_regret_winner_8[slice10],\n",
    "       train_regret_winner_9[slice10],\n",
    "       train_regret_winner_10[slice10],\n",
    "       train_regret_winner_11[slice10],\n",
    "       train_regret_winner_12[slice10],\n",
    "       train_regret_winner_13[slice10],\n",
    "       train_regret_winner_14[slice10],\n",
    "       train_regret_winner_15[slice10],\n",
    "       train_regret_winner_16[slice10],\n",
    "       train_regret_winner_17[slice10],\n",
    "       train_regret_winner_18[slice10],\n",
    "       train_regret_winner_19[slice10],\n",
    "       train_regret_winner_20[slice10]]\n",
    "\n",
    "loser10_results = pd.DataFrame(loser10).sort_values(by=[0], ascending=False)\n",
    "winner10_results = pd.DataFrame(winner10).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser10 = np.asarray(loser10_results[4:5][0])[0]\n",
    "median_loser10 = np.asarray(loser10_results[9:10][0])[0]\n",
    "upper_loser10 = np.asarray(loser10_results[14:15][0])[0]\n",
    "\n",
    "lower_winner10 = np.asarray(winner10_results[4:5][0])[0]\n",
    "median_winner10 = np.asarray(winner10_results[9:10][0])[0]\n",
    "upper_winner10 = np.asarray(winner10_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration20 :\n",
    "\n",
    "slice20 = 19\n",
    "\n",
    "loser20 = [train_regret_loser_1[slice20],\n",
    "       train_regret_loser_2[slice20],\n",
    "       train_regret_loser_3[slice20],\n",
    "       train_regret_loser_4[slice20],\n",
    "       train_regret_loser_5[slice20],\n",
    "       train_regret_loser_6[slice20],\n",
    "       train_regret_loser_7[slice20],\n",
    "       train_regret_loser_8[slice20],\n",
    "       train_regret_loser_9[slice20],\n",
    "       train_regret_loser_10[slice20],\n",
    "       train_regret_loser_11[slice20],\n",
    "       train_regret_loser_12[slice20],\n",
    "       train_regret_loser_13[slice20],\n",
    "       train_regret_loser_14[slice20],\n",
    "       train_regret_loser_15[slice20],\n",
    "       train_regret_loser_16[slice20],\n",
    "       train_regret_loser_17[slice20],\n",
    "       train_regret_loser_18[slice20],\n",
    "       train_regret_loser_19[slice20],\n",
    "       train_regret_loser_20[slice20]]\n",
    "\n",
    "winner20 = [train_regret_winner_1[slice20],\n",
    "       train_regret_winner_2[slice20],\n",
    "       train_regret_winner_3[slice20],\n",
    "       train_regret_winner_4[slice20],\n",
    "       train_regret_winner_5[slice20],\n",
    "       train_regret_winner_6[slice20],\n",
    "       train_regret_winner_7[slice20],\n",
    "       train_regret_winner_8[slice20],\n",
    "       train_regret_winner_9[slice20],\n",
    "       train_regret_winner_10[slice20],\n",
    "       train_regret_winner_11[slice20],\n",
    "       train_regret_winner_12[slice20],\n",
    "       train_regret_winner_13[slice20],\n",
    "       train_regret_winner_14[slice20],\n",
    "       train_regret_winner_15[slice20],\n",
    "       train_regret_winner_16[slice20],\n",
    "       train_regret_winner_17[slice20],\n",
    "       train_regret_winner_18[slice20],\n",
    "       train_regret_winner_19[slice20],\n",
    "       train_regret_winner_20[slice20]]\n",
    "\n",
    "loser20_results = pd.DataFrame(loser20).sort_values(by=[0], ascending=False)\n",
    "winner20_results = pd.DataFrame(winner20).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser20 = np.asarray(loser20_results[4:5][0])[0]\n",
    "median_loser20 = np.asarray(loser20_results[9:10][0])[0]\n",
    "upper_loser20 = np.asarray(loser20_results[14:15][0])[0]\n",
    "\n",
    "lower_winner20 = np.asarray(winner20_results[4:5][0])[0]\n",
    "median_winner20 = np.asarray(winner20_results[9:10][0])[0]\n",
    "upper_winner20 = np.asarray(winner20_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration30 :\n",
    "\n",
    "slice30 = 29\n",
    "\n",
    "loser30 = [train_regret_loser_1[slice30],\n",
    "       train_regret_loser_2[slice30],\n",
    "       train_regret_loser_3[slice30],\n",
    "       train_regret_loser_4[slice30],\n",
    "       train_regret_loser_5[slice30],\n",
    "       train_regret_loser_6[slice30],\n",
    "       train_regret_loser_7[slice30],\n",
    "       train_regret_loser_8[slice30],\n",
    "       train_regret_loser_9[slice30],\n",
    "       train_regret_loser_10[slice30],\n",
    "       train_regret_loser_11[slice30],\n",
    "       train_regret_loser_12[slice30],\n",
    "       train_regret_loser_13[slice30],\n",
    "       train_regret_loser_14[slice30],\n",
    "       train_regret_loser_15[slice30],\n",
    "       train_regret_loser_16[slice30],\n",
    "       train_regret_loser_17[slice30],\n",
    "       train_regret_loser_18[slice30],\n",
    "       train_regret_loser_19[slice30],\n",
    "       train_regret_loser_20[slice30]]\n",
    "\n",
    "winner30 = [train_regret_winner_1[slice30],\n",
    "       train_regret_winner_2[slice30],\n",
    "       train_regret_winner_3[slice30],\n",
    "       train_regret_winner_4[slice30],\n",
    "       train_regret_winner_5[slice30],\n",
    "       train_regret_winner_6[slice30],\n",
    "       train_regret_winner_7[slice30],\n",
    "       train_regret_winner_8[slice30],\n",
    "       train_regret_winner_9[slice30],\n",
    "       train_regret_winner_10[slice30],\n",
    "       train_regret_winner_11[slice30],\n",
    "       train_regret_winner_12[slice30],\n",
    "       train_regret_winner_13[slice30],\n",
    "       train_regret_winner_14[slice30],\n",
    "       train_regret_winner_15[slice30],\n",
    "       train_regret_winner_16[slice30],\n",
    "       train_regret_winner_17[slice30],\n",
    "       train_regret_winner_18[slice30],\n",
    "       train_regret_winner_19[slice30],\n",
    "       train_regret_winner_20[slice30]]\n",
    "\n",
    "loser30_results = pd.DataFrame(loser30).sort_values(by=[0], ascending=False)\n",
    "winner30_results = pd.DataFrame(winner30).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser30 = np.asarray(loser30_results[4:5][0])[0]\n",
    "median_loser30 = np.asarray(loser30_results[9:10][0])[0]\n",
    "upper_loser30 = np.asarray(loser30_results[14:15][0])[0]\n",
    "\n",
    "lower_winner30 = np.asarray(winner30_results[4:5][0])[0]\n",
    "median_winner30 = np.asarray(winner30_results[9:10][0])[0]\n",
    "upper_winner30 = np.asarray(winner30_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summarise arrays: 'Loser'\n",
    "\n",
    "lower_loser = [lower_loser1,\n",
    "            lower_loser2,\n",
    "            lower_loser3,\n",
    "            lower_loser4,\n",
    "            lower_loser5,\n",
    "            lower_loser6,\n",
    "            lower_loser7,\n",
    "            lower_loser8,\n",
    "            lower_loser9,\n",
    "            lower_loser10,\n",
    "            lower_loser11,\n",
    "            lower_loser12,\n",
    "            lower_loser13,\n",
    "            lower_loser14,\n",
    "            lower_loser15,\n",
    "            lower_loser16,\n",
    "            lower_loser17,\n",
    "            lower_loser18,\n",
    "            lower_loser19,\n",
    "            lower_loser20,\n",
    "            lower_loser21,\n",
    "            lower_loser22,\n",
    "            lower_loser23,\n",
    "            lower_loser24,\n",
    "            lower_loser25,\n",
    "            lower_loser26,\n",
    "            lower_loser27,\n",
    "            lower_loser28,\n",
    "            lower_loser29,\n",
    "            lower_loser30,\n",
    "            lower_loser31]\n",
    "\n",
    "median_loser = [median_loser1,\n",
    "            median_loser2,\n",
    "            median_loser3,\n",
    "            median_loser4,\n",
    "            median_loser5,\n",
    "            median_loser6,\n",
    "            median_loser7,\n",
    "            median_loser8,\n",
    "            median_loser9,\n",
    "            median_loser10,\n",
    "            median_loser11,\n",
    "            median_loser12,\n",
    "            median_loser13,\n",
    "            median_loser14,\n",
    "            median_loser15,\n",
    "            median_loser16,\n",
    "            median_loser17,\n",
    "            median_loser18,\n",
    "            median_loser19,\n",
    "            median_loser20,\n",
    "            median_loser21,\n",
    "            median_loser22,\n",
    "            median_loser23,\n",
    "            median_loser24,\n",
    "            median_loser25,\n",
    "            median_loser26,\n",
    "            median_loser27,\n",
    "            median_loser28,\n",
    "            median_loser29,\n",
    "            median_loser30,\n",
    "            median_loser31]\n",
    "\n",
    "upper_loser = [upper_loser1,\n",
    "            upper_loser2,\n",
    "            upper_loser3,\n",
    "            upper_loser4,\n",
    "            upper_loser5,\n",
    "            upper_loser6,\n",
    "            upper_loser7,\n",
    "            upper_loser8,\n",
    "            upper_loser9,\n",
    "            upper_loser10,\n",
    "            upper_loser11,\n",
    "            upper_loser12,\n",
    "            upper_loser13,\n",
    "            upper_loser14,\n",
    "            upper_loser15,\n",
    "            upper_loser16,\n",
    "            upper_loser17,\n",
    "            upper_loser18,\n",
    "            upper_loser19,\n",
    "            upper_loser20,\n",
    "            upper_loser21,\n",
    "            upper_loser22,\n",
    "            upper_loser23,\n",
    "            upper_loser24,\n",
    "            upper_loser25,\n",
    "            upper_loser26,\n",
    "            upper_loser27,\n",
    "            upper_loser28,\n",
    "            upper_loser29,\n",
    "            upper_loser30,\n",
    "            upper_loser31]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summarise arrays: 'Winner'\n",
    "\n",
    "lower_winner = [lower_winner1,\n",
    "            lower_winner2,\n",
    "            lower_winner3,\n",
    "            lower_winner4,\n",
    "            lower_winner5,\n",
    "            lower_winner6,\n",
    "            lower_winner7,\n",
    "            lower_winner8,\n",
    "            lower_winner9,\n",
    "            lower_winner10,\n",
    "            lower_winner11,\n",
    "            lower_winner12,\n",
    "            lower_winner13,\n",
    "            lower_winner14,\n",
    "            lower_winner15,\n",
    "            lower_winner16,\n",
    "            lower_winner17,\n",
    "            lower_winner18,\n",
    "            lower_winner19,\n",
    "            lower_winner20,\n",
    "            lower_winner21,\n",
    "            lower_winner22,\n",
    "            lower_winner23,\n",
    "            lower_winner24,\n",
    "            lower_winner25,\n",
    "            lower_winner26,\n",
    "            lower_winner27,\n",
    "            lower_winner28,\n",
    "            lower_winner29,\n",
    "            lower_winner30,\n",
    "            lower_winner31]\n",
    "\n",
    "median_winner = [median_winner1,\n",
    "            median_winner2,\n",
    "            median_winner3,\n",
    "            median_winner4,\n",
    "            median_winner5,\n",
    "            median_winner6,\n",
    "            median_winner7,\n",
    "            median_winner8,\n",
    "            median_winner9,\n",
    "            median_winner10,\n",
    "            median_winner11,\n",
    "            median_winner12,\n",
    "            median_winner13,\n",
    "            median_winner14,\n",
    "            median_winner15,\n",
    "            median_winner16,\n",
    "            median_winner17,\n",
    "            median_winner18,\n",
    "            median_winner19,\n",
    "            median_winner20,\n",
    "            median_winner21,\n",
    "            median_winner22,\n",
    "            median_winner23,\n",
    "            median_winner24,\n",
    "            median_winner25,\n",
    "            median_winner26,\n",
    "            median_winner27,\n",
    "            median_winner28,\n",
    "            median_winner29,\n",
    "            median_winner30,\n",
    "            median_winner31]\n",
    "\n",
    "upper_winner = [upper_winner1,\n",
    "            upper_winner2,\n",
    "            upper_winner3,\n",
    "            upper_winner4,\n",
    "            upper_winner5,\n",
    "            upper_winner6,\n",
    "            upper_winner7,\n",
    "            upper_winner8,\n",
    "            upper_winner9,\n",
    "            upper_winner10,\n",
    "            upper_winner11,\n",
    "            upper_winner12,\n",
    "            upper_winner13,\n",
    "            upper_winner14,\n",
    "            upper_winner15,\n",
    "            upper_winner16,\n",
    "            upper_winner17,\n",
    "            upper_winner18,\n",
    "            upper_winner19,\n",
    "            upper_winner20,\n",
    "            upper_winner21,\n",
    "            upper_winner22,\n",
    "            upper_winner23,\n",
    "            upper_winner24,\n",
    "            upper_winner25,\n",
    "            upper_winner26,\n",
    "            upper_winner27,\n",
    "            upper_winner28,\n",
    "            upper_winner29,\n",
    "            upper_winner30,\n",
    "            upper_winner31]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEYCAYAAACp5wpbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdeXxcddnw/881k0xmsjZN0jVtk7Z0X9I2tGDZlwJVKQVFEFlEBR4WxRtRHvkpeCP3jSzCo4IKiIi0CrKJCAJls5UC3dK9pVu6pmnStEmTzCSZme/vjzNJJ+lMMlkmk5lc79frvDo5c86Z62TSuea7izEGpZRSKhRbrANQSinVd2mSUEopFZYmCaWUUmFpklBKKRWWJgmllFJhaZJQSikVliYJ1e+JyLUiYkTkuaB9Twf2XRf4WUTk2yJSIiJuEakSkSUiMjfonA8D5zRvDSKyWkTOiHL854nIR9F8DdV/iY6TUApE5BVgIbAAqAfeBV4zxiwMPP8A8CPgBeAlIB24BxgCzDDGbBGRD4Fi4BJAgFzgV4DfGDM4irF/CBQZYwZE6zVU/6UlCaUsNwDlwO+Bp4AK4EYAERkO/AD4lzHmCmPMS8aYZ4FLgf8HJAVdxwssC2yfAYeBg81PisjpIvKJiNSLyE4R+b8iYgs8ly0ifwyUUqpE5CURGRF4bpqIfCwidSJyVET+KiIZInIvcCaQJSL6jU/1uKSOD1Eq8RljKkXkRuC1wK7LjDGHAo9nA3bgX83Hi4gT2AzcC/iCLpUFuIN+bgTODZwzEngL2AlcDZwC/A9WqeN/gOeAeVgllirgIeBNEZkVeJ2xwJXAKOA7gWOfwyoBjcEqwSjVo7QkodRxXwh6PDHE88kAIpKLlQiat6eCjqkFTgXmAl8GPgf+KSLDsEoeacAPjDEvG2PuxEo0V4vIAOBLwMvGmMeMMc8BjwBTgBnAEiAPuA8YD/wYeMUYsxM4AniNMUu6/ytQqjVNEkoBInIWVpXSp0AJcK+IzA48vRyrtPClwM9HsRLB+SEu5TPGfGKM+dgY8wbwJJAJnAb4Q700YAJbqOcAjDHmCWAW8CxWsngNeLwTt6hUl2h1k+r3At/i/wR4gGsAB7ASWCwiRcaYgyLyEHCXiLwKLAKcWNU8AE1Bl0sSkfMCj7Ow2jp8wAZgFfAg8KCIpANzgAnAj4wx1SKyBLhMRL6HVd30fWAdsEZE3gNOBm4CXgG+CBQEXqcRcInI14C/GWNCJSOlusYYo5tu/XoDFmN9k781aN8PAvueDdr3LWAFVpVSXeDxj4D0wPMfcrxUYLAasfcC1wVd4xzgE6xqqp2B822B53KxklVVYPsbkB94bgxWe0ZNYHsbGBN47htYVU57gIGx/n3qllibdoFVSikVlrZJKKWUCkuThFJKqbA0SSillApLk4RSSqmwEq4LbG5urikoKIh1GEopFTdWrVpVaYzJC/VcwiWJgoICVq5cGeswlFIqbojI7nDPaXWTUkqpsDRJKKWUCkuThFJKqbASrk1CqWhoampi3759eDyeWIeiVJc5nU7y8/NJTk6O+BxNEkpFYN++fWRkZFBQUICIdHyCUn2MMYbDhw+zb98+CgsLIz5Pq5uUioDH4yEnJ0cThIpbIkJOTk6nS8OaJJSKkCYIFe+68jesSUIppVRY2iYRcHR3Nf9+tRKG50NKSqzDCWnQIDjllFhHoQB48smevd4NN3R4SHl5Od///vf55JNPyM7OxuFw8MMf/pCFCxfy4YcfsmDBAgoLC2loaOCKK67gnnvuaXV+aWkpEydOZPz48S37/uu//otrrrmGgoICMjIyEBGys7N57rnnGDVqFGB9+7zqqqt4/vnnAfB6vQwdOpQ5c+bwxhtvtHqN4Dg8Hg9f+tKXePjhh7v72+nQs88+y7x58xg2bFiHx61cuZLf/OY3ADz55JP88pe/BCA9PZ2HH36Ys846C4CzzjqLsrIynE4nDoeDp556iqKioqjeR18UsyQhIncADwN5xpjKNs8VAb/FWvbRB9xvjHkhmvE01TdxcOV+WH0AcnJhyBAYMAD6UBXDwYNWOHPmxDoS1duMMVxyySVce+21LF68GIDdu3fz+uuvtxxz+umn88Ybb1BXV0dRURFf/vKXmTlzZqvrjBkzhpKSkpCv8cEHH5Cbm8s999zDz3/+c556ylq6Oy0tjQ0bNuB2u3G5XLz77rsMHz48bKzNcbjdbmbMmMHChQuZO3dud38F+Hw+7HZ7yOeeffZZpkyZ0mGSCPbGG2/w+9//nmXLlpGbm8vq1au5+OKL+fTTT1vub9GiRRQXF/PHP/6RO++8k3fffbfb9xFvYlLdJCIjgHlYK2mFUg9cY4yZDFwIPBZYYjL6/AYqKmD9eli5Evbtg6amjs/rJWvXWpvqX95//30cDgc33XRTy75Ro0Zx2223nXBsWloas2bNYvv27V16rVNPPZX9+/e32jd//nz++c9/AvCXv/yFK6+8ssPruFwuioqKWq5VV1fH9ddfz+zZs5kxYwZ///vfAaivr+fyyy9n0qRJLFy4kDlz5rRMrZOens4dd9zB9OnTWb58OatWreLMM89k1qxZXHDBBZSVlfHSSy+xcuVKrrrqKoqKinC73RHd5y9+8QseeughcnNzAZg5cybf/OY3efzxE5cOD/U76S9i1SbxKPBDQi/+jjHmc2PMtsDjA8AhrMXfe5fbDTt3wqefwJYtUH2010MI5dNPrXBU/7Fx48YTSgXhHD58mE8++YTJkyef8NyOHTsoKipq2ZYuXXrCMf/617+45JJLWu274oor+Otf/4rH42HdunXMiaA4e+TIEbZt28YZZ5wBwP33388555zDZ599xgcffMCdd95JXV0dTzzxBNnZ2WzatIn77ruPVatWtVyjrq6OOXPmsHbtWubMmcNtt93GSy+9xKpVq7j++uu5++67+cpXvkJxcTGLFi2ipKQEl8vFT3/601alrFA2btzIrFmzWu0rLi5m06ZNEf1O+oter24SkQXAfmPM2kha2kVkNtbC9DvaOeYGrAXnGTlyZA9FGsRv4NAha0tNhbS0yM5zOiEvF9IzejykpUutppNOdHdWCeSWW25h2bJlOBwOVqxYAcDSpUuZMWMGNpuNu+66K2SSaK+66eyzz6aqqor09HTuu+++Vs9NmzaN0tJS/vKXvzB//vx2Y1u6dCnTp09n27Zt3H777QwZMgSAd955h9dff72ljcLj8bBnzx6WLVvG9773PQCmTJnCtGnTWq5lt9u57LLLANi6dSsbNmzg/PPPB6zqp6FDh4aM4b//+7/bjTFSV111FY2NjdTW1ob9vSW6qCQJEVkCDAnx1N3Aj7GqmiK5zlDgz8C1xhh/uOOMMU8CTwIUFxdHd9Hu+npri9TeveByQW4O5OZBRs8kDGPgvffgoougnephlSAmT57Myy+/3PLz448/TmVlJcXFxS37mtsCuuqDDz5gwIABXHXVVdxzzz0tDbrNLr74Yn7wgx/w4Ycfcvjw4bDXaY5j165dnHLKKVx++eUUFRVhjOHll19u1XDeEafT2dIOYYxh8uTJLF++vGs32MakSZNYtWoV55xzTsu+VatWtfqdLlq0iFmzZnHnnXdy22238corr/TIa8eTqFQ3GWPOM8ZMabsBO4FCYK2IlAL5wGoROSGhiEgm8E/gbmPMJ9GIs9e43bB3H6xZA599ZlVh1dR0+7J+P7zzjtWEohLbOeecg8fj4be//W3LvvrOfFmJUFJSEo899hjPPfccVVVVrZ67/vrrueeee5g6dWpE1yosLOSuu+7iF7/4BQAXXHABv/71rzHG+h63Zs0aAObOncuLL74IwKZNm1i/fn3I640fP56KioqWJNHU1MTGjRsByMjI4NixY5261x/+8If86Ec/akl4JSUlvPrqq9x4442tjhMR7rvvPj755BO29MN63l6tbjLGrAcGNf8cSBTFIXo3OYBXgeeMMS/1ZoxR5/FYjeH79oEz5XhPqkirsNpoaoK33oKLL7Y6Y6leEkGX1Z4kIrz22mt8//vf58EHHyQvL4+0tLSWD+BINbdJNLv++uv57ne/2+qYoUOHcuWVV/L444/zk5/8pGV/fn7+Ccd25KabbuLhhx+mtLSUn/zkJ9x+++1MmzYNv99PYWEhb7zxBjfffDPXXnstkyZNYsKECUyePJmsrKwTruVwOHjppZf47ne/S3V1NV6vl9tvv53Jkydz3XXXcdNNN+FyuVi+fDn/+7//S3FxMRdffHHY2C6++GIOHDjA3Llz8Xq9HDx4kLVr15KXd2Lzp8vl4o477uChhx7iD3/4Q6d+B/FOmrN6TF48KEmISDFwkzHm2yLyDeCPwMagw68zxnRYKVhcXGy6suhQxeZKXr3/xAarXiECw4bBqFGQ1LW8nZYGCxZAenoPx6YA2Lx5MxMnTox1GAnJ5/PR1NSE0+lkx44dnHfeeWzduhWHw9FrMXi9Xr75zW/i9/t5/vnnE3p0fai/ZRFZZYwpDnV8TAfTGWMKgh6vBL4dePw88HyMwup9xsD+/VbDeGEBDB7S6fEZdXXwz39aicLpjEqUSkVFfX09Z599Nk1NTRhjeOKJJ3o1QYBVzfbnP/+5V18zXuiI676kqQk+3wZlB2HMGMjM7NTp1dXw5pvwpS9BL/8fU6rLMjIydMnhPkznbuqLjh2DkhLYuhUaGzp1amWlNQZQKaV6gpYk+rLycutTf9Qoq83CFllOP3AgynEppfoNTRJ9nc9ndZk9eBDyh0OYuWuCVdWm4vGka9uEUqrbNEnEi/p6q70iEoMHUV4+gcAknkop1WXaJpGI3B7KymIdhFIqEWiSSERuNwcPxjoIpVQi0OqmRNTURGW5F683qatj81QHYrDmEGDNpLp48WLsdjs2m43f//73LdNIHDx4ELvd3jJi+LPPPsPlcjF16lS8Xi8TJ07kT3/6E6mpqa2uabfbW021ccUVV3DXXXe17Pd6vRQWFvLnP/+ZAYFh/Z1ZiCj4NUJdK1qOHj3K4sWLufnmmzs8Nj09ndraWgD27dvHLbfcwqZNm/D5fMyfP59HHnmElMBiZJHei9vt5sILL+T9998Puw5GdzUvFmW320lKSmLlypU0NjZy3nnn8f7775PUAx8AWpJIUP46D+XlsY5C9aTly5fzxhtvsHr1atatW8eSJUsYMWIEJSUllJSUcNNNN/H973+/5WeHw4HL5aKkpIQNGzbgcDj43e9+d8J1m49p3u66665W+zds2MDAgQNbrbMQvBAR0OFCRO1dqzuMMfj9oef+PHr0KE888USnr3fppZdyySWXsG3bNrZt24bb7eaHP/xhyzGR3sszzzzDpZdeGrUE0eyDDz6gpKSkZayJw+Hg3HPP5YUXemadNk0Sicrt1naJBFNWVkZubm7LN9rc3NxOrcR2+umnx3wholDXev7555k9ezZFRUXceOON+Hw+AO677z7Gjx/PaaedxpVXXtkyxXhpaSnjx4/nmmuuYcqUKezduzfkNe66666WuaruvPPOiGJ7//33cTqdfPOb3wSsUsOjjz7Kc88911LS6Oj30mzRokUsWLAAgOrqagYPHtzy3KxZs6iuro4opq645JJLWLRoUY9cS5NEovJ4tF0iwcybN4+9e/cybtw4br75Zj766KOIz/V6vbz11lshZ3B1u92tFiJq+w3U5/Px3nvvnTBZXlcWImp7rc2bN/PCCy/wn//8h5KSEux2O4sWLWLFihW8/PLLrF27lrfeeuuEEdnbtm3j5ptvZuPGjdTX14e8xgMPPNCyfsZDDz0EWIntQDsDiUItRJSZmUlBQcEJCTbc7wWgsbGRnTt3UlBQAEBWVhb19fV4vV4Apk+fzrp160447/TTT2/1XjRvS5YsCRmviDBv3jxmzZrFk0F1oFOmTGlZZ6S7tMY6UbndlJdb04lHOAZP9XHp6emsWrWKpUuX8sEHH/C1r32NBx54gOuuuy7sOc0JAKwPoG9961snHNNcfRLu3P379zNx4sSWxX6adWYhonDXeu+991i1ahUnn3xyy3GDBg2iqqqKBQsW4HQ6cTqdfPnLX251vVGjRnHKKae0e43mFfGCvfnmm+3GGYmOfi8AlZWVJ7RTDBkyhLKyMkaMGMGWLVtaFmMKFmqlwPYsW7aM4cOHc+jQIc4//3wmTJjAGWecgd1ux+FwcOzYMTK6uYaNfnwkKo8Hn0/Xmkg0druds846i5/97Gf85je/abUQUSjB7Q2//vWvOzVxXvO5u3fvxhgTsu69eSGijqqawl3LGMO1117bEuPWrVu59957O4wtLWhq/a5eI5TmhYiC1dTUcPDgwZbFkiL5vbhcLjweT6t9w4YN48CBA7z00kvk5uZy0kknnXBeZ0sSze1AgwYNYuHChXz22WctzzU0NODsgRG1miQSlcdqUNQqp8SxdetWtm07PqCypKSEUb0wYjI1NZVf/epXPPLIIy3VJc06uxBR22ude+65vPTSSxw6dAiAqqoqdu/ezdy5c/nHP/6Bx+Ohtra23RX3wl2jKwsRnXvuudTX1/Pcc88BVpXSHXfcwa233orL5Wr3XoJlZ2fj8/laJYphw4bx5ptv8uCDD/LMM8+EfP2lS5e26kTQvJ133nknHFtXV9dyf3V1dbzzzjtMmTIFsNY5z83NJTk5uVP3H4pWNyUqTwP4/ZSV2Zg+PdbBJJ5eXnMIgNraWm677TaOHj1KUlISY8eObVUP3VXBVVIAF154IQ888ECrY2bMmMG0adP4y1/+wtVXX92yvysLEbW91s9//nPmzZuH3+8nOTmZxx9/nFNOOYWLL76YadOmMXjwYKZOnRpyISKwvv2Hu8bcuXOZMmUKF110EQ899BDz58/n6aefDtvgLyK8+uqr3HLLLdx3331UVFTwta99jbvvvjuiewk2b948li1b1vIBP2zYMBYvXsz7779Pbm5up35noZSXl7Nw4ULAanP6+te/zoUXXghYPZ6++MUvdvs1IMaLDkVDVxcd2v3hTt5+el8UIoqh4mIcA1K59tpOL0+h2tBFh3pfbW0t6enp1NfXc8YZZ/Dkk08yc+bMXo3h448/5sorr+TVV1/t9GuvXr2aRx99NCbrVFx66aU88MADjBs37oTn4mrRob7Cd6SGM89zMCIlnalFdqaOqsFuS4Dk6XHT2JjKkSMwcGCsg1Gqc2644QY2bdqEx+Ph2muv7fUEAfCFL3yB3bt3d+ncmTNncvbZZ+Pz+aI+ViJYY2Mjl1xyScgE0RWaJIAGm4urpv2H59ZMYdnHI8ha6WbO2EpOG1PG4ExPxxfoq9xW7GVlmiRU/Fm8eHGsQ+i266+/vtdf0+FwcM011/TY9bThGkjNSub2RSfz2zP/yqtJX+WMpiUs2TSMn/5jNr9cMo0VpXk0+eKwviYwGlYH1SmlukqTRJAD+XOo+/LX+H3u3exlBN8b+CcO1zp4+j8T+dGrp/DiqtGUVbs6vlBfEehZoT2clFJdpdVNbdSlDuKN8x6jeN0zPLbxOu7Ouo/H5vyOt8um8+Hnw3hvSz5ZrgZcyT6cyV5cyT5cDm/Qz8f3DcmsZ3Tusdg1GgdKEvX1UFPT6SWzVRvGGER7AKg41pWOSpokQjC2JFYU3UDZoOmc/fH93LvqYs6dfQerT57Pp7sGUVadiqcpCXeTHXdjEkfrHbibknA3JdHgbd1ANSyrjrPGHWBO4SGcyb7evZEGDxgDIpSVaZLoDqfTyeHDh8nJydFEoeKSMYbDhw93eoCdJol27Bs2h5fn/4Fzl/0353z8c4aNWUN28XfxJYX/Jfv8tCSQLQez+fDzoSxecRIvrynklMJDnDnuAMMH1PfODfgNNDSA00lZGQQGjKouyM/PZ9++fVToEHYVx5xOJ/n5+Z06R5NEB+pT83jjvEcpXvdHZmx8nkGVmyiZ/HWMdNylbU4SXDMRttQO5/Wy2Xy4fTIfbRvG1MxSLh6ygtNyNpNs81GTPpSK3EnRuQG3G5xObZfopuTkZAoLC2MdhlK9TpNEBKzqp+8Eqp9+zjkf39+p888DbgUqyeGPfJPf1dzE/TVfZRDlfJunuUye4vPzH6Qhpf36oGSbn4FpDZ1r4wg0XtfUWG0TbdabUUqpdsV0xLWI3AE8DOQZYyrDHJMJbAJeM8bc2tE1uzriumJzJa/ev6nD45Ka6kmr716Vg9/AuophvLt7HGvKh+PvRCeztJQmRufWMDq3hjG5NYzKOYYzOfSiKwDk58Po0QCcey6MGdOt0JVSCahPjrgWkRHAPGBPB4feB/w7+hFFxpucSnVW9ydVKxwAN5y0i8O1B8hd8hd8Ymfl9G+3e467MYnSwxnsrMxk/f4cAGxiGD6gltG5xxidZyWO3HTP8dJGYKI/sLrCapJQSnVGLKubHgV+CPw93AEiMgsYDPwLCJnl4l1OegNTxjdx6ur/xwvZRR0moDOxRsbVNSSxs9JKGDsrM/lk1yA+2mZNWjbA1cAPzl9LXoanZdQ16KA6pVTnxSRJiMgCYL8xZm247oQiYgMeAb6BVa3f3vVuAG4AGDlyZM8G2wu2F5zHnDW/Y9yud1hR9J2IzklL8TJ1+BGmDj8CWIsLHahOY93+gfx9bSF7qtKtJBE0VXFVldXZKbD6pVJKdShqI65FZImIbAixLQB+DPy0g0vcDLxpjOlwalZjzJPGmGJjTHFeXl5PhN+r3K4c9g0pZuyud8G0077QDpsN8rPrOGOsVVyodgcWl/H5oLGh5bjy8m6Hq5TqR6KWJIwx5xljprTdgJ1AIbBWREqBfGC1iLRdy+9U4NbAMQ8D14jIAySobaMvIKO+nKGH1nbrOmkpXuw2//EkAVrlpJTqsl6vbjLGrAcGNf8cSALFbXs3GWOuCjrmusAxd/VSmL2uNP80GpNSGbfzbcoGz+jydUQg09lItScoSXg8EFiwRZOEUqoz+tQEfyJSLCJPxzqOWPAlOdk18kwK93yE3du96cmzXI1tShLHezhVVkKblRaVUiqsmCcJY0xBcynCGLPSGHNCP1BjzLORjJGId58XzsPhradg33+6dZ0sVyM17jYliQC/HwJLASulVIdiniTUcWWDi6hNHcRJu97u1nWynOFLEqBVTkqpyGmS6EvExrbC88kvW4nLfbjLl8l0NXKswYHPH+he7NEkoZTqGk0Sfcy2wnnYjI+xpe91+RpZrkYAajzJ1o4mLzQ1tTx/6JBV7aSUUh3RJNHHHM0q4NDA8Zy0650uX6M5SVSHaZfweq0GbKWU6ogmiT5oW+E8co9sI/vozi6dHzpJaJWTUqrzNEn0QTsKzsUv9i6XJkImCXfrbrW6voRSKhKaJPogjzObvcNmM3bXEsTf+SVPM51NCCZsdRNYSSKGs8QrpeKEJok+6vPCC0h3VzCsfE2nz7XbDOkpTa3HSrTpBtvQAEeOdDdKpVSi0yQRkJWfwcjuLxPRY/bkf4GG5PRuVTm1mpqjTZIArXJSSnVMk0SAIyOFC/+/kznjZDdJ9tjXw/jsKewceSaFe/9NkvfED/iOZLadmqOx0ZoRNsjnn8POndYU4r7O12oppfoBXeM6mMvFhBvPZNjf3ubDT5wcrHbFNJxtoy9g4o5/UrB3KdsL53Xq3CxXI2XVbRa09nggLa3lx0OHYMkS67EIZGRAdjYMGHD83wEDwOFAKdVPaZJoy+kk86sX8OXUN1lbUsvK3bn4/aEXRoq2g3lTqUkbwkm73ul8knA2UuNxYAzHlzJ1u1sliWDGQE2Nte3e3fq57Gz46le7cANKqbin1U2hOJ3Il75I0UwbC4tKGZjW0PE50SA2thXOY/jBVaTWd270W5arEZ/fRl1D0PcAT9dmlz1yRBu5leqvNEmEk5ICX/wiOaOzWDijlOkjuj6XUndsK7wAm/EztnRJp87LbB4r4Qk/oK4z9u/v8qlKqTimSaI9DgfMn4996CDmFFZw8fTdZDibOj6vB9Vk5lOeO6nTM8NGMqCuMzRJKNU/aZLoSCBRMGQIQ7LcfGXWTmaMPNyrPaC2Fc4j5+hOBh7ZHvE5Hc3f1FkHDuikgEr1R5okIpGcDBddBEOHkmw3nFxQwRUnb2fSsCPYbNFPFjtGnYPPltSpMROZzjBJoouf9E1NUFHRpVOVUnFMezdFqjlR/OtfcOAAqQ4fp40tZ9rwKlbuzmP7ocyovXRDShZ7hp3CuJ3/AjruaeVNcrJ20hWkJHlbJwljrKHWrq517d2/HwYP7tKpSqk4pUmiM5KS4MILW3X1yQTOAaZXCZ+tSWbv/i4UzkpLO+w+tHH8pQwrL2HS56+1e5zgJ8nXSMXAcWS5TqfG02aQg8fT5SSxbx/MnNmlU5VScUqTRGclJUFe3gm7c/LgovHWFNyfftrJdaTz8jpMEgeGzOJPl/+zw0u53Ie5+pVLSauvsKbmcLdJEm63NfChCw4dsqqdkpO7dLpSKg5pkuhhQ4fCJZdYhYPS0ghPGpUN1dUdHlZZ66SqLqXdYzwpA/CLnfRAkthTld76gBBzOEXK77eS4MiRXb6EUirOaJKIkoICa4tMCrh9HRY/jtY7eGl1YbsjwI3NTr0rJ3xJohs9nMDq5aRJQqn+Q3s39RWjOp6CdkBqI5OGHu3wuNrUPFLdlWQ6G2nwJuFpCnqbuzGgDqx2CaVU/6FJoq+IIEkAzBxZiSOp/W6s9am5LdVNQOvGa7enW6sNVVV1q8ZKKRVnNEn0FQMHQnp6h4c5k33MGtX+gIW61EFWdVOosRJ+vzVteDfo6Gul+g9NEn1JhKWJyUOPkOkKPz1InSuXZK+bnGSraurEdonuFQU0SSjVf8QsSYjIHSJiRCQ3zPMjReQdEdksIptEpKB3I4yBCJOEzQanFJaHfb4u1eqiO5QyIESS6GZ9kSYJpfqPmCQJERkBzAP2tHPYc8BDxpiJwGygMyMP4tPQoREPQijIrWVoVn3I5+pSrbw7pGkfdpu/9VrX0K2J/gBqayPqsauUSgCxKkk8CvwQCNmCKiKTgCRjzLsAxphaY0zoT8REYrfDiBERH37qmNClibrUQQCkuyvIdLZZ6xq63Q0WtDShVH/R60lCRBYA+40xa9s5bBxwVEReEZE1Islv858AACAASURBVPKQiNjbueYNIrJSRFZWxPssdJ0YhJCb3sC4wSd+pa9z5QC0P+q6mzRJKNU/RCVJiMgSEdkQYlsA/Bj4aQeXSAJOB34AnAyMBq4Ld7Ax5kljTLExpjgvxJQZcWXkyKD1Rjt2ckHFCdOW++0O3CkDSKuvjMqAOrAG1XWjJ61SKk5EnCREZJyInCIihR0da4w5zxgzpe0G7AQKgbUiUgrkA6tFZEibS+wDSowxO40xXuA1oH9MLed0dmqq1bQUL9PzT1w1ry41r6Ub7AlJwuu1JmHqhoYGqOzciqpKqTjUbpIQkUwRuV9EDgKbgY+B7SKyX0R+JiIZnXkxY8x6Y8wgY0yBMaYAKxnMNMYcbHPoCmCAiDQXC84BNnXmteJahL2cmk3PP0yqw9tqX11qLmnuCjJdjdQ2OPC1ncpDq5yUUhHoqCSxDhiP1chcDJyE1dPox4HH7bUrdIqIFIvI0wDGGB9WVdN7IrIeaxGFp3rqtfq8TiaJJLthdmHrtpiWkkTLqOs2vaa08VopFYGOJvg71RhTJiIjjTF7AEQkGThmjPlTuDEOkQqUJpofrwS+HfTzu8C07lw/bg0YAJmZUFMT8SknDapmw/5sKmudANS58nA1VJOdUgdYYyWyU4NGWndzQB3AwYNWzVWSThOpVMLqsE1CRMYBu0RkZuDx6cByAGOM1kpHSydLEyJwyujjQ0maB9QNxiphRKOHk88H5eHH9CmlEkBHSeJCYAtWdc8KrHaJJUCc9zONA51MEgDDBtRTkHsMOD6gbpixpm2tdrdZh6KbA+qaaZWTUomtoyTxJ+BsYDfwJawG5DOAU6MclxoyBByOjo9rY07hIWw201KSGO7bg2Codvd8mwTo1OFKJbp2k4Qxxm+M+cgYUwg4gC8CyYQZKa16kM3WqdHXzbJcTUwYcrQlSWR5yklPaTpxao7GRvB5Q1yhcyorre6wSqnEFNE4CRF5CPgdcDvwHWBRNINSAV2ocgLISWugKTmNxqRU0uoryXSFmJoDeqzK6cCBHrmMUqoPinQw3XXAmUAd8DRwVpTiUcFGjOjU6OtmzWMm6lJzw0/NAT1W5aTtEkolrkiTRB1Wm4QNq42iLGoRqeNSUqy2iU5KS7FGU9el5pHmbidJ9NASc9ouoVTiijRJ3AncB2RgDXL7cdQiUq11ocrpeEni+NQcNR7HiXMt9VCSqKmBY8d65FJKqT4m0iThBcYCc4ARxpgXoxeSaqULScKZ5AOsAXWp7iqynB58fht1DW1GvVVWapWTUqpdkSaJZwCXMWaFDqDrZVlZ1gjsTrDZrNJEXWoeNuMjz34ECDGgzuuFrVt7ZDpXTRJKJaZIJ1TYCCwSkeWAB8AYo1VOvWXkSDh6tFOnWEkisEKdWPMnVntSGE6btZuqq2HvHhjZtZ5Uzfbvt3JNF9rZlVJ9WKQliXxgCLAQuBK4ImoRqRMVFHT6lOaSBASPug6zNOruPZ2aJyoUjweOHOnWJZRSfVBESSIwtXdh0DY62oGpIIMGWT2dOiE4SeR7S4EQ1U3NjIEtW6zqp27QXk5KJZ5IB9MdaLPtE5FPReS0aAeosBoZOrGsKVhJwpOShc+WTF7DAVKSvOGTBFhFge3buxWmtksolXgibZP4CGtdiXeAC7DGSXiBJ4FJ0QlNtTJqFGzbFvHhqQ4viI06V077A+qCHToE2dmdWhkvWFkZLFvWpVP7HJsNxo61CnFK9WeRJolZwPnGmN0i8nvgn1ijrrWCobfk51ufXH5/RIe3GivRMqAugiqr7duttSxcrk6H6PXCpgRaP3DDBmss4/TpnV56XKmEEWnDdQbwLRGZhjVFRzZwCXDi4soqOhwOGDo04sObk0R90Ap1J6xOF4rPB1u3RJyMEt3Bg/D22/Dii7B5c7ebbZSKO5EmibuxljAtAX4U+Pl04NdRikuFMi3yhfrSUqxPs9rUPNLqK8lyRlDd1KzmGOzZ05UIE1Z1NSxdCosXw+rVPTYGUak+L6LqJmPMMyLyOjAa2GmMqRSRPxtjmqIbnmplxAir/uPgwQ4PdSUHShKuXJJ8DQxMPkaDNx9Pkw1ncgSlhD17IHsAZHVuIF+i83hg5UooKYFx46y8nZkZ66iUip6IkoSIjAIeA+YCvxGR5YE1qFVvmz0bXn+9w8NsNnAm+6gNdIMdZKsAJlLtduBMjvBr8JatMHMmJEdQTdXPNLe/bNoE6enWwPjmwfEDBliP09O1HUPFv0gbrp/FWp0uDat94nFgXJRiUu0ZMsRqRY2gOigtxUt9IEkMDUzcW+NxMDgzwiTR0GA1ZE+c2OVw+4PaWmtr2wXYbj+ePLKywOmM7Hpjx0Jqas/HqVRXdKZ301XAxcA/gJuiFpHq2MknR5QkUh1eqk4Ydd3JJVErKqxusV2Ysry/8/mgqsraOqOhwXqLleoLIk0SK4AXARfwAPBp1CJSHcvJgTFjYMeOdg9LdXjZ58rBIIzsaNR1e7Zvg9LSzp/XFw0caDUm9GGbN1u1fHZ7rCNRKvIkcS3wMFbX1zKsZUxVLBUXw86d7c7gmurwYmxJuJ3ZDGvYhd3m71qS8BtrTexEcPhwn5+J0OOx8n8fz2Wqn+iwC6yIpAOHjTFXGGMmG2MuQ5cvjb2sLBg/vt1DmsdK1AYG1GU6IxxQl8iamuJihaSNG2MdgVKWdpOEiNyGNWCuSkSuFpEcEfkHVkN2t4nIHSJiRCQ3zPMPishGEdksIr8S6cNf/2Jh1qx26ySCB9Sld2ZAXaI72venq62osGZJUSrWOipJ/F/gBeC3wP3AW1iliO9094VFZAQwDwjZAisiX8DqcjsNmAKcDJzZ3ddNKGlpMHly+Kcdx9e6TnVXRjZ/U39Q1feTBGhpQvUNHSWJPOB7xpj/AnKBFGCGMeaZHnjtR7FGcYerVDeAE3AEXjcZKO+B100sRUVhxzG0zN/kysXZeIwBKW5NEmBVN8XB/Bo7dvTYMuRKdVlHScKONdsrQANwnTGme/NJAyKyANhvjFkb7hhjzHLgA6yG8jLgbWPM5jDXu0FEVorIyoqKiu6GF1+czrDTdbgcgbWuA91gc+1HqG1w4PX181o7Yzq90l8s+P1WTyelYimS3k3PiEgTkArcKyJ1AMaYr7d3kogswVrNrq27gR9jVTW1d/5YYCLWqngA74rI6caYpW2PNcY8iTVtOcXFxd1fsDneTJ1q1U20mVDIbjM4k30tSWKwzarkrvE4GJjW0Oth9ilVVZAbsimsT9m82Sos2iKdZU2pHtbRn96/saqZhgIfA5mBxx1OR2qMOc8YM6XtBuwECoG1IlKKlQRWi0jbhLIQ+MQYU2uMqcVqDzm1E/fWfzgc1idJCMEr1B0fda2N1/HQeA1QV5c4Q1RUfOqoJDHPGBO2g7yIJBljOlW5a4xZD7Qs5RJIFMXGmMo2h+4BviMi/wsIVqP1Y515rX5l8mRYv976VAmS6vByMNX6xjzcvxfo4oC6RONpsH5XaWmxjqRDGzfCaF0wWMVIRyWJLSLyCxE5TUSyRMQW6AZ7uoj8D9CjNaYiUiwiTwd+fAnYAawH1gJrjTH/6MnXSyh2u9Ulto1UhxdvkosGRzojmnYB6FiJZnFSmigr6/zUHkr1lI6SxFysHk7vA1VAE3AIWAI0d2HtFmNMQXMpwhiz0hjz7cBjnzHmRmPMRGPMpEAPK9WeceOsQXZBjvdwymNk4zYEoyWJZnHSFRa0O6yKnXaThDGmzBhzPVaiuAj4BtYa17nGmKuNMbt6IUYVKZvNmq4jSPAypgM85aSnNGmSaFZdbc3CFwe2bbMm/lOqt0XaZ+JHWIPopgLnAD8SkZtEpPMLIavoGj3amgAw4HiSyCWtvoJMHXV9nN9vJYo44PXC1q2xjkL1R5FO8HcZMBaowCpVHMHqEjsPuDQ6oakuEbGqnZYvB1qXJFLdVWRlNWhJItiRI9bMsHFg0yart7NOTqN6U6QliY3Al4wxQ4AFwDtY02RcEK3AVDcEtUsEJwnBkJN8TBuugx2Jn3aJmhrYuzfWUaj+JtIkcS5WKQLgIFb7RCoQwWLJqtcFLboc3HANkJdURbU7GX//G3IYWn39CYMQ+zJtwFa9LdIk8TbwoYisBj7EKkl8B/hPlOJS3ZGR0fIwyW5wJPmPj7qWQ/iNjfqGSGsa+4E46QoLVkkiTppRVIKINElcgzUj7CdYU2pch7VS3VXRCUt1i90O6ektP1qjrq0BdUPNAUAH1LUSR11hwWqbUKq3RJQkjDEejo+TqDTGuI0xS4wxh6Maneq6oCqntJQmGhyZeO0O8v3WzOzVHm2XaHH0qNXTKU5s3WqtnaRUb4goSYjIz4A/YHWD/YOI3BPNoFQPaNsuIWINqGuy1sXWkkQQrzcuVqtr1thojZtQqjdEWt30HeAiY8x04IvATdELSfWIUI3XqXmMarQ+XTRJtBFHvZxAG7BV74k0SaQBzRPwHcbq2aT6spBJIpdBnj2kJHmpduuAulbiLEkcOQJ7Qq7pqFTPijRJvIK1nsOrWD2dXo5eSKpHhClJpNXrMqYhHTsWdxX9H3xgjZ1QKpraTRIiMk5ExgG/wFpPIgVrtbiHeyE21R0hx0rkYvc3kZ1SrwPqQomz0kRDA7z9ttVGoVS0dNRZfgvH16AOngzgcqylTVVf5XBYS5t6PKS1lCSsZTxykmvYXtvhulH9z5EjMGhQx8f1IUeOWCWKefN0ug4VHR0libN7JQoVHZmZ4PG0apMAGGQ7zCr3qFhG1jcdOWKtfx1nn7a7d8PKlXDyybGORCWidpOEMeaj3gpERUFmJhw6RJLdkGwPHnVdToPXjqfJhjM5fsYHRF1jo7VaXdBAxHixZg1kZ8PYsbGORCUaXV49kbUaUOfF7czGLzaG6ajr8OKsXSLYRx9BRUXHxynVGZokElmbxmtjS6LemUO+rxTQZUxDiuMk4fPBO+9YcxYq1VM0SSSyMN1gRzbtBLQkEVJNNfi8sY6iy+rqrEQRJwvuqTigSSKRhUgS9am5FDRYS5zpCnUh+A0cje9pVg8dgqVLYx2FShSaJBJZaiokWX0TmpNEbWoeIzzbsNv8WpII50hVrCPots8/h3XrYh2FSgSaJBJdoDTRUpJw5ZLirSPL2aBtEuEcORrrCHrEp5/qSnaq+3TlmUSXmQlVVa1KEgDZjjotSYTjdlubyxXrSLrFGHjvPZg/H9LSYh1Nz3C5wKZfbXuVJolE17YkEUgSuUlH2evWUddhHTkS90kCrKEfr70W6yh6zsyZUFwc6yj6F83JiS4rC+CEksQg+2FtuG7PofK4WoiovygpsdaIUr1Hk0SiC5QkHEl+kuyGepc1NccQyqltcOD1xdcUFL2m5pjV+mtMx8eqXuP3w7JlsY6if+n1JCEi94rIfhEpCWzzwxx3oYhsFZHtInJXb8eZMNp0g/UlpeBJyWKYsVo0azzaLhHWoUOwc2eso1BtHDgA27fHOor+I1YliUeNMUWB7c22T4qIHXgcuAiYBFwpIpN6O8iEkJbW0tIXPGX4CO9uQAfUdWj/fu0i1ActX65TpPeWvlrdNBvYbozZaYxpBP4KLIhxTPHJZoOMDADSHNaiOtaoa13rOmK7dsHBslhHoYK43bBiRayj6B9ilSRuFZF1IvKMiGSHeH44EPz1bV9gX0gicoOIrBSRlRU6w9mJ2vRwqkvNZXTjJgBK9uXg12r3jm3bDpWVHR+nes2mTfqW9IaoJAkRWSIiG0JsC4DfAmOAIqAMeKS7r2eMedIYU2yMKc7Ly+vu5RLPCUkijzENW5g3oZTlO4ew6NOTtCNPR4yBLVugWrvW9BXGWNOPaN+C6IrKOAljzHmRHCciTwFvhHhqPzAi6Of8wD7VFSGSBMA14z7Fbrfx1saRNPlsXHvqVux9tQKyL/D7YeMmmD49cUanxbmKCti8GSZpi2XUxKJ3U/AIroXAhhCHrQBOEpFCEXEAVwCv90Z8CaltknBZSSLdXcElRaUsmL6LT0sH8/R/JmqX2I54vbB+PXg8sY5EBXz2mU6PHk2x+N74oIisF5F1WMujfh9ARIaJyJsAxhgvcCvwNrAZeNEYszEGsSaGMCWJtHqrQnf+lL18ZeYOVu/J4/dLJ9GkiaJ9jY1WotDuNX1CYyN88kmso0hcvT4thzHm6jD7DwDzg35+Ezihe6zqgubeTSmt17pOcx9v5D9/4n4cdj+LV5zE4x9O4eYzN+JI0oaKsNxu2LABpk1rmWlXxc727TBhAgwbFutIEo/+dfcHSUmQloajrg67zdCYnE5Tkou0+tY9wc4cV0aS3c+fPxnHrz6Ywq1nbcSZrKvXhFVbCxs3Qm5uZMfbbZCUDMlJgX+TrfdGZ6zrEcuWwWWXgd0e60gSiyaJ/iIzE+rqSHV4OeZJps6Ve0KSAJg7ppxku58/fjyBx96fynfPXk+qQxNFWNXV1tYdSUnHE0Zycuw+5ex2GDkSnM7YvH43HT1qraExY0asI0ksmiT6i8xMKCsjLSWQJFLzWtok2ppdUEGy3c9TyybyyyXTuP2c9aQ743dJzz7P67W2vqCiwkoUw4fHZQln9WoYM6bVbDSqmzRJ9BchBtQNLV8b9vAZIw7zf87YyO/+PZlHlkzn4uml9HRztt1mEDHYxWATg4i1z3ps7RcBQTvC9wa7zTA4043s2mXNgjv2pJZZhOOFzwcffwwXXhjrSBKHJon+IkQPpzR3JRg/SOhvjFOHH+HWszfwxIeT+d2/J/daqCp2BmXUc8ZJZZxaWE563VoYMgQKC61qsDixZw98+KFVa2azYX3RkPCPY2XYMBg4MHavHylNEv1Fm3Ul6lx52IwPl+cIbldO2NMmDjnK/Zd8xtH6np3jyRjBbwS/sR77Aj8bI/j8tDz2G+2O21vqGpNYvnMwL60ew2slhcwcWckZJx1gbOVKZMxoGDw41iFG7PPPYx1BZAYPtgYCFhb23U5yfTQs1ePCjpWoaDdJAGQ6m8h0NkU3PtUnnD72IPuPprJ0+1A+2TmYz0oHMTSzjjNOKmPO9ArSphTqaPMeVF5ubR9/DOPGwcSJMGBArKNqTZNEf+FwQEpKqzYJsAbUVbafI1Q/M3xAPVcU7+DSol2s2J3H0m1DeWHVWF4p8VE8qpLTv3CU0WNtMa2q6TFis5apTUsFe+w+DhsarPGZ69fD0KFW6aKgoG9059Uk0Z9kZpJaVQNAbdoQAPKqtrJ7xGmxjEr1UY4kP3PHlDN3TDl7q9L49/ahfLprEMt3JpHmaKIg5xiFuccoyKmhMOdY/PeAczohNdUqKaWlWY9TU3u9l1dZmbU5nTB+PEQ6Z2l+vvVdsKdpkuhPsrJIdVQB4HEOoDT/NCZvfYV1Ey6nMSUjxsGpvmzEwDqumr2dy2bsYvWeXLZXZFJ6OIN/bhiJCbQb5aa7Kcw5RkHuMQpzjjEiuza+Ru17PNZWVXV8n4hV0kh1he3gEbVwgLVrIj/+8h+MxDG856sCNUn0J5mZOJN92GwGv19YOe16vvLm9Uzb8iIrp38r1tGpOOBM9vGFMeV8YUw5AJ4mG3uqMth1OIPSygy2V2SyYvcgAGzix9XDAzFHDTzGjadvwpncS8nHGGv2wHiYQdAbnTlJNEn0J0GN17WeZKqyx7Bj5FlM2fI31o+/jAZnH2sxU32eM9nPuMHVjBt8fNR5tdvBrsoMSg9n4G7quUp1r9/GxzuG8NSyidx85kad1r6XaJLoT9okCYBV077J6D0fMX3zX/lsxk2xjE4liCxXI0UjDlM04nCPX3vUwFoWfXYSf105lq+fvD0xGs/7OM3F/UmbbrAAR7MK2F5wHpO3vorLXRXuTKX6hDNOKuOCSXv497ZhvLMpP9bh9AuaJPqT1FRISiLN0boXyqqp12H3N1G0aXGMAlMqcpcUlXLyqEO8UjKaFaW6XHG0aZLobzIzW5UkAGoy89lWOI+Jn/+d1BAzwyrVl9gErj11K2Pzqnl2+Xi2HdLZ/KJJk0R/EyJJAKyeei0242PGhudjEJRSnZNsN9x85kZy0j088dFkDta4Yh1SwtIk0d+ESRLH0oeyZcwXmbDjDdJrD8YgMKU6Jy3Fy21nbcAuhl9/MIUaT/xMQhhPNEn0N2GSBMCaKVcDwowNz/VuTEp1UV6Gh1vO2ki128HjH06m0asfaT1Nf6P9TTtJoi5tEJtP+jLjd/6LjGP7ezkwpbqmMPcY3567hd2HM/jDfybgj6NB3vFAk0R/EzTqOpQ1k7+B32Zn1vo/9XJgSnVd0YjDXF68g5J9ufxt9ZhYh5NQdDBdf5OejthtuJJ91DWc+Pa7XTlsHLeQqVv+xprJV1GdNSoGQSrVeeeMP0BlrZP3tuTjTPYyJrcm1iH1qsH/sbMgCv9dNUn0NzYbpKeT6vCGTBIAayd9nUnbXmfW+md5/7R7ejlApbruKzN3cqQuhTc39L8vNy9s8LPg6z1/XU0S/VFmJmkpTVQcc4Z82uMcwIbxl1K0cTFrplzNkQGjezlApbrGJvCd0zezt2pvv1vV8MLbxgI9P5uzJon+KDOTVEf78+qsm3gFkz9/jeJ1f+TdM+7rpcCU6j6bwKic2liH0eumT4lOi32vN1yLyL0isl9ESgLb/BDHjBCRD0Rkk4hsFJHv9XacCa2dHk7NGlIyWT/hqxTu/Tc5VXGyYLBSqsfFqnfTo8aYosD2ZojnvcAdxphJwCnALSIyqXdDTGARJAmAdRO+iseRQfG6Z3ohKKVUX9Qnq5uMMWVAWeDxMRHZDAwHNsU0sEQRYZJocqSzbuLXmL32aaZvXIwnJasXglOq93jtDvYOO0VXZmxHrJLErSJyDbASq8RwJNyBIlIAzAA+beeYG4AbAEaOHNmjgSakCJMEwIbxlzH581eZU/L7KAelVGx47Q52jTiTLWO/SNmgInSRitaikiREZAkwJMRTdwO/Be4DTODfR4Drw1wnHXgZuN0YE7bTszHmSeBJgOLi4tCjxNRxSUmkZkW2Yro3OZUXvvw8KY39ryFQJb5U92HG7fwXY0uXcFLpu1SnD2frmPl8PvpC6lNzYx1enxCVJGGMOS+S40TkKeCNMM8lYyWIRcaYV3owPAW4ctMQsZbw7Yg3ORVvcmr0g1Kql9WlDaIidyKfzPw/jN7zEeN3vMnstU9RvO4Z9g6bw5axX2LPsDkYW5+sme8VvX7nIjI00OYAsBDYEOIYAf4AbDbG/LI34+svJCsTV7KX+sb++8evVDNfkpNtoy9g2+gLyKzZx/gdbzJ+51uM2v8x9c6BbCucR70rp8PrGBHqUgdRlVVATcbwhEgusbiDB0WkCKu6qRS4EUBEhgFPG2PmA3OBq4H1IlISOO/HYXpCqa7IzCTVUalJQqk2ajLzWTHjBlZOv56RBz5lwvY3mLrlb9iMr1PX8dmSOZo5giNZBYGtkCMDCqhJHxZXyaPXIzXGXB1m/wFgfuDxMkBbj6IpK4tUh64boVQ4xpbE7vy57M6fi93bgM3f1OE5NuMno7aM7OpdZFeXkn20lEGHNzN29/stx1jJYySelJ5dUS/1cD689XqPXhP6aBdY1Qs60cNJqf7Ol5SCj5SIjm1IyaQyZ3yrfUleNwOqd5NdXcrAo7sYUL0bR1Ndj8Yovs6VdCKlSaK/yswkLUWThFK9wZvkojJnApU5E6L2GpffP50BUbiurifRX6WkkJqmNXpKqfZpSaIfGzNWyHPuwtOUhKfJfnzz2lvtczcl4dfRJ3GryWfD79cvBKprNEn0Y46cDHKPHgIaYh2KiqKj9Q5eXzsKT5M91qGoOKTVTf1ZZs/2rlB904DURi6cvJckuxYHVedpkujPcnKslepUwhuU6WHepH1h1zZXKhytburPRo+GESOgvBz274cDB6CyMrK5OlTcyc+u46xxZby/ZVisQ1FxRJNEf5ecDPn51gbQ2AhlZVbCOHAADre/gp2KL2MH1eBpsvPxjsGxDkXFCU0SqjWHA0aNsjYAj8dKGm53bONSPWYK4F7vZM3m0Guc9zs+X/ub32/929dL2FGa4lyThGqf0wmFhbGOQvWwkyeB+9+wZUusI1E9Jkr9ULTVUql+6rTToKAg1lGovk6ThFL9lM0G554LQ4fGOhLVl2mSUKofs9vhggus3tBKhaJJQql+zuGAiy6CjIxYR6L6Im24VkqRmgpf/arViUf1LV6v1bnQ4wn9b/PjKHVu0iShlLIkJVmb6ltSUiAtLXavr9VNSimlwtIkoZRSKixNEkoppcLSJKGUUiosTRJKKaXC0iShlFIqLE0SSimlwtIkoZRSKixNEkoppcIS09cX0ugkEakAdnfx9FygsgfDiaVEuZdEuQ/Qe+mLEuU+oHv3MsoYkxfqiYRLEt0hIiuNMcWxjqMnJMq9JMp9gN5LX5Qo9wHRuxetblJKKRWWJgmllFJhaZJo7clYB9CDEuVeEuU+QO+lL0qU+4Ao3Yu2SSillApLSxJKKaXC0iShlFIqLE0SgIhcKCJbRWS7iNwV63i6Q0RKRWS9iJSIyMpYx9MZIvKMiBwSkQ1B+waKyLsisi3wb3YsY4xUmHu5V0T2B96bEhGZH8sYIyEiI0TkAxHZJCIbReR7gf1x9760cy/x+L44ReQzEVkbuJefBfYXisingc+yF0TE0e3X6u9tEiJiBz4Hzgf2ASuAK40xm2IaWBeJSClQbIyJuwFCInIGUAs8Z4yZEtj3IFBljHkgkMCzjTE/imWckQhzL/cCtcaYh2MZW2eIyFBgqDFmtYhkAKuAS4DriLP3pZ17uZz4e18ESDPG1IpIMrAM+B7wX8Arxpi/isjvgLXGmN9257W0JAGzge3GmJ3GmEbgr8CCGMfULxlj/g1Utdm9APhT4PGfsP5T93lh7iXuGGPKjDGrA4+PAZuB4cTh+9LOvcQdY6kN/Jgc2AxwDvBSYH+PvC+aJKw/kr1BP+8jTv9wnJ+KqQAABn9JREFUAgzwjoisEpEbYh1MDxhsjCkLPD4IDI5lMD3gVhFZF6iO6vNVNMFEpACYAXxKnL8vbe4F4vB9ERG7iJQAh4B3gR3AUWOMN3BIj3yWaZJIPKcZY2YCFwG3BKo9EoKx6kbjuX70t8AYoAgoAx6JbTiRE5F04GXgdmNMTfBz8fa+hLiXuHxfjDE+Y0wRkI9VIzIhGq+jSQL2AyOCfs4P7ItLxpj9gX8PAa9i/fHEs/JAXXJznfKhGMfTZcaY8sB/bD/wFHHy3gTqvF8GFhljXgnsjsv3JdS9xOv70swYcxT4ADgVGCAiSYGneuSzTJOE1VB9UqBXgAO4Ang9xjF1iYikBRrkEJE0YB6wof2z+rzXgWsDj68F/h7DWLql+UM1YCFx8N4EGkj/AGw2xvwy6Km4e1/C3Uucvi95IjIg8NiF1fFmM1ay+ErgsB55X/p97yaAQJe3xwA78Iwx5v4Yh9QlIjIaq/QAkAQsjqd7EZG/AGdhTXlcDtwDvAa8CIzEmgL+cmNMn28QDnMvZ2FVaRigFLgxqF6/TxKR04ClwHrAH9j9Y6y6/Lh6X9q5lyuJv/dlGlbDtB3ry/6Lxpj/DnwG/BUYCKwBvmGMaejWa2mSUEopFY5WNymllApLk4RSSqmwNEkopZQKS5OEUkqpsDRJKKWUCkuThFJKqbA0SSillApLk4RSqteIyLki8udYx6Eip0lCRYWIFAUWRDlLRExg84lIlYjc04Xr2UXkehE5YVbLoNcIO8FZ0DGTg6/T0bnBz0fyOh3F3ZVrBF3rThE5YW2AcPfWHcFxdyfmEKZjjQRWcUKThIqWR7EmS2s2GxgCPA/cKyLjOnm907Dm3ckI8dwyIBtr8ahwmo/JbXOdSM7tzOu01Tburlyj2R+A60VkYpi42t5bdwTH3Z2Y25oOrBGRFBF5VkT+JzCnkuqjNEmoHiciU7DmKfpH0O5jxpgKjs9K6RARm4g8IiKVInJYRH4rIg4RKRaRDSLSINYyjBdwfIGbzYG1AIKdBhwBxgV96304cN21geObj3mhzXWCzx0oIu+LiCdQ4vlxO6/zbFAJyYjIH8Oc3zbu5mtMCHXvgd9fyHsIzI30MfDtMHG1ujcRSRZrfYRqsZbnPV9ErhOROhH5j4isaeeeW+IGvhF03/9/e/cOGlUQhXH8P2ChkUBQUgiKYCM+0ETQQi210ELQRkHBQgSNoE3EB1ooVhbBQkRTKEhQiIUKQhQs1BDER6EkAU3lAwsVQh4k2MRjcWaTy83O7oqsWc33a5ZM7p09Z29yz51hmUlds9TnnrcGXzH2EfDYzE6b1gaqaSoSUg2bgBEz+5hpexlCGAcuAG1m1gccBI7gq1Zuia8ngL343+ZmoA1oAFpiPxuATxXEMIavjLkS2J1pP1+inyXAa3xd/nvA0RL9t+BP17eAkRhnsfNTcW+leO7lcujFP99i8rkdAHYBG/GRXQcwF6gDLuO7lqVyTsWdumalYgYml+leBtwGTplZRyIPqSFzyh8i8tsWAqO5tp34dMWgmY3FtmbgvZk9AQghPMdvSoeBRUAXMIzf4L7Gc0bN7GfwfZVPxrZiexN3mll/CGEQmJdpL6z+Wegne84wfnNrB+bjN9SizGw8hNCK74+83cx645Nz/vzxxPttS+ReLocRfIXPYvK5rcGni3rwolvP1P98l5kNxameYjlPxh3bC1LXrLtEzAUr8KX5FwATiRykxmgkIdXwHX/KzvpiZp8zBQLgLbA8TlU045umvMCffgeBdcBDfPRRuKksjtMyV/HlnZvwJ+G8wvH5qYz+XD9Zx4BVwCF8+evkXHkI4QBwDmgFXgXf7azY+fm4C+4nci+XQwNTBTMvn9s7vPDtA84C1zM5/SiT82TcQDbu1DUrFXPBWny6bA9wI4TwT215OlupSEg1dAN1IYSlZY5rB67gG7c/jq8XgafAenzksQO/EffHn+8Ay8xsyMw+mNkHpm54lZjI9pP73V18Q/k3+NNuPdCY6OdMfL2Ez9c/SJw/kHi/Z4ncy1mNjwwqya09xnQzxjvA9BHetJhDCI3kPu/M8alrVom1QJ+ZDeBTVJ1xCkpqmPaTkKoIIfQAN83s2kzH8r8IvuvgN6DJzN7PdDwyO2gkIdVynOnfwpE/sx8vvCoQ8tdoJCEiIkkaSYiISJKKhIiIJKlIiIhIkoqEiIgkqUiIiEiSioSIiCSpSIiISNIvGzvp576kCfoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Visualise!\n",
    "\n",
    "title = obj_func\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(median_loser, color = 'Red')\n",
    "plt.plot(median_winner, color = 'Blue')\n",
    "\n",
    "xstar = np.arange(0, max_iter+1, step=1)\n",
    "plt.fill_between(xstar, lower_loser, upper_loser, facecolor = 'Red', alpha=0.4, label='GP ERM Regret: IQR')\n",
    "plt.fill_between(xstar, lower_winner, upper_winner, facecolor = 'Blue', alpha=0.4, label='STP ERM Regret: IQR ' r'($\\nu$' ' = {})'.format(df))\n",
    "\n",
    "plt.title(title, weight = 'bold', family = 'Arial')\n",
    "plt.xlabel('(Post-initialization) iteration $\\it{k}$', weight = 'bold', family = 'Arial') # x-axis label\n",
    "plt.ylabel('log(Regret)', weight = 'bold', family = 'Arial') # y-axis label\n",
    "plt.legend(loc=0) # add plot legend\n",
    "\n",
    "plt.show() #visualise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
