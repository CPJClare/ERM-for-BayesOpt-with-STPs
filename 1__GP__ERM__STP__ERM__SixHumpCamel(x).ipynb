{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SixHumpCamel synthetic function:\n",
    "\n",
    "GP ERM versus STP nu = 5 ERM (winner)\n",
    "\n",
    "https://www.sfu.ca/~ssurjano/sumsqu.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import modules:\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "from pyGPGO.logger import EventLogger\n",
    "from pyGPGO.GPGO import GPGO\n",
    "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\n",
    "from pyGPGO.surrogates.tStudentProcess import tStudentProcess, logpdf\n",
    "from pyGPGO.acquisition import Acquisition\n",
    "from pyGPGO.covfunc import squaredExponential\n",
    "\n",
    "from collections import OrderedDict\n",
    "from joblib import Parallel, delayed\n",
    "from numpy.linalg import slogdet, inv, cholesky, solve\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.special import gamma\n",
    "from scipy.stats import norm, t\n",
    "from matplotlib.pyplot import rc\n",
    "\n",
    "rc('text', usetex=False)\n",
    "plt.rcParams['text.latex.preamble']=[r'\\usepackage{amsmath}']\n",
    "plt.rcParams['text.latex.preamble'] = [r'\\boldmath']\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inputs:\n",
    "\n",
    "obj_func = 'SixHumpCamel'\n",
    "n_test = 50 # test points\n",
    "df = 5 # nu\n",
    "\n",
    "util_loser = 'RegretMinimized'\n",
    "util_winner = 'tRegretMinimized'\n",
    "n_init = 5 # random initialisations\n",
    "\n",
    "cov_func = squaredExponential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Objective function:\n",
    "\n",
    "if obj_func == 'SixHumpCamel':\n",
    "    \n",
    "    # True y bounds:\n",
    "    y_lb = -1.0316\n",
    "    operator = -1 # targets global minimum \n",
    "    y_global_orig = y_lb * operator # targets global minimum\n",
    "            \n",
    "# Constraints:\n",
    "    lb_x1 = -3\n",
    "    ub_x1 = +3\n",
    "    \n",
    "    lb_x2 = -2\n",
    "    ub_x2 = +2\n",
    "    \n",
    "# Input array dimension(s):\n",
    "    dim = 2\n",
    "\n",
    "# 2-D inputs' parameter bounds:\n",
    "    param = {'x1_training': ('cont', [lb_x1, ub_x1]),\n",
    "             'x2_training': ('cont', [lb_x2, ub_x2])}\n",
    "    \n",
    "    max_iter = (10 * dim)*0 + 100  # iterations of Bayesian optimisation\n",
    "    \n",
    "# Test data:\n",
    "    x1_test = np.linspace(lb_x1, ub_x1, n_test)\n",
    "    x2_test = np.linspace(lb_x2, ub_x2, n_test)\n",
    "    Xstar_d = np.column_stack((x1_test, x2_test))\n",
    "\n",
    "    def f_syn_polarity(x1_training, x2_training):\n",
    "        return operator * ((4 - 2.1 * x1_training ** 2 + 1 / 3 * x1_training ** 4) * x1_training ** 2 +\n",
    "                (x1_training * x2_training) + (-4 + 4 * x2_training ** 2) * x2_training ** 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cumulative Regret Calculator:\n",
    "\n",
    "def min_max_array(x):\n",
    "    new_list = []\n",
    "    for i, num in enumerate(x):\n",
    "            new_list.append(np.min(x[0:i+1]))\n",
    "    return new_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set-seeds:\n",
    "\n",
    "run_num_1 = 111\n",
    "run_num_2 = 113\n",
    "run_num_3 = 3333\n",
    "run_num_4 = 444\n",
    "run_num_5 = 5555\n",
    "run_num_6 = 6\n",
    "run_num_7 = 777\n",
    "run_num_8 = 887\n",
    "run_num_9 = 99\n",
    "run_num_10 = 1000\n",
    "run_num_11 = 1113\n",
    "run_num_12 = 1234\n",
    "run_num_13 = 2345\n",
    "run_num_14 = 88\n",
    "run_num_15 = 1556\n",
    "run_num_16 = 1666\n",
    "run_num_17 = 717\n",
    "run_num_18 = 8\n",
    "run_num_19 = 1998\n",
    "run_num_20 = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Acquisition function - ERM:\n",
    "\n",
    "class Acquisition_new(Acquisition):    \n",
    "    def __init__(self, mode, eps=1e-06, **params):\n",
    "        \n",
    "        self.params = params\n",
    "        self.eps = eps\n",
    "\n",
    "        mode_dict = {\n",
    "            'RegretMinimized': self.RegretMinimized,\n",
    "            'tRegretMinimized': self.tRegretMinimized\n",
    "        }\n",
    "\n",
    "        self.f = mode_dict[mode]\n",
    "   \n",
    "    def RegretMinimized(self, tau, mean, std):\n",
    "        \n",
    "        z = (mean - y_global_orig - self.eps) / (std + self.eps)\n",
    "        return z * (std + self.eps) * norm.cdf(z) + (std + self.eps) * norm.pdf(z)[0]\n",
    "    \n",
    "    def tRegretMinimized(self, tau, mean, std, nu=3.0):\n",
    "        \n",
    "        gamma = (mean - y_global_orig - self.eps) / (std + self.eps)\n",
    "        return gamma * (std + self.eps) * t.cdf(gamma, df=nu) + (std + self.eps) * (nu + gamma ** 2)/(nu - 1) * t.pdf(gamma, df=nu)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.67302105 -1.32372098]. \t  -5.793449752432556 \t -0.8736935954900025\n",
      "init   \t [-0.38364588  1.07704989]. \t  -0.8736935954900025 \t -0.8736935954900025\n",
      "init   \t [-1.22804817 -1.40334817]. \t  -11.759316761133794 \t -0.8736935954900025\n",
      "init   \t [-2.86513005 -0.31910203]. \t  -76.26436708944966 \t -0.8736935954900025\n",
      "init   \t [-1.56790715 -0.64937523]. \t  -2.1371114994016214 \t -0.8736935954900025\n",
      "1      \t [-0.93715312 -0.14094362]. \t  -2.1732366761917628 \t -0.8736935954900025\n",
      "2      \t [-0.07304745  1.9144092 ]. \t  -38.94939404436368 \t -0.8736935954900025\n",
      "3      \t [-0.78050993  0.72945565]. \t  \u001b[92m-0.167569287879497\u001b[0m \t -0.167569287879497\n",
      "4      \t [0.30116645 0.52574138]. \t  \u001b[92m0.29590610486999536\u001b[0m \t 0.29590610486999536\n",
      "5      \t [-0.24800953  0.70022966]. \t  \u001b[92m0.935121440318956\u001b[0m \t 0.935121440318956\n",
      "6      \t [1.5609464  0.14143977]. \t  -2.243079860045657 \t 0.935121440318956\n",
      "7      \t [ 1.98277411 -2.        ]. \t  -47.55707867715 \t 0.935121440318956\n",
      "8      \t [3. 2.]. \t  -162.89999999999998 \t 0.935121440318956\n",
      "9      \t [ 1.04581603 -0.12582446]. \t  -2.1050148479641027 \t 0.935121440318956\n",
      "10     \t [-0.33128604  0.80795693]. \t  0.760132762399243 \t 0.935121440318956\n",
      "11     \t [-1.48211326  1.64684486]. \t  -18.319370010217696 \t 0.935121440318956\n",
      "12     \t [0.20276379 0.02789873]. \t  -0.16347208264014565 \t 0.935121440318956\n",
      "13     \t [ 1.86388429 -0.23019458]. \t  -1.8976200882948167 \t 0.935121440318956\n",
      "14     \t [ 0.26096036 -1.76606431]. \t  -26.238185894466334 \t 0.935121440318956\n",
      "15     \t [ 0.99354472 -0.93019942]. \t  -0.8323521358113928 \t 0.935121440318956\n",
      "16     \t [-0.00435298  0.50492238]. \t  0.7619174732096596 \t 0.935121440318956\n",
      "17     \t [ 0.61502719 -0.71637269]. \t  0.2092855324522761 \t 0.935121440318956\n",
      "18     \t [-2.06825954 -1.82702401]. \t  -39.77174015579872 \t 0.935121440318956\n",
      "19     \t [ 2.79836475 -0.36511083]. \t  -61.13109513906215 \t 0.935121440318956\n",
      "20     \t [ 0.08952598 -0.45795008]. \t  0.6720196307983171 \t 0.935121440318956\n",
      "21     \t [ 1.55366239 -0.32439664]. \t  -1.2269816968095526 \t 0.935121440318956\n",
      "22     \t [-0.03248066  0.71572693]. \t  \u001b[92m1.0184279216230645\u001b[0m \t 1.0184279216230645\n",
      "23     \t [-1.0331679  -0.80591207]. \t  -2.2044185478435976 \t 1.0184279216230645\n",
      "24     \t [-0.22125    -0.31741033]. \t  0.10135530706690432 \t 1.0184279216230645\n",
      "25     \t [-0.06617994  0.69130856]. \t  \u001b[92m1.026319568264805\u001b[0m \t 1.026319568264805\n",
      "26     \t [-0.12064446  0.3855229 ]. \t  0.4948853256820529 \t 1.026319568264805\n",
      "27     \t [ 1.69944873 -0.45869842]. \t  -0.6219762540829291 \t 1.026319568264805\n",
      "28     \t [-0.06057763  0.75063218]. \t  1.0147182520433624 \t 1.026319568264805\n",
      "29     \t [-1.58777894  0.64712721]. \t  -0.07714900223152943 \t 1.026319568264805\n",
      "30     \t [-1.30880562  0.71542016]. \t  -0.42957053211949625 \t 1.026319568264805\n",
      "31     \t [-2.85455508  1.61243532]. \t  -85.54204862515923 \t 1.026319568264805\n",
      "32     \t [-1.76511934 -0.15155532]. \t  -2.3365582004902055 \t 1.026319568264805\n",
      "33     \t [-1.7508726   0.38394134]. \t  -0.9552273236159621 \t 1.026319568264805\n",
      "34     \t [-0.1198851  -0.78435006]. \t  0.7958223749138317 \t 1.026319568264805\n",
      "35     \t [ 0.13030098 -0.71771591]. \t  1.0252954392152667 \t 1.026319568264805\n",
      "36     \t [-1.80852718  0.38025444]. \t  -1.0984653743738888 \t 1.026319568264805\n",
      "37     \t [-1.78809222  1.4077139 ]. \t  -7.4806184141338425 \t 1.026319568264805\n",
      "38     \t [-0.39087178 -0.64675075]. \t  0.1572014174845049 \t 1.026319568264805\n",
      "39     \t [ 0.07675711 -0.69114445]. \t  \u001b[92m1.027563848132161\u001b[0m \t 1.027563848132161\n",
      "40     \t [-2.25205148  1.19447344]. \t  -9.501171268168793 \t 1.027563848132161\n",
      "41     \t [-1.81306811  0.89572291]. \t  -0.0386096181568919 \t 1.027563848132161\n",
      "42     \t [0.11986892 1.20162661]. \t  -2.7649172983455013 \t 1.027563848132161\n",
      "43     \t [-0.18441338  0.78811711]. \t  0.9530333021585169 \t 1.027563848132161\n",
      "44     \t [-1.52807232  1.13158191]. \t  -1.8414164933370722 \t 1.027563848132161\n",
      "45     \t [1.08878427 1.92103978]. \t  -44.1520376928909 \t 1.027563848132161\n",
      "46     \t [ 0.0761291  -0.69372883]. \t  \u001b[92m1.0282960675613686\u001b[0m \t 1.0282960675613686\n",
      "47     \t [-1.77367045  1.64443934]. \t  -17.6955156113525 \t 1.0282960675613686\n",
      "48     \t [-1.95465509  0.67202311]. \t  -0.9144651821575858 \t 1.0282960675613686\n",
      "49     \t [-0.33830206 -1.5749701 ]. \t  -15.65361112403584 \t 1.0282960675613686\n",
      "50     \t [0.96700511 1.6088807 ]. \t  -20.1797643715767 \t 1.0282960675613686\n",
      "51     \t [-0.63320781  1.30818715]. \t  -5.328849189545264 \t 1.0282960675613686\n",
      "52     \t [1.78943477 1.05874623]. \t  -4.657173885277427 \t 1.0282960675613686\n",
      "53     \t [ 0.24251203 -0.5235617 ]. \t  0.6948246801657919 \t 1.0282960675613686\n",
      "54     \t [1.5095283  0.74482543]. \t  -2.2909762346483764 \t 1.0282960675613686\n",
      "55     \t [-0.76016924 -1.6482706 ]. \t  -21.584220412392074 \t 1.0282960675613686\n",
      "56     \t [-1.39263147  1.44762328]. \t  -9.458376248624448 \t 1.0282960675613686\n",
      "57     \t [-0.02252971  0.75824675]. \t  0.9925903742455113 \t 1.0282960675613686\n",
      "58     \t [1.17939302 0.48776517]. \t  -2.247908063207417 \t 1.0282960675613686\n",
      "59     \t [0.59966949 1.15925269]. \t  -3.7259669311157895 \t 1.0282960675613686\n",
      "60     \t [1.4978104  1.68247342]. \t  -25.417164401686623 \t 1.0282960675613686\n",
      "61     \t [-0.79823274 -0.23445295]. \t  -1.7617083174416392 \t 1.0282960675613686\n",
      "62     \t [0.38030497 1.23549112]. \t  -4.219773213399278 \t 1.0282960675613686\n",
      "63     \t [-1.6531405   1.26525901]. \t  -3.8071160185632826 \t 1.0282960675613686\n",
      "64     \t [-1.47517746  1.13816911]. \t  -2.0467536045476122 \t 1.0282960675613686\n",
      "65     \t [ 0.04446127 -0.78391692]. \t  0.9744903444205099 \t 1.0282960675613686\n",
      "66     \t [-0.08423858 -0.23792483]. \t  0.16529356485900776 \t 1.0282960675613686\n",
      "67     \t [ 2.14377368 -1.45496872]. \t  -12.72342743307032 \t 1.0282960675613686\n",
      "68     \t [ 0.16023755 -0.76028763]. \t  0.9961418762046389 \t 1.0282960675613686\n",
      "69     \t [-0.08627193  0.75742197]. \t  1.0139693537690022 \t 1.0282960675613686\n",
      "70     \t [-2.27538876 -1.35300516]. \t  -19.839781473580203 \t 1.0282960675613686\n",
      "71     \t [1.86722164 0.27060845]. \t  -2.7798413326718903 \t 1.0282960675613686\n",
      "72     \t [-0.12988261  0.68633578]. \t  1.0189102949372086 \t 1.0282960675613686\n",
      "73     \t [0.71379781 1.66023714]. \t  -22.087172660864255 \t 1.0282960675613686\n",
      "74     \t [-0.4026587  -1.88690108]. \t  -37.81861699002644 \t 1.0282960675613686\n",
      "75     \t [-0.36505783  0.38931278]. \t  0.15993082202549325 \t 1.0282960675613686\n",
      "76     \t [-0.76437006 -0.06979844]. \t  -1.720627974182256 \t 1.0282960675613686\n",
      "77     \t [ 0.31622755 -0.56341955]. \t  0.6655256602171276 \t 1.0282960675613686\n",
      "78     \t [1.50877989 1.95950441]. \t  -48.725209640449066 \t 1.0282960675613686\n",
      "79     \t [-0.63462875 -0.18815498]. \t  -1.2749621822997672 \t 1.0282960675613686\n",
      "80     \t [-0.75974962 -1.7818196 ]. \t  -30.64699317939762 \t 1.0282960675613686\n",
      "81     \t [ 0.97444179 -0.22800717]. \t  -1.7708006741633173 \t 1.0282960675613686\n",
      "82     \t [1.32274211 0.57888504]. \t  -2.2297831175980805 \t 1.0282960675613686\n",
      "83     \t [ 1.09004211 -1.94185986]. \t  -42.02348857257047 \t 1.0282960675613686\n",
      "84     \t [1.74306568 1.61403765]. \t  -21.65610178465473 \t 1.0282960675613686\n",
      "85     \t [-2.6672884   0.65360609]. \t  -39.47632188643165 \t 1.0282960675613686\n",
      "86     \t [ 1.08795635 -1.7951684 ]. \t  -29.043000392860524 \t 1.0282960675613686\n",
      "87     \t [-0.58486961  1.27081664]. \t  -4.365318131187895 \t 1.0282960675613686\n",
      "88     \t [-1.58461732  0.13930054]. \t  -1.7837788225475826 \t 1.0282960675613686\n",
      "89     \t [1.29854219 0.70445066]. \t  -2.2868519260290716 \t 1.0282960675613686\n",
      "90     \t [-0.05184237  0.73552739]. \t  1.0206719398153363 \t 1.0282960675613686\n",
      "91     \t [-0.04926054  0.75311248]. \t  1.009352932694486 \t 1.0282960675613686\n",
      "92     \t [ 2.15048771 -0.66737539]. \t  -4.13124619636116 \t 1.0282960675613686\n",
      "93     \t [-0.17662727  0.73279209]. \t  1.0012046919843132 \t 1.0282960675613686\n",
      "94     \t [ 0.11333463 -0.70660871]. \t  \u001b[92m1.0290480670839535\u001b[0m \t 1.0290480670839535\n",
      "95     \t [ 2.42228947 -1.65386767]. \t  -33.486234540202446 \t 1.0290480670839535\n",
      "96     \t [-2.42244289 -1.42115387]. \t  -30.19710499829729 \t 1.0290480670839535\n",
      "97     \t [-0.31163253  1.16961094]. \t  -2.018095017814412 \t 1.0290480670839535\n",
      "98     \t [ 2.52120677 -0.8352527 ]. \t  -23.23708434097477 \t 1.0290480670839535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [ 1.73673603 -0.96623492]. \t  -0.1807152065744506 \t 1.0290480670839535\n",
      "100    \t [-0.09590284  0.63470091]. \t  0.9865013931367435 \t 1.0290480670839535\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\n",
    "\n",
    "np.random.seed(run_num_1)\n",
    "surrogate_loser_1 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_1 = GPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_1.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 2.11191296 -1.7043856 ]. \t  -24.175973303775706 \t -2.2733752808550527\n",
      "init   \t [ 2.36959058 -0.25402581]. \t  -14.417173913696448 \t -2.2733752808550527\n",
      "init   \t [-2.23393359  0.3034315 ]. \t  -8.078384314238003 \t -2.2733752808550527\n",
      "init   \t [ 2.04282552 -0.2595178 ]. \t  -3.564649730949853 \t -2.2733752808550527\n",
      "init   \t [1.17546336 0.73855239]. \t  -2.2733752808550527 \t -2.2733752808550527\n",
      "1      \t [ 1.15696415 -0.34291717]. \t  \u001b[92m-1.5792328485400045\u001b[0m \t -1.5792328485400045\n",
      "2      \t [1.60259527 0.08121045]. \t  -2.1721691951550346 \t -1.5792328485400045\n",
      "3      \t [0.72219245 1.74530995]. \t  -27.75337391833869 \t -1.5792328485400045\n",
      "4      \t [-3. -2.]. \t  -162.89999999999998 \t -1.5792328485400045\n",
      "5      \t [ 1.66202741 -0.63776504]. \t  \u001b[92m-0.026098956189019362\u001b[0m \t -0.026098956189019362\n",
      "6      \t [-0.03019684  0.24438903]. \t  \u001b[92m0.22836934045908666\u001b[0m \t 0.22836934045908666\n",
      "7      \t [0.49745879 0.37247973]. \t  -0.5736356477870388 \t 0.22836934045908666\n",
      "8      \t [-0.20022727 -0.40189988]. \t  \u001b[92m0.30425333420351636\u001b[0m \t 0.30425333420351636\n",
      "9      \t [ 1.68168007 -0.41728565]. \t  -0.7791662803424317 \t 0.30425333420351636\n",
      "10     \t [ 0.50908846 -1.18377706]. \t  -2.548353120429341 \t 0.30425333420351636\n",
      "11     \t [1.94010885 1.06633308]. \t  -5.771842917675719 \t 0.30425333420351636\n",
      "12     \t [ 0.31348502 -0.35373046]. \t  0.17563792275485451 \t 0.30425333420351636\n",
      "13     \t [-1.03446173 -0.00530849]. \t  -2.289508657660735 \t 0.30425333420351636\n",
      "14     \t [-3.          1.22224409]. \t  -108.18446248270295 \t 0.30425333420351636\n",
      "15     \t [ 1.12828656 -0.9698438 ]. \t  -1.0587830708302863 \t 0.30425333420351636\n",
      "16     \t [-0.43977002 -1.17762934]. \t  -3.3610763568667164 \t 0.30425333420351636\n",
      "17     \t [-1.84426985 -0.17329162]. \t  -2.630123444597507 \t 0.30425333420351636\n",
      "18     \t [-0.37165112 -0.02267267]. \t  -0.5196831634034867 \t 0.30425333420351636\n",
      "19     \t [-0.31633902  0.68965521]. \t  \u001b[92m0.8362021915252301\u001b[0m \t 0.8362021915252301\n",
      "20     \t [ 0.20870977 -0.72443872]. \t  \u001b[92m0.978453036569203\u001b[0m \t 0.978453036569203\n",
      "21     \t [0.0798901  0.68382019]. \t  0.915728843592086 \t 0.978453036569203\n",
      "22     \t [ 0.4705197  -0.69589917]. \t  0.5402004135791172 \t 0.978453036569203\n",
      "23     \t [-0.00315429 -0.74352158]. \t  \u001b[92m0.986453274759761\u001b[0m \t 0.986453274759761\n",
      "24     \t [-0.10009624  0.64131387]. \t  \u001b[92m0.9928440662615338\u001b[0m \t 0.9928440662615338\n",
      "25     \t [-1.69618354  0.20884572]. \t  -1.5427060895551934 \t 0.9928440662615338\n",
      "26     \t [-0.13385615  0.73528114]. \t  \u001b[92m1.0208183788548912\u001b[0m \t 1.0208183788548912\n",
      "27     \t [ 0.08037485 -0.72256931]. \t  \u001b[92m1.0303687102869248\u001b[0m \t 1.0303687102869248\n",
      "28     \t [-0.94756007  1.47131169]. \t  -10.831322363135858 \t 1.0303687102869248\n",
      "29     \t [-0.10350334  0.6945203 ]. \t  1.028029107639676 \t 1.0303687102869248\n",
      "30     \t [-0.55330569 -1.9282616 ]. \t  -42.53130635324554 \t 1.0303687102869248\n",
      "31     \t [2.2316572  0.70391909]. \t  -9.58112801346515 \t 1.0303687102869248\n",
      "32     \t [2.66224111 1.9054783 ]. \t  -84.81841961256475 \t 1.0303687102869248\n",
      "33     \t [-0.14549017  0.65040713]. \t  0.9871981755970161 \t 1.0303687102869248\n",
      "34     \t [-1.06399978 -0.70405953]. \t  -2.069777083252896 \t 1.0303687102869248\n",
      "35     \t [ 0.1235709  -0.68984284]. \t  1.0223274036498695 \t 1.0303687102869248\n",
      "36     \t [-0.00691102  0.71684901]. \t  1.003993324956122 \t 1.0303687102869248\n",
      "37     \t [-0.09001087 -0.70324145]. \t  0.9043115921918863 \t 1.0303687102869248\n",
      "38     \t [1.20821799 1.07536779]. \t  -4.423824629190195 \t 1.0303687102869248\n",
      "39     \t [-1.43663077 -1.95937682]. \t  -48.65557496698203 \t 1.0303687102869248\n",
      "40     \t [-1.18604578 -0.62546756]. \t  -2.1883461710378285 \t 1.0303687102869248\n",
      "41     \t [1.51121874 0.64860887]. \t  -2.158066466727826 \t 1.0303687102869248\n",
      "42     \t [ 0.001911   -0.77300886]. \t  0.963404307308214 \t 1.0303687102869248\n",
      "43     \t [ 0.13090245 -0.73760147]. \t  1.0208632690035335 \t 1.0303687102869248\n",
      "44     \t [-2.27546516  1.05426529]. \t  -8.778753399171318 \t 1.0303687102869248\n",
      "45     \t [-1.76746819  1.20839752]. \t  -2.7163588216432144 \t 1.0303687102869248\n",
      "46     \t [-2.57067255  0.4268469 ]. \t  -29.228832380413056 \t 1.0303687102869248\n",
      "47     \t [-2.81904323  1.97213767]. \t  -105.85075856177869 \t 1.0303687102869248\n",
      "48     \t [-1.15860497 -1.50114465]. \t  -15.42904142345882 \t 1.0303687102869248\n",
      "49     \t [-0.83549937 -1.30350278]. \t  -7.722945864915705 \t 1.0303687102869248\n",
      "50     \t [-1.89214966  0.84477724]. \t  -0.284345414908568 \t 1.0303687102869248\n",
      "51     \t [2.96379456 0.22237478]. \t  -99.4978816164872 \t 1.0303687102869248\n",
      "52     \t [ 2.47769042 -1.0686117 ]. \t  -20.533261295807936 \t 1.0303687102869248\n",
      "53     \t [2.93011755 1.64268014]. \t  -113.64611579697947 \t 1.0303687102869248\n",
      "54     \t [-0.84027301  0.36321686]. \t  -1.1313841567784735 \t 1.0303687102869248\n",
      "55     \t [-1.23207633  0.56761209]. \t  -0.82604010282297 \t 1.0303687102869248\n",
      "56     \t [1.92485735 0.45537205]. \t  -3.165357890014924 \t 1.0303687102869248\n",
      "57     \t [2.88057046 0.26111487]. \t  -79.5367356274671 \t 1.0303687102869248\n",
      "58     \t [ 0.10671594 -0.72062059]. \t  1.030131361019288 \t 1.0303687102869248\n",
      "59     \t [-1.93985613  0.33712633]. \t  -2.020391385944096 \t 1.0303687102869248\n",
      "60     \t [-0.1372666   0.77745327]. \t  0.9884677140556731 \t 1.0303687102869248\n",
      "61     \t [2.30360534 0.00581503]. \t  -11.914920370314794 \t 1.0303687102869248\n",
      "62     \t [ 0.06054849 -0.77093444]. \t  0.996442573984819 \t 1.0303687102869248\n",
      "63     \t [1.61382399 1.61781357]. \t  -21.6050231733215 \t 1.0303687102869248\n",
      "64     \t [ 0.12055112 -0.65800655]. \t  1.0036649497844463 \t 1.0303687102869248\n",
      "65     \t [-0.04232208  0.71929355]. \t  1.0220753960045983 \t 1.0303687102869248\n",
      "66     \t [ 0.1231373  -1.97281926]. \t  -44.840327152198974 \t 1.0303687102869248\n",
      "67     \t [ 0.12672596 -0.80216317]. \t  0.9556275589106191 \t 1.0303687102869248\n",
      "68     \t [ 0.05182957 -0.75818359]. \t  1.0061607500647776 \t 1.0303687102869248\n",
      "69     \t [-1.6784934  -1.01690178]. \t  -3.9028009049976027 \t 1.0303687102869248\n",
      "70     \t [ 0.40081555 -0.20565458]. \t  -0.3453450310759652 \t 1.0303687102869248\n",
      "71     \t [-2.47670402  0.2944167 ]. \t  -21.409173377901208 \t 1.0303687102869248\n",
      "72     \t [ 2.31159292 -0.81827272]. \t  -9.493471172091168 \t 1.0303687102869248\n",
      "73     \t [ 0.12626477 -0.69760157]. \t  1.0241306341459926 \t 1.0303687102869248\n",
      "74     \t [-0.09481064 -0.11165014]. \t  0.0028690320535212158 \t 1.0303687102869248\n",
      "75     \t [-2.85902183 -0.67450772]. \t  -75.36940988507637 \t 1.0303687102869248\n",
      "76     \t [2.59171689 1.09948148]. \t  -36.99881218323877 \t 1.0303687102869248\n",
      "77     \t [-0.03911843  0.71549453]. \t  1.0213034008135675 \t 1.0303687102869248\n",
      "78     \t [ 0.87500532 -0.65176966]. \t  -0.4334457811383149 \t 1.0303687102869248\n",
      "79     \t [-1.51393183  1.20427673]. \t  -2.9385972199817028 \t 1.0303687102869248\n",
      "80     \t [ 0.13567873 -0.6955568 ]. \t  1.0203971069895057 \t 1.0303687102869248\n",
      "81     \t [0.80095779 1.66342919]. \t  -22.67928971311677 \t 1.0303687102869248\n",
      "82     \t [-0.01808215 -0.73119089]. \t  0.9806711139121456 \t 1.0303687102869248\n",
      "83     \t [ 0.01580952 -0.74634041]. \t  0.9977927008092208 \t 1.0303687102869248\n",
      "84     \t [ 2.389094   -0.75934143]. \t  -13.609064836510703 \t 1.0303687102869248\n",
      "85     \t [-0.12686207 -0.68427057]. \t  0.8453203260181767 \t 1.0303687102869248\n",
      "86     \t [-0.52273784  0.83621451]. \t  0.33529392790640367 \t 1.0303687102869248\n",
      "87     \t [2.7672583  0.20580894]. \t  -57.57731382171728 \t 1.0303687102869248\n",
      "88     \t [-0.50532771 -1.40228085]. \t  -9.199869130023037 \t 1.0303687102869248\n",
      "89     \t [ 2.08784573 -0.92626063]. \t  -2.721546899008986 \t 1.0303687102869248\n",
      "90     \t [2.45266727 1.52614456]. \t  -36.757287253058195 \t 1.0303687102869248\n",
      "91     \t [-1.435657    1.76607725]. \t  -26.143691970884298 \t 1.0303687102869248\n",
      "92     \t [-0.03710924  0.61043453]. \t  0.9522562120363517 \t 1.0303687102869248\n",
      "93     \t [-0.52505796  0.39306676]. \t  -0.22121615591223176 \t 1.0303687102869248\n",
      "94     \t [-2.3539009   1.71382247]. \t  -33.11986351303693 \t 1.0303687102869248\n",
      "95     \t [ 1.89971677 -1.11108798]. \t  -1.7998449946112847 \t 1.0303687102869248\n",
      "96     \t [1.71512374 0.84382188]. \t  -2.706752792866186 \t 1.0303687102869248\n",
      "97     \t [0.06952256 0.69500834]. \t  0.9312456932089062 \t 1.0303687102869248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98     \t [-1.77087388  0.75861102]. \t  0.14872796222869067 \t 1.0303687102869248\n",
      "99     \t [ 1.5261282 -0.8115012]. \t  0.0018335549230428239 \t 1.0303687102869248\n",
      "100    \t [-0.12819565  0.71412009]. \t  1.025978875259258 \t 1.0303687102869248\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\n",
    "\n",
    "np.random.seed(run_num_2)\n",
    "surrogate_loser_2 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_2 = GPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_2.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.50945364 -1.5629949 ]. \t  -13.89572751148896 \t -0.027690663940903623\n",
      "init   \t [-0.08327233 -0.00067529]. \t  -0.027690663940903623 \t -0.027690663940903623\n",
      "init   \t [ 0.94265999 -1.05646116]. \t  -1.6525931811945593 \t -0.027690663940903623\n",
      "init   \t [ 0.67676934 -1.52139039]. \t  -12.56543686771716 \t -0.027690663940903623\n",
      "init   \t [ 1.27321378 -0.50975783]. \t  -0.9673942177133168 \t -0.027690663940903623\n",
      "1      \t [ 0.55583856 -0.53062014]. \t  \u001b[92m0.05886828563667812\u001b[0m \t 0.05886828563667812\n",
      "2      \t [-0.93812422  0.56405229]. \t  -0.7241197257913029 \t 0.05886828563667812\n",
      "3      \t [-3. -2.]. \t  -162.89999999999998 \t 0.05886828563667812\n",
      "4      \t [-0.35662755  0.6623391 ]. \t  \u001b[92m0.7457243275072701\u001b[0m \t 0.7457243275072701\n",
      "5      \t [3. 2.]. \t  -162.89999999999998 \t 0.7457243275072701\n",
      "6      \t [-3.  2.]. \t  -150.89999999999998 \t 0.7457243275072701\n",
      "7      \t [-0.62328834  0.22953003]. \t  -0.9138615689602275 \t 0.7457243275072701\n",
      "8      \t [-0.19514789  1.86977414]. \t  -34.68981134147559 \t 0.7457243275072701\n",
      "9      \t [-0.11250342  0.36633991]. \t  0.4556980288279966 \t 0.7457243275072701\n",
      "10     \t [-0.64396831  0.80970934]. \t  0.10313400787413951 \t 0.7457243275072701\n",
      "11     \t [ 2.18620258 -0.81081723]. \t  -4.866487216589105 \t 0.7457243275072701\n",
      "12     \t [-0.48410145  0.53146093]. \t  0.24159898277000913 \t 0.7457243275072701\n",
      "13     \t [-0.39917558 -0.62406496]. \t  0.11661458806764047 \t 0.7457243275072701\n",
      "14     \t [0.08178303 0.72254602]. \t  \u001b[92m0.9122991670588343\u001b[0m \t 0.9122991670588343\n",
      "15     \t [-0.02022456  0.63533429]. \t  \u001b[92m0.9740800195290358\u001b[0m \t 0.9740800195290358\n",
      "16     \t [-1.52788137 -0.16502753]. \t  -2.280354205870446 \t 0.9740800195290358\n",
      "17     \t [ 2.96628381 -1.38106257]. \t  -102.50746023195177 \t 0.9740800195290358\n",
      "18     \t [ 1.94272894 -0.51836816]. \t  -1.3107380061153258 \t 0.9740800195290358\n",
      "19     \t [ 1.6485475  -0.85852299]. \t  0.1392183035984681 \t 0.9740800195290358\n",
      "20     \t [-0.80278648 -0.655194  ]. \t  -1.3408677496767925 \t 0.9740800195290358\n",
      "21     \t [0.09520836 0.65531563]. \t  0.8816065907550134 \t 0.9740800195290358\n",
      "22     \t [-1.84430119  0.14300644]. \t  -2.0833107106429565 \t 0.9740800195290358\n",
      "23     \t [ 2.48640317 -0.30470559]. \t  -22.133533458614437 \t 0.9740800195290358\n",
      "24     \t [0.80561089 0.80642902]. \t  -1.542674591971854 \t 0.9740800195290358\n",
      "25     \t [2.28626513 1.30269706]. \t  -18.84581186380636 \t 0.9740800195290358\n",
      "26     \t [1.62790578 1.25891918]. \t  -7.813188725055089 \t 0.9740800195290358\n",
      "27     \t [1.91770827 0.81031674]. \t  -3.5400446862162775 \t 0.9740800195290358\n",
      "28     \t [2.03252232 1.94878724]. \t  -50.648580740820705 \t 0.9740800195290358\n",
      "29     \t [2.69500782 0.72422615]. \t  -46.94108517932003 \t 0.9740800195290358\n",
      "30     \t [-0.09280874  0.7023051 ]. \t  \u001b[92m1.0306985929027668\u001b[0m \t 1.0306985929027668\n",
      "31     \t [-1.20200077  0.81677325]. \t  -0.5308281839469812 \t 1.0306985929027668\n",
      "32     \t [-0.0729474   0.66301092]. \t  1.0125384217834554 \t 1.0306985929027668\n",
      "33     \t [-0.98876642  1.71764403]. \t  -23.53245957557891 \t 1.0306985929027668\n",
      "34     \t [-1.88246282  0.25176509]. \t  -1.9256364935732981 \t 1.0306985929027668\n",
      "35     \t [ 0.25480188 -0.62146833]. \t  0.8556349328375543 \t 1.0306985929027668\n",
      "36     \t [ 0.35695422 -0.78750712]. \t  0.7470816407854051 \t 1.0306985929027668\n",
      "37     \t [-0.61527902 -1.25310886]. \t  -5.584422742384657 \t 1.0306985929027668\n",
      "38     \t [-2.85030537 -0.69498865]. \t  -73.61434162247694 \t 1.0306985929027668\n",
      "39     \t [-1.52140777 -1.52416182]. \t  -16.754476452081555 \t 1.0306985929027668\n",
      "40     \t [-0.05277617  0.64527684]. \t  0.9949624838866779 \t 1.0306985929027668\n",
      "41     \t [ 1.15332523 -1.99656566]. \t  -47.703299551702244 \t 1.0306985929027668\n",
      "42     \t [1.07341296 0.21966895]. \t  -2.382886423019017 \t 1.0306985929027668\n",
      "43     \t [-0.54372494 -0.35602197]. \t  -0.7584532578463083 \t 1.0306985929027668\n",
      "44     \t [-0.03481541 -0.77704563]. \t  0.9250037876103374 \t 1.0306985929027668\n",
      "45     \t [-2.68790805 -0.88129439]. \t  -46.66635119968425 \t 1.0306985929027668\n",
      "46     \t [-1.85916826 -0.91781474]. \t  -3.677151823072371 \t 1.0306985929027668\n",
      "47     \t [-1.61111715  0.48236638]. \t  -0.5720610369045324 \t 1.0306985929027668\n",
      "48     \t [ 0.05058689 -0.63793275]. \t  0.9874215370770407 \t 1.0306985929027668\n",
      "49     \t [-2.82242365  0.16270713]. \t  -66.54411412468377 \t 1.0306985929027668\n",
      "50     \t [-2.09761025  1.6427845 ]. \t  -20.230439203197196 \t 1.0306985929027668\n",
      "51     \t [-2.26578346 -1.39609948]. \t  -20.852309596099982 \t 1.0306985929027668\n",
      "52     \t [-0.00220962 -0.68415958]. \t  0.9943917420282791 \t 1.0306985929027668\n",
      "53     \t [-1.62314669  0.99022259]. \t  -0.37414367227150785 \t 1.0306985929027668\n",
      "54     \t [2.02561443 0.82840007]. \t  -4.9006351078639785 \t 1.0306985929027668\n",
      "55     \t [-2.32292919  0.97403634]. \t  -10.353010193082945 \t 1.0306985929027668\n",
      "56     \t [ 0.04991704 -0.6346857 ]. \t  0.9839566224674201 \t 1.0306985929027668\n",
      "57     \t [-2.0371764   0.60926246]. \t  -2.082722851895475 \t 1.0306985929027668\n",
      "58     \t [1.28133908 1.92025246]. \t  -44.47958177219236 \t 1.0306985929027668\n",
      "59     \t [-1.65258663  0.18611014]. \t  -1.6097238562181924 \t 1.0306985929027668\n",
      "60     \t [-1.38051878  0.14670585]. \t  -2.016401009076795 \t 1.0306985929027668\n",
      "61     \t [-0.77790184 -1.87867914]. \t  -38.89674298101128 \t 1.0306985929027668\n",
      "62     \t [-0.5910202 -0.0648883]. \t  -1.1767762356526867 \t 1.0306985929027668\n",
      "63     \t [-2.53301849  0.01625332]. \t  -27.21681695433323 \t 1.0306985929027668\n",
      "64     \t [ 0.90975857 -0.92924577]. \t  -0.7442177043139835 \t 1.0306985929027668\n",
      "65     \t [-1.77629651 -1.886067  ]. \t  -41.92231808284836 \t 1.0306985929027668\n",
      "66     \t [2.44049402 0.38693755]. \t  -20.191583296663858 \t 1.0306985929027668\n",
      "67     \t [ 0.04731297 -0.7139353 ]. \t  1.0244582094928152 \t 1.0306985929027668\n",
      "68     \t [ 0.29288521 -1.3619114 ]. \t  -6.270961703110473 \t 1.0306985929027668\n",
      "69     \t [1.42169473 0.7488354 ]. \t  -2.33751822645579 \t 1.0306985929027668\n",
      "70     \t [2.5870525  0.28373031]. \t  -33.074948865719705 \t 1.0306985929027668\n",
      "71     \t [-0.48707467 -0.38844806]. \t  -0.5119313273843653 \t 1.0306985929027668\n",
      "72     \t [ 0.00615173 -0.74432965]. \t  0.9927520361408804 \t 1.0306985929027668\n",
      "73     \t [ 1.5797356  -1.23899433]. \t  -3.412952074281728 \t 1.0306985929027668\n",
      "74     \t [ 0.07595577 -0.60476274]. \t  0.9508227723858835 \t 1.0306985929027668\n",
      "75     \t [ 2.03724327 -1.85181545]. \t  -33.807305855072876 \t 1.0306985929027668\n",
      "76     \t [-0.10215466  0.69211414]. \t  1.0274283897674474 \t 1.0306985929027668\n",
      "77     \t [ 0.14509126 -0.76693684]. \t  0.9968860719654099 \t 1.0306985929027668\n",
      "78     \t [-0.23920191 -0.1224888 ]. \t  -0.19224352643091214 \t 1.0306985929027668\n",
      "79     \t [ 0.03293267 -0.75069151]. \t  1.004238324531852 \t 1.0306985929027668\n",
      "80     \t [ 2.33635455 -0.16672648]. \t  -12.979379570775944 \t 1.0306985929027668\n",
      "81     \t [-0.41667052 -0.11155714]. \t  -0.6302256936198231 \t 1.0306985929027668\n",
      "82     \t [1.00194036 0.73239216]. \t  -1.9755369383083268 \t 1.0306985929027668\n",
      "83     \t [-0.19467155  0.71394043]. \t  0.9900164712700178 \t 1.0306985929027668\n",
      "84     \t [-1.05265745  0.89282795]. \t  -0.7207040572239737 \t 1.0306985929027668\n",
      "85     \t [-2.59530879  1.9820506 ]. \t  -74.40555179730832 \t 1.0306985929027668\n",
      "86     \t [-0.09640934  0.64451237]. \t  0.9965079098223476 \t 1.0306985929027668\n",
      "87     \t [ 0.19637835 -0.30262325]. \t  0.24104999713041617 \t 1.0306985929027668\n",
      "88     \t [ 0.03049975 -0.76975237]. \t  0.9855192814944557 \t 1.0306985929027668\n",
      "89     \t [-0.0640559  -0.79551586]. \t  0.8620735189213845 \t 1.0306985929027668\n",
      "90     \t [-0.09398358  0.68882134]. \t  1.0269637209067533 \t 1.0306985929027668\n",
      "91     \t [ 0.14185097 -0.73013908]. \t  1.0195484312971521 \t 1.0306985929027668\n",
      "92     \t [ 0.1335903  -0.68689956]. \t  1.0178706170302674 \t 1.0306985929027668\n",
      "93     \t [ 1.04276395 -0.95564549]. \t  -0.9816636218497132 \t 1.0306985929027668\n",
      "94     \t [-1.00181589 -0.71221086]. \t  -1.9499409567324595 \t 1.0306985929027668\n",
      "95     \t [ 0.08592029 -0.70692452]. \t  \u001b[92m1.031324021353481\u001b[0m \t 1.031324021353481\n",
      "96     \t [ 2.52572856 -0.10205333]. \t  -26.294017680311253 \t 1.031324021353481\n",
      "97     \t [-0.00316329 -0.69411294]. \t  0.9964382781101411 \t 1.031324021353481\n",
      "98     \t [-2.6643835  -1.96777663]. \t  -91.54494845216861 \t 1.031324021353481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [ 0.08281988 -0.70858959]. \t  \u001b[92m1.0313298409369251\u001b[0m \t 1.0313298409369251\n",
      "100    \t [ 2.8492419  -0.03804816]. \t  -72.30079939605417 \t 1.0313298409369251\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\n",
    "\n",
    "np.random.seed(run_num_3)\n",
    "surrogate_loser_3 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_3 = GPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_3.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [2.03616628 1.28517986]. \t  -11.164343885550666 \t -1.5094648555070385\n",
      "init   \t [0.85324299 0.66889047]. \t  -1.5094648555070385 \t -1.5094648555070385\n",
      "init   \t [-2.76516057 -1.73731045]. \t  -85.9872978949554 \t -1.5094648555070385\n",
      "init   \t [-1.34099616 -0.73455799]. \t  -2.3318567753236477 \t -1.5094648555070385\n",
      "init   \t [-1.83844553 -1.21162847]. \t  -7.376051913381579 \t -1.5094648555070385\n",
      "1      \t [-1.01437897 -1.75256636]. \t  -29.483576565451546 \t -1.5094648555070385\n",
      "2      \t [-1.91438989 -0.42850765]. \t  -3.0825209393641564 \t -1.5094648555070385\n",
      "3      \t [ 3. -2.]. \t  -150.89999999999998 \t -1.5094648555070385\n",
      "4      \t [-0.05640391  2.        ]. \t  -47.8998965367624 \t -1.5094648555070385\n",
      "5      \t [1.00244671 0.20551426]. \t  -2.2814346947109962 \t -1.5094648555070385\n",
      "6      \t [-2.96701333  1.47701968]. \t  -105.80292056646262 \t -1.5094648555070385\n",
      "7      \t [-1.69152227 -0.76906951]. \t  -2.395299314041103 \t -1.5094648555070385\n",
      "8      \t [-0.37581582  0.09326528]. \t  \u001b[92m-0.4544568130064052\u001b[0m \t -0.4544568130064052\n",
      "9      \t [-1.12120405 -0.00457023]. \t  -2.3770078630629117 \t -0.4544568130064052\n",
      "10     \t [-0.05116708 -0.07685558]. \t  \u001b[92m0.009097192783050417\u001b[0m \t 0.009097192783050417\n",
      "11     \t [0.13620185 0.23751596]. \t  \u001b[92m0.10709191246407179\u001b[0m \t 0.10709191246407179\n",
      "12     \t [2.90089528 1.58797867]. \t  -103.54551315512943 \t 0.10709191246407179\n",
      "13     \t [1.52380481 1.05219382]. \t  -4.216311003251198 \t 0.10709191246407179\n",
      "14     \t [-0.64070024 -0.29532758]. \t  -1.1819495636764243 \t 0.10709191246407179\n",
      "15     \t [ 0.35644758 -0.03360426]. \t  -0.45851296913255374 \t 0.10709191246407179\n",
      "16     \t [1.56922006 1.92106752]. \t  -44.825015921670634 \t 0.10709191246407179\n",
      "17     \t [2.2289841  0.59820475]. \t  -9.330734784046534 \t 0.10709191246407179\n",
      "18     \t [-1.06612122  0.70212874]. \t  -0.5745874498456677 \t 0.10709191246407179\n",
      "19     \t [0.59968305 0.33681097]. \t  -0.9820857720893055 \t 0.10709191246407179\n",
      "20     \t [ 0.61162402 -0.85555746]. \t  0.08810934323476172 \t 0.10709191246407179\n",
      "21     \t [-0.06687928  0.79188882]. \t  \u001b[92m0.9705062451128493\u001b[0m \t 0.9705062451128493\n",
      "22     \t [ 1.23470571 -0.85777865]. \t  -0.5616924861630658 \t 0.9705062451128493\n",
      "23     \t [ 0.96276072 -0.60166938]. \t  -0.665758755828095 \t 0.9705062451128493\n",
      "24     \t [ 1.03774098 -1.41537451]. \t  -8.859174953180483 \t 0.9705062451128493\n",
      "25     \t [1.71515554 0.59658607]. \t  -2.185970345519278 \t 0.9705062451128493\n",
      "26     \t [-0.29715605  0.61722153]. \t  0.7896690612223893 \t 0.9705062451128493\n",
      "27     \t [ 1.76825411 -0.48852082]. \t  -0.5752010388217574 \t 0.9705062451128493\n",
      "28     \t [ 0.12113733 -0.68158152]. \t  \u001b[92m1.0192932476930665\u001b[0m \t 1.0192932476930665\n",
      "29     \t [ 0.20457736 -0.61970175]. \t  0.9092273196485889 \t 1.0192932476930665\n",
      "30     \t [-2.79086856 -0.21948544]. \t  -61.69526754144576 \t 1.0192932476930665\n",
      "31     \t [-1.68171016  0.10517426]. \t  -1.8355176611791315 \t 1.0192932476930665\n",
      "32     \t [-1.30307549  1.18672325]. \t  -3.12289688899998 \t 1.0192932476930665\n",
      "33     \t [ 0.23317622 -0.84945214]. \t  0.7903707510127012 \t 1.0192932476930665\n",
      "34     \t [-0.02325013 -0.56856418]. \t  0.8596784265581336 \t 1.0192932476930665\n",
      "35     \t [0.02150614 0.69156258]. \t  0.9813868345065434 \t 1.0192932476930665\n",
      "36     \t [-0.50383496  0.98811789]. \t  -0.2954197517958001 \t 1.0192932476930665\n",
      "37     \t [-2.33313211  0.64413321]. \t  -10.840325346586532 \t 1.0192932476930665\n",
      "38     \t [ 0.08925651 -0.67917935]. \t  \u001b[92m1.0228918623095424\u001b[0m \t 1.0228918623095424\n",
      "39     \t [ 0.17652761 -0.72445466]. \t  1.002800402176357 \t 1.0228918623095424\n",
      "40     \t [0.18489196 0.77016641]. \t  0.6885906675508231 \t 1.0228918623095424\n",
      "41     \t [-0.37402625 -0.93030868]. \t  -0.401642068037274 \t 1.0228918623095424\n",
      "42     \t [1.74432768 0.36192449]. \t  -2.2947272832239722 \t 1.0228918623095424\n",
      "43     \t [-0.00170113 -0.65256742]. \t  0.9768820080365259 \t 1.0228918623095424\n",
      "44     \t [-0.00075889 -0.4882414 ]. \t  0.7258463677604814 \t 1.0228918623095424\n",
      "45     \t [-2.01313241  1.69248607]. \t  -21.863720800237815 \t 1.0228918623095424\n",
      "46     \t [-0.34952555 -1.65696159]. \t  -20.20655931026264 \t 1.0228918623095424\n",
      "47     \t [0.19945342 0.8981783 ]. \t  0.288711967883419 \t 1.0228918623095424\n",
      "48     \t [-0.12224617  0.73296553]. \t  \u001b[92m1.0247467842373679\u001b[0m \t 1.0247467842373679\n",
      "49     \t [-1.72158534  0.80701522]. \t  0.2111083670045243 \t 1.0247467842373679\n",
      "50     \t [-2.80745569 -0.78639926]. \t  -65.54659070114327 \t 1.0247467842373679\n",
      "51     \t [2.64482093 0.05739439]. \t  -39.45595239398992 \t 1.0247467842373679\n",
      "52     \t [2.88321692 0.50180912]. \t  -80.31301853059882 \t 1.0247467842373679\n",
      "53     \t [2.75631719 0.8736854 ]. \t  -57.033858950697564 \t 1.0247467842373679\n",
      "54     \t [ 1.94759015 -1.44321963]. \t  -9.360926545341943 \t 1.0247467842373679\n",
      "55     \t [ 1.15616927 -1.00246659]. \t  -1.2515516194025211 \t 1.0247467842373679\n",
      "56     \t [-1.6016663  -0.88536188]. \t  -2.809173142980251 \t 1.0247467842373679\n",
      "57     \t [0.00372818 1.07337042]. \t  -0.7051205099094748 \t 1.0247467842373679\n",
      "58     \t [-1.66448557 -0.56728501]. \t  -2.1228155949450414 \t 1.0247467842373679\n",
      "59     \t [-2.79303402  0.44502954]. \t  -59.775048507785044 \t 1.0247467842373679\n",
      "60     \t [ 2.48071995 -0.68503503]. \t  -20.07684884118497 \t 1.0247467842373679\n",
      "61     \t [ 0.15628587 -0.67105499]. \t  0.998548834118897 \t 1.0247467842373679\n",
      "62     \t [ 1.53081709 -1.67480463]. \t  -19.81874943512973 \t 1.0247467842373679\n",
      "63     \t [-1.00928588 -0.90835668]. \t  -2.587461425158197 \t 1.0247467842373679\n",
      "64     \t [ 1.73293054 -0.90303612]. \t  0.06557521846813807 \t 1.0247467842373679\n",
      "65     \t [0.21002109 0.56319727]. \t  0.5756621194672236 \t 1.0247467842373679\n",
      "66     \t [-1.10009728 -0.35363513]. \t  -2.3073517237133614 \t 1.0247467842373679\n",
      "67     \t [ 1.12482493 -1.58099808]. \t  -15.588889124059172 \t 1.0247467842373679\n",
      "68     \t [0.16930928 0.90281358]. \t  0.3371181676665549 \t 1.0247467842373679\n",
      "69     \t [-2.91393861 -0.96208655]. \t  -89.14848637067882 \t 1.0247467842373679\n",
      "70     \t [-0.03498823  0.69033745]. \t  1.0170634816410133 \t 1.0247467842373679\n",
      "71     \t [-0.88618045  0.86335414]. \t  -0.4833480521606608 \t 1.0247467842373679\n",
      "72     \t [0.19842418 0.9788918 ]. \t  -0.1883852766947534 \t 1.0247467842373679\n",
      "73     \t [-0.42996539 -0.08880044]. \t  -0.6767031144041831 \t 1.0247467842373679\n",
      "74     \t [-1.23406737 -1.61124739]. \t  -20.961775855228854 \t 1.0247467842373679\n",
      "75     \t [ 2.35794808 -0.98124886]. \t  -12.156894774937614 \t 1.0247467842373679\n",
      "76     \t [0.4971928 0.406593 ]. \t  -0.5157145329080287 \t 1.0247467842373679\n",
      "77     \t [2.10915116 1.65383641]. \t  -28.053107444365978 \t 1.0247467842373679\n",
      "78     \t [-0.11734432  0.72929147]. \t  \u001b[92m1.0268349729106456\u001b[0m \t 1.0268349729106456\n",
      "79     \t [ 0.6369389  -1.68238681]. \t  -20.95134466822598 \t 1.0268349729106456\n",
      "80     \t [-2.40525527 -1.66380939]. \t  -40.98022631262636 \t 1.0268349729106456\n",
      "81     \t [1.12943309 0.75342911]. \t  -2.246502150436677 \t 1.0268349729106456\n",
      "82     \t [-2.75694262  1.45525316]. \t  -60.907760328896764 \t 1.0268349729106456\n",
      "83     \t [-1.95619532  0.94174824]. \t  -0.9905924109691995 \t 1.0268349729106456\n",
      "84     \t [2.16973958 1.86296958]. \t  -45.40950165469089 \t 1.0268349729106456\n",
      "85     \t [2.62045572 0.94140433]. \t  -38.43933928147885 \t 1.0268349729106456\n",
      "86     \t [-0.06380384  0.71039889]. \t  \u001b[92m1.0289901281202716\u001b[0m \t 1.0289901281202716\n",
      "87     \t [-2.92248883  1.13975918]. \t  -86.87723689891227 \t 1.0289901281202716\n",
      "88     \t [ 2.93258401 -0.95958499]. \t  -87.99895438247323 \t 1.0289901281202716\n",
      "89     \t [ 0.06072783 -0.72264918]. \t  1.0271867430459622 \t 1.0289901281202716\n",
      "90     \t [ 1.84648262 -0.49219817]. \t  -0.7944946539199708 \t 1.0289901281202716\n",
      "91     \t [2.99130591 1.36234056]. \t  -116.88989274527349 \t 1.0289901281202716\n",
      "92     \t [ 1.61357124 -0.5160565 ]. \t  -0.44783734847806367 \t 1.0289901281202716\n",
      "93     \t [1.63628062 1.15385615]. \t  -5.706261461664688 \t 1.0289901281202716\n",
      "94     \t [0.02448335 0.35365705]. \t  0.42666421846708025 \t 1.0289901281202716\n",
      "95     \t [-0.38776684 -0.49921305]. \t  -0.00026002408885739126 \t 1.0289901281202716\n",
      "96     \t [ 1.98787556 -0.08699199]. \t  -3.380044472638614 \t 1.0289901281202716\n",
      "97     \t [1.16087083 0.00538577]. \t  -2.3986441238140785 \t 1.0289901281202716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98     \t [-1.79473501  1.46034985]. \t  -9.276890587165429 \t 1.0289901281202716\n",
      "99     \t [-0.38490882  1.11186483]. \t  -1.287859305303532 \t 1.0289901281202716\n",
      "100    \t [ 1.88937607 -0.95511048]. \t  -0.5568805346409538 \t 1.0289901281202716\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\n",
    "\n",
    "np.random.seed(run_num_4)\n",
    "surrogate_loser_4 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_4 = GPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_4.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.34411965 0.16704251]. \t  -0.3937629170174044 \t 0.3730261027784846\n",
      "init   \t [ 0.3625427  -0.47180763]. \t  0.3730261027784846 \t 0.3730261027784846\n",
      "init   \t [-2.28543493  1.38399646]. \t  -14.95148366216595 \t 0.3730261027784846\n",
      "init   \t [1.43844967 1.72943026]. \t  -28.545251683089745 \t 0.3730261027784846\n",
      "init   \t [2.15596804 0.8324185 ]. \t  -7.6401968083208285 \t 0.3730261027784846\n",
      "1      \t [ 2.81160912 -0.08415915]. \t  -64.79173078590617 \t 0.3730261027784846\n",
      "2      \t [-0.23106522 -0.45348889]. \t  0.3410229294819404 \t 0.3730261027784846\n",
      "3      \t [-3. -2.]. \t  -162.89999999999998 \t 0.3730261027784846\n",
      "4      \t [ 0.0789118  -0.25498825]. \t  0.23846077990721387 \t 0.3730261027784846\n",
      "5      \t [ 0.12012353 -1.48392355]. \t  -10.466640607038924 \t 0.3730261027784846\n",
      "6      \t [0.90299428 0.07482027]. \t  -2.091365268965974 \t 0.3730261027784846\n",
      "7      \t [-0.74640357 -0.30672528]. \t  -1.5223368401391302 \t 0.3730261027784846\n",
      "8      \t [3.         1.73320159]. \t  -138.1794234168509 \t 0.3730261027784846\n",
      "9      \t [1.5446228  0.73589959]. \t  -2.2601762627851594 \t 0.3730261027784846\n",
      "10     \t [-0.34933316  1.387776  ]. \t  -7.105687500746312 \t 0.3730261027784846\n",
      "11     \t [-0.56980314 -0.75746869]. \t  -0.5421105193649437 \t 0.3730261027784846\n",
      "12     \t [-2.96453478  1.94657731]. \t  -135.7248957934397 \t 0.3730261027784846\n",
      "13     \t [-1.77239096  0.83964886]. \t  0.14459279157497595 \t 0.3730261027784846\n",
      "14     \t [-1.55266112  1.54727847]. \t  -13.05620861785108 \t 0.3730261027784846\n",
      "15     \t [-2.32928803  0.69693762]. \t  -10.499719307800447 \t 0.3730261027784846\n",
      "16     \t [0.01615899 0.72974001]. \t  \u001b[92m0.9829335016898544\u001b[0m \t 0.9829335016898544\n",
      "17     \t [-1.93969194  1.05991811]. \t  -1.5745429516476164 \t 0.9829335016898544\n",
      "18     \t [-1.17228418  0.37831003]. \t  -1.46210767892256 \t 0.9829335016898544\n",
      "19     \t [-1.62460303  0.24599812]. \t  -1.4301056809276036 \t 0.9829335016898544\n",
      "20     \t [0.09482291 0.91456236]. \t  0.42475726110870216 \t 0.9829335016898544\n",
      "21     \t [ 0.97140808 -0.60636015]. \t  -0.66570467940294 \t 0.9829335016898544\n",
      "22     \t [ 1.64565052 -1.8481631 ]. \t  -32.015586964356885 \t 0.9829335016898544\n",
      "23     \t [1.83846767 0.42491363]. \t  -2.589584805300085 \t 0.9829335016898544\n",
      "24     \t [-0.02378041  0.59582521]. \t  0.9278165174691358 \t 0.9829335016898544\n",
      "25     \t [-1.95084447 -1.05593106]. \t  -5.753903332764851 \t 0.9829335016898544\n",
      "26     \t [-1.50162579 -0.92593893]. \t  -3.0649843501923475 \t 0.9829335016898544\n",
      "27     \t [-1.95876152 -0.43696331]. \t  -3.498097426729177 \t 0.9829335016898544\n",
      "28     \t [-0.01518318  0.8066197 ]. \t  0.9205610223113977 \t 0.9829335016898544\n",
      "29     \t [ 0.62926623 -0.40990959]. \t  -0.4582112676937422 \t 0.9829335016898544\n",
      "30     \t [ 0.86424664 -0.90411914]. \t  -0.5766837885142329 \t 0.9829335016898544\n",
      "31     \t [ 0.63388754 -1.32783878]. \t  -5.830374991726465 \t 0.9829335016898544\n",
      "32     \t [ 0.05770152 -0.64065586]. \t  \u001b[92m0.9915882005324195\u001b[0m \t 0.9915882005324195\n",
      "33     \t [ 0.25800663 -0.70729968]. \t  0.9254252670220189 \t 0.9915882005324195\n",
      "34     \t [0.01053681 0.73141925]. \t  0.9869563421790106 \t 0.9915882005324195\n",
      "35     \t [-2.45517733 -0.86928745]. \t  -22.21160294481286 \t 0.9915882005324195\n",
      "36     \t [-0.73732793 -1.31955173]. \t  -7.742916969456498 \t 0.9915882005324195\n",
      "37     \t [ 0.05651427 -0.71226656]. \t  \u001b[92m1.027284641349812\u001b[0m \t 1.027284641349812\n",
      "38     \t [-1.10399041 -1.86500434]. \t  -38.897831837209274 \t 1.027284641349812\n",
      "39     \t [-1.21281292 -1.62213654]. \t  -21.53850489606362 \t 1.027284641349812\n",
      "40     \t [ 1.59900716 -1.81179592]. \t  -29.14495570125768 \t 1.027284641349812\n",
      "41     \t [-2.78399023 -1.87129091]. \t  -100.30034839322298 \t 1.027284641349812\n",
      "42     \t [ 0.14469391 -0.72665586]. \t  1.0191723577766891 \t 1.027284641349812\n",
      "43     \t [-0.5066195   0.70470737]. \t  0.46302306143381566 \t 1.027284641349812\n",
      "44     \t [-2.14028291  1.7231771 ]. \t  -26.00064756068846 \t 1.027284641349812\n",
      "45     \t [-0.3756004  0.2067667]. \t  -0.28208318095697904 \t 1.027284641349812\n",
      "46     \t [ 0.2331444  -0.77053813]. \t  0.9332320338070067 \t 1.027284641349812\n",
      "47     \t [-2.28577133 -1.23976423]. \t  -17.25036356190636 \t 1.027284641349812\n",
      "48     \t [-1.50067069  0.92554643]. \t  -0.28466284252014035 \t 1.027284641349812\n",
      "49     \t [-0.02416309  0.7657793 ]. \t  0.9862966519762904 \t 1.027284641349812\n",
      "50     \t [1.65241809 1.38526686]. \t  -11.3939211718474 \t 1.027284641349812\n",
      "51     \t [-0.2291505  -0.62290786]. \t  0.602799330676699 \t 1.027284641349812\n",
      "52     \t [-0.18948295  0.70595758]. \t  0.9928328705202233 \t 1.027284641349812\n",
      "53     \t [-0.84730347 -1.46758405]. \t  -13.096412091357216 \t 1.027284641349812\n",
      "54     \t [0.73188169 0.2254517 ]. \t  -1.5633216755247445 \t 1.027284641349812\n",
      "55     \t [ 0.13160956 -0.75572143]. \t  1.0105748646378223 \t 1.027284641349812\n",
      "56     \t [2.78270860e-04 6.49628242e-01]. \t  0.975493432825064 \t 1.027284641349812\n",
      "57     \t [-2.73765081 -1.74375766]. \t  -81.94293174449048 \t 1.027284641349812\n",
      "58     \t [-2.12866504 -0.37413172]. \t  -6.334222581869954 \t 1.027284641349812\n",
      "59     \t [2.31694503 0.40924027]. \t  -12.912886046617707 \t 1.027284641349812\n",
      "60     \t [ 2.26559285 -0.83148234]. \t  -7.544644863245212 \t 1.027284641349812\n",
      "61     \t [ 1.77547482 -0.91226199]. \t  -0.004768356986522004 \t 1.027284641349812\n",
      "62     \t [-1.42640702 -1.42301748]. \t  -12.584748621560806 \t 1.027284641349812\n",
      "63     \t [-0.89526163 -1.05356796]. \t  -3.4602208165454043 \t 1.027284641349812\n",
      "64     \t [ 0.77973851 -1.87471528]. \t  -35.61904688587131 \t 1.027284641349812\n",
      "65     \t [2.04271653 0.02346745]. \t  -4.390058271514452 \t 1.027284641349812\n",
      "66     \t [1.45073363 0.33172567]. \t  -2.3136173297813474 \t 1.027284641349812\n",
      "67     \t [ 1.66456456 -0.55198868]. \t  -0.2853457169404895 \t 1.027284641349812\n",
      "68     \t [ 0.163275   -0.77495171]. \t  0.9809401316865627 \t 1.027284641349812\n",
      "69     \t [-1.87221732  0.1381844 ]. \t  -2.2411284602456933 \t 1.027284641349812\n",
      "70     \t [-2.39422297 -1.80228989]. \t  -50.23774986710231 \t 1.027284641349812\n",
      "71     \t [ 2.15851782 -0.44039846]. \t  -5.1879436534507795 \t 1.027284641349812\n",
      "72     \t [ 0.11396787 -0.78972675]. \t  0.9772268983380612 \t 1.027284641349812\n",
      "73     \t [-2.49974569  0.1867351 ]. \t  -23.726181972301156 \t 1.027284641349812\n",
      "74     \t [ 0.11163985 -0.70772203]. \t  \u001b[92m1.029478689896796\u001b[0m \t 1.029478689896796\n",
      "75     \t [-2.23134452 -0.0290725 ]. \t  -9.060634452141048 \t 1.029478689896796\n",
      "76     \t [-0.02442857  0.6942312 ]. \t  1.013270596144027 \t 1.029478689896796\n",
      "77     \t [2.86394139 1.843679  ]. \t  -113.36538306615888 \t 1.029478689896796\n",
      "78     \t [-1.17724415  0.33368658]. \t  -1.6087738020855884 \t 1.029478689896796\n",
      "79     \t [ 0.07705502 -0.7509969 ]. \t  1.0178100143550097 \t 1.029478689896796\n",
      "80     \t [ 0.29857531 -0.85700682]. \t  0.6958579526210769 \t 1.029478689896796\n",
      "81     \t [ 1.35543692 -0.59289011]. \t  -0.6122565050807526 \t 1.029478689896796\n",
      "82     \t [-2.1751507   1.01815312]. \t  -5.1571279723890875 \t 1.029478689896796\n",
      "83     \t [-0.03688358  0.76079807]. \t  0.9977768481429655 \t 1.029478689896796\n",
      "84     \t [-0.09468621  0.70924885]. \t  \u001b[92m1.0314259105576675\u001b[0m \t 1.0314259105576675\n",
      "85     \t [-1.36919712  0.41587675]. \t  -1.1729811270120105 \t 1.0314259105576675\n",
      "86     \t [0.77036719 0.86864376]. \t  -1.6322536922725321 \t 1.0314259105576675\n",
      "87     \t [-0.08958938  0.61735895]. \t  0.966820546758358 \t 1.0314259105576675\n",
      "88     \t [ 0.16384412 -0.78389894]. \t  0.9701258030438284 \t 1.0314259105576675\n",
      "89     \t [ 2.17590065 -0.84576374]. \t  -4.586281595793041 \t 1.0314259105576675\n",
      "90     \t [-0.08988776  0.71156691]. \t  \u001b[92m1.0316186898180488\u001b[0m \t 1.0316186898180488\n",
      "91     \t [ 2.74295744 -0.15740264]. \t  -52.65936821401287 \t 1.0316186898180488\n",
      "92     \t [ 2.59494145 -1.37071372]. \t  -36.53834520005145 \t 1.0316186898180488\n",
      "93     \t [-2.25927235  0.34047687]. \t  -8.853917990078358 \t 1.0316186898180488\n",
      "94     \t [-1.60483318  1.39877428]. \t  -7.308463097182967 \t 1.0316186898180488\n",
      "95     \t [-0.12380703  0.69031718]. \t  1.0224436989579837 \t 1.0316186898180488\n",
      "96     \t [2.38486964 0.39298819]. \t  -16.561846062575402 \t 1.0316186898180488\n",
      "97     \t [ 0.07577561 -0.69760554]. \t  1.0292503691137052 \t 1.0316186898180488\n",
      "98     \t [-2.65823165  1.50589863]. \t  -48.51357447859141 \t 1.0316186898180488\n",
      "99     \t [-1.09238915  0.81764816]. \t  -0.5697297134987273 \t 1.0316186898180488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [-1.065235   -1.76307983]. \t  -30.41611267523959 \t 1.0316186898180488\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\n",
    "\n",
    "np.random.seed(run_num_5)\n",
    "surrogate_loser_5 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_5 = GPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_5.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 2.35716091 -0.67208078]. \t  -11.995980169715628 \t 0.3096187466378422\n",
      "init   \t [ 1.92737474 -1.8332135 ]. \t  -31.167927894156623 \t 0.3096187466378422\n",
      "init   \t [-2.35405992  0.38020826]. \t  -13.013381745677465 \t 0.3096187466378422\n",
      "init   \t [ 0.17890417 -0.32477029]. \t  0.3096187466378422 \t 0.3096187466378422\n",
      "init   \t [-0.9875529   0.49007773]. \t  -0.9989196280666852 \t 0.3096187466378422\n",
      "1      \t [-0.12191844  0.49506843]. \t  \u001b[92m0.741453601410202\u001b[0m \t 0.741453601410202\n",
      "2      \t [-0.37098235 -0.24344843]. \t  -0.3789004257898174 \t 0.741453601410202\n",
      "3      \t [-0.669951    1.49296697]. \t  -11.359290133322727 \t 0.741453601410202\n",
      "4      \t [0.64869573 0.24332304]. \t  -1.2712393080277513 \t 0.741453601410202\n",
      "5      \t [-0.18875744  0.20424528]. \t  0.05858970564507704 \t 0.741453601410202\n",
      "6      \t [-0.20392166 -1.33670575]. \t  -6.058546494320597 \t 0.741453601410202\n",
      "7      \t [0.41775451 1.05670176]. \t  -1.5982037606762858 \t 0.741453601410202\n",
      "8      \t [3. 2.]. \t  -162.89999999999998 \t 0.741453601410202\n",
      "9      \t [-3. -2.]. \t  -162.89999999999998 \t 0.741453601410202\n",
      "10     \t [-0.04515023  0.57978425]. \t  \u001b[92m0.9106443623061353\u001b[0m \t 0.9106443623061353\n",
      "11     \t [-0.07540089  0.85796567]. \t  0.8190355668322429 \t 0.9106443623061353\n",
      "12     \t [ 0.51571504 -0.7701858 ]. \t  0.44088788826517666 \t 0.9106443623061353\n",
      "13     \t [ 0.53016302 -0.51458355]. \t  0.08574088835462013 \t 0.9106443623061353\n",
      "14     \t [-0.02073442  0.65038988]. \t  \u001b[92m0.988054489733031\u001b[0m \t 0.988054489733031\n",
      "15     \t [ 0.23033716 -0.75181801]. \t  0.9497922304217868 \t 0.988054489733031\n",
      "16     \t [-2.93993004  1.9819571 ]. \t  -133.1039740457407 \t 0.988054489733031\n",
      "17     \t [-1.62990592  0.17267372]. \t  -1.6581228167467315 \t 0.988054489733031\n",
      "18     \t [-1.28190255  0.04903936]. \t  -2.3090401732286376 \t 0.988054489733031\n",
      "19     \t [ 1.4020299  -0.06344982]. \t  -2.175263089107674 \t 0.988054489733031\n",
      "20     \t [ 2.65066632 -0.13111749]. \t  -39.63588027963832 \t 0.988054489733031\n",
      "21     \t [ 1.38167895 -0.55557483]. \t  -0.6807864018502735 \t 0.988054489733031\n",
      "22     \t [ 0.20987165 -1.93480733]. \t  -40.846711969417285 \t 0.988054489733031\n",
      "23     \t [-0.11269532 -0.70218072]. \t  0.8702118339700617 \t 0.988054489733031\n",
      "24     \t [-1.01438666 -1.69837124]. \t  -25.721097984822222 \t 0.988054489733031\n",
      "25     \t [0.00104752 0.75330168]. \t  0.9810012604209467 \t 0.988054489733031\n",
      "26     \t [ 0.09120335 -0.67357402]. \t  \u001b[92m1.0197310763253764\u001b[0m \t 1.0197310763253764\n",
      "27     \t [ 2.95885518 -1.37818516]. \t  -100.49291343525168 \t 1.0197310763253764\n",
      "28     \t [ 1.83559548 -0.56220473]. \t  -0.49072037048500294 \t 1.0197310763253764\n",
      "29     \t [-1.69983582 -0.47112305]. \t  -2.1763907126599795 \t 1.0197310763253764\n",
      "30     \t [-2.7686966  -0.13574454]. \t  -57.716559004142354 \t 1.0197310763253764\n",
      "31     \t [-1.2260299  -0.71123319]. \t  -2.271966769039133 \t 1.0197310763253764\n",
      "32     \t [-0.11935141  0.71103793]. \t  \u001b[92m1.028185177802214\u001b[0m \t 1.028185177802214\n",
      "33     \t [-1.3416263   0.91318161]. \t  -0.560822563009315 \t 1.028185177802214\n",
      "34     \t [-0.03984047  0.72294706]. \t  1.0204062549676756 \t 1.028185177802214\n",
      "35     \t [ 2.53367887 -0.35266827]. \t  -25.990881745808792 \t 1.028185177802214\n",
      "36     \t [ 1.72961388 -0.86037823]. \t  0.1605510191323053 \t 1.028185177802214\n",
      "37     \t [-2.86338355 -1.38491355]. \t  -86.3555363466685 \t 1.028185177802214\n",
      "38     \t [ 2.82050773 -0.44522179]. \t  -64.84804885355754 \t 1.028185177802214\n",
      "39     \t [-1.76056921  0.65975877]. \t  -0.004302697335522243 \t 1.028185177802214\n",
      "40     \t [ 0.46918183 -1.93111569]. \t  -40.58740832883037 \t 1.028185177802214\n",
      "41     \t [0.35628443 0.68180985]. \t  0.2775457558364143 \t 1.028185177802214\n",
      "42     \t [ 0.070658   -0.86232102]. \t  0.803652930065609 \t 1.028185177802214\n",
      "43     \t [-0.61929583  0.54890956]. \t  -0.06200497758542112 \t 1.028185177802214\n",
      "44     \t [-0.24932822  0.74696857]. \t  0.9321789350043674 \t 1.028185177802214\n",
      "45     \t [-0.10195464  0.38407504]. \t  0.500819219076793 \t 1.028185177802214\n",
      "46     \t [ 2.17226571 -1.76336922]. \t  -29.545419149518928 \t 1.028185177802214\n",
      "47     \t [-0.00278237 -0.62369932]. \t  0.9489504180662062 \t 1.028185177802214\n",
      "48     \t [ 1.59696989 -0.83888727]. \t  0.10184090170387616 \t 1.028185177802214\n",
      "49     \t [0.1057779  0.63686963]. \t  0.8524966220729688 \t 1.028185177802214\n",
      "50     \t [ 0.05868269 -0.71205515]. \t  1.027838305561759 \t 1.028185177802214\n",
      "51     \t [-1.1381645   1.52345037]. \t  -12.911047886815567 \t 1.028185177802214\n",
      "52     \t [-0.22025487  0.71312573]. \t  0.9676324422585564 \t 1.028185177802214\n",
      "53     \t [-0.21087362  0.81136128]. \t  0.8971025504292355 \t 1.028185177802214\n",
      "54     \t [ 2.40649414 -0.53453161]. \t  -15.374250651300464 \t 1.028185177802214\n",
      "55     \t [ 1.85239383 -1.13546471]. \t  -1.8552913758824587 \t 1.028185177802214\n",
      "56     \t [-0.01009399  0.64001979]. \t  0.9733824978349569 \t 1.028185177802214\n",
      "57     \t [-1.18232872 -0.730649  ]. \t  -2.2669457097328936 \t 1.028185177802214\n",
      "58     \t [0.95681507 1.31153758]. \t  -8.36744877747643 \t 1.028185177802214\n",
      "59     \t [ 1.73797702 -0.35136805]. \t  -1.0650705072639377 \t 1.028185177802214\n",
      "60     \t [-1.96668278 -1.27530915]. \t  -9.926198408177598 \t 1.028185177802214\n",
      "61     \t [2.07965784 1.54916583]. \t  -21.645796951056074 \t 1.028185177802214\n",
      "62     \t [1.79926657 0.6059405 ]. \t  -2.410937783661596 \t 1.028185177802214\n",
      "63     \t [ 1.31448403 -0.11568594]. \t  -2.156512325751887 \t 1.028185177802214\n",
      "64     \t [-2.91565639  0.60580754]. \t  -84.33043212592362 \t 1.028185177802214\n",
      "65     \t [-2.8110672  -1.00878065]. \t  -67.86265076008986 \t 1.028185177802214\n",
      "66     \t [-0.06864935  0.64692368]. \t  0.9990442407498064 \t 1.028185177802214\n",
      "67     \t [0.71127764 0.56739309]. \t  -1.059732591878914 \t 1.028185177802214\n",
      "68     \t [-0.80793547  0.26417126]. \t  -1.335852925179555 \t 1.028185177802214\n",
      "69     \t [-1.87701998 -1.16950057]. \t  -6.810399190289848 \t 1.028185177802214\n",
      "70     \t [2.62019305 1.47188718]. \t  -50.309870190637255 \t 1.028185177802214\n",
      "71     \t [-2.23506261 -1.0726989 ]. \t  -12.221866657394914 \t 1.028185177802214\n",
      "72     \t [ 0.18188601 -0.71918624]. \t  0.9995787822765818 \t 1.028185177802214\n",
      "73     \t [ 0.3954629 -0.0086061]. \t  -0.5717769588223559 \t 1.028185177802214\n",
      "74     \t [ 0.58043634 -1.47269991]. \t  -10.407376176186657 \t 1.028185177802214\n",
      "75     \t [-1.66307653  1.10260861]. \t  -1.2668489324928376 \t 1.028185177802214\n",
      "76     \t [2.33595046 0.77983323]. \t  -14.324929778720128 \t 1.028185177802214\n",
      "77     \t [1.3986583  1.96047331]. \t  -48.740762532781545 \t 1.028185177802214\n",
      "78     \t [ 0.67201117 -1.79122895]. \t  -28.549006930803053 \t 1.028185177802214\n",
      "79     \t [-0.79921115 -1.97378121]. \t  -48.48867856065587 \t 1.028185177802214\n",
      "80     \t [-0.21156133  1.27782219]. \t  -4.037729088654012 \t 1.028185177802214\n",
      "81     \t [1.71113178 0.95239292]. \t  -3.368169503521101 \t 1.028185177802214\n",
      "82     \t [-2.43590768  0.58794679]. \t  -17.098110317723936 \t 1.028185177802214\n",
      "83     \t [-2.52668816  0.23811163]. \t  -25.86433456497141 \t 1.028185177802214\n",
      "84     \t [ 0.12329002 -0.75104704]. \t  1.0158582232518898 \t 1.028185177802214\n",
      "85     \t [-0.0681401   0.76531021]. \t  1.0042434964472557 \t 1.028185177802214\n",
      "86     \t [0.01309009 0.71240814]. \t  0.9897626472955292 \t 1.028185177802214\n",
      "87     \t [ 0.12421091 -0.74203078]. \t  1.020708204125009 \t 1.028185177802214\n",
      "88     \t [-1.08728413  0.614188  ]. \t  -0.7370833259314306 \t 1.028185177802214\n",
      "89     \t [-0.01773171 -0.7071651 ]. \t  0.9862032835088613 \t 1.028185177802214\n",
      "90     \t [-1.17958267 -0.21085851]. \t  -2.4767151198401147 \t 1.028185177802214\n",
      "91     \t [-2.4632363  -1.88681742]. \t  -62.52141456818632 \t 1.028185177802214\n",
      "92     \t [ 2.28414527 -1.19344572]. \t  -10.737017723627515 \t 1.028185177802214\n",
      "93     \t [-1.08257047  0.13255206]. \t  -2.1275261860267647 \t 1.028185177802214\n",
      "94     \t [-0.0163336   1.02756611]. \t  -0.220347337255167 \t 1.028185177802214\n",
      "95     \t [ 0.01938034 -0.65838524]. \t  0.9935532728474701 \t 1.028185177802214\n",
      "96     \t [-2.64572875 -1.27727253]. \t  -46.93032312880587 \t 1.028185177802214\n",
      "97     \t [-0.00473055 -0.66039161]. \t  0.9804623599827457 \t 1.028185177802214\n",
      "98     \t [-0.17842741  0.69956862]. \t  0.999144801016171 \t 1.028185177802214\n",
      "99     \t [-0.21374517  0.70545031]. \t  0.9723682522942024 \t 1.028185177802214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [-0.07948172  0.62429612]. \t  0.9758103921078468 \t 1.028185177802214\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\n",
    "\n",
    "np.random.seed(run_num_6)\n",
    "surrogate_loser_6 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_6 = GPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_6.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-2.08401759 -0.79057356]. \t  -5.778591021444629 \t -0.6139922708386365\n",
      "init   \t [-2.62778151 -0.16055863]. \t  -37.56206986588009 \t -0.6139922708386365\n",
      "init   \t [2.01152031 1.70798819]. \t  -29.692842569565112 \t -0.6139922708386365\n",
      "init   \t [1.36193391 1.07398489]. \t  -4.49224583043401 \t -0.6139922708386365\n",
      "init   \t [-1.3847696   0.57611717]. \t  -0.6139922708386365 \t -0.6139922708386365\n",
      "1      \t [-1.33546514 -1.33946171]. \t  -9.833387638121035 \t -0.6139922708386365\n",
      "2      \t [0.2838874  0.69494898]. \t  \u001b[92m0.4926474344665255\u001b[0m \t 0.4926474344665255\n",
      "3      \t [1.34037633 0.15528739]. \t  -2.4550945739258507 \t 0.4926474344665255\n",
      "4      \t [-0.87185046  1.09262018]. \t  -1.946473086581968 \t 0.4926474344665255\n",
      "5      \t [-1.05617971  0.14038367]. \t  -2.086034244247926 \t 0.4926474344665255\n",
      "6      \t [0.72509872 0.89824793]. \t  -1.5989560308506285 \t 0.4926474344665255\n",
      "7      \t [-2.46851409 -1.80673456]. \t  -55.84426900955606 \t 0.4926474344665255\n",
      "8      \t [-1.58749501 -0.58897987]. \t  -2.1072275772073694 \t 0.4926474344665255\n",
      "9      \t [-0.06021863  0.42633027]. \t  \u001b[92m0.6060823344468371\u001b[0m \t 0.6060823344468371\n",
      "10     \t [-0.81834752  0.63697275]. \t  -0.35134076309665596 \t 0.6060823344468371\n",
      "11     \t [1.86938375 0.48547085]. \t  -2.7453181438303353 \t 0.6060823344468371\n",
      "12     \t [0.36375839 0.43282163]. \t  -0.041765999433472145 \t 0.6060823344468371\n",
      "13     \t [-1.9015448  2.       ]. \t  -46.962495267115244 \t 0.6060823344468371\n",
      "14     \t [-0.09508795  1.23944688]. \t  -3.2132176596792803 \t 0.6060823344468371\n",
      "15     \t [ 1.97819702 -1.71049586]. \t  -22.624102117582986 \t 0.6060823344468371\n",
      "16     \t [ 0.12032623 -1.86380164]. \t  -34.20612263486066 \t 0.6060823344468371\n",
      "17     \t [ 2.90296293 -0.88025528]. \t  -80.81165676528063 \t 0.6060823344468371\n",
      "18     \t [1.42003398 0.58129899]. \t  -2.190601261801846 \t 0.6060823344468371\n",
      "19     \t [ 0.77436659 -0.58444295]. \t  -0.36316671490529995 \t 0.6060823344468371\n",
      "20     \t [ 1.31343166 -0.54298105]. \t  -0.8173572114281388 \t 0.6060823344468371\n",
      "21     \t [ 1.01041903 -0.81376985]. \t  -0.53262560331891 \t 0.6060823344468371\n",
      "22     \t [-0.0848125   0.03703862]. \t  -0.02004286362349033 \t 0.6060823344468371\n",
      "23     \t [-1.2298219   0.84431664]. \t  -0.5421808330192731 \t 0.6060823344468371\n",
      "24     \t [2.90542942 0.9071926 ]. \t  -86.68689063945362 \t 0.6060823344468371\n",
      "25     \t [0.17074975 0.20096563]. \t  0.005864325293150258 \t 0.6060823344468371\n",
      "26     \t [-0.187739   -0.56880381]. \t  \u001b[92m0.6302681203753914\u001b[0m \t 0.6302681203753914\n",
      "27     \t [1.39416022 1.9784682 ]. \t  -50.677890879010995 \t 0.6302681203753914\n",
      "28     \t [-0.03130072 -0.72812491]. \t  \u001b[92m0.9696523141581839\u001b[0m \t 0.9696523141581839\n",
      "29     \t [-0.04562359 -0.58870442]. \t  0.8706647307812317 \t 0.9696523141581839\n",
      "30     \t [ 1.71523979 -0.09345485]. \t  -1.884849177923638 \t 0.9696523141581839\n",
      "31     \t [-0.53281432  1.71756302]. \t  -23.069246721273927 \t 0.9696523141581839\n",
      "32     \t [ 2.25589818 -1.79479828]. \t  -34.47555745554888 \t 0.9696523141581839\n",
      "33     \t [-0.36004187  0.78662898]. \t  0.7428213766085785 \t 0.9696523141581839\n",
      "34     \t [-1.87500732  1.5058791 ]. \t  -11.266393258109213 \t 0.9696523141581839\n",
      "35     \t [-2.06343242  1.05488176]. \t  -3.015380333743159 \t 0.9696523141581839\n",
      "36     \t [-2.92395011  1.78224008]. \t  -111.44648393053825 \t 0.9696523141581839\n",
      "37     \t [-1.73358009  0.94342478]. \t  -0.07519986442727106 \t 0.9696523141581839\n",
      "38     \t [-2.58195158  1.04187562]. \t  -29.776016537517318 \t 0.9696523141581839\n",
      "39     \t [-2.52711463 -0.53737245]. \t  -27.25506880824635 \t 0.9696523141581839\n",
      "40     \t [0.08151787 0.59227592]. \t  0.836177326986924 \t 0.9696523141581839\n",
      "41     \t [-2.66974222  1.32330721]. \t  -44.25179993477073 \t 0.9696523141581839\n",
      "42     \t [2.12310088 1.84371406]. \t  -42.42829347931966 \t 0.9696523141581839\n",
      "43     \t [1.20102993 1.13867345]. \t  -5.30654927068663 \t 0.9696523141581839\n",
      "44     \t [-1.26688507  0.95187126]. \t  -0.8421450545719095 \t 0.9696523141581839\n",
      "45     \t [-0.14819344 -0.78051211]. \t  0.7497995212476773 \t 0.9696523141581839\n",
      "46     \t [ 2.20702255 -0.10006412]. \t  -7.921342044964864 \t 0.9696523141581839\n",
      "47     \t [ 1.22830956 -0.33254151]. \t  -1.5976329368265905 \t 0.9696523141581839\n",
      "48     \t [ 1.8809176  -1.75878782]. \t  -25.220761712644656 \t 0.9696523141581839\n",
      "49     \t [1.21250649 1.14353358]. \t  -5.39680775060438 \t 0.9696523141581839\n",
      "50     \t [-0.50423155  1.21533856]. \t  -3.0924040456911426 \t 0.9696523141581839\n",
      "51     \t [-1.48759749 -0.07008751]. \t  -2.2648817888920205 \t 0.9696523141581839\n",
      "52     \t [ 0.85477092 -0.22929246]. \t  -1.5362746925688928 \t 0.9696523141581839\n",
      "53     \t [1.87037849 0.33092018]. \t  -2.7929414657368534 \t 0.9696523141581839\n",
      "54     \t [ 2.85999473 -1.45185161]. \t  -79.82478357806254 \t 0.9696523141581839\n",
      "55     \t [ 1.85806924 -0.84590557]. \t  -0.1101598636411667 \t 0.9696523141581839\n",
      "56     \t [ 1.41768042 -0.76215083]. \t  -0.2084062749618627 \t 0.9696523141581839\n",
      "57     \t [-0.68608189  0.05452516]. \t  -1.4030431217535346 \t 0.9696523141581839\n",
      "58     \t [ 0.10777843 -0.72889819]. \t  \u001b[92m1.028460684445148\u001b[0m \t 1.028460684445148\n",
      "59     \t [-0.00860593 -0.78279415]. \t  0.9421017861585248 \t 1.028460684445148\n",
      "60     \t [-0.71228017  1.56528679]. \t  -14.629359817589183 \t 1.028460684445148\n",
      "61     \t [-0.08972386  0.78315247]. \t  0.9868291510041477 \t 1.028460684445148\n",
      "62     \t [2.83701769 0.95688008]. \t  -72.36055116619683 \t 1.028460684445148\n",
      "63     \t [ 2.89550491 -0.68849145]. \t  -79.37229654354084 \t 1.028460684445148\n",
      "64     \t [ 1.12307156 -0.00925813]. \t  -2.362471481871678 \t 1.028460684445148\n",
      "65     \t [-2.31629345  0.35737844]. \t  -11.21801410637446 \t 1.028460684445148\n",
      "66     \t [-2.20651298 -0.45591465]. \t  -8.51287292237533 \t 1.028460684445148\n",
      "67     \t [ 1.90233713 -0.76710649]. \t  -0.34328865900458594 \t 1.028460684445148\n",
      "68     \t [ 0.13684218 -0.73710472]. \t  1.0191904180533808 \t 1.028460684445148\n",
      "69     \t [0.03967151 0.71908542]. \t  0.964015252325956 \t 1.028460684445148\n",
      "70     \t [-2.1289087   0.69118165]. \t  -3.5555779832535146 \t 1.028460684445148\n",
      "71     \t [2.27601261 0.04318991]. \t  -10.795542546218192 \t 1.028460684445148\n",
      "72     \t [ 0.10382921 -0.75539975]. \t  1.0156004830014775 \t 1.028460684445148\n",
      "73     \t [-1.36389509  1.56942999]. \t  -14.594391196357211 \t 1.028460684445148\n",
      "74     \t [-2.07828263 -0.23852778]. \t  -5.24056221098302 \t 1.028460684445148\n",
      "75     \t [-0.91419713 -0.72463519]. \t  -1.7357666953058564 \t 1.028460684445148\n",
      "76     \t [-0.03530569  0.65626515]. \t  0.9989683250865011 \t 1.028460684445148\n",
      "77     \t [-0.15999772  0.65753484]. \t  0.9858726103392562 \t 1.028460684445148\n",
      "78     \t [-0.14352453  0.97269335]. \t  0.26196003421164393 \t 1.028460684445148\n",
      "79     \t [1.68582026 0.37829297]. \t  -2.2051799001338055 \t 1.028460684445148\n",
      "80     \t [ 1.53943768 -1.39690109]. \t  -7.396929103144068 \t 1.028460684445148\n",
      "81     \t [0.29850144 0.75169909]. \t  0.41871435526569234 \t 1.028460684445148\n",
      "82     \t [-2.05853931 -1.20006989]. \t  -9.6113730410618 \t 1.028460684445148\n",
      "83     \t [2.70584702 0.07317875]. \t  -47.71789337336684 \t 1.028460684445148\n",
      "84     \t [ 1.61813427 -1.41849948]. \t  -7.910856169388076 \t 1.028460684445148\n",
      "85     \t [ 0.04472985 -0.76885355]. \t  0.9931731482258748 \t 1.028460684445148\n",
      "86     \t [-1.6924592  -0.31644914]. \t  -2.23660983264908 \t 1.028460684445148\n",
      "87     \t [-0.63113889 -1.01439375]. \t  -2.040767516238305 \t 1.028460684445148\n",
      "88     \t [-0.49715951  0.23486027]. \t  -0.5401806250858956 \t 1.028460684445148\n",
      "89     \t [1.99126132 0.79967047]. \t  -4.294183455666888 \t 1.028460684445148\n",
      "90     \t [-2.83890557  0.13291947]. \t  -69.88361423184833 \t 1.028460684445148\n",
      "91     \t [ 0.11791946 -0.64920987]. \t  0.9966739424163479 \t 1.028460684445148\n",
      "92     \t [ 0.21363143 -0.10393113]. \t  -0.1132682463885391 \t 1.028460684445148\n",
      "93     \t [0.02640109 1.95463056]. \t  -43.15941460461513 \t 1.028460684445148\n",
      "94     \t [ 0.17675447 -0.67155499]. \t  0.9861619164999932 \t 1.028460684445148\n",
      "95     \t [-0.10512433  0.68224647]. \t  1.0230002426460265 \t 1.028460684445148\n",
      "96     \t [ 0.14167691 -0.70925847]. \t  1.0210024037349084 \t 1.028460684445148\n",
      "97     \t [ 0.19176328 -0.71077231]. \t  0.9919225433847412 \t 1.028460684445148\n",
      "98     \t [ 1.68596276 -1.4508121 ]. \t  -8.914251805503532 \t 1.028460684445148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [ 0.0800936  -0.70385952]. \t  \u001b[92m1.0307170638615488\u001b[0m \t 1.0307170638615488\n",
      "100    \t [-0.13987136  0.65554394]. \t  0.9944900394745256 \t 1.0307170638615488\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\n",
    "\n",
    "np.random.seed(run_num_7)\n",
    "surrogate_loser_7 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_7 = GPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_7.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-2.34300075 -1.83113994]. \t  -49.66879811482005 \t 0.22190529745233212\n",
      "init   \t [ 0.59815551 -0.80051718]. \t  0.22190529745233212 \t 0.22190529745233212\n",
      "init   \t [ 1.77967069 -0.5352155 ]. \t  -0.42351602320787596 \t 0.22190529745233212\n",
      "init   \t [ 2.28365006 -0.80863347]. \t  -8.27265764318726 \t 0.22190529745233212\n",
      "init   \t [-1.02538042  0.27486766]. \t  -1.710375599764969 \t 0.22190529745233212\n",
      "1      \t [1.29037445 0.20200115]. \t  -2.480997034389709 \t 0.22190529745233212\n",
      "2      \t [ 1.18473638 -0.80920456]. \t  -0.5361185956171156 \t 0.22190529745233212\n",
      "3      \t [ 1.745343   -0.14991732]. \t  -1.7709072181220349 \t 0.22190529745233212\n",
      "4      \t [-0.04553522 -0.2940538 ]. \t  \u001b[92m0.294289344340327\u001b[0m \t 0.294289344340327\n",
      "5      \t [ 0.0496607  -0.88825695]. \t  \u001b[92m0.7001745436561713\u001b[0m \t 0.7001745436561713\n",
      "6      \t [-1.53206188  1.32201802]. \t  -5.331602106816116 \t 0.7001745436561713\n",
      "7      \t [ 1.26057476 -0.42564457]. \t  -1.2610701939933218 \t 0.7001745436561713\n",
      "8      \t [0.61353053 1.33362384]. \t  -7.58291013697965 \t 0.7001745436561713\n",
      "9      \t [ 0.39195307 -1.43455974]. \t  -8.71285239128954 \t 0.7001745436561713\n",
      "10     \t [ 0.10994135 -0.64579632]. \t  \u001b[92m0.9954366410115363\u001b[0m \t 0.9954366410115363\n",
      "11     \t [-0.25071601  0.39313884]. \t  0.37802656351648806 \t 0.9954366410115363\n",
      "12     \t [-3.  2.]. \t  -150.89999999999998 \t 0.9954366410115363\n",
      "13     \t [-0.9893803   1.38026127]. \t  -7.747796527373921 \t 0.9954366410115363\n",
      "14     \t [-1.42246108  0.76813767]. \t  -0.1970376007396586 \t 0.9954366410115363\n",
      "15     \t [ 0.16856295 -0.74523981]. \t  \u001b[92m1.0013848564518768\u001b[0m \t 1.0013848564518768\n",
      "16     \t [0.43390964 0.53273868]. \t  -0.09900533215528107 \t 1.0013848564518768\n",
      "17     \t [1.47163416 1.10462825]. \t  -4.899536069152372 \t 1.0013848564518768\n",
      "18     \t [-1.14555173  0.88120302]. \t  -0.6824264177709897 \t 1.0013848564518768\n",
      "19     \t [2.88967721 1.36537269]. \t  -91.44288653479214 \t 1.0013848564518768\n",
      "20     \t [-0.30730913 -0.64084393]. \t  0.4118441404303309 \t 1.0013848564518768\n",
      "21     \t [1.0225967  0.85490409]. \t  -2.355039942775524 \t 1.0013848564518768\n",
      "22     \t [1.10283818 1.95152523]. \t  -47.293865859347314 \t 1.0013848564518768\n",
      "23     \t [0.11878852 0.74134389]. \t  0.8460742351120865 \t 1.0013848564518768\n",
      "24     \t [-0.42301578 -1.04797595]. \t  -1.5253764391027507 \t 1.0013848564518768\n",
      "25     \t [1.5845592  0.61026497]. \t  -2.1127778838801374 \t 1.0013848564518768\n",
      "26     \t [0.00178673 0.39117344]. \t  0.517698552246121 \t 1.0013848564518768\n",
      "27     \t [ 1.88678313 -1.43312695]. \t  -8.618528568836634 \t 1.0013848564518768\n",
      "28     \t [-2.8569924  -0.31221082]. \t  -74.55055021051692 \t 1.0013848564518768\n",
      "29     \t [-2.19674658  1.74682719]. \t  -29.05991138123884 \t 1.0013848564518768\n",
      "30     \t [-0.32335014  0.89041939]. \t  0.5492350687058929 \t 1.0013848564518768\n",
      "31     \t [-0.10952098  1.11036338]. \t  -1.074679393744134 \t 1.0013848564518768\n",
      "32     \t [ 0.05183994 -0.70781622]. \t  \u001b[92m1.0259547618839664\u001b[0m \t 1.0259547618839664\n",
      "33     \t [-0.12787045  0.71512024]. \t  \u001b[92m1.0260797537026323\u001b[0m \t 1.0260797537026323\n",
      "34     \t [-0.17290339  0.71482243]. \t  1.0053993895396158 \t 1.0260797537026323\n",
      "35     \t [ 0.10720259 -0.67581912]. \t  1.0192681706072977 \t 1.0260797537026323\n",
      "36     \t [-2.64881066  0.3572587 ]. \t  -38.42520682620916 \t 1.0260797537026323\n",
      "37     \t [ 2.99367092 -1.84723403]. \t  -134.51474621210156 \t 1.0260797537026323\n",
      "38     \t [ 0.46761963 -0.45741137]. \t  0.09794985364525177 \t 1.0260797537026323\n",
      "39     \t [ 0.06044746 -0.71427139]. \t  \u001b[92m1.028173506356821\u001b[0m \t 1.028173506356821\n",
      "40     \t [-2.71533089 -1.62402195]. \t  -70.62024298757024 \t 1.028173506356821\n",
      "41     \t [ 0.16533661 -0.76573377]. \t  0.9889974498063511 \t 1.028173506356821\n",
      "42     \t [ 0.01401377 -0.74050587]. \t  1.0002413334480986 \t 1.028173506356821\n",
      "43     \t [2.74096889 0.74717986]. \t  -53.93365770139119 \t 1.028173506356821\n",
      "44     \t [-1.03532418 -1.97686049]. \t  -49.78921925317441 \t 1.028173506356821\n",
      "45     \t [1.84284901 0.18675245]. \t  -2.6298676813666293 \t 1.028173506356821\n",
      "46     \t [-1.57591629  1.85810017]. \t  -33.029232992836036 \t 1.028173506356821\n",
      "47     \t [0.00406456 0.66029699]. \t  0.9808620648452958 \t 1.028173506356821\n",
      "48     \t [-1.90559418  1.02129907]. \t  -1.0284902330103085 \t 1.028173506356821\n",
      "49     \t [-0.03110939  0.64384788]. \t  0.9869469246807769 \t 1.028173506356821\n",
      "50     \t [-0.57756502  1.07621416]. \t  -1.2245376903795397 \t 1.028173506356821\n",
      "51     \t [1.52050383 1.19365077]. \t  -6.378267640774154 \t 1.028173506356821\n",
      "52     \t [-1.26458625 -0.87970678]. \t  -2.8019793707299465 \t 1.028173506356821\n",
      "53     \t [-2.82495844  0.51092602]. \t  -65.37940866147089 \t 1.028173506356821\n",
      "54     \t [-1.84395531  0.02676023]. \t  -2.3733454070778044 \t 1.028173506356821\n",
      "55     \t [ 1.83967182 -1.66597754]. \t  -19.05207730965373 \t 1.028173506356821\n",
      "56     \t [-1.8416812  -0.31836561]. \t  -2.6369055361297424 \t 1.028173506356821\n",
      "57     \t [ 1.80811905 -0.92581519]. \t  -0.11568910277788064 \t 1.028173506356821\n",
      "58     \t [-0.15612816  0.6883878 ]. \t  1.0084861958479863 \t 1.028173506356821\n",
      "59     \t [-1.85086687  0.08896547]. \t  -2.263032534114449 \t 1.028173506356821\n",
      "60     \t [-1.57806562  0.11406319]. \t  -1.8544341899589911 \t 1.028173506356821\n",
      "61     \t [-0.39456501 -0.24192619]. \t  -0.44813147456540553 \t 1.028173506356821\n",
      "62     \t [-2.10497047  0.87975072]. \t  -2.9400379728272563 \t 1.028173506356821\n",
      "63     \t [-0.07083412  0.46505753]. \t  0.6909333218769065 \t 1.028173506356821\n",
      "64     \t [ 2.31706773 -1.89638128]. \t  -45.48156446785852 \t 1.028173506356821\n",
      "65     \t [ 0.04036884 -0.70894721]. \t  1.022079211726255 \t 1.028173506356821\n",
      "66     \t [-2.96225484 -0.50634735]. \t  -99.36079221562287 \t 1.028173506356821\n",
      "67     \t [ 0.13872231 -0.75847507]. \t  1.006346225062667 \t 1.028173506356821\n",
      "68     \t [-1.61999573 -1.36908186]. \t  -10.832711919491926 \t 1.028173506356821\n",
      "69     \t [1.29456908 0.91053371]. \t  -2.9863539416802425 \t 1.028173506356821\n",
      "70     \t [ 0.30283035 -0.15330221]. \t  -0.21119936841362813 \t 1.028173506356821\n",
      "71     \t [-0.07680614  0.78252199]. \t  0.9860970770716072 \t 1.028173506356821\n",
      "72     \t [-0.66585064  0.9565221 ]. \t  -0.4414721104708425 \t 1.028173506356821\n",
      "73     \t [ 0.83605536 -0.03075759]. \t  -1.854268583443862 \t 1.028173506356821\n",
      "74     \t [-2.65418964 -1.04802879]. \t  -43.712444501721095 \t 1.028173506356821\n",
      "75     \t [-1.95802404 -0.93238991]. \t  -4.623918931824608 \t 1.028173506356821\n",
      "76     \t [ 2.65928074 -0.8611894 ]. \t  -38.095995525025415 \t 1.028173506356821\n",
      "77     \t [-1.77438788e+00 -4.20943140e-04]. \t  -2.181051972440066 \t 1.028173506356821\n",
      "78     \t [ 2.96206493 -1.69132174]. \t  -114.8531583137974 \t 1.028173506356821\n",
      "79     \t [-0.18429283 -1.93121574]. \t  -41.2104708174101 \t 1.028173506356821\n",
      "80     \t [0.50354726 1.40187561]. \t  -9.178460961291158 \t 1.028173506356821\n",
      "81     \t [ 1.8494766  -0.49234186]. \t  -0.8070627204068261 \t 1.028173506356821\n",
      "82     \t [-1.58589212  1.4667759 ]. \t  -9.66241801894752 \t 1.028173506356821\n",
      "83     \t [ 2.12917079 -0.60851793]. \t  -3.802860591425924 \t 1.028173506356821\n",
      "84     \t [-1.62576351  1.82523629]. \t  -30.15860884407847 \t 1.028173506356821\n",
      "85     \t [-0.46050736 -1.01357847]. \t  -1.3361208843171213 \t 1.028173506356821\n",
      "86     \t [ 0.27969873 -0.04764548]. \t  -0.2778466622892492 \t 1.028173506356821\n",
      "87     \t [0.38289648 0.48997109]. \t  -0.00021019484094653151 \t 1.028173506356821\n",
      "88     \t [-1.36318854  0.16967846]. \t  -1.9772451551515737 \t 1.028173506356821\n",
      "89     \t [ 0.45605372 -0.35690517]. \t  -0.1367085318367286 \t 1.028173506356821\n",
      "90     \t [ 0.19950363 -0.63976149]. \t  0.9388239357260448 \t 1.028173506356821\n",
      "91     \t [-0.18401169  0.70978786]. \t  0.9975050826924644 \t 1.028173506356821\n",
      "92     \t [-0.1187756   1.80241837]. \t  -29.06360017085305 \t 1.028173506356821\n",
      "93     \t [ 2.27106201 -1.31394768]. \t  -12.534642518238567 \t 1.028173506356821\n",
      "94     \t [0.96610622 1.40140075]. \t  -11.101235624535835 \t 1.028173506356821\n",
      "95     \t [-1.25641567  1.01054434]. \t  -1.2094612531411373 \t 1.028173506356821\n",
      "96     \t [-2.9526468  -0.65418685]. \t  -97.08852405515171 \t 1.028173506356821\n",
      "97     \t [-0.08834708  0.71535162]. \t  \u001b[92m1.031555998209891\u001b[0m \t 1.031555998209891\n",
      "98     \t [-0.05068548  1.93404227]. \t  -40.916101305328624 \t 1.031555998209891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [-0.08204208 -1.10118477]. \t  -1.1484120974862886 \t 1.031555998209891\n",
      "100    \t [-0.15130269  0.71868847]. \t  1.0171752939520076 \t 1.031555998209891\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\n",
    "\n",
    "np.random.seed(run_num_8)\n",
    "surrogate_loser_8 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_8 = GPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_8.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.03367135 -0.0476864 ]. \t  -2.224696248130898 \t -2.224696248130898\n",
      "init   \t [ 1.95297104 -1.87421445]. \t  -34.846635035334884 \t -2.224696248130898\n",
      "init   \t [1.84829978 0.26246968]. \t  -2.674972725698239 \t -2.224696248130898\n",
      "init   \t [-1.21426501 -1.81321712]. \t  -34.68894768245752 \t -2.224696248130898\n",
      "init   \t [ 2.9437644  -1.97269707]. \t  -133.08463964627106 \t -2.224696248130898\n",
      "1      \t [ 0.99919526 -1.87802298]. \t  -36.00572890352069 \t -2.224696248130898\n",
      "2      \t [0.33113194 0.99935031]. \t  \u001b[92m-0.7395127602691062\u001b[0m \t -0.7395127602691062\n",
      "3      \t [-3.  2.]. \t  -150.89999999999998 \t -0.7395127602691062\n",
      "4      \t [3. 2.]. \t  -162.89999999999998 \t -0.7395127602691062\n",
      "5      \t [ 1.67002537 -0.17243671]. \t  -1.649161781835133 \t -0.7395127602691062\n",
      "6      \t [1.12969368 0.43622179]. \t  -2.253883281745309 \t -0.7395127602691062\n",
      "7      \t [-0.16570513  0.54535842]. \t  \u001b[92m0.8179509003220468\u001b[0m \t 0.8179509003220468\n",
      "8      \t [-0.05751053  0.77012399]. \t  \u001b[92m0.9964196538279392\u001b[0m \t 0.9964196538279392\n",
      "9      \t [-0.03611517  2.        ]. \t  -47.932983303064326 \t 0.9964196538279392\n",
      "10     \t [0.30658819 0.5243986 ]. \t  0.279006630511489 \t 0.9964196538279392\n",
      "11     \t [-2.87236259 -1.07188338]. \t  -81.0213916588206 \t 0.9964196538279392\n",
      "12     \t [-0.87807407  0.26395771]. \t  -1.497412890073796 \t 0.9964196538279392\n",
      "13     \t [0.10935452 0.71330834]. \t  0.8741522283761534 \t 0.9964196538279392\n",
      "14     \t [-0.67861359  0.6859553 ]. \t  0.032763675448674534 \t 0.9964196538279392\n",
      "15     \t [-0.41191561 -0.0296586 ]. \t  -0.6285697718919135 \t 0.9964196538279392\n",
      "16     \t [-0.27314109  0.6927226 ]. \t  0.9007153596706121 \t 0.9964196538279392\n",
      "17     \t [ 0.26701021 -0.07354288]. \t  -0.23347061416041662 \t 0.9964196538279392\n",
      "18     \t [ 2.99894711 -0.09395282]. \t  -108.28539198623508 \t 0.9964196538279392\n",
      "19     \t [1.37934276 0.19139133]. \t  -2.4272139130134565 \t 0.9964196538279392\n",
      "20     \t [ 1.44139883 -0.79972248]. \t  -0.16035224860897612 \t 0.9964196538279392\n",
      "21     \t [1.42915131 0.91986021]. \t  -3.0434012089311997 \t 0.9964196538279392\n",
      "22     \t [1.81745983 1.88303539]. \t  -41.843742047563325 \t 0.9964196538279392\n",
      "23     \t [1.14520647 1.75818012]. \t  -30.25656645742504 \t 0.9964196538279392\n",
      "24     \t [ 1.32328683 -0.56641903]. \t  -0.7337670912495751 \t 0.9964196538279392\n",
      "25     \t [-0.42259568  0.49992645]. \t  0.31184867961930723 \t 0.9964196538279392\n",
      "26     \t [-2.76250716  0.89203855]. \t  -53.25869439108361 \t 0.9964196538279392\n",
      "27     \t [-1.69225979  0.3880026 ]. \t  -0.8932344876693016 \t 0.9964196538279392\n",
      "28     \t [-0.1165043  -1.20253177]. \t  -2.7742965638888464 \t 0.9964196538279392\n",
      "29     \t [-0.38870818 -0.88453742]. \t  -0.2204247578534827 \t 0.9964196538279392\n",
      "30     \t [ 1.61637563 -0.71805284]. \t  0.09894930589342121 \t 0.9964196538279392\n",
      "31     \t [-0.015529    0.69284194]. \t  \u001b[92m1.008199453457621\u001b[0m \t 1.008199453457621\n",
      "32     \t [-1.4688038   0.60984982]. \t  -0.3724067869300681 \t 1.008199453457621\n",
      "33     \t [-1.76825634  1.49344461]. \t  -10.501847986739971 \t 1.008199453457621\n",
      "34     \t [-1.27059303  1.26578692]. \t  -4.6381505811165695 \t 1.008199453457621\n",
      "35     \t [-0.41621929 -1.69107162]. \t  -22.60869382067305 \t 1.008199453457621\n",
      "36     \t [-0.08896973 -0.72659801]. \t  0.9007001180700751 \t 1.008199453457621\n",
      "37     \t [-0.49508589  0.40682219]. \t  -0.10532062250841523 \t 1.008199453457621\n",
      "38     \t [-0.53216428  1.2966519 ]. \t  -4.863848069779513 \t 1.008199453457621\n",
      "39     \t [-1.99202267  0.24380253]. \t  -2.924056629448654 \t 1.008199453457621\n",
      "40     \t [-2.29863479 -0.12409438]. \t  -11.902097916644355 \t 1.008199453457621\n",
      "41     \t [-0.87650631 -1.01097242]. \t  -2.9610558415542174 \t 1.008199453457621\n",
      "42     \t [ 0.47621191 -0.75926556]. \t  0.5351724058246855 \t 1.008199453457621\n",
      "43     \t [-1.18293077 -0.76795044]. \t  -2.339263296591527 \t 1.008199453457621\n",
      "44     \t [1.45206898 0.98275837]. \t  -3.5174771415413497 \t 1.008199453457621\n",
      "45     \t [-0.01607511  0.75685761]. \t  0.9899142336146882 \t 1.008199453457621\n",
      "46     \t [-2.75310137 -0.51940669]. \t  -55.46388912746703 \t 1.008199453457621\n",
      "47     \t [ 0.09026541 -0.90406529]. \t  0.6463500900963194 \t 1.008199453457621\n",
      "48     \t [0.93836213 1.902392  ]. \t  -41.821636818569516 \t 1.008199453457621\n",
      "49     \t [-2.52410618 -0.37430821]. \t  -26.909505225810012 \t 1.008199453457621\n",
      "50     \t [-1.8968713  -0.93606366]. \t  -4.074291143892804 \t 1.008199453457621\n",
      "51     \t [-1.70019451  0.97764946]. \t  -0.23540301265572364 \t 1.008199453457621\n",
      "52     \t [-1.84743823  0.7883419 ]. \t  -0.04485003447364899 \t 1.008199453457621\n",
      "53     \t [ 0.170916   -0.54824899]. \t  0.8195609564870565 \t 1.008199453457621\n",
      "54     \t [-1.51966336  0.75220658]. \t  -0.017440683202022744 \t 1.008199453457621\n",
      "55     \t [-2.56793601 -1.98618002]. \t  -82.21299883561676 \t 1.008199453457621\n",
      "56     \t [ 0.13846224 -0.72520902]. \t  \u001b[92m1.0218073476478498\u001b[0m \t 1.0218073476478498\n",
      "57     \t [-0.79440385  0.0028139 ]. \t  -1.7694768026248533 \t 1.0218073476478498\n",
      "58     \t [-2.19133823 -0.89059662]. \t  -8.988892584154872 \t 1.0218073476478498\n",
      "59     \t [0.95929284 0.93192789]. \t  -2.5994911534475658 \t 1.0218073476478498\n",
      "60     \t [ 0.19120558 -0.74859182]. \t  0.985099542116302 \t 1.0218073476478498\n",
      "61     \t [ 0.03264569 -0.7185973 ]. \t  1.0181250346717692 \t 1.0218073476478498\n",
      "62     \t [ 2.10329515 -1.05429921]. \t  -3.7348255282934106 \t 1.0218073476478498\n",
      "63     \t [0.68662848 1.92131871]. \t  -42.51503427646867 \t 1.0218073476478498\n",
      "64     \t [-0.13220191  0.75868485]. \t  1.0081668120498384 \t 1.0218073476478498\n",
      "65     \t [2.34770383 1.69343593]. \t  -39.46458088155146 \t 1.0218073476478498\n",
      "66     \t [0.00205563 1.33166075]. \t  -5.488133297389344 \t 1.0218073476478498\n",
      "67     \t [ 0.06466461 -0.74848175]. \t  1.0172027564334127 \t 1.0218073476478498\n",
      "68     \t [-0.22480738  0.75437177]. \t  0.9536691424243549 \t 1.0218073476478498\n",
      "69     \t [ 0.0409425  -0.65114443]. \t  0.9968495810748208 \t 1.0218073476478498\n",
      "70     \t [ 1.90146709 -1.22487579]. \t  -3.4385320379368345 \t 1.0218073476478498\n",
      "71     \t [-0.11351374  0.67742707]. \t  1.0189493657478614 \t 1.0218073476478498\n",
      "72     \t [ 0.0946507  -0.72133732]. \t  \u001b[92m1.0309555339238001\u001b[0m \t 1.0309555339238001\n",
      "73     \t [ 0.34797029 -0.59597515]. \t  0.6693611429450617 \t 1.0309555339238001\n",
      "74     \t [ 0.0865381  -0.72285594]. \t  1.0306881131547756 \t 1.0309555339238001\n",
      "75     \t [ 0.20284511 -0.17902922]. \t  -0.0006405939934639587 \t 1.0309555339238001\n",
      "76     \t [ 0.06661796 -0.74995015]. \t  1.0166620315084538 \t 1.0309555339238001\n",
      "77     \t [ 0.11980624 -0.66905245]. \t  1.012204213731107 \t 1.0309555339238001\n",
      "78     \t [-1.02033212 -1.19297079]. \t  -5.890629022588897 \t 1.0309555339238001\n",
      "79     \t [-0.68421345 -1.28284427]. \t  -6.574707115676258 \t 1.0309555339238001\n",
      "80     \t [ 0.0803055  -1.25773149]. \t  -3.606636904367955 \t 1.0309555339238001\n",
      "81     \t [1.17077375 0.20753904]. \t  -2.4738270937380227 \t 1.0309555339238001\n",
      "82     \t [-2.71459008  0.25166402]. \t  -47.90519265973831 \t 1.0309555339238001\n",
      "83     \t [-0.16283618  0.67009116]. \t  0.9941278881942943 \t 1.0309555339238001\n",
      "84     \t [-0.95532538 -0.85972632]. \t  -2.2048844752583614 \t 1.0309555339238001\n",
      "85     \t [-0.18280673  0.78603776]. \t  0.9567930499642578 \t 1.0309555339238001\n",
      "86     \t [2.61956597 0.75564002]. \t  -37.27138351629192 \t 1.0309555339238001\n",
      "87     \t [-0.34134508 -0.78996824]. \t  0.23071146764321804 \t 1.0309555339238001\n",
      "88     \t [-2.09907457  0.74081977]. \t  -2.8230845382388163 \t 1.0309555339238001\n",
      "89     \t [-1.36466746 -0.70704187]. \t  -2.28385117953788 \t 1.0309555339238001\n",
      "90     \t [1.67255505 1.45330367]. \t  -13.8792202407375 \t 1.0309555339238001\n",
      "91     \t [-1.61862826 -0.97299392]. \t  -3.4328156026929944 \t 1.0309555339238001\n",
      "92     \t [-1.60387543 -1.47769845]. \t  -14.775435162999196 \t 1.0309555339238001\n",
      "93     \t [ 1.30313978 -0.44213513]. \t  -1.1638794116202833 \t 1.0309555339238001\n",
      "94     \t [-1.83231722 -0.88869639]. \t  -3.337389277584356 \t 1.0309555339238001\n",
      "95     \t [-1.74961224 -1.02428229]. \t  -4.1262762804269535 \t 1.0309555339238001\n",
      "96     \t [ 0.0337198  -0.62460306]. \t  0.9682295408809853 \t 1.0309555339238001\n",
      "97     \t [-1.52109416 -0.93110702]. \t  -3.0965668760517855 \t 1.0309555339238001\n",
      "98     \t [-0.96007511  0.7899094 ]. \t  -0.4669214584322007 \t 1.0309555339238001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [-0.66300677  0.88183712]. \t  -0.10450903626885111 \t 1.0309555339238001\n",
      "100    \t [-0.17807022  0.64705619]. \t  0.9640356102682575 \t 1.0309555339238001\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\n",
    "\n",
    "np.random.seed(run_num_9)\n",
    "surrogate_loser_9 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_9 = GPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_9.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.92153751 -1.53997223]. \t  -13.677695110590259 \t -6.372423095293032\n",
      "init   \t [ 2.70169719 -0.07123439]. \t  -46.72852427361676 \t -6.372423095293032\n",
      "init   \t [ 2.23484721 -1.15066928]. \t  -8.26768569212749 \t -6.372423095293032\n",
      "init   \t [-2.75574225 -0.41122215]. \t  -55.82511776655432 \t -6.372423095293032\n",
      "init   \t [-1.60120682  1.3669629 ]. \t  -6.372423095293032 \t -6.372423095293032\n",
      "1      \t [ 2.58113859 -2.        ]. \t  -74.84682024424548 \t -6.372423095293032\n",
      "2      \t [ 1.57470162 -0.81229953]. \t  \u001b[92m0.08837188195077916\u001b[0m \t 0.08837188195077916\n",
      "3      \t [-0.16836986  2.        ]. \t  -47.774973884193805 \t 0.08837188195077916\n",
      "4      \t [-2.49962425  2.        ]. \t  -67.31814071799164 \t 0.08837188195077916\n",
      "5      \t [-1.21362495  0.74715066]. \t  -0.5077121230126085 \t 0.08837188195077916\n",
      "6      \t [-1.00460368 -1.88464791]. \t  -40.39026631870184 \t 0.08837188195077916\n",
      "7      \t [ 1.73224944 -0.97707968]. \t  -0.23468266608155064 \t 0.08837188195077916\n",
      "8      \t [ 0.52111376 -0.22732585]. \t  -0.623561182377702 \t 0.08837188195077916\n",
      "9      \t [-1.32471796  0.98994364]. \t  -0.9639570804829939 \t 0.08837188195077916\n",
      "10     \t [-0.17999855 -0.14544558]. \t  -0.07075718400671668 \t 0.08837188195077916\n",
      "11     \t [ 0.12099119 -0.37646287]. \t  \u001b[92m0.4739962657887828\u001b[0m \t 0.4739962657887828\n",
      "12     \t [ 0.7462506  -0.76713968]. \t  -0.09271599182304457 \t 0.4739962657887828\n",
      "13     \t [-0.54967934 -0.13782872]. \t  -1.0272868633296435 \t 0.4739962657887828\n",
      "14     \t [2.69481592 1.85940993]. \t  -84.95549658392102 \t 0.4739962657887828\n",
      "15     \t [ 0.59916777 -0.52908489]. \t  -0.05748911540703616 \t 0.4739962657887828\n",
      "16     \t [-1.58818898  0.5876488 ]. \t  -0.24034320668621223 \t 0.4739962657887828\n",
      "17     \t [-1.25160292  0.37715646]. \t  -1.4340171483967747 \t 0.4739962657887828\n",
      "18     \t [-2.78787591 -1.96479765]. \t  -110.38179581993715 \t 0.4739962657887828\n",
      "19     \t [ 1.24875125 -0.86771665]. \t  -0.5673217357331832 \t 0.4739962657887828\n",
      "20     \t [-1.39574473  0.78893703]. \t  -0.2459029307261329 \t 0.4739962657887828\n",
      "21     \t [ 0.11324526 -0.78232445]. \t  \u001b[92m0.9874369949259401\u001b[0m \t 0.9874369949259401\n",
      "22     \t [0.29550887 0.31781972]. \t  -0.06420251557533652 \t 0.9874369949259401\n",
      "23     \t [ 0.34080812 -0.75607904]. \t  0.8003476218015324 \t 0.9874369949259401\n",
      "24     \t [ 0.06131269 -0.7757057 ]. \t  \u001b[92m0.991165990753101\u001b[0m \t 0.991165990753101\n",
      "25     \t [0.08255045 0.15424011]. \t  0.05300270726969513 \t 0.991165990753101\n",
      "26     \t [2.13476842 0.6143083 ]. \t  -6.535692351817299 \t 0.991165990753101\n",
      "27     \t [1.52478423 0.73892288]. \t  -2.2727232877362256 \t 0.991165990753101\n",
      "28     \t [1.55488417 0.18797203]. \t  -2.2623773378815177 \t 0.991165990753101\n",
      "29     \t [2.67669027 0.30631829]. \t  -43.933735469628125 \t 0.991165990753101\n",
      "30     \t [ 0.06527374 -0.67772351]. \t  \u001b[92m1.0206100231097044\u001b[0m \t 1.0206100231097044\n",
      "31     \t [ 1.63428609 -0.88268599]. \t  0.07694139238262276 \t 1.0206100231097044\n",
      "32     \t [-2.86195139  0.61201958]. \t  -72.35739599088193 \t 1.0206100231097044\n",
      "33     \t [-0.2509088   0.54609945]. \t  0.730586897383692 \t 1.0206100231097044\n",
      "34     \t [-0.27040837 -0.96833813]. \t  -0.3094825676788334 \t 1.0206100231097044\n",
      "35     \t [0.01585826 0.63383449]. \t  0.9503274134701899 \t 1.0206100231097044\n",
      "36     \t [ 0.08296447 -0.70147903]. \t  \u001b[92m1.030513445938779\u001b[0m \t 1.030513445938779\n",
      "37     \t [-0.0364743 -0.6080386]. \t  0.9046034771237113 \t 1.030513445938779\n",
      "38     \t [-2.89034399  1.27204281]. \t  -81.52572547728954 \t 1.030513445938779\n",
      "39     \t [ 0.81638987 -1.42533591]. \t  -9.051161659801737 \t 1.030513445938779\n",
      "40     \t [ 0.12451982 -0.6586628 ]. \t  1.002989111153076 \t 1.030513445938779\n",
      "41     \t [-0.08702852  1.43540279]. \t  -8.644411983134919 \t 1.030513445938779\n",
      "42     \t [-0.19633567  0.82243514]. \t  0.8859167143682395 \t 1.030513445938779\n",
      "43     \t [ 1.60287997 -1.00834182]. \t  -0.5199392992897433 \t 1.030513445938779\n",
      "44     \t [-2.74504984  0.48123736]. \t  -51.488734250744066 \t 1.030513445938779\n",
      "45     \t [2.67638905 0.66092075]. \t  -44.19808676366892 \t 1.030513445938779\n",
      "46     \t [-0.11444584  0.84084159]. \t  0.8727787895521331 \t 1.030513445938779\n",
      "47     \t [ 1.66682285 -0.63849379]. \t  -0.02176355265571417 \t 1.030513445938779\n",
      "48     \t [0.1903842  0.98731711]. \t  -0.23193249536012223 \t 1.030513445938779\n",
      "49     \t [-0.67818938 -1.69170675]. \t  -23.88900013268641 \t 1.030513445938779\n",
      "50     \t [ 0.12723363 -0.70932283]. \t  1.0260056436343195 \t 1.030513445938779\n",
      "51     \t [ 1.10803087 -1.3704361 ]. \t  -7.4405067847072495 \t 1.030513445938779\n",
      "52     \t [-0.15632649  0.7504683 ]. \t  1.004837166059053 \t 1.030513445938779\n",
      "53     \t [ 0.00148233 -0.75727758]. \t  0.9795227801906831 \t 1.030513445938779\n",
      "54     \t [2.79581991 1.08658394]. \t  -66.04549951077904 \t 1.030513445938779\n",
      "55     \t [ 0.08032289 -0.62943855]. \t  0.9817349169977259 \t 1.030513445938779\n",
      "56     \t [ 0.06745902 -0.71344767]. \t  1.0296445155167424 \t 1.030513445938779\n",
      "57     \t [ 2.0281526  -0.41041358]. \t  -2.7284919541159884 \t 1.030513445938779\n",
      "58     \t [-1.27305634  0.16225623]. \t  -2.0767178133917406 \t 1.030513445938779\n",
      "59     \t [1.92506769 1.9352494 ]. \t  -47.79856746651636 \t 1.030513445938779\n",
      "60     \t [-2.07614895  0.12052891]. \t  -4.612118253698222 \t 1.030513445938779\n",
      "61     \t [ 0.02957982 -0.71012929]. \t  1.0174338440773691 \t 1.030513445938779\n",
      "62     \t [ 2.68823408 -0.07644032]. \t  -44.80791360734754 \t 1.030513445938779\n",
      "63     \t [-2.6634366  1.2686209]. \t  -42.23689288604318 \t 1.030513445938779\n",
      "64     \t [-1.96101917  1.01096365]. \t  -1.390926440574144 \t 1.030513445938779\n",
      "65     \t [ 1.86619423 -1.93773023]. \t  -40.298911362434026 \t 1.030513445938779\n",
      "66     \t [-2.06022129 -0.21581547]. \t  -4.901236965991056 \t 1.030513445938779\n",
      "67     \t [-1.42047153 -0.78074965]. \t  -2.4165963153759584 \t 1.030513445938779\n",
      "68     \t [-0.51218763 -1.91364132]. \t  -40.88454432653726 \t 1.030513445938779\n",
      "69     \t [ 0.03119401 -0.68746469]. \t  1.0145531523293214 \t 1.030513445938779\n",
      "70     \t [-2.05895995 -1.86768905]. \t  -43.17687580612669 \t 1.030513445938779\n",
      "71     \t [ 2.0583589  -0.83649929]. \t  -2.039986723574948 \t 1.030513445938779\n",
      "72     \t [ 0.45243668 -0.26156537]. \t  -0.3603770730775245 \t 1.030513445938779\n",
      "73     \t [-0.14602888  0.7081339 ]. \t  1.0190535192624401 \t 1.030513445938779\n",
      "74     \t [2.98269366 0.65422908]. \t  -105.05867184138486 \t 1.030513445938779\n",
      "75     \t [-2.54004172  1.68853225]. \t  -44.73619958935012 \t 1.030513445938779\n",
      "76     \t [0.78801118 1.19049578]. \t  -5.057649231580321 \t 1.030513445938779\n",
      "77     \t [-2.88210765  1.73017947]. \t  -98.25984341909299 \t 1.030513445938779\n",
      "78     \t [ 0.05798271 -0.77096229]. \t  0.9956457401294769 \t 1.030513445938779\n",
      "79     \t [ 0.16231511 -0.76012353]. \t  0.9952425598083661 \t 1.030513445938779\n",
      "80     \t [-2.80964243  0.40516224]. \t  -63.001967894053784 \t 1.030513445938779\n",
      "81     \t [ 2.96091139 -1.62288339]. \t  -110.67896439468223 \t 1.030513445938779\n",
      "82     \t [ 0.10763693 -0.65051043]. \t  1.0003422793997896 \t 1.030513445938779\n",
      "83     \t [-1.71323206  1.37418432]. \t  -6.433885877833734 \t 1.030513445938779\n",
      "84     \t [-2.95256081 -0.16017044]. \t  -96.48707819542581 \t 1.030513445938779\n",
      "85     \t [-2.18775637 -1.12846761]. \t  -11.447744437308861 \t 1.030513445938779\n",
      "86     \t [ 0.32999429 -1.66886543]. \t  -19.747340796148112 \t 1.030513445938779\n",
      "87     \t [-1.51538819  0.06635407]. \t  -2.029914658998604 \t 1.030513445938779\n",
      "88     \t [ 2.13563403 -0.24182188]. \t  -5.448300544973153 \t 1.030513445938779\n",
      "89     \t [-2.45383138 -0.18643227]. \t  -21.04011729421002 \t 1.030513445938779\n",
      "90     \t [-0.01958063  0.59731677]. \t  0.9281230613370918 \t 1.030513445938779\n",
      "91     \t [ 0.6765134  -0.71638581]. \t  0.06118090557916078 \t 1.030513445938779\n",
      "92     \t [1.15290845 0.05935896]. \t  -2.4437662926618007 \t 1.030513445938779\n",
      "93     \t [-0.01181787  0.6729364 ]. \t  0.9984990878226371 \t 1.030513445938779\n",
      "94     \t [ 0.11154192 -0.70472584]. \t  1.0291192993207856 \t 1.030513445938779\n",
      "95     \t [-0.11155897  0.63997966]. \t  0.989231007265727 \t 1.030513445938779\n",
      "96     \t [-1.18736519  1.04948797]. \t  -1.6001087753452512 \t 1.030513445938779\n",
      "97     \t [-0.08400921  0.71516168]. \t  \u001b[92m1.0314294790796208\u001b[0m \t 1.0314294790796208\n",
      "98     \t [-0.95675261 -0.3437366 ]. \t  -2.069648106712903 \t 1.0314294790796208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [ 2.88993136 -1.21569296]. \t  -80.421519592609 \t 1.0314294790796208\n",
      "100    \t [2.95282291 0.70006488]. \t  -97.24887489921754 \t 1.0314294790796208\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\n",
    "\n",
    "np.random.seed(run_num_10)\n",
    "surrogate_loser_10 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_10 = GPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_10.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [2.88621398 1.2849914 ]. \t  -88.29186392936245 \t -1.3768038348803564\n",
      "init   \t [ 0.87536168 -0.30524795]. \t  -1.3768038348803564 \t -1.3768038348803564\n",
      "init   \t [-1.78613413 -0.03237292]. \t  -2.2646252293989613 \t -1.3768038348803564\n",
      "init   \t [-2.16865001 -0.18991586]. \t  -7.310805476408371 \t -1.3768038348803564\n",
      "init   \t [-2.31759223 -1.98830205]. \t  -63.86369356519016 \t -1.3768038348803564\n",
      "1      \t [-1.49201268  0.70463596]. \t  \u001b[92m-0.12368954045433\u001b[0m \t -0.12368954045433\n",
      "2      \t [-0.45308407 -0.03216398]. \t  -0.7459653765230012 \t -0.12368954045433\n",
      "3      \t [ 1.05436088 -1.42426222]. \t  -9.153262547928108 \t -0.12368954045433\n",
      "4      \t [-3.  2.]. \t  -150.89999999999998 \t -0.12368954045433\n",
      "5      \t [-0.76594485  0.81298218]. \t  -0.17211557732865268 \t -0.12368954045433\n",
      "6      \t [-1.25679659  0.44503189]. \t  -1.1977675868724482 \t -0.12368954045433\n",
      "7      \t [-0.36294215  1.68137575]. \t  -20.54115923248786 \t -0.12368954045433\n",
      "8      \t [0.21833137 0.1829002 ]. \t  \u001b[92m-0.09653783089941748\u001b[0m \t -0.09653783089941748\n",
      "9      \t [-0.01303573 -0.64033091]. \t  \u001b[92m0.9585902006942616\u001b[0m \t 0.9585902006942616\n",
      "10     \t [ 0.0693997  -0.27663533]. \t  0.2826646400132813 \t 0.9585902006942616\n",
      "11     \t [ 3. -2.]. \t  -150.89999999999998 \t 0.9585902006942616\n",
      "12     \t [-0.20856665 -1.03699856]. \t  -0.7105213907569612 \t 0.9585902006942616\n",
      "13     \t [ 0.39604382 -0.88814247]. \t  0.441103776268105 \t 0.9585902006942616\n",
      "14     \t [-0.18327676 -0.6282388 ]. \t  0.7084879170701088 \t 0.9585902006942616\n",
      "15     \t [ 0.50430093 -1.92791392]. \t  -40.307236540458206 \t 0.9585902006942616\n",
      "16     \t [ 1.15195345 -0.81468984]. \t  -0.5576976618042925 \t 0.9585902006942616\n",
      "17     \t [ 0.77654391 -0.7787171 ]. \t  -0.1621200383700261 \t 0.9585902006942616\n",
      "18     \t [-0.20576693  0.34482121]. \t  0.324388263207919 \t 0.9585902006942616\n",
      "19     \t [ 1.69940457 -0.25802557]. \t  -1.37894768894172 \t 0.9585902006942616\n",
      "20     \t [ 1.48500689 -0.07060689]. \t  -2.058544326881689 \t 0.9585902006942616\n",
      "21     \t [-1.1869001   1.36586043]. \t  -7.237349357192135 \t 0.9585902006942616\n",
      "22     \t [ 0.03554955 -0.75298165]. \t  \u001b[92m1.003770414333272\u001b[0m \t 1.003770414333272\n",
      "23     \t [-1.76950043  0.25577592]. \t  -1.4715192151055598 \t 1.003770414333272\n",
      "24     \t [-0.95521572 -1.06706791]. \t  -3.8053183751625403 \t 1.003770414333272\n",
      "25     \t [-1.78686797  0.34020014]. \t  -1.1957818858157583 \t 1.003770414333272\n",
      "26     \t [ 1.89310866 -0.95189241]. \t  -0.5643228107184426 \t 1.003770414333272\n",
      "27     \t [ 1.84666589 -0.70195988]. \t  -0.14241716686047412 \t 1.003770414333272\n",
      "28     \t [-0.70171499 -0.46429048]. \t  -1.1496538318568794 \t 1.003770414333272\n",
      "29     \t [ 0.29235642 -0.69172901]. \t  0.8736248586690547 \t 1.003770414333272\n",
      "30     \t [ 2.47715485 -0.32312617]. \t  -21.31600615236496 \t 1.003770414333272\n",
      "31     \t [-1.60814438 -0.39630624]. \t  -2.172734221647407 \t 1.003770414333272\n",
      "32     \t [-2.62528448  0.49761074]. \t  -34.89236242277745 \t 1.003770414333272\n",
      "33     \t [-2.84471413 -1.38775983]. \t  -82.57609248232174 \t 1.003770414333272\n",
      "34     \t [-2.81598185  1.04472462]. \t  -63.33651911687615 \t 1.003770414333272\n",
      "35     \t [0.11174621 0.5023075 ]. \t  0.6488512378946321 \t 1.003770414333272\n",
      "36     \t [-0.03319448  0.79596349]. \t  0.9506658273058585 \t 1.003770414333272\n",
      "37     \t [-0.15751271  0.85234849]. \t  0.8310969981086489 \t 1.003770414333272\n",
      "38     \t [-0.40375941  1.89230518]. \t  -36.79946486551535 \t 1.003770414333272\n",
      "39     \t [ 0.06166358 -0.76529036]. \t  1.0026543597050042 \t 1.003770414333272\n",
      "40     \t [-2.90117951 -1.46434979]. \t  -97.71886353155439 \t 1.003770414333272\n",
      "41     \t [ 0.07044918 -0.68206161]. \t  \u001b[92m1.0234080985991807\u001b[0m \t 1.0234080985991807\n",
      "42     \t [ 0.0936433  -0.66308675]. \t  1.0126265556046126 \t 1.0234080985991807\n",
      "43     \t [1.19588378 1.70614667]. \t  -26.691262660646913 \t 1.0234080985991807\n",
      "44     \t [1.37579412 0.30093761]. \t  -2.3925647295717716 \t 1.0234080985991807\n",
      "45     \t [0.79691694 1.092594  ]. \t  -3.5746374065231636 \t 1.0234080985991807\n",
      "46     \t [-0.01017729  0.09984365]. \t  0.04007936285370981 \t 1.0234080985991807\n",
      "47     \t [2.82544107 0.32685592]. \t  -68.22940741215106 \t 1.0234080985991807\n",
      "48     \t [-0.84808781 -1.15124713]. \t  -4.615959633772685 \t 1.0234080985991807\n",
      "49     \t [-0.83222183  0.59999437]. \t  -0.452854965556835 \t 1.0234080985991807\n",
      "50     \t [-1.01297559  1.22118176]. \t  -3.9470400971033053 \t 1.0234080985991807\n",
      "51     \t [ 1.57295258 -0.96784742]. \t  -0.33059879533611636 \t 1.0234080985991807\n",
      "52     \t [-2.75526272 -1.67922377]. \t  -80.32772808622109 \t 1.0234080985991807\n",
      "53     \t [-2.00942936 -1.00411499]. \t  -5.908005156330962 \t 1.0234080985991807\n",
      "54     \t [ 1.42606026 -1.23164889]. \t  -3.633523845217296 \t 1.0234080985991807\n",
      "55     \t [ 1.65985302 -1.65079666]. \t  -18.115849921007353 \t 1.0234080985991807\n",
      "56     \t [-0.06900149 -1.68501767]. \t  -21.024274713613476 \t 1.0234080985991807\n",
      "57     \t [-0.26597277  0.42024479]. \t  0.4208632346927586 \t 1.0234080985991807\n",
      "58     \t [ 0.70304787 -1.37012648]. \t  -7.128276134389456 \t 1.0234080985991807\n",
      "59     \t [2.51110724 1.44589507]. \t  -38.048639387264956 \t 1.0234080985991807\n",
      "60     \t [1.81954223 1.09986889]. \t  -5.337214643874206 \t 1.0234080985991807\n",
      "61     \t [-2.36156302 -0.91920798]. \t  -16.45867051626573 \t 1.0234080985991807\n",
      "62     \t [ 0.07810499 -0.66550892]. \t  1.0146154162804344 \t 1.0234080985991807\n",
      "63     \t [ 1.92745182 -1.09265228]. \t  -1.7879926467221452 \t 1.0234080985991807\n",
      "64     \t [1.28382217 1.38446992]. \t  -11.186711424391039 \t 1.0234080985991807\n",
      "65     \t [-0.05328788  1.35059511]. \t  -5.952410054366719 \t 1.0234080985991807\n",
      "66     \t [0.59229679 0.50726533]. \t  -0.695233212282974 \t 1.0234080985991807\n",
      "67     \t [2.28820001 1.96924646]. \t  -60.36695879294162 \t 1.0234080985991807\n",
      "68     \t [-1.7936637  -1.32138328]. \t  -9.813495848696078 \t 1.0234080985991807\n",
      "69     \t [-0.07057971  0.18675095]. \t  0.127945270687914 \t 1.0234080985991807\n",
      "70     \t [ 0.0127297  -0.70754001]. \t  1.008357144187566 \t 1.0234080985991807\n",
      "71     \t [-0.13984841  0.71297915]. \t  1.0220012692033893 \t 1.0234080985991807\n",
      "72     \t [-1.03620367  1.01245747]. \t  -1.3401459475904591 \t 1.0234080985991807\n",
      "73     \t [ 0.69834889 -1.19663496]. \t  -3.12830403023527 \t 1.0234080985991807\n",
      "74     \t [1.39055452 0.0702551 ]. \t  -2.3707200523887493 \t 1.0234080985991807\n",
      "75     \t [-1.1834965  -1.41171051]. \t  -11.984792929440168 \t 1.0234080985991807\n",
      "76     \t [-1.91163791  0.1638563 ]. \t  -2.4227614503194763 \t 1.0234080985991807\n",
      "77     \t [ 2.90355641 -0.5537879 ]. \t  -81.74299183075733 \t 1.0234080985991807\n",
      "78     \t [ 0.09251057 -0.76871682]. \t  1.0039653887246158 \t 1.0234080985991807\n",
      "79     \t [-0.2094308  -1.47319762]. \t  -10.639724372690395 \t 1.0234080985991807\n",
      "80     \t [2.29297314 0.912743  ]. \t  -12.963521977474125 \t 1.0234080985991807\n",
      "81     \t [ 0.41832801 -0.85291199]. \t  0.5123784196557906 \t 1.0234080985991807\n",
      "82     \t [0.82349402 1.38105833]. \t  -9.910297015095287 \t 1.0234080985991807\n",
      "83     \t [ 0.18464951 -0.70243342]. \t  0.995576690142621 \t 1.0234080985991807\n",
      "84     \t [ 2.49697937 -1.06633592]. \t  -22.056959876333448 \t 1.0234080985991807\n",
      "85     \t [-1.9039389   1.29214545]. \t  -4.794974384183428 \t 1.0234080985991807\n",
      "86     \t [-2.394412    0.04328208]. \t  -16.611638546871088 \t 1.0234080985991807\n",
      "87     \t [-2.9442584   0.83301228]. \t  -90.7038455136966 \t 1.0234080985991807\n",
      "88     \t [-1.95744191  0.87594868]. \t  -0.8178366777045933 \t 1.0234080985991807\n",
      "89     \t [-2.61651762  1.39054953]. \t  -39.500320653278514 \t 1.0234080985991807\n",
      "90     \t [-0.62303647  1.72432567]. \t  -23.650281643898392 \t 1.0234080985991807\n",
      "91     \t [-0.34059905 -0.8592493 ]. \t  0.043885211548270964 \t 1.0234080985991807\n",
      "92     \t [-0.15046339  0.70093556]. \t  1.0156786499109973 \t 1.0234080985991807\n",
      "93     \t [ 0.92488281 -1.6168044 ]. \t  -17.47523928938321 \t 1.0234080985991807\n",
      "94     \t [ 0.13896094 -0.772967  ]. \t  0.9929444737170331 \t 1.0234080985991807\n",
      "95     \t [1.73158128 0.4847771 ]. \t  -2.219664094025473 \t 1.0234080985991807\n",
      "96     \t [0.16438231 0.17275979]. \t  -0.019137404813361625 \t 1.0234080985991807\n",
      "97     \t [-0.02086051  0.65419053]. \t  0.9911504769009783 \t 1.0234080985991807\n",
      "98     \t [-0.064117    1.04860763]. \t  -0.38714987860657246 \t 1.0234080985991807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [-1.57141442  1.27918624]. \t  -4.246101476539559 \t 1.0234080985991807\n",
      "100    \t [ 2.92572179 -0.67351473]. \t  -86.47125216976039 \t 1.0234080985991807\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\n",
    "\n",
    "np.random.seed(run_num_11)\n",
    "surrogate_loser_11 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_11 = GPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_11.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-1.8508833   0.48843508]. \t  -0.8285352707628978 \t 0.04378866326980524\n",
      "init   \t [-0.37363357  1.14143433]. \t  -1.670326523786612 \t 0.04378866326980524\n",
      "init   \t [ 1.67985485 -0.90962958]. \t  0.04378866326980524 \t 0.04378866326980524\n",
      "init   \t [-1.34121447  1.20748871]. \t  -3.3921424222614553 \t 0.04378866326980524\n",
      "init   \t [2.74883612 1.50373054]. \t  -69.67081810821485 \t 0.04378866326980524\n",
      "1      \t [ 1.36865588 -1.53139099]. \t  -12.837582610424613 \t 0.04378866326980524\n",
      "2      \t [ 2.01907328 -0.67017216]. \t  -1.6471745213009104 \t 0.04378866326980524\n",
      "3      \t [ 0.95632687 -0.32053923]. \t  -1.4814491220278343 \t 0.04378866326980524\n",
      "4      \t [-2.52185414 -0.42846579]. \t  -26.72553497150102 \t 0.04378866326980524\n",
      "5      \t [-1.03910342  0.3660143 ]. \t  -1.4458929377440608 \t 0.04378866326980524\n",
      "6      \t [-2.434201    1.15945148]. \t  -18.34577343196723 \t 0.04378866326980524\n",
      "7      \t [-1.47562192  0.5276854 ]. \t  -0.6120664191308257 \t 0.04378866326980524\n",
      "8      \t [-0.65079799  2.        ]. \t  -48.0411741284007 \t 0.04378866326980524\n",
      "9      \t [-0.03651927  0.5134332 ]. \t  \u001b[92m0.7899052791479299\u001b[0m \t 0.7899052791479299\n",
      "10     \t [-0.55874873  0.75255645]. \t  0.3486262521261403 \t 0.7899052791479299\n",
      "11     \t [-0.68182621 -0.81871942]. \t  -1.1134213200050573 \t 0.7899052791479299\n",
      "12     \t [0.22307286 0.93451028]. \t  0.04020922368880081 \t 0.7899052791479299\n",
      "13     \t [ 0.03221771 -0.38128678]. \t  0.5051120347511271 \t 0.7899052791479299\n",
      "14     \t [-0.51766729 -0.35534593]. \t  -0.6701704794358964 \t 0.7899052791479299\n",
      "15     \t [ 0.47904722 -0.55358755]. \t  0.30398313795682164 \t 0.7899052791479299\n",
      "16     \t [-0.83132317 -2.        ]. \t  -51.53407072745749 \t 0.7899052791479299\n",
      "17     \t [ 2.31542293 -1.25072605]. \t  -13.085265399650483 \t 0.7899052791479299\n",
      "18     \t [-1.44252754 -0.13020417]. \t  -2.354994764354096 \t 0.7899052791479299\n",
      "19     \t [-0.06720929  0.79749381]. \t  \u001b[92m0.9615934031884311\u001b[0m \t 0.9615934031884311\n",
      "20     \t [0.14427793 0.58986659]. \t  0.74005190942764 \t 0.9615934031884311\n",
      "21     \t [0.29842325 0.08959069]. \t  -0.3346936586703786 \t 0.9615934031884311\n",
      "22     \t [-0.00189491 -0.74426156]. \t  \u001b[92m0.9869435940775936\u001b[0m \t 0.9869435940775936\n",
      "23     \t [ 1.51258176 -0.69507829]. \t  -0.10094614286422676 \t 0.9869435940775936\n",
      "24     \t [ 2.99550529 -0.45718293]. \t  -105.60224688104036 \t 0.9869435940775936\n",
      "25     \t [ 1.69287965 -0.46364454]. \t  -0.6018128861330503 \t 0.9869435940775936\n",
      "26     \t [ 1.80909271 -0.76912914]. \t  0.07501228331487741 \t 0.9869435940775936\n",
      "27     \t [1.29985675 0.52093886]. \t  -2.257435621107326 \t 0.9869435940775936\n",
      "28     \t [ 0.58458935 -0.83163814]. \t  0.2042657853247679 \t 0.9869435940775936\n",
      "29     \t [-0.18447187 -0.69256024]. \t  0.7368831911707445 \t 0.9869435940775936\n",
      "30     \t [-1.65284816 -0.97238512]. \t  -3.4522292218358306 \t 0.9869435940775936\n",
      "31     \t [-2.70880324 -1.70215847]. \t  -74.57223000826514 \t 0.9869435940775936\n",
      "32     \t [1.87635432 1.24449673]. \t  -8.334172120069411 \t 0.9869435940775936\n",
      "33     \t [1.58938744 1.84458695]. \t  -37.70688640722808 \t 0.9869435940775936\n",
      "34     \t [1.97257784 0.44109598]. \t  -3.650109450636586 \t 0.9869435940775936\n",
      "35     \t [-1.837763   -0.88272457]. \t  -3.3310873710124618 \t 0.9869435940775936\n",
      "36     \t [-2.24322914 -1.36402883]. \t  -18.890759911801425 \t 0.9869435940775936\n",
      "37     \t [0.02073162 0.66950814]. \t  0.9736852872082312 \t 0.9869435940775936\n",
      "38     \t [-0.0651702  -1.20238265]. \t  -2.672886050392152 \t 0.9869435940775936\n",
      "39     \t [1.72610936 0.72147309]. \t  -2.3391847567846917 \t 0.9869435940775936\n",
      "40     \t [-2.45673118 -0.14933551]. \t  -21.21041555327551 \t 0.9869435940775936\n",
      "41     \t [-0.09990478  0.66854638]. \t  \u001b[92m1.015820583675183\u001b[0m \t 1.015820583675183\n",
      "42     \t [-0.00416547  0.67687597]. \t  0.9957481200262653 \t 1.015820583675183\n",
      "43     \t [ 0.25235405 -1.06776635]. \t  -0.6158846625426558 \t 1.015820583675183\n",
      "44     \t [ 2.88701349 -1.34849216]. \t  -82.51936413894451 \t 1.015820583675183\n",
      "45     \t [-0.107537    0.65360158]. \t  1.0031075900127144 \t 1.015820583675183\n",
      "46     \t [ 0.04831812 -0.06504942]. \t  0.010670036435382228 \t 1.015820583675183\n",
      "47     \t [ 1.09435985 -1.59371871]. \t  -16.252352467078627 \t 1.015820583675183\n",
      "48     \t [-2.32926869 -1.14458586]. \t  -17.4122586560799 \t 1.015820583675183\n",
      "49     \t [2.97237129 0.86403187]. \t  -103.10932787910203 \t 1.015820583675183\n",
      "50     \t [2.11670865 1.93998777]. \t  -51.45584114792257 \t 1.015820583675183\n",
      "51     \t [ 0.30291667 -1.86842167]. \t  -34.5679467648692 \t 1.015820583675183\n",
      "52     \t [-0.01638743  0.7556086 ]. \t  0.9911760358422166 \t 1.015820583675183\n",
      "53     \t [ 0.31161461 -0.70407949]. \t  0.850409749511773 \t 1.015820583675183\n",
      "54     \t [-2.88617111 -0.57472514]. \t  -81.04646365654582 \t 1.015820583675183\n",
      "55     \t [-2.7191748   1.89455628]. \t  -81.53487914351996 \t 1.015820583675183\n",
      "56     \t [-1.63261223  0.79611001]. \t  0.17370621835981903 \t 1.015820583675183\n",
      "57     \t [-0.12549952  0.73024472]. \t  \u001b[92m1.0247402765264615\u001b[0m \t 1.0247402765264615\n",
      "58     \t [ 0.02521145 -0.65355836]. \t  0.9927003561827339 \t 1.0247402765264615\n",
      "59     \t [ 0.11700194 -0.66453601]. \t  1.0097484210932042 \t 1.0247402765264615\n",
      "60     \t [-0.10939444  0.78034791]. \t  0.9903231300311188 \t 1.0247402765264615\n",
      "61     \t [-0.2230927   0.77553679]. \t  0.9379216260841357 \t 1.0247402765264615\n",
      "62     \t [1.00250607 1.67031535]. \t  -23.887367103614753 \t 1.0247402765264615\n",
      "63     \t [ 2.03276638 -0.94796054]. \t  -1.898761720426021 \t 1.0247402765264615\n",
      "64     \t [ 2.05737341 -1.31175784]. \t  -6.847153012497693 \t 1.0247402765264615\n",
      "65     \t [ 2.05294824 -0.04145615]. \t  -4.418864376096343 \t 1.0247402765264615\n",
      "66     \t [ 0.91101854 -1.01279506]. \t  -1.2468445427077464 \t 1.0247402765264615\n",
      "67     \t [-1.45950325  1.38951564]. \t  -7.373869993156253 \t 1.0247402765264615\n",
      "68     \t [-1.14379772 -0.9990079 ]. \t  -3.5199274814126724 \t 1.0247402765264615\n",
      "69     \t [-0.13867673  0.79162449]. \t  0.9694487707594929 \t 1.0247402765264615\n",
      "70     \t [-0.11312156  0.66204084]. \t  1.0088198059878934 \t 1.0247402765264615\n",
      "71     \t [ 0.08695291 -0.74229564]. \t  1.0240162851663395 \t 1.0247402765264615\n",
      "72     \t [1.86328927 0.21380227]. \t  -2.7480195517584076 \t 1.0247402765264615\n",
      "73     \t [-1.08174226  1.74116814]. \t  -25.093102583029605 \t 1.0247402765264615\n",
      "74     \t [ 0.28125908 -1.77199494]. \t  -26.682743625884864 \t 1.0247402765264615\n",
      "75     \t [1.02921709 1.38264057]. \t  -10.671534166266749 \t 1.0247402765264615\n",
      "76     \t [ 0.11694851 -0.7630651 ]. \t  1.0078511553572105 \t 1.0247402765264615\n",
      "77     \t [ 0.60381498 -1.20212563]. \t  -3.0424168445531055 \t 1.0247402765264615\n",
      "78     \t [ 1.38690398 -1.03172382]. \t  -1.1400628159400346 \t 1.0247402765264615\n",
      "79     \t [0.47823896 1.91941125]. \t  -41.28190383168363 \t 1.0247402765264615\n",
      "80     \t [1.87919569 0.75327489]. \t  -3.050423098347907 \t 1.0247402765264615\n",
      "81     \t [-0.08535765 -1.48175638]. \t  -10.655775753871147 \t 1.0247402765264615\n",
      "82     \t [-0.03599048  0.4888465 ]. \t  0.739871329880445 \t 1.0247402765264615\n",
      "83     \t [ 0.16167637 -0.63669909]. \t  0.9640036600467126 \t 1.0247402765264615\n",
      "84     \t [-0.21630035  0.73482275]. \t  0.9699730006286056 \t 1.0247402765264615\n",
      "85     \t [-0.55261944  1.11049629]. \t  -1.571855403543893 \t 1.0247402765264615\n",
      "86     \t [-0.00835113 -0.68797991]. \t  0.9911275677308704 \t 1.0247402765264615\n",
      "87     \t [-0.11505374  0.77714354]. \t  0.9936068771787001 \t 1.0247402765264615\n",
      "88     \t [-2.86285431 -0.71178107]. \t  -76.27373555122846 \t 1.0247402765264615\n",
      "89     \t [ 1.80031842 -1.36858942]. \t  -6.330494902776361 \t 1.0247402765264615\n",
      "90     \t [-0.15464849  0.66019155]. \t  0.9911701974689976 \t 1.0247402765264615\n",
      "91     \t [-0.95302087 -0.50270019]. \t  -1.8741129246672243 \t 1.0247402765264615\n",
      "92     \t [ 1.80445316 -0.66892002]. \t  -0.07103172905641963 \t 1.0247402765264615\n",
      "93     \t [-0.85269625 -1.5213297 ]. \t  -15.392364583001486 \t 1.0247402765264615\n",
      "94     \t [-2.84071651  0.48809332]. \t  -68.57962407536536 \t 1.0247402765264615\n",
      "95     \t [-2.05457297  1.97172799]. \t  -45.39332497851873 \t 1.0247402765264615\n",
      "96     \t [ 0.3143381  -1.58899008]. \t  -15.27628234675918 \t 1.0247402765264615\n",
      "97     \t [1.47101432 1.85895123]. \t  -38.87908100913851 \t 1.0247402765264615\n",
      "98     \t [-1.84385328  1.19822411]. \t  -2.7181663616412473 \t 1.0247402765264615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [-1.47616923 -1.88652854]. \t  -41.40825666775498 \t 1.0247402765264615\n",
      "100    \t [-0.6322706  -0.54470962]. \t  -0.7944667087506341 \t 1.0247402765264615\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\n",
    "\n",
    "np.random.seed(run_num_12)\n",
    "surrogate_loser_12 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_12 = GPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_12.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.96386586 -0.34559739]. \t  -1.4171064314549415 \t -1.4171064314549415\n",
      "init   \t [ 1.00861535 -1.08022631]. \t  -1.9362798876488938 \t -1.4171064314549415\n",
      "init   \t [1.84607002 0.54727386]. \t  -2.606716488173805 \t -1.4171064314549415\n",
      "init   \t [-1.96683689 -0.95845653]. \t  -4.9306493500188235 \t -1.4171064314549415\n",
      "init   \t [ 2.49191994 -0.14873796]. \t  -23.22037861975103 \t -1.4171064314549415\n",
      "1      \t [1.31005933 1.26173611]. \t  -7.787111565332043 \t -1.4171064314549415\n",
      "2      \t [ 0.43242082 -2.        ]. \t  -47.81186343553399 \t -1.4171064314549415\n",
      "3      \t [ 1.33270836 -0.88429756]. \t  \u001b[92m-0.4870135931577916\u001b[0m \t -0.4870135931577916\n",
      "4      \t [2.38805277 1.31001373]. \t  -24.381446893352688 \t -0.4870135931577916\n",
      "5      \t [-3.  2.]. \t  -150.89999999999998 \t -0.4870135931577916\n",
      "6      \t [-3. -2.]. \t  -162.89999999999998 \t -0.4870135931577916\n",
      "7      \t [-1.46667321 -0.44283794]. \t  -2.223987287878843 \t -0.4870135931577916\n",
      "8      \t [-1.73961311 -0.6865412 ]. \t  -2.3087298048291416 \t -0.4870135931577916\n",
      "9      \t [-0.58506742  0.21552186]. \t  -0.833260699196913 \t -0.4870135931577916\n",
      "10     \t [-1.37186716 -1.15223027]. \t  -5.632542500554207 \t -0.4870135931577916\n",
      "11     \t [-0.92432765  0.04450565]. \t  -2.0434390386925823 \t -0.4870135931577916\n",
      "12     \t [-0.46780613 -0.19935973]. \t  -0.7188934949334121 \t -0.4870135931577916\n",
      "13     \t [0.29466439 0.80799102]. \t  \u001b[92m0.3367671138883971\u001b[0m \t 0.3367671138883971\n",
      "14     \t [1.31492263 0.50902516]. \t  -2.2625297271412563 \t 0.3367671138883971\n",
      "15     \t [0.0349059  1.21093654]. \t  -2.782602526326895 \t 0.3367671138883971\n",
      "16     \t [ 2.39459191 -1.89610523]. \t  -49.51480204750642 \t 0.3367671138883971\n",
      "17     \t [0.0285412  0.52359908]. \t  \u001b[92m0.7777767827507556\u001b[0m \t 0.7777767827507556\n",
      "18     \t [0.11811478 0.76598634]. \t  \u001b[92m0.824037210221053\u001b[0m \t 0.824037210221053\n",
      "19     \t [-0.95839959 -0.34582277]. \t  -2.070948914728223 \t 0.824037210221053\n",
      "20     \t [ 1.08321173 -0.74600562]. \t  -0.5453914232582714 \t 0.824037210221053\n",
      "21     \t [ 1.81053674 -0.06463817]. \t  -2.15428724522402 \t 0.824037210221053\n",
      "22     \t [-0.24421332  0.47108851]. \t  0.5745802046747878 \t 0.824037210221053\n",
      "23     \t [0.7930704 1.7306475]. \t  -27.043472590755005 \t 0.824037210221053\n",
      "24     \t [ 0.08299261 -0.11120322]. \t  0.030630408920149933 \t 0.824037210221053\n",
      "25     \t [-0.48325168  1.11502071]. \t  -1.494818425982562 \t 0.824037210221053\n",
      "26     \t [ 2.55651886 -1.9862726 ]. \t  -70.90235447738172 \t 0.824037210221053\n",
      "27     \t [ 1.55639598 -1.80871066]. \t  -29.013317327871505 \t 0.824037210221053\n",
      "28     \t [ 1.90344864 -1.08303389]. \t  -1.529354674870678 \t 0.824037210221053\n",
      "29     \t [ 0.35259878 -0.81303233]. \t  0.7174778129130254 \t 0.824037210221053\n",
      "30     \t [-0.06124098  0.83519581]. \t  \u001b[92m0.8800686721216469\u001b[0m \t 0.8800686721216469\n",
      "31     \t [ 0.86720838 -1.58874497]. \t  -15.972587040590096 \t 0.8800686721216469\n",
      "32     \t [-2.39903864  0.67523273]. \t  -14.396055314849475 \t 0.8800686721216469\n",
      "33     \t [-2.79255387 -0.2255001 ]. \t  -62.00397383228671 \t 0.8800686721216469\n",
      "34     \t [-1.46057876  0.91622901]. \t  -0.33511334669296833 \t 0.8800686721216469\n",
      "35     \t [-1.6440341  -1.07331517]. \t  -4.516873609960143 \t 0.8800686721216469\n",
      "36     \t [-1.75530258  0.6386886 ]. \t  -0.051351041266828656 \t 0.8800686721216469\n",
      "37     \t [ 0.08557911 -0.98719519]. \t  0.15449333551347597 \t 0.8800686721216469\n",
      "38     \t [-1.71838241  0.86045747]. \t  0.16434913394980144 \t 0.8800686721216469\n",
      "39     \t [-1.93179542 -0.90794685]. \t  -4.180232284485839 \t 0.8800686721216469\n",
      "40     \t [ 1.747245   -1.80644961]. \t  -28.509827715705118 \t 0.8800686721216469\n",
      "41     \t [-0.01215502 -0.72553865]. \t  \u001b[92m0.9878009488939742\u001b[0m \t 0.9878009488939742\n",
      "42     \t [ 0.04715145 -0.73247423]. \t  \u001b[92m1.0203201601930978\u001b[0m \t 1.0203201601930978\n",
      "43     \t [-2.41854561  1.57273742]. \t  -29.033047115733403 \t 1.0203201601930978\n",
      "44     \t [-1.60992792  2.        ]. \t  -46.844144858075786 \t 1.0203201601930978\n",
      "45     \t [-0.17399644  0.77564032]. \t  0.9744703404468986 \t 1.0203201601930978\n",
      "46     \t [-1.25297873  0.62479867]. \t  -0.6588926050167921 \t 1.0203201601930978\n",
      "47     \t [-0.19427374 -0.76524269]. \t  0.6740308143097935 \t 1.0203201601930978\n",
      "48     \t [ 1.30105651 -1.29726398]. \t  -5.279596075054974 \t 1.0203201601930978\n",
      "49     \t [-0.96661677 -0.13451372]. \t  -2.2349332119192056 \t 1.0203201601930978\n",
      "50     \t [2.83626181 1.26246322]. \t  -77.17141313318591 \t 1.0203201601930978\n",
      "51     \t [-0.90902633 -1.72516509]. \t  -27.153851646091333 \t 1.0203201601930978\n",
      "52     \t [-0.08557321  0.7377791 ]. \t  \u001b[92m1.0260991674638449\u001b[0m \t 1.0260991674638449\n",
      "53     \t [-2.49303784  1.43513443]. \t  -28.921217089979546 \t 1.0260991674638449\n",
      "54     \t [-1.69252462 -0.78432174]. \t  -2.442051036097802 \t 1.0260991674638449\n",
      "55     \t [-0.94419946  0.19469842]. \t  -1.8034552125236238 \t 1.0260991674638449\n",
      "56     \t [ 0.10044682 -0.67460576]. \t  1.019550605930943 \t 1.0260991674638449\n",
      "57     \t [2.01794246 1.90540455]. \t  -46.020810099682826 \t 1.0260991674638449\n",
      "58     \t [1.79663667 0.88683149]. \t  -3.163427451513118 \t 1.0260991674638449\n",
      "59     \t [0.6869659  0.96378109]. \t  -1.8528468462154646 \t 1.0260991674638449\n",
      "60     \t [2.2222916  1.10391883]. \t  -12.205153863231672 \t 1.0260991674638449\n",
      "61     \t [-2.99710815 -0.0030341 ]. \t  -108.09255324458013 \t 1.0260991674638449\n",
      "62     \t [ 1.43774044 -1.23302165]. \t  -3.631107448170682 \t 1.0260991674638449\n",
      "63     \t [0.39880528 1.71915394]. \t  -24.387740824734028 \t 1.0260991674638449\n",
      "64     \t [-1.2297937   0.39450762]. \t  -1.3884714176753867 \t 1.0260991674638449\n",
      "65     \t [-0.65559679 -1.92096486]. \t  -42.3242368576616 \t 1.0260991674638449\n",
      "66     \t [ 1.78891087 -1.60164163]. \t  -15.41476226025681 \t 1.0260991674638449\n",
      "67     \t [1.22694393 0.14558724]. \t  -2.4953548654668545 \t 1.0260991674638449\n",
      "68     \t [-0.0557093  0.7131464]. \t  \u001b[92m1.027040684817492\u001b[0m \t 1.027040684817492\n",
      "69     \t [ 1.07294836 -1.73211895]. \t  -24.476554163319086 \t 1.027040684817492\n",
      "70     \t [-2.32270043  1.14837687]. \t  -11.813225294815636 \t 1.027040684817492\n",
      "71     \t [1.15969125 1.68067308]. \t  -24.95735053577011 \t 1.027040684817492\n",
      "72     \t [-1.19657914  0.13310674]. \t  -2.1716268447924287 \t 1.027040684817492\n",
      "73     \t [2.18610107 1.50484932]. \t  -22.28156656774463 \t 1.027040684817492\n",
      "74     \t [-0.11627878  0.68244715]. \t  1.02095757853666 \t 1.027040684817492\n",
      "75     \t [-0.08551962 -0.45798396]. \t  0.5947092947821175 \t 1.027040684817492\n",
      "76     \t [ 1.62086379 -0.75164169]. \t  0.14273549843694178 \t 1.027040684817492\n",
      "77     \t [0.2102203  0.87408539]. \t  0.36471432165255346 \t 1.027040684817492\n",
      "78     \t [ 1.20210704 -0.84362883]. \t  -0.5660323794188019 \t 1.027040684817492\n",
      "79     \t [ 0.43868533 -1.32577587]. \t  -5.439835621608516 \t 1.027040684817492\n",
      "80     \t [0.60578523 1.84009159]. \t  -34.630798189964956 \t 1.027040684817492\n",
      "81     \t [ 1.70130709 -0.76429569]. \t  0.20456437072344746 \t 1.027040684817492\n",
      "82     \t [0.6419439  0.94451402]. \t  -1.5363892153679015 \t 1.027040684817492\n",
      "83     \t [ 0.08449206 -0.71513343]. \t  \u001b[92m1.0314530828837465\u001b[0m \t 1.0314530828837465\n",
      "84     \t [-0.97621935 -1.78289534]. \t  -31.635843118061782 \t 1.0314530828837465\n",
      "85     \t [-1.90434584  0.91978009]. \t  -0.5131550202266988 \t 1.0314530828837465\n",
      "86     \t [-0.16305448  0.73387875]. \t  1.0088402439342428 \t 1.0314530828837465\n",
      "87     \t [-1.71792663 -0.20698257]. \t  -2.274162579555493 \t 1.0314530828837465\n",
      "88     \t [-0.55281132  1.98414813]. \t  -46.18651465663658 \t 1.0314530828837465\n",
      "89     \t [ 1.60525888 -0.94601304]. \t  -0.17195007905176535 \t 1.0314530828837465\n",
      "90     \t [1.04047799 0.37476817]. \t  -2.199132117765278 \t 1.0314530828837465\n",
      "91     \t [1.84590816 1.96331092]. \t  -50.07202860187505 \t 1.0314530828837465\n",
      "92     \t [ 2.63088989 -1.92145613]. \t  -72.31278555254752 \t 1.0314530828837465\n",
      "93     \t [-0.89164821  1.41926786]. \t  -8.927469161027126 \t 1.0314530828837465\n",
      "94     \t [-0.5559054  -1.92776883]. \t  -42.495221664314876 \t 1.0314530828837465\n",
      "95     \t [-0.05174774  0.200276  ]. \t  0.15367406674279468 \t 1.0314530828837465\n",
      "96     \t [-2.11805478  0.63698247]. \t  -3.4627648976880536 \t 1.0314530828837465\n",
      "97     \t [-1.25272698  1.42225095]. \t  -8.88772114223142 \t 1.0314530828837465\n",
      "98     \t [-1.61716448  1.57313368]. \t  -14.114876185932932 \t 1.0314530828837465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [ 1.79900006 -0.13919475]. \t  -1.9228507851187322 \t 1.0314530828837465\n",
      "100    \t [-0.02473527  0.77148006]. \t  0.9803981624053298 \t 1.0314530828837465\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\n",
    "\n",
    "np.random.seed(run_num_13)\n",
    "surrogate_loser_13 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_13 = GPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_13.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.8853063  0.02859875]. \t  -2.0275945291834825 \t -2.0275945291834825\n",
      "init   \t [0.17004828 1.58514082]. \t  -15.586853032333524 \t -2.0275945291834825\n",
      "init   \t [1.19994714 0.85718841]. \t  -2.6498170273522446 \t -2.0275945291834825\n",
      "init   \t [ 1.30403029 -1.10872215]. \t  -2.0500441913631415 \t -2.0275945291834825\n",
      "init   \t [-1.94907286 -0.17263405]. \t  -3.38472265204694 \t -2.0275945291834825\n",
      "1      \t [2.22212993 0.71391762]. \t  -9.267467852913526 \t -2.0275945291834825\n",
      "2      \t [-3.  2.]. \t  -150.89999999999998 \t -2.0275945291834825\n",
      "3      \t [-1.76631702 -0.9542249 ]. \t  -3.5211107459231856 \t -2.0275945291834825\n",
      "4      \t [ 3. -2.]. \t  -150.89999999999998 \t -2.0275945291834825\n",
      "5      \t [ 0.59863203 -1.33262018]. \t  -5.892789939167 \t -2.0275945291834825\n",
      "6      \t [ 1.22521833 -0.55778933]. \t  \u001b[92m-0.8592166103441335\u001b[0m \t -0.8592166103441335\n",
      "7      \t [2.86493458 1.95006643]. \t  -123.89427694145698 \t -0.8592166103441335\n",
      "8      \t [1.84408725 0.19782139]. \t  -2.640568209557654 \t -0.8592166103441335\n",
      "9      \t [-2.43372984 -1.17158059]. \t  -24.18099320584752 \t -0.8592166103441335\n",
      "10     \t [-1.19668044 -0.34246918]. \t  -2.39623121039255 \t -0.8592166103441335\n",
      "11     \t [-1.2698406  -1.97667729]. \t  -50.33481686808442 \t -0.8592166103441335\n",
      "12     \t [-1.65539649 -0.53709486]. \t  -2.119064475951476 \t -0.8592166103441335\n",
      "13     \t [-0.27828293  0.11499358]. \t  \u001b[92m-0.21313090597920756\u001b[0m \t -0.21313090597920756\n",
      "14     \t [ 0.97294219 -0.92051979]. \t  -0.7744551750891745 \t -0.21313090597920756\n",
      "15     \t [-0.75450226  0.45429017]. \t  -0.6601265802077101 \t -0.21313090597920756\n",
      "16     \t [-0.05270931  0.49262473]. \t  \u001b[92m0.7500128897410501\u001b[0m \t 0.7500128897410501\n",
      "17     \t [0.12705145 0.45934689]. \t  0.5435318103932159 \t 0.7500128897410501\n",
      "18     \t [-0.92110091 -0.80118432]. \t  -1.9041520701096513 \t 0.7500128897410501\n",
      "19     \t [ 0.99111508 -1.86840252]. \t  -35.14963296511663 \t 0.7500128897410501\n",
      "20     \t [-0.23629567 -0.86913022]. \t  0.3168839462767482 \t 0.7500128897410501\n",
      "21     \t [-0.04143346 -0.83864002]. \t  \u001b[92m0.7930401690402418\u001b[0m \t 0.7930401690402418\n",
      "22     \t [2.90520404 0.32450415]. \t  -85.14757073466616 \t 0.7930401690402418\n",
      "23     \t [1.70649561 0.67367013]. \t  -2.2297154552443628 \t 0.7930401690402418\n",
      "24     \t [0.54746647 0.71098197]. \t  -0.4085657348914663 \t 0.7930401690402418\n",
      "25     \t [0.90161569 1.91494816]. \t  -42.889750016103875 \t 0.7930401690402418\n",
      "26     \t [-0.43938629  0.80575097]. \t  0.5685839737469045 \t 0.7930401690402418\n",
      "27     \t [ 2.70659158 -1.61929638]. \t  -60.28030912160772 \t 0.7930401690402418\n",
      "28     \t [ 2.14080046 -0.91975417]. \t  -3.820636547509968 \t 0.7930401690402418\n",
      "29     \t [ 1.85660516 -1.09728028]. \t  -1.4337583833973955 \t 0.7930401690402418\n",
      "30     \t [-0.54522033  1.84954709]. \t  -33.128697758108025 \t 0.7930401690402418\n",
      "31     \t [-1.66765019  0.1739341 ]. \t  -1.64465429536343 \t 0.7930401690402418\n",
      "32     \t [ 0.15039673 -0.56293855]. \t  \u001b[92m0.8611552539810666\u001b[0m \t 0.8611552539810666\n",
      "33     \t [-0.12304489 -0.6593215 ]. \t  0.8417400486838296 \t 0.8611552539810666\n",
      "34     \t [-0.21679237  0.76764576]. \t  \u001b[92m0.9511444158953275\u001b[0m \t 0.9511444158953275\n",
      "35     \t [1.09516905 1.14525105]. \t  -5.240779269036343 \t 0.9511444158953275\n",
      "36     \t [ 1.13094065 -1.21259074]. \t  -3.7733070913793583 \t 0.9511444158953275\n",
      "37     \t [-1.08179979 -1.60495721]. \t  -20.312793644925716 \t 0.9511444158953275\n",
      "38     \t [ 2.16573892 -1.28897526]. \t  -8.562451589721018 \t 0.9511444158953275\n",
      "39     \t [0.35808799 0.65486896]. \t  0.26616963652630576 \t 0.9511444158953275\n",
      "40     \t [ 0.15327534 -0.10396972]. \t  -0.03411115926070339 \t 0.9511444158953275\n",
      "41     \t [0.6593961 1.8653659]. \t  -37.111440235724615 \t 0.9511444158953275\n",
      "42     \t [2.10011274 0.31309679]. \t  -5.693889565865355 \t 0.9511444158953275\n",
      "43     \t [-0.10791116  0.68471664]. \t  \u001b[92m1.0237089685534349\u001b[0m \t 1.0237089685534349\n",
      "44     \t [-2.22394161  0.79089015]. \t  -6.04654147429078 \t 1.0237089685534349\n",
      "45     \t [-2.21840895  0.4034016 ]. \t  -7.115241544566829 \t 1.0237089685534349\n",
      "46     \t [-2.88621869  1.55338924]. \t  -89.43820997480013 \t 1.0237089685534349\n",
      "47     \t [-1.64989912  0.87866362]. \t  0.10247195430325118 \t 1.0237089685534349\n",
      "48     \t [-0.09212575  0.72308019]. \t  \u001b[92m1.0307291834318548\u001b[0m \t 1.0307291834318548\n",
      "49     \t [ 0.10220664 -0.80951208]. \t  0.944696954014915 \t 1.0307291834318548\n",
      "50     \t [-0.07532625  1.99383881]. \t  -47.18587221993711 \t 1.0307291834318548\n",
      "51     \t [ 0.10648995 -0.76516185]. \t  1.007168876615758 \t 1.0307291834318548\n",
      "52     \t [ 0.43781402 -1.43377894]. \t  -8.74527030585627 \t 1.0307291834318548\n",
      "53     \t [-2.42007473 -0.09673544]. \t  -18.556136911437832 \t 1.0307291834318548\n",
      "54     \t [0.07551448 0.3643988 ]. \t  0.41035802448501757 \t 1.0307291834318548\n",
      "55     \t [0.03270282 0.74571641]. \t  0.9587517948146356 \t 1.0307291834318548\n",
      "56     \t [ 0.48430012 -1.70372237]. \t  -22.09313973590703 \t 1.0307291834318548\n",
      "57     \t [-2.3157445  -0.21862567]. \t  -12.789620113637667 \t 1.0307291834318548\n",
      "58     \t [-1.28151099  1.06928366]. \t  -1.6670903211021577 \t 1.0307291834318548\n",
      "59     \t [1.64034857 1.96576279]. \t  -49.549044592541584 \t 1.0307291834318548\n",
      "60     \t [-1.93488522  0.17919941]. \t  -2.5615017531809587 \t 1.0307291834318548\n",
      "61     \t [-1.44121622 -1.88793319]. \t  -41.515838564113196 \t 1.0307291834318548\n",
      "62     \t [ 0.12585199 -0.65868511]. \t  1.0025726435130191 \t 1.0307291834318548\n",
      "63     \t [-2.44222575  0.55814753]. \t  -17.657978830799458 \t 1.0307291834318548\n",
      "64     \t [ 0.03643459 -0.67015704]. \t  1.0087516990154541 \t 1.0307291834318548\n",
      "65     \t [2.55096516 0.46749753]. \t  -29.466997174844437 \t 1.0307291834318548\n",
      "66     \t [-0.07051334  0.72067078]. \t  1.0294800290370425 \t 1.0307291834318548\n",
      "67     \t [0.01954082 0.74888036]. \t  0.969042035983855 \t 1.0307291834318548\n",
      "68     \t [ 2.98527148 -1.09888872]. \t  -102.51457751696026 \t 1.0307291834318548\n",
      "69     \t [ 0.04879414 -0.67179856]. \t  1.0137866845821022 \t 1.0307291834318548\n",
      "70     \t [ 0.01219218 -0.73988296]. \t  0.9994290287373999 \t 1.0307291834318548\n",
      "71     \t [-0.81959939  0.91554542]. \t  -0.5476097190111732 \t 1.0307291834318548\n",
      "72     \t [ 0.04350959 -0.63626204]. \t  0.9838888963714706 \t 1.0307291834318548\n",
      "73     \t [-2.96057673  1.65924245]. \t  -112.57863068680221 \t 1.0307291834318548\n",
      "74     \t [ 1.75642857 -0.7788879 ]. \t  0.18181974765279496 \t 1.0307291834318548\n",
      "75     \t [-1.95708465  1.74740684]. \t  -24.903281358737694 \t 1.0307291834318548\n",
      "76     \t [ 2.4734786  -1.65451984]. \t  -37.13482608911188 \t 1.0307291834318548\n",
      "77     \t [ 0.95603388 -1.84167032]. \t  -32.84434949799984 \t 1.0307291834318548\n",
      "78     \t [-2.48615387 -1.88059049]. \t  -63.76760783501793 \t 1.0307291834318548\n",
      "79     \t [ 2.02351394 -1.69285078]. \t  -22.014836770849858 \t 1.0307291834318548\n",
      "80     \t [ 0.14347648 -0.70125623]. \t  1.0188871964587585 \t 1.0307291834318548\n",
      "81     \t [-2.18901025  0.51048487]. \t  -5.735334761615723 \t 1.0307291834318548\n",
      "82     \t [-1.55490134 -1.18913026]. \t  -6.297215392267328 \t 1.0307291834318548\n",
      "83     \t [-0.7067576  -1.05849114]. \t  -2.803303437001512 \t 1.0307291834318548\n",
      "84     \t [ 2.03972401 -1.32236183]. \t  -6.83631869632155 \t 1.0307291834318548\n",
      "85     \t [ 1.00833903 -1.50728794]. \t  -12.28532548398758 \t 1.0307291834318548\n",
      "86     \t [1.48891204 0.59208065]. \t  -2.149505595439773 \t 1.0307291834318548\n",
      "87     \t [1.99594863 0.34924764]. \t  -3.950720095656112 \t 1.0307291834318548\n",
      "88     \t [2.45008353 0.59344997]. \t  -20.98449250856693 \t 1.0307291834318548\n",
      "89     \t [-0.15638063  1.41651631]. \t  -7.95344433075686 \t 1.0307291834318548\n",
      "90     \t [ 2.44570897 -0.88399802]. \t  -17.282280926879103 \t 1.0307291834318548\n",
      "91     \t [-0.09347063  0.66943619]. \t  1.01702981816735 \t 1.0307291834318548\n",
      "92     \t [-0.48323742  0.50783211]. \t  0.18713766016017597 \t 1.0307291834318548\n",
      "93     \t [ 1.46615004 -0.7306553 ]. \t  -0.13904126292789754 \t 1.0307291834318548\n",
      "94     \t [-0.46281242  0.29894351]. \t  -0.2998321858296047 \t 1.0307291834318548\n",
      "95     \t [0.7504753  0.84422357]. \t  -1.4608226973697307 \t 1.0307291834318548\n",
      "96     \t [ 1.75428451 -0.73863175]. \t  0.15089273627477506 \t 1.0307291834318548\n",
      "97     \t [ 1.9030235  -1.07106191]. \t  -1.4133324654497175 \t 1.0307291834318548\n",
      "98     \t [ 2.91042024 -0.23785788]. \t  -84.88874768631914 \t 1.0307291834318548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [ 2.69790981 -0.2587271 ]. \t  -45.45107101086421 \t 1.0307291834318548\n",
      "100    \t [-0.98991277 -1.91084742]. \t  -42.832123111171335 \t 1.0307291834318548\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\n",
    "\n",
    "np.random.seed(run_num_14)\n",
    "surrogate_loser_14 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_14 = GPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_14.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.46729115 -0.97799468]. \t  -0.15324775214966774 \t -0.10477079024230418\n",
      "init   \t [-1.48483878  0.83961742]. \t  -0.10477079024230418 \t -0.10477079024230418\n",
      "init   \t [-0.31468582 -1.09222817]. \t  -1.6403350770531386 \t -0.10477079024230418\n",
      "init   \t [-0.58974261  1.52903094]. \t  -12.761482581115207 \t -0.10477079024230418\n",
      "init   \t [-0.3767192   1.51370072]. \t  -11.790964688915137 \t -0.10477079024230418\n",
      "1      \t [-2.12826797  0.32652419]. \t  -4.934123393463919 \t -0.10477079024230418\n",
      "2      \t [-0.88114718  0.18471356]. \t  -1.7011777695465604 \t -0.10477079024230418\n",
      "3      \t [1.30747098 1.31648618]. \t  -9.17002452824579 \t -0.10477079024230418\n",
      "4      \t [ 0.99645987 -1.97738227]. \t  -45.770750922776465 \t -0.10477079024230418\n",
      "5      \t [ 0.46125796 -0.28130057]. \t  -0.3379606132854113 \t -0.10477079024230418\n",
      "6      \t [ 0.25749526 -0.69779905]. \t  \u001b[92m0.9229155845722549\u001b[0m \t 0.9229155845722549\n",
      "7      \t [ 0.98466881 -0.4899728 ]. \t  -0.9957501949142898 \t 0.9229155845722549\n",
      "8      \t [ 0.47743892 -0.68312734]. \t  0.5150829177410041 \t 0.9229155845722549\n",
      "9      \t [ 2.18035285 -0.06687561]. \t  -7.205171443921705 \t 0.9229155845722549\n",
      "10     \t [-3.  2.]. \t  -150.89999999999998 \t 0.9229155845722549\n",
      "11     \t [-1.38067625  0.49998483]. \t  -0.8627199769170979 \t 0.9229155845722549\n",
      "12     \t [-0.22558852  0.50400929]. \t  0.6735185773894292 \t 0.9229155845722549\n",
      "13     \t [-3.         -0.69952834]. \t  -109.99903957465264 \t 0.9229155845722549\n",
      "14     \t [-1.8325366   0.64519493]. \t  -0.2197602862835628 \t 0.9229155845722549\n",
      "15     \t [-0.25808623  0.2194048 ]. \t  -0.017305489446127836 \t 0.9229155845722549\n",
      "16     \t [0.55001786 0.59367052]. \t  -0.4407388487642898 \t 0.9229155845722549\n",
      "17     \t [0.92754134 0.01192162]. \t  -2.109723330951134 \t 0.9229155845722549\n",
      "18     \t [0.08728408 0.64163789]. \t  0.8824547845376621 \t 0.9229155845722549\n",
      "19     \t [ 2.857054  -0.5307976]. \t  -71.69727766213151 \t 0.9229155845722549\n",
      "20     \t [2.24225599 0.6516924 ]. \t  -9.874536126355009 \t 0.9229155845722549\n",
      "21     \t [ 1.84514496 -0.12996827]. \t  -2.124961143833328 \t 0.9229155845722549\n",
      "22     \t [ 0.15302981 -0.92372258]. \t  0.5496518367599035 \t 0.9229155845722549\n",
      "23     \t [-0.90985098  0.80071061]. \t  -0.41243977398451237 \t 0.9229155845722549\n",
      "24     \t [-0.65283978 -0.61839555]. \t  -0.8081678184059434 \t 0.9229155845722549\n",
      "25     \t [0.69644078 1.87769703]. \t  -38.412384059828305 \t 0.9229155845722549\n",
      "26     \t [-1.21411749 -1.82230836]. \t  -35.44120904135842 \t 0.9229155845722549\n",
      "27     \t [-0.09552351 -0.61588959]. \t  0.846589031422058 \t 0.9229155845722549\n",
      "28     \t [-0.4179639   0.86765849]. \t  0.4704906179734629 \t 0.9229155845722549\n",
      "29     \t [2.59661688 1.79081733]. \t  -66.63587545198251 \t 0.9229155845722549\n",
      "30     \t [-0.11242265  0.72179608]. \t  \u001b[92m1.0291633506171936\u001b[0m \t 1.0291633506171936\n",
      "31     \t [-0.04905967  0.61613668]. \t  0.962651089289744 \t 1.0291633506171936\n",
      "32     \t [ 2.72757434 -0.40090776]. \t  -49.15178086195256 \t 1.0291633506171936\n",
      "33     \t [-0.1158351   0.69995743]. \t  1.027381046851688 \t 1.0291633506171936\n",
      "34     \t [ 0.1546346  -0.71857804]. \t  1.0155958974209214 \t 1.0291633506171936\n",
      "35     \t [-0.84449985  0.69228797]. \t  -0.3226029563542343 \t 1.0291633506171936\n",
      "36     \t [1.00737241 0.08566848]. \t  -2.3020919903908 \t 1.0291633506171936\n",
      "37     \t [-0.06770591  0.6339393 ]. \t  0.9861182745260134 \t 1.0291633506171936\n",
      "38     \t [-0.07124171  0.70816797]. \t  \u001b[92m1.0301946013322725\u001b[0m \t 1.0301946013322725\n",
      "39     \t [-1.45413003  0.50560714]. \t  -0.7236899118730549 \t 1.0301946013322725\n",
      "40     \t [-0.18822731  0.71327892]. \t  0.9948542678033747 \t 1.0301946013322725\n",
      "41     \t [-0.20066151  0.66974516]. \t  0.9661299434538988 \t 1.0301946013322725\n",
      "42     \t [-0.67226021  0.90796726]. \t  -0.22015371246861315 \t 1.0301946013322725\n",
      "43     \t [2.62829925 1.14763938]. \t  -41.98916315898547 \t 1.0301946013322725\n",
      "44     \t [0.25510179 0.69925623]. \t  0.5696248375504335 \t 1.0301946013322725\n",
      "45     \t [ 2.47180275 -1.37287676]. \t  -25.349737741921395 \t 1.0301946013322725\n",
      "46     \t [ 1.78994273 -1.15104644]. \t  -1.8833844184388606 \t 1.0301946013322725\n",
      "47     \t [ 1.79549503 -0.76434081]. \t  0.10566390202407683 \t 1.0301946013322725\n",
      "48     \t [1.9879498  1.83868299]. \t  -39.43411598304237 \t 1.0301946013322725\n",
      "49     \t [-1.6482847  -1.23789187]. \t  -7.354931116601106 \t 1.0301946013322725\n",
      "50     \t [ 2.18057362 -1.97053779]. \t  -47.85755457335124 \t 1.0301946013322725\n",
      "51     \t [-1.38827904 -0.28345532]. \t  -2.3930328622164616 \t 1.0301946013322725\n",
      "52     \t [0.01707227 0.70188657]. \t  0.9866351317247339 \t 1.0301946013322725\n",
      "53     \t [-2.9113778 -0.6819999]. \t  -87.00909785522047 \t 1.0301946013322725\n",
      "54     \t [-2.09719261 -0.78627487]. \t  -6.034853641759609 \t 1.0301946013322725\n",
      "55     \t [ 0.19928146 -1.91554461]. \t  -38.951961089367835 \t 1.0301946013322725\n",
      "56     \t [0.7293508  1.03744074]. \t  -2.6688100575816995 \t 1.0301946013322725\n",
      "57     \t [-2.52322304 -1.74120743]. \t  -55.40078308820749 \t 1.0301946013322725\n",
      "58     \t [-0.62219426  1.0020765 ]. \t  -0.646334716846837 \t 1.0301946013322725\n",
      "59     \t [-1.31535873 -0.02814128]. \t  -2.394620274990766 \t 1.0301946013322725\n",
      "60     \t [ 0.19607251 -0.67715481]. \t  0.975202355365616 \t 1.0301946013322725\n",
      "61     \t [-0.10614547  0.75700587]. \t  1.0142015858372546 \t 1.0301946013322725\n",
      "62     \t [-0.08734134  0.64435134]. \t  0.9971146726685534 \t 1.0301946013322725\n",
      "63     \t [-2.72348769  0.70805455]. \t  -47.23312268740964 \t 1.0301946013322725\n",
      "64     \t [ 0.86875936 -0.65100078]. \t  -0.42370541355190106 \t 1.0301946013322725\n",
      "65     \t [-1.57538804  0.42502143]. \t  -0.826382623869653 \t 1.0301946013322725\n",
      "66     \t [-0.02301843  0.72670201]. \t  1.0114512361700414 \t 1.0301946013322725\n",
      "67     \t [0.08753176 0.75359478]. \t  0.8850680157934472 \t 1.0301946013322725\n",
      "68     \t [2.82208785 0.56733025]. \t  -67.76999401806682 \t 1.0301946013322725\n",
      "69     \t [1.77682387 0.91680534]. \t  -3.27919681246194 \t 1.0301946013322725\n",
      "70     \t [ 0.66000406 -1.27355475]. \t  -4.565955876600416 \t 1.0301946013322725\n",
      "71     \t [-0.02913911 -0.90641129]. \t  0.5565346979403937 \t 1.0301946013322725\n",
      "72     \t [1.47981044 0.97489083]. \t  -3.4435457383386563 \t 1.0301946013322725\n",
      "73     \t [-1.29970164  1.57028808]. \t  -14.7879592976306 \t 1.0301946013322725\n",
      "74     \t [-2.79788892  0.7094115 ]. \t  -59.54381372619115 \t 1.0301946013322725\n",
      "75     \t [-0.14107122  0.67301945]. \t  1.0073155385870756 \t 1.0301946013322725\n",
      "76     \t [ 0.0872596  -0.70906487]. \t  \u001b[92m1.031506611614352\u001b[0m \t 1.031506611614352\n",
      "77     \t [-0.07424501  0.77499964]. \t  0.9950532079782564 \t 1.031506611614352\n",
      "78     \t [ 2.95015327 -0.13826422]. \t  -95.01606706240212 \t 1.031506611614352\n",
      "79     \t [-1.8449315   1.08919984]. \t  -1.3050210984911756 \t 1.031506611614352\n",
      "80     \t [ 1.09064255 -1.04456335]. \t  -1.6061120261755024 \t 1.031506611614352\n",
      "81     \t [2.3243922  1.96256259]. \t  -61.37740809352031 \t 1.031506611614352\n",
      "82     \t [0.1558512  1.05658319]. \t  -0.7802321876099787 \t 1.031506611614352\n",
      "83     \t [-0.09826194 -1.5896447 ]. \t  -15.629061696141676 \t 1.031506611614352\n",
      "84     \t [1.00367686 0.17366662]. \t  -2.2964708748809715 \t 1.031506611614352\n",
      "85     \t [0.5785536  1.84994887]. \t  -35.34601009790853 \t 1.031506611614352\n",
      "86     \t [ 0.11849792 -0.7452084 ]. \t  1.0203036554856164 \t 1.031506611614352\n",
      "87     \t [-2.47854072 -0.00312966]. \t  -22.60745912620147 \t 1.031506611614352\n",
      "88     \t [-0.0217407  -0.61538547]. \t  0.9258754636758458 \t 1.031506611614352\n",
      "89     \t [-0.59130309 -0.81206048]. \t  -0.7379459896098622 \t 1.031506611614352\n",
      "90     \t [-0.86941445 -0.20246397]. \t  -1.9864156929290815 \t 1.031506611614352\n",
      "91     \t [1.94169735 0.05078113]. \t  -3.182549720479885 \t 1.031506611614352\n",
      "92     \t [-1.21929528  0.07409524]. \t  -2.288374901218474 \t 1.031506611614352\n",
      "93     \t [-2.71023071  1.26752489]. \t  -48.64488377839112 \t 1.031506611614352\n",
      "94     \t [2.24354666 0.86524169]. \t  -10.626450767349077 \t 1.031506611614352\n",
      "95     \t [0.01989147 0.67569128]. \t  0.9774285647622631 \t 1.031506611614352\n",
      "96     \t [ 2.23148288 -1.70035642]. \t  -27.081269546199533 \t 1.031506611614352\n",
      "97     \t [1.62122784 0.21600302]. \t  -2.2308047436755936 \t 1.031506611614352\n",
      "98     \t [ 1.2162527 -0.2986543]. \t  -1.712581562735385 \t 1.031506611614352\n",
      "99     \t [-1.27928006 -0.50410063]. \t  -2.269551004138511 \t 1.031506611614352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [ 0.15803657 -0.74048775]. \t  1.0090865496155785 \t 1.031506611614352\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\n",
    "\n",
    "np.random.seed(run_num_15)\n",
    "surrogate_loser_15 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_15 = GPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_15.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 2.18368999 -0.73453599]. \t  -4.868217712547965 \t 0.15481417016151777\n",
      "init   \t [1.03290467 0.03164667]. \t  -2.310710708702905 \t 0.15481417016151777\n",
      "init   \t [ 1.68996256 -0.86527329]. \t  0.15481417016151777 \t 0.15481417016151777\n",
      "init   \t [-1.5977473   0.25519875]. \t  -1.420015353077075 \t 0.15481417016151777\n",
      "init   \t [2.25014619 0.87577168]. \t  -10.939300534748709 \t 0.15481417016151777\n",
      "1      \t [ 1.00985701 -1.38885985]. \t  -8.0135863880244 \t 0.15481417016151777\n",
      "2      \t [-3. -2.]. \t  -162.89999999999998 \t 0.15481417016151777\n",
      "3      \t [-1.49519072  1.00020557]. \t  -0.6773842439076193 \t 0.15481417016151777\n",
      "4      \t [-3.  2.]. \t  -150.89999999999998 \t 0.15481417016151777\n",
      "5      \t [-0.78766292  0.9890635 ]. \t  -0.8887681231591341 \t 0.15481417016151777\n",
      "6      \t [-0.97361154  1.11769552]. \t  -2.3458988599770754 \t 0.15481417016151777\n",
      "7      \t [-1.25461811  0.63257035]. \t  -0.6393902750501259 \t 0.15481417016151777\n",
      "8      \t [0.27070396 1.05977269]. \t  -1.121966185510173 \t 0.15481417016151777\n",
      "9      \t [-1.90768754  0.50421999]. \t  -1.090295683914801 \t 0.15481417016151777\n",
      "10     \t [-0.03357332  0.51433132]. \t  \u001b[92m0.790989949130232\u001b[0m \t 0.790989949130232\n",
      "11     \t [-0.14448104  0.8499634 ]. \t  \u001b[92m0.8423022893978533\u001b[0m \t 0.8423022893978533\n",
      "12     \t [1.22343918 2.        ]. \t  -52.847032050428126 \t 0.8423022893978533\n",
      "13     \t [-0.75371193 -0.73114257]. \t  -1.2115823410133308 \t 0.8423022893978533\n",
      "14     \t [ 1.83488093 -1.32971119]. \t  -5.37701565932792 \t 0.8423022893978533\n",
      "15     \t [ 0.68939697 -0.51758881]. \t  -0.3211704413224429 \t 0.8423022893978533\n",
      "16     \t [ 1.42211637 -0.71903314]. \t  -0.2362625555448956 \t 0.8423022893978533\n",
      "17     \t [ 0.00152732 -0.55664215]. \t  \u001b[92m0.8562134885929198\u001b[0m \t 0.8562134885929198\n",
      "18     \t [0.27446137 0.62924383]. \t  0.49444715013862267 \t 0.8562134885929198\n",
      "19     \t [ 0.14666144 -0.24473092]. \t  0.17604666117368392 \t 0.8562134885929198\n",
      "20     \t [ 0.04959141 -0.81967294]. \t  \u001b[92m0.9126756388727633\u001b[0m \t 0.9126756388727633\n",
      "21     \t [2.95642866 1.51867499]. \t  -113.65092779445001 \t 0.9126756388727633\n",
      "22     \t [1.73411388 0.57456856]. \t  -2.2146984186505403 \t 0.9126756388727633\n",
      "23     \t [2.67912602 0.26364761]. \t  -44.23169528294859 \t 0.9126756388727633\n",
      "24     \t [ 0.1807103  -1.96951697]. \t  -44.44296998401471 \t 0.9126756388727633\n",
      "25     \t [-0.18610861 -0.85796922]. \t  0.48129038675012836 \t 0.9126756388727633\n",
      "26     \t [-1.74476969 -0.40249317]. \t  -2.278709154576716 \t 0.9126756388727633\n",
      "27     \t [1.75432254 0.88293776]. \t  -2.998256802540766 \t 0.9126756388727633\n",
      "28     \t [-0.0263908   0.65667486]. \t  \u001b[92m0.9956235774067448\u001b[0m \t 0.9956235774067448\n",
      "29     \t [ 2.7787833  -1.93238217]. \t  -94.60890787685028 \t 0.9956235774067448\n",
      "30     \t [0.0254775  0.79436433]. \t  0.9085066659785855 \t 0.9956235774067448\n",
      "31     \t [ 1.16006307 -1.42403575]. \t  -9.07788399308901 \t 0.9956235774067448\n",
      "32     \t [-0.590951   -1.87775729]. \t  -37.890679461121174 \t 0.9956235774067448\n",
      "33     \t [1.58476494 1.21281658]. \t  -6.773319884253189 \t 0.9956235774067448\n",
      "34     \t [0.0011967  0.69418676]. \t  \u001b[92m0.997852411692912\u001b[0m \t 0.997852411692912\n",
      "35     \t [-0.92499965 -1.60176692]. \t  -19.64329435847691 \t 0.997852411692912\n",
      "36     \t [ 1.59121066 -0.50113126]. \t  -0.526099360979377 \t 0.997852411692912\n",
      "37     \t [-0.43552111  0.14346979]. \t  -0.5423118285680255 \t 0.997852411692912\n",
      "38     \t [-1.65152292  1.87485109]. \t  -34.317232286957825 \t 0.997852411692912\n",
      "39     \t [-2.08942913  0.06695537]. \t  -5.016291437368888 \t 0.997852411692912\n",
      "40     \t [ 0.11093152 -1.98101709]. \t  -45.73599653867532 \t 0.997852411692912\n",
      "41     \t [-0.49690119  1.97896082]. \t  -45.56532430579879 \t 0.997852411692912\n",
      "42     \t [2.21827117 1.96065619]. \t  -56.63377226491582 \t 0.997852411692912\n",
      "43     \t [ 0.04940441 -0.75562736]. \t  \u001b[92m1.0074321494287037\u001b[0m \t 1.0074321494287037\n",
      "44     \t [-0.10645872  0.71478019]. \t  \u001b[92m1.0305538272659756\u001b[0m \t 1.0305538272659756\n",
      "45     \t [-0.89775871 -1.6124698 ]. \t  -20.122860266160462 \t 1.0305538272659756\n",
      "46     \t [0.99149593 1.09304245]. \t  -4.233879355183807 \t 1.0305538272659756\n",
      "47     \t [-0.32324254 -0.37129543]. \t  -0.039996073069283766 \t 1.0305538272659756\n",
      "48     \t [ 0.162227   -0.67779611]. \t  0.9995438639326215 \t 1.0305538272659756\n",
      "49     \t [1.40229604 0.22165236]. \t  -2.4039258096964105 \t 1.0305538272659756\n",
      "50     \t [-0.0451006  -0.09975659]. \t  0.026782738471090778 \t 1.0305538272659756\n",
      "51     \t [-2.40975476  0.82515777]. \t  -14.827831552487309 \t 1.0305538272659756\n",
      "52     \t [-0.98902035 -0.98631503]. \t  -3.0850448128396297 \t 1.0305538272659756\n",
      "53     \t [-1.86370466 -1.82081755]. \t  -36.625260451302594 \t 1.0305538272659756\n",
      "54     \t [ 1.20875711 -1.30544156]. \t  -5.623231024010353 \t 1.0305538272659756\n",
      "55     \t [-0.93842378 -1.43326778]. \t  -12.129471971749108 \t 1.0305538272659756\n",
      "56     \t [-0.10384646  0.714943  ]. \t  \u001b[92m1.0308550455040408\u001b[0m \t 1.0308550455040408\n",
      "57     \t [-1.76279819  0.64155545]. \t  -0.054119100443345114 \t 1.0308550455040408\n",
      "58     \t [ 2.55713393 -1.85211105]. \t  -58.17204951785956 \t 1.0308550455040408\n",
      "59     \t [ 0.13169418 -0.72033082]. \t  1.0246945864787034 \t 1.0308550455040408\n",
      "60     \t [-0.20861753  0.74525062]. \t  0.9730614138466095 \t 1.0308550455040408\n",
      "61     \t [-1.14461476  0.52225754]. \t  -0.9943654894472209 \t 1.0308550455040408\n",
      "62     \t [ 1.98461331 -0.39381778]. \t  -2.2384753512446833 \t 1.0308550455040408\n",
      "63     \t [-0.00913986 -0.71092033]. \t  0.9930511794555171 \t 1.0308550455040408\n",
      "64     \t [ 0.22835015 -0.7083746 ]. \t  0.9588319866793009 \t 1.0308550455040408\n",
      "65     \t [-0.5267027   1.94130659]. \t  -41.669520784412626 \t 1.0308550455040408\n",
      "66     \t [-2.28575626 -1.68877761]. \t  -36.10148036780675 \t 1.0308550455040408\n",
      "67     \t [-2.08193529 -1.02056755]. \t  -7.326488782809572 \t 1.0308550455040408\n",
      "68     \t [ 2.15379535 -0.1148423 ]. \t  -6.340509925469137 \t 1.0308550455040408\n",
      "69     \t [1.90139351 1.03652246]. \t  -5.0549803120941 \t 1.0308550455040408\n",
      "70     \t [ 1.87066426 -0.05430607]. \t  -2.4523633639431703 \t 1.0308550455040408\n",
      "71     \t [-0.14323188  0.76567151]. \t  0.9987298338020258 \t 1.0308550455040408\n",
      "72     \t [-0.30060069 -0.20153082]. \t  -0.24926219649462772 \t 1.0308550455040408\n",
      "73     \t [0.97526732 0.73008194]. \t  -1.907972443799164 \t 1.0308550455040408\n",
      "74     \t [ 0.13192895 -0.63195665]. \t  0.9738805783926698 \t 1.0308550455040408\n",
      "75     \t [-0.0897056   0.78608958]. \t  0.982827695221619 \t 1.0308550455040408\n",
      "76     \t [-1.66110534  1.01313952]. \t  -0.47688773519325334 \t 1.0308550455040408\n",
      "77     \t [-1.86750568  0.70426339]. \t  -0.2324756996214492 \t 1.0308550455040408\n",
      "78     \t [-0.0307248   0.02334333]. \t  -0.0008785049358762055 \t 1.0308550455040408\n",
      "79     \t [ 0.13185308 -0.7798099 ]. \t  0.9871669093224957 \t 1.0308550455040408\n",
      "80     \t [2.80156731 0.96082609]. \t  -65.60661081112652 \t 1.0308550455040408\n",
      "81     \t [-2.89226964 -1.95592372]. \t  -130.53011185358824 \t 1.0308550455040408\n",
      "82     \t [-0.08641601  0.72006289]. \t  \u001b[92m1.0311034228951594\u001b[0m \t 1.0311034228951594\n",
      "83     \t [-0.13307561  0.65766346]. \t  0.9991256924045604 \t 1.0311034228951594\n",
      "84     \t [ 0.21706747 -0.73010678]. \t  0.9702659255941001 \t 1.0311034228951594\n",
      "85     \t [-1.05880544 -1.26619925]. \t  -7.524050429022926 \t 1.0311034228951594\n",
      "86     \t [0.3473684  1.15989825]. \t  -2.714141654336306 \t 1.0311034228951594\n",
      "87     \t [ 0.76379738 -1.69355155]. \t  -21.823395040306888 \t 1.0311034228951594\n",
      "88     \t [-2.43203835 -0.79476976]. \t  -20.169540227053655 \t 1.0311034228951594\n",
      "89     \t [0.18337496 0.88162559]. \t  0.398687728010978 \t 1.0311034228951594\n",
      "90     \t [ 0.40001542 -1.3566967 ]. \t  -6.234058703140568 \t 1.0311034228951594\n",
      "91     \t [1.53165474 1.54256082]. \t  -17.62277393064972 \t 1.0311034228951594\n",
      "92     \t [-0.10296326 -1.54889969]. \t  -13.6278251233956 \t 1.0311034228951594\n",
      "93     \t [-2.30940352 -1.23800489]. \t  -18.292581845782465 \t 1.0311034228951594\n",
      "94     \t [ 0.00372845 -0.74159598]. \t  0.992723552577655 \t 1.0311034228951594\n",
      "95     \t [-0.99061714  1.71493951]. \t  -23.35342319187397 \t 1.0311034228951594\n",
      "96     \t [ 0.15352823 -0.75439378]. \t  1.0035946854663889 \t 1.0311034228951594\n",
      "97     \t [-1.70697496  1.38578229]. \t  -6.776549152514686 \t 1.0311034228951594\n",
      "98     \t [ 0.20155206 -0.72713384]. \t  0.9842053783441223 \t 1.0311034228951594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [-2.12458244  1.61086327]. \t  -19.056246462062564 \t 1.0311034228951594\n",
      "100    \t [-0.01343013  0.6652738 ]. \t  0.9950293221425383 \t 1.0311034228951594\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\n",
    "\n",
    "np.random.seed(run_num_16)\n",
    "surrogate_loser_16 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_16 = GPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_16.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.91094417 0.9220785 ]. \t  -2.3942909876906837 \t -0.5838248233399961\n",
      "init   \t [-1.83979583  0.07870895]. \t  -2.2368016331814964 \t -0.5838248233399961\n",
      "init   \t [1.0582996  1.21777949]. \t  -6.46789670014418 \t -0.5838248233399961\n",
      "init   \t [0.20107719 1.63213672]. \t  -18.215840822341526 \t -0.5838248233399961\n",
      "init   \t [-0.33911323 -0.97611116]. \t  -0.5838248233399961 \t -0.5838248233399961\n",
      "1      \t [1.13213455 0.01299289]. \t  -2.3928963638963574 \t -0.5838248233399961\n",
      "2      \t [0.47727594 0.30619857]. \t  -0.6124145357534219 \t -0.5838248233399961\n",
      "3      \t [-0.85766014 -1.81420745]. \t  -33.66120969958933 \t -0.5838248233399961\n",
      "4      \t [-0.24894107 -0.46100906]. \t  \u001b[92m0.31477749397480403\u001b[0m \t 0.31477749397480403\n",
      "5      \t [ 0.3283862  -1.04779217]. \t  -0.49305340924925994 \t 0.31477749397480403\n",
      "6      \t [-3.          0.51148913]. \t  -106.59283053857072 \t 0.31477749397480403\n",
      "7      \t [-1.31704565 -0.20891068]. \t  -2.467738349647818 \t 0.31477749397480403\n",
      "8      \t [-1.44088248  0.48424505]. \t  -0.8200122021217195 \t 0.31477749397480403\n",
      "9      \t [ 3. -2.]. \t  -150.89999999999998 \t 0.31477749397480403\n",
      "10     \t [ 0.04634518 -0.73675737]. \t  \u001b[92m1.018232062789341\u001b[0m \t 1.018232062789341\n",
      "11     \t [1.45076767 0.53945396]. \t  -2.1813907185536836 \t 1.018232062789341\n",
      "12     \t [2.92691825 1.45802502]. \t  -103.56372379292706 \t 1.018232062789341\n",
      "13     \t [ 0.0093861  -1.06405503]. \t  -0.5891386810201593 \t 1.018232062789341\n",
      "14     \t [1.09416666 0.41391158]. \t  -2.23587627829629 \t 1.018232062789341\n",
      "15     \t [ 1.83197558 -0.24686772]. \t  -1.6904850228241102 \t 1.018232062789341\n",
      "16     \t [0.07892163 0.8027183 ]. \t  0.8284596740740193 \t 1.018232062789341\n",
      "17     \t [0.43008179 0.5388358 ]. \t  -0.0777074541894186 \t 1.018232062789341\n",
      "18     \t [-0.13963323  0.59876753]. \t  0.9263504619925327 \t 1.018232062789341\n",
      "19     \t [-1.00981011  1.08670255]. \t  -2.0059396777206446 \t 1.018232062789341\n",
      "20     \t [0.18777925 0.49748522]. \t  0.5130927212989009 \t 1.018232062789341\n",
      "21     \t [-0.31840793  0.81692042]. \t  0.7637811614328618 \t 1.018232062789341\n",
      "22     \t [-2.59421844 -1.38180644]. \t  -43.94155432342252 \t 1.018232062789341\n",
      "23     \t [2.01348983 0.04922234]. \t  -4.001648681577594 \t 1.018232062789341\n",
      "24     \t [ 0.93794917 -1.57471347]. \t  -15.320850092271204 \t 1.018232062789341\n",
      "25     \t [-0.07489019  0.71567881]. \t  \u001b[92m1.030634172873397\u001b[0m \t 1.030634172873397\n",
      "26     \t [ 0.72465278 -0.70557673]. \t  -0.05839428172986938 \t 1.030634172873397\n",
      "27     \t [ 0.1203504  -0.57429792]. \t  0.8957721457123444 \t 1.030634172873397\n",
      "28     \t [-0.65148094  0.68896455]. \t  0.101377493494474 \t 1.030634172873397\n",
      "29     \t [-1.35471937  1.9619322 ]. \t  -43.538461102405925 \t 1.030634172873397\n",
      "30     \t [-1.93228055  1.18346564]. \t  -2.967104989893754 \t 1.030634172873397\n",
      "31     \t [-0.21800843  0.71297827]. \t  0.9697543451263169 \t 1.030634172873397\n",
      "32     \t [-1.43111299 -0.59190564]. \t  -2.1839003640556665 \t 1.030634172873397\n",
      "33     \t [-0.1175397   0.76424929]. \t  1.0066915011705022 \t 1.030634172873397\n",
      "34     \t [-1.52790622  0.91660941]. \t  -0.19653127920935298 \t 1.030634172873397\n",
      "35     \t [-0.00412997 -0.69620151]. \t  0.9961197002105225 \t 1.030634172873397\n",
      "36     \t [-2.42197371  1.84728455]. \t  -46.941052440101735 \t 1.030634172873397\n",
      "37     \t [-2.2038044  -1.05044206]. \t  -10.85069669643094 \t 1.030634172873397\n",
      "38     \t [-1.93926691 -0.45688236]. \t  -3.2972828228992768 \t 1.030634172873397\n",
      "39     \t [-1.71581059 -1.08432951]. \t  -4.7675326588951314 \t 1.030634172873397\n",
      "40     \t [ 0.16407685 -0.68002028]. \t  0.9997594657028358 \t 1.030634172873397\n",
      "41     \t [-0.78308704 -0.71217801]. \t  -1.2979775588026223 \t 1.030634172873397\n",
      "42     \t [-2.12005203  0.72207186]. \t  -3.2922894799006936 \t 1.030634172873397\n",
      "43     \t [ 0.00396788 -0.73537917]. \t  0.9962020647101746 \t 1.030634172873397\n",
      "44     \t [-1.9318373   0.86079393]. \t  -0.5751171370886103 \t 1.030634172873397\n",
      "45     \t [-0.07270525  0.71842764]. \t  1.0301061017031308 \t 1.030634172873397\n",
      "46     \t [2.57017079 1.79643139]. \t  -64.23765088309628 \t 1.030634172873397\n",
      "47     \t [ 0.22681119 -1.29316655]. \t  -4.403915479673336 \t 1.030634172873397\n",
      "48     \t [ 2.15982273 -1.25616497]. \t  -7.733379726496054 \t 1.030634172873397\n",
      "49     \t [ 1.86660992 -0.90445782]. \t  -0.25891182550108094 \t 1.030634172873397\n",
      "50     \t [ 2.24485755 -0.84218651]. \t  -6.770869564566761 \t 1.030634172873397\n",
      "51     \t [2.54033949 1.42592283]. \t  -39.96754511275637 \t 1.030634172873397\n",
      "52     \t [1.69541705 0.81596996]. \t  -2.5566819402525507 \t 1.030634172873397\n",
      "53     \t [-0.248762    1.33304453]. \t  -5.430951350760462 \t 1.030634172873397\n",
      "54     \t [-0.85430215 -0.38910705]. \t  -1.7488269490371118 \t 1.030634172873397\n",
      "55     \t [-2.88248936  0.38871025]. \t  -77.82619363645762 \t 1.030634172873397\n",
      "56     \t [-0.06082658  0.72249963]. \t  1.0272394153584885 \t 1.030634172873397\n",
      "57     \t [-2.79334878  0.95228897]. \t  -58.711832529534135 \t 1.030634172873397\n",
      "58     \t [-2.05051606  1.21270545]. \t  -4.752518000759231 \t 1.030634172873397\n",
      "59     \t [ 0.01148549 -0.95595997]. \t  0.32533342447844954 \t 1.030634172873397\n",
      "60     \t [-0.02611478  0.68642678]. \t  1.0118769606015303 \t 1.030634172873397\n",
      "61     \t [ 0.17929713 -0.7595871 ]. \t  0.9860620313161906 \t 1.030634172873397\n",
      "62     \t [-2.6966726  0.1213066]. \t  -45.837779417492754 \t 1.030634172873397\n",
      "63     \t [ 0.05568978 -0.74705507]. \t  1.0157197247992749 \t 1.030634172873397\n",
      "64     \t [ 0.05678926 -0.68515891]. \t  1.0222964284674123 \t 1.030634172873397\n",
      "65     \t [-0.15795444  0.74720813]. \t  1.0059235750052078 \t 1.030634172873397\n",
      "66     \t [1.65581427 1.1997202 ]. \t  -6.56684895211724 \t 1.030634172873397\n",
      "67     \t [-2.79092106  1.87072585]. \t  -91.04520775490309 \t 1.030634172873397\n",
      "68     \t [-2.72328735 -0.03660815]. \t  -50.22543373541524 \t 1.030634172873397\n",
      "69     \t [2.20967779 0.24191683]. \t  -8.58161737385623 \t 1.030634172873397\n",
      "70     \t [-0.0548514   1.10793357]. \t  -1.0683666874262419 \t 1.030634172873397\n",
      "71     \t [-0.06066418  0.6552652 ]. \t  1.0051059964855433 \t 1.030634172873397\n",
      "72     \t [2.41857944 1.72118125]. \t  -45.67790883537593 \t 1.030634172873397\n",
      "73     \t [-1.21721353  0.37638637]. \t  -1.4561829761753438 \t 1.030634172873397\n",
      "74     \t [ 0.08829875 -0.76730917]. \t  1.005177522522379 \t 1.030634172873397\n",
      "75     \t [2.31581916 0.26280664]. \t  -12.820353321665147 \t 1.030634172873397\n",
      "76     \t [ 0.26182785 -0.74040568]. \t  0.9201121748068555 \t 1.030634172873397\n",
      "77     \t [ 0.15703372 -0.67447123]. \t  1.0004164544659797 \t 1.030634172873397\n",
      "78     \t [-1.4930781  1.6574841]. \t  -18.899555094610104 \t 1.030634172873397\n",
      "79     \t [2.81309369 0.18958664]. \t  -65.7295870887448 \t 1.030634172873397\n",
      "80     \t [ 0.22770293 -0.76375639]. \t  0.9443425561417805 \t 1.030634172873397\n",
      "81     \t [0.46681699 1.07306979]. \t  -1.9740220329833469 \t 1.030634172873397\n",
      "82     \t [2.85242299 1.9098808 ]. \t  -117.14490381232577 \t 1.030634172873397\n",
      "83     \t [ 1.10182886 -1.37356323]. \t  -7.535483616365992 \t 1.030634172873397\n",
      "84     \t [ 0.07965184 -0.74731186]. \t  1.0205542341796388 \t 1.030634172873397\n",
      "85     \t [2.84489502 1.65781661]. \t  -95.46908078915511 \t 1.030634172873397\n",
      "86     \t [-1.08520893 -1.61539391]. \t  -20.895577754342526 \t 1.030634172873397\n",
      "87     \t [-0.66717693  1.60934127]. \t  -16.792151593581714 \t 1.030634172873397\n",
      "88     \t [-0.83344645 -0.7140815 ]. \t  -1.4725157938370281 \t 1.030634172873397\n",
      "89     \t [-0.14401812  0.65986007]. \t  0.9962826453892079 \t 1.030634172873397\n",
      "90     \t [-0.00116813  0.71396141]. \t  1.000449002627258 \t 1.030634172873397\n",
      "91     \t [ 0.97808965 -1.21057817]. \t  -3.7412707018661573 \t 1.030634172873397\n",
      "92     \t [-1.47511109 -1.23407059]. \t  -7.2009415769992575 \t 1.030634172873397\n",
      "93     \t [0.23459966 0.23693481]. \t  -0.05748087029356072 \t 1.030634172873397\n",
      "94     \t [-0.43418483  0.62553449]. \t  0.5426607798626597 \t 1.030634172873397\n",
      "95     \t [-0.08591447  0.71205314]. \t  \u001b[92m1.03156766146739\u001b[0m \t 1.03156766146739\n",
      "96     \t [-2.80059451 -0.98251403]. \t  -65.63850827229243 \t 1.03156766146739\n",
      "97     \t [-0.47859325 -0.19908632]. \t  -0.7530598121334977 \t 1.03156766146739\n",
      "98     \t [2.04633673 0.04001267]. \t  -4.477789381508556 \t 1.03156766146739\n",
      "99     \t [2.45543409 1.89781373]. \t  -62.976893431328726 \t 1.03156766146739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [0.78846872 1.53984025]. \t  -15.973491298829508 \t 1.03156766146739\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\n",
    "\n",
    "np.random.seed(run_num_17)\n",
    "surrogate_loser_17 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_17 = GPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_17.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [2.24057642 1.87416265]. \t  -48.82879577505094 \t -0.03190064766213015\n",
      "init   \t [2.21516724 0.12342277]. \t  -8.660623551418242 \t -0.03190064766213015\n",
      "init   \t [-1.60363003 -1.95440478]. \t  -48.28332373939171 \t -0.03190064766213015\n",
      "init   \t [-0.41718709 -0.39059456]. \t  -0.2801229037700297 \t -0.03190064766213015\n",
      "init   \t [ 0.13604803 -0.08643282]. \t  -0.03190064766213015 \t -0.03190064766213015\n",
      "1      \t [-0.7915585   0.36468929]. \t  -1.0139151812710225 \t -0.03190064766213015\n",
      "2      \t [ 2.62087196 -1.3222448 ]. \t  -38.19228889832653 \t -0.03190064766213015\n",
      "3      \t [-3.  2.]. \t  -150.89999999999998 \t -0.03190064766213015\n",
      "4      \t [ 0.36487142 -0.61183585]. \t  \u001b[92m0.6639918373230551\u001b[0m \t 0.6639918373230551\n",
      "5      \t [ 0.22539049 -0.69499916]. \t  \u001b[92m0.9576657826671826\u001b[0m \t 0.9576657826671826\n",
      "6      \t [ 0.36708218 -1.22260349]. \t  -3.011070611645343 \t 0.9576657826671826\n",
      "7      \t [-0.16577965  1.85141002]. \t  -33.08758207573004 \t 0.9576657826671826\n",
      "8      \t [ 0.04893066 -0.42729865]. \t  0.6083318766004618 \t 0.9576657826671826\n",
      "9      \t [ 0.12582291 -0.69206111]. \t  \u001b[92m1.0225038826294923\u001b[0m \t 1.0225038826294923\n",
      "10     \t [1.11345104 0.15166399]. \t  -2.4454854662367813 \t 1.0225038826294923\n",
      "11     \t [-1.49587996 -0.04322049]. \t  -2.227624776261556 \t 1.0225038826294923\n",
      "12     \t [-2.85174389 -0.66002883]. \t  -73.82578365948193 \t 1.0225038826294923\n",
      "13     \t [-1.09397978  0.04312097]. \t  -2.2961101124198096 \t 1.0225038826294923\n",
      "14     \t [-0.2289851   0.51182253]. \t  0.686540824828949 \t 1.0225038826294923\n",
      "15     \t [-0.00347317 -0.77295862]. \t  0.9592694103159312 \t 1.0225038826294923\n",
      "16     \t [-1.41251368  0.59112099]. \t  -0.5243123706545838 \t 1.0225038826294923\n",
      "17     \t [-0.80479565  0.72152936]. \t  -0.2213990230456927 \t 1.0225038826294923\n",
      "18     \t [ 1.07950036 -0.83532284]. \t  -0.5917436309775882 \t 1.0225038826294923\n",
      "19     \t [ 1.29444128 -0.45728857]. \t  -1.1210568439486899 \t 1.0225038826294923\n",
      "20     \t [-1.56876767  0.29600157]. \t  -1.3095444879031297 \t 1.0225038826294923\n",
      "21     \t [2.87690738 0.24906094]. \t  -78.72379852933588 \t 1.0225038826294923\n",
      "22     \t [1.74014107 0.23391415]. \t  -2.3121095532098135 \t 1.0225038826294923\n",
      "23     \t [0.29099247 0.5463768 ]. \t  0.35479239051485123 \t 1.0225038826294923\n",
      "24     \t [ 1.91336779 -0.25043255]. \t  -2.139547170037043 \t 1.0225038826294923\n",
      "25     \t [ 1.66947248 -1.93659216]. \t  -40.07945159497484 \t 1.0225038826294923\n",
      "26     \t [-0.01737654 -0.66394796]. \t  0.9732494693763833 \t 1.0225038826294923\n",
      "27     \t [ 0.67539255 -0.8171781 ]. \t  0.02001919246102657 \t 1.0225038826294923\n",
      "28     \t [-2.60021909  0.9563041 ]. \t  -31.2719439793236 \t 1.0225038826294923\n",
      "29     \t [ 1.90741339 -1.02250575]. \t  -1.0485644032316137 \t 1.0225038826294923\n",
      "30     \t [-1.52283517  1.92386952]. \t  -39.202584247376464 \t 1.0225038826294923\n",
      "31     \t [-2.93196646 -1.69547913]. \t  -117.48019821846701 \t 1.0225038826294923\n",
      "32     \t [-0.75263632 -1.5985435 ]. \t  -18.75342307967317 \t 1.0225038826294923\n",
      "33     \t [-0.45206572 -1.48669755]. \t  -12.104767100266098 \t 1.0225038826294923\n",
      "34     \t [ 1.03141014 -1.9831555 ]. \t  -46.373907674674726 \t 1.0225038826294923\n",
      "35     \t [0.79277552 1.43699572]. \t  -11.702790040325198 \t 1.0225038826294923\n",
      "36     \t [-1.09710754  1.5353724 ]. \t  -13.468246995670192 \t 1.0225038826294923\n",
      "37     \t [-2.69086437 -1.22676121]. \t  -51.74440884466302 \t 1.0225038826294923\n",
      "38     \t [-1.82488921 -0.98855805]. \t  -4.057300153008309 \t 1.0225038826294923\n",
      "39     \t [-2.76177044  1.9548925 ]. \t  -93.98362702954604 \t 1.0225038826294923\n",
      "40     \t [-0.42711216  0.88580295]. \t  0.3924045404793665 \t 1.0225038826294923\n",
      "41     \t [-2.00389929  1.19793775]. \t  -3.880520963644551 \t 1.0225038826294923\n",
      "42     \t [-0.43762129  0.67418825]. \t  0.5953997198965988 \t 1.0225038826294923\n",
      "43     \t [ 0.1059613  -0.73584125]. \t  \u001b[92m1.0264472648565535\u001b[0m \t 1.0264472648565535\n",
      "44     \t [-0.75156827  0.75964351]. \t  -0.10229366183722222 \t 1.0264472648565535\n",
      "45     \t [ 0.00341448 -0.72174587]. \t  1.0006676538931332 \t 1.0264472648565535\n",
      "46     \t [ 0.09425095 -0.65123438]. \t  1.0029725113892194 \t 1.0264472648565535\n",
      "47     \t [1.38817035 1.1798556 ]. \t  -6.116116966453971 \t 1.0264472648565535\n",
      "48     \t [ 0.15510418 -0.66959604]. \t  0.9981714277153473 \t 1.0264472648565535\n",
      "49     \t [ 0.09510098 -0.72671615]. \t  \u001b[92m1.0299440415408652\u001b[0m \t 1.0299440415408652\n",
      "50     \t [ 1.39689306 -1.92142211]. \t  -39.3538013909084 \t 1.0299440415408652\n",
      "51     \t [ 0.32859218 -0.0127359 ]. \t  -0.4029951711077447 \t 1.0299440415408652\n",
      "52     \t [ 0.14630184 -0.69764612]. \t  1.0167023381576823 \t 1.0299440415408652\n",
      "53     \t [-2.84223556 -0.84811832]. \t  -72.59955350318138 \t 1.0299440415408652\n",
      "54     \t [0.10933846 1.79458702]. \t  -28.849145298752564 \t 1.0299440415408652\n",
      "55     \t [ 1.68425434 -0.8699521 ]. \t  0.14417110895038465 \t 1.0299440415408652\n",
      "56     \t [-1.4244854  -0.52674357]. \t  -2.20336333382401 \t 1.0299440415408652\n",
      "57     \t [-2.55463205  1.28137014]. \t  -30.2575255844188 \t 1.0299440415408652\n",
      "58     \t [0.07337205 0.8489675 ]. \t  0.7213216136573339 \t 1.0299440415408652\n",
      "59     \t [-0.05213195  0.75526361]. \t  1.0086802474667467 \t 1.0299440415408652\n",
      "60     \t [ 0.97012635 -0.69282134]. \t  -0.5118467337645538 \t 1.0299440415408652\n",
      "61     \t [-0.09629965  0.69006861]. \t  1.0272724384237262 \t 1.0299440415408652\n",
      "62     \t [ 0.90131944 -0.04121331]. \t  -1.9983814024464437 \t 1.0299440415408652\n",
      "63     \t [-1.84978151  1.68859447]. \t  -20.44559432253321 \t 1.0299440415408652\n",
      "64     \t [ 0.10069779 -0.75084103]. \t  1.0190009304245184 \t 1.0299440415408652\n",
      "65     \t [-1.94219693 -1.65945027]. \t  -25.63984931620373 \t 1.0299440415408652\n",
      "66     \t [ 1.74209392 -0.79830464]. \t  0.20026084575978864 \t 1.0299440415408652\n",
      "67     \t [ 2.84732368 -1.41880092]. \t  -76.14126246054262 \t 1.0299440415408652\n",
      "68     \t [ 0.11170653 -0.72651241]. \t  1.028473253764476 \t 1.0299440415408652\n",
      "69     \t [-1.85453607  1.25378907]. \t  -3.749098678664283 \t 1.0299440415408652\n",
      "70     \t [-0.18158659  0.67245193]. \t  0.9833422299209952 \t 1.0299440415408652\n",
      "71     \t [-2.43737172 -0.01060075]. \t  -19.5625535312107 \t 1.0299440415408652\n",
      "72     \t [-0.13591375  0.65926674]. \t  0.9993361469746563 \t 1.0299440415408652\n",
      "73     \t [-0.08337479  0.76605364]. \t  1.0060020217980716 \t 1.0299440415408652\n",
      "74     \t [-1.15234777 -1.08191252]. \t  -4.434342465676329 \t 1.0299440415408652\n",
      "75     \t [0.4855699  0.88039393]. \t  -0.5609382439949269 \t 1.0299440415408652\n",
      "76     \t [-0.04056383  0.68071721]. \t  1.0156711908380995 \t 1.0299440415408652\n",
      "77     \t [ 0.17092523 -1.92118805]. \t  -39.51569133924025 \t 1.0299440415408652\n",
      "78     \t [-1.29268562  1.99943502]. \t  -47.72767416217624 \t 1.0299440415408652\n",
      "79     \t [-0.6379283  -1.97115382]. \t  -47.404963989223546 \t 1.0299440415408652\n",
      "80     \t [-0.015983    0.36855873]. \t  0.474405968954032 \t 1.0299440415408652\n",
      "81     \t [-0.17697495  1.45243478]. \t  -9.229007287194559 \t 1.0299440415408652\n",
      "82     \t [ 1.86029389 -1.63189365]. \t  -17.187688793382062 \t 1.0299440415408652\n",
      "83     \t [0.05649968 1.93521163]. \t  -41.24332959688418 \t 1.0299440415408652\n",
      "84     \t [1.76901617 0.77944067]. \t  -2.5926560214294785 \t 1.0299440415408652\n",
      "85     \t [ 0.56058972 -1.21033858]. \t  -3.1057681552973615 \t 1.0299440415408652\n",
      "86     \t [-0.01309983  0.71297119]. \t  1.008376023774279 \t 1.0299440415408652\n",
      "87     \t [-0.06073714  0.77421198]. \t  0.9927712045725299 \t 1.0299440415408652\n",
      "88     \t [-1.35760287 -1.11474798]. \t  -5.0452611426740095 \t 1.0299440415408652\n",
      "89     \t [-0.76963046 -1.55434442]. \t  -16.582089597861742 \t 1.0299440415408652\n",
      "90     \t [-2.76971627 -1.3347041 ]. \t  -66.85092440362372 \t 1.0299440415408652\n",
      "91     \t [-0.42433228 -0.76250859]. \t  -0.004167086705356149 \t 1.0299440415408652\n",
      "92     \t [-1.67424272 -0.46906555]. \t  -2.1524663184686608 \t 1.0299440415408652\n",
      "93     \t [-1.27523867 -0.69605985]. \t  -2.2734013795154975 \t 1.0299440415408652\n",
      "94     \t [2.31831579 1.73660466]. \t  -40.93072020895758 \t 1.0299440415408652\n",
      "95     \t [-0.05916504  0.72790852]. \t  1.0255261626276844 \t 1.0299440415408652\n",
      "96     \t [-0.11888172  0.64680051]. \t  0.9941135071804182 \t 1.0299440415408652\n",
      "97     \t [-0.85603745 -1.96652865]. \t  -47.971185074003685 \t 1.0299440415408652\n",
      "98     \t [-1.42667977  1.75155667]. \t  -25.13096446186454 \t 1.0299440415408652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [ 2.93228413 -1.23583275]. \t  -90.62857274863212 \t 1.0299440415408652\n",
      "100    \t [-0.29463268  1.09391463]. \t  -0.9506042993461121 \t 1.0299440415408652\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\n",
    "\n",
    "np.random.seed(run_num_18)\n",
    "surrogate_loser_18 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_18 = GPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_18.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.35667346 1.04596796]. \t  -1.260209694806778 \t -1.260209694806778\n",
      "init   \t [-1.53281067  1.08675788]. \t  -1.318361654110369 \t -1.260209694806778\n",
      "init   \t [-0.87316751  1.1867896 ]. \t  -3.241714295197782 \t -1.260209694806778\n",
      "init   \t [-0.92399987 -0.22450226]. \t  -2.107788234046822 \t -1.260209694806778\n",
      "init   \t [-1.18017846 -0.23154432]. \t  -2.468366013151972 \t -1.260209694806778\n",
      "1      \t [-2.6937212   1.50277262]. \t  -53.123940891052406 \t -1.260209694806778\n",
      "2      \t [1.1791765  0.66235955]. \t  -2.1938988924775304 \t -1.260209694806778\n",
      "3      \t [-1.35556192  0.5882584 ]. \t  \u001b[92m-0.624963540344051\u001b[0m \t -0.624963540344051\n",
      "4      \t [-0.14296728 -1.09287656]. \t  -1.1657794508744628 \t -0.624963540344051\n",
      "5      \t [0.99535331 1.90215572]. \t  -42.01173315105823 \t -0.624963540344051\n",
      "6      \t [0.45523383 0.25106708]. \t  \u001b[92m-0.6197773365429832\u001b[0m \t -0.6197773365429832\n",
      "7      \t [0.44021484 0.65955151]. \t  \u001b[92m-0.005958538267673119\u001b[0m \t -0.005958538267673119\n",
      "8      \t [ 1.92528835 -0.2206769 ]. \t  -2.339713028365267 \t -0.005958538267673119\n",
      "9      \t [1.0932032  0.13744039]. \t  -2.4261337203904847 \t -0.005958538267673119\n",
      "10     \t [2.69616501 0.38574781]. \t  -46.68423672984286 \t -0.005958538267673119\n",
      "11     \t [ 2.10781668 -1.27926985]. \t  -7.02258077227836 \t -0.005958538267673119\n",
      "12     \t [-0.09520733 -0.45466171]. \t  \u001b[92m0.5765683938166284\u001b[0m \t 0.5765683938166284\n",
      "13     \t [-0.75292199 -2.        ]. \t  -51.159268013505766 \t 0.5765683938166284\n",
      "14     \t [ 0.47062336 -1.31517233]. \t  -5.21603275106072 \t 0.5765683938166284\n",
      "15     \t [-1.03172653  1.80781976]. \t  -30.067241323023197 \t 0.5765683938166284\n",
      "16     \t [-0.13393583  1.01235891]. \t  -0.03744666239657306 \t 0.5765683938166284\n",
      "17     \t [-0.77757188 -0.69795626]. \t  -1.2678375877035273 \t 0.5765683938166284\n",
      "18     \t [-2.62496851 -0.94779358]. \t  -39.02907970043853 \t 0.5765683938166284\n",
      "19     \t [-0.05640245 -0.73074754]. \t  \u001b[92m0.9414585313118096\u001b[0m \t 0.9414585313118096\n",
      "20     \t [-1.1697871   0.87749386]. \t  -0.6605386243031159 \t 0.9414585313118096\n",
      "21     \t [-1.73923605  0.76351821]. \t  0.18986702760846264 \t 0.9414585313118096\n",
      "22     \t [-1.50233709  0.78255906]. \t  -0.03777987735096633 \t 0.9414585313118096\n",
      "23     \t [ 1.92674847 -0.55628429]. \t  -1.0355656278777778 \t 0.9414585313118096\n",
      "24     \t [-1.96647175  0.31124575]. \t  -2.378575003427972 \t 0.9414585313118096\n",
      "25     \t [-0.30161947 -0.73949771]. \t  0.4214028973928532 \t 0.9414585313118096\n",
      "26     \t [ 2.97758847 -1.55812178]. \t  -111.92513088694884 \t 0.9414585313118096\n",
      "27     \t [ 1.6095449  -1.75203444]. \t  -24.656114571914493 \t 0.9414585313118096\n",
      "28     \t [ 1.64920897 -1.23686716]. \t  -3.253677787357346 \t 0.9414585313118096\n",
      "29     \t [2.00897948 1.5775873 ]. \t  -21.84135769623316 \t 0.9414585313118096\n",
      "30     \t [-0.30707288  0.03577979]. \t  -0.34268149842498496 \t 0.9414585313118096\n",
      "31     \t [ 1.82196605 -0.8294947 ]. \t  0.03923401947033911 \t 0.9414585313118096\n",
      "32     \t [2.61633004 1.91005909]. \t  -79.54102215180171 \t 0.9414585313118096\n",
      "33     \t [-0.16469302  0.51573154]. \t  0.7589172725673777 \t 0.9414585313118096\n",
      "34     \t [-0.85970772  1.10068474]. \t  -2.022515243888862 \t 0.9414585313118096\n",
      "35     \t [2.98561421 0.70523437]. \t  -105.99224564957036 \t 0.9414585313118096\n",
      "36     \t [-0.03892457  0.75567245]. \t  \u001b[92m1.0031713457562699\u001b[0m \t 1.0031713457562699\n",
      "37     \t [ 1.95780413 -1.37790233]. \t  -7.377192974630941 \t 1.0031713457562699\n",
      "38     \t [ 1.92312212 -0.95400459]. \t  -0.770036529519903 \t 1.0031713457562699\n",
      "39     \t [ 0.92176742 -0.69056286]. \t  -0.45266112060336383 \t 1.0031713457562699\n",
      "40     \t [ 1.40510009 -1.35368486]. \t  -6.476637162373654 \t 1.0031713457562699\n",
      "41     \t [ 1.41198193 -1.63313684]. \t  -17.74913813628892 \t 1.0031713457562699\n",
      "42     \t [-0.3615254  -1.73951775]. \t  -25.637680752411736 \t 1.0031713457562699\n",
      "43     \t [-0.85257763 -0.51197168]. \t  -1.5888576668774759 \t 1.0031713457562699\n",
      "44     \t [-1.37633581 -0.83965887]. \t  -2.631232565221661 \t 1.0031713457562699\n",
      "45     \t [ 0.62029188 -0.52614174]. \t  -0.12001388081785569 \t 1.0031713457562699\n",
      "46     \t [-0.13963214  0.73526178]. \t  \u001b[92m1.0188768065018763\u001b[0m \t 1.0188768065018763\n",
      "47     \t [1.16135382 0.68659946]. \t  -2.1933345180377493 \t 1.0188768065018763\n",
      "48     \t [-0.19770199  0.7174539 ]. \t  0.9878169811127523 \t 1.0188768065018763\n",
      "49     \t [-0.35546651 -0.74355793]. \t  0.25193582438135464 \t 1.0188768065018763\n",
      "50     \t [ 1.50646609 -1.33221999]. \t  -5.65177010784323 \t 1.0188768065018763\n",
      "51     \t [2.08783872 0.44943854]. \t  -5.436344038665195 \t 1.0188768065018763\n",
      "52     \t [-1.45305386 -0.34856998]. \t  -2.3008774639519785 \t 1.0188768065018763\n",
      "53     \t [1.65376027 0.95022269]. \t  -3.271828619864523 \t 1.0188768065018763\n",
      "54     \t [2.20044236 0.12277229]. \t  -8.184137643768748 \t 1.0188768065018763\n",
      "55     \t [-0.27248587 -1.68949914]. \t  -21.918853808438246 \t 1.0188768065018763\n",
      "56     \t [-2.9649158   0.71469175]. \t  -96.20285419413186 \t 1.0188768065018763\n",
      "57     \t [-0.03908832  1.52429823]. \t  -12.246916075350724 \t 1.0188768065018763\n",
      "58     \t [ 0.06118469 -0.60839842]. \t  0.9548343161575438 \t 1.0188768065018763\n",
      "59     \t [-2.81943489 -1.60024499]. \t  -87.03388190982164 \t 1.0188768065018763\n",
      "60     \t [-0.07933293  0.76480393]. \t  1.0067333196639645 \t 1.0188768065018763\n",
      "61     \t [ 0.14353488 -0.71219178]. \t  \u001b[92m1.0204954013647223\u001b[0m \t 1.0204954013647223\n",
      "62     \t [-1.58257969  0.18530123]. \t  -1.6563179025136354 \t 1.0204954013647223\n",
      "63     \t [-1.58032713  0.8188882 ]. \t  0.09374647447396878 \t 1.0204954013647223\n",
      "64     \t [-2.60818333  0.03771494]. \t  -34.85945773019259 \t 1.0204954013647223\n",
      "65     \t [-2.55474319 -1.27128561]. \t  -36.557144218172 \t 1.0204954013647223\n",
      "66     \t [-0.03789537 -1.65611557]. \t  -19.187648864389928 \t 1.0204954013647223\n",
      "67     \t [ 2.69738938 -0.55125018]. \t  -43.99173716040437 \t 1.0204954013647223\n",
      "68     \t [0.4258326  0.81592786]. \t  -0.11559474197424568 \t 1.0204954013647223\n",
      "69     \t [ 1.1347592  -0.26344266]. \t  -1.823091754041529 \t 1.0204954013647223\n",
      "70     \t [-2.5967251  -1.10896766]. \t  -37.69588921345289 \t 1.0204954013647223\n",
      "71     \t [-0.10400095  0.6400702 ]. \t  0.99092468711703 \t 1.0204954013647223\n",
      "72     \t [ 0.11557289 -0.70501129]. \t  \u001b[92m1.0283906665434865\u001b[0m \t 1.0283906665434865\n",
      "73     \t [-0.05610261  0.7138529 ]. \t  1.027112230628762 \t 1.0283906665434865\n",
      "74     \t [0.82403448 0.31942618]. \t  -1.7489445201560758 \t 1.0283906665434865\n",
      "75     \t [ 0.15774598 -0.77761722]. \t  0.9805872426076191 \t 1.0283906665434865\n",
      "76     \t [ 0.15308587 -0.67024391]. \t  0.9997011589866988 \t 1.0283906665434865\n",
      "77     \t [-1.86597671  0.33298528]. \t  -1.5233391741694335 \t 1.0283906665434865\n",
      "78     \t [-0.37470388 -1.86731947]. \t  -35.90668535930074 \t 1.0283906665434865\n",
      "79     \t [-1.48232832  1.24797864]. \t  -3.809308878301353 \t 1.0283906665434865\n",
      "80     \t [-0.07080778  0.42071336]. \t  0.5924708603077229 \t 1.0283906665434865\n",
      "81     \t [0.21493049 0.85901793]. \t  0.4086308083959589 \t 1.0283906665434865\n",
      "82     \t [ 2.33954439 -0.84215168]. \t  -10.84472818874074 \t 1.0283906665434865\n",
      "83     \t [-0.14517488  0.6584975 ]. \t  0.9945982160404532 \t 1.0283906665434865\n",
      "84     \t [-2.66400556  1.14826982]. \t  -40.38804091939894 \t 1.0283906665434865\n",
      "85     \t [-0.35692482  1.8012942 ]. \t  -28.96591328040761 \t 1.0283906665434865\n",
      "86     \t [-1.79036479  1.36361529]. \t  -6.174035781723444 \t 1.0283906665434865\n",
      "87     \t [-1.34978112 -1.60704806]. \t  -20.850960601027268 \t 1.0283906665434865\n",
      "88     \t [-0.2051117  -1.31876297]. \t  -5.5769146212012695 \t 1.0283906665434865\n",
      "89     \t [-0.83881349  0.88334614]. \t  -0.46421823057038614 \t 1.0283906665434865\n",
      "90     \t [ 0.01520152 -0.6470992 ]. \t  0.9824981953250173 \t 1.0283906665434865\n",
      "91     \t [-0.09334287  0.64079587]. \t  0.9931657537994472 \t 1.0283906665434865\n",
      "92     \t [2.66476303 1.48070808]. \t  -56.27023939005207 \t 1.0283906665434865\n",
      "93     \t [ 1.29508841 -0.30486184]. \t  -1.6421082069525126 \t 1.0283906665434865\n",
      "94     \t [1.30075989 0.18903078]. \t  -2.4787070683515418 \t 1.0283906665434865\n",
      "95     \t [-1.36987247 -0.74622182]. \t  -2.3490503986980915 \t 1.0283906665434865\n",
      "96     \t [-1.28869068 -1.9300101 ]. \t  -45.465960802926766 \t 1.0283906665434865\n",
      "97     \t [1.79005905 0.64195341]. \t  -2.4021734601384312 \t 1.0283906665434865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98     \t [-2.21651463  0.47165672]. \t  -6.754605027893274 \t 1.0283906665434865\n",
      "99     \t [-2.96590443  1.52060883]. \t  -107.20873029801652 \t 1.0283906665434865\n",
      "100    \t [ 0.15531981 -0.63170859]. \t  0.9620783809565833 \t 1.0283906665434865\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\n",
    "\n",
    "np.random.seed(run_num_19)\n",
    "surrogate_loser_19 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_19 = GPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_19.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.42310371 0.25811502]. \t  -0.5111512856704337 \t 0.9146183273252478\n",
      "init   \t [-0.069349   -0.65408899]. \t  0.9146183273252478 \t 0.9146183273252478\n",
      "init   \t [-0.74479093  0.12814348]. \t  -1.4695211123708676 \t 0.9146183273252478\n",
      "init   \t [-2.59136227  0.33811624]. \t  -31.819735064764004 \t 0.9146183273252478\n",
      "init   \t [-1.57261342 -1.35697367]. \t  -10.421507729581124 \t 0.9146183273252478\n",
      "1      \t [-0.1410994  -0.15343052]. \t  -0.008508372031263561 \t 0.9146183273252478\n",
      "2      \t [ 0.5005314  -1.42718009]. \t  -8.608756717094366 \t 0.9146183273252478\n",
      "3      \t [1.38215761 0.87577001]. \t  -2.7970299264195067 \t 0.9146183273252478\n",
      "4      \t [-0.39610392  1.66004094]. \t  -19.273014065521274 \t 0.9146183273252478\n",
      "5      \t [1.02776732 0.0387556 ]. \t  -2.3087827753134285 \t 0.9146183273252478\n",
      "6      \t [-0.4732578 -0.7758918]. \t  -0.20311220017100673 \t 0.9146183273252478\n",
      "7      \t [3. 2.]. \t  -162.89999999999998 \t 0.9146183273252478\n",
      "8      \t [0.91712204 0.80482995]. \t  -1.902565927851525 \t 0.9146183273252478\n",
      "9      \t [1.09063056 0.59591867]. \t  -2.08157898064945 \t 0.9146183273252478\n",
      "10     \t [ 3. -2.]. \t  -150.89999999999998 \t 0.9146183273252478\n",
      "11     \t [ 0.34255669 -0.41440248]. \t  0.26990781880783854 \t 0.9146183273252478\n",
      "12     \t [-0.28548568 -1.26341423]. \t  -4.479679859597242 \t 0.9146183273252478\n",
      "13     \t [1.03096635 1.82076077]. \t  -34.85724569420337 \t 0.9146183273252478\n",
      "14     \t [-0.06424198  0.66310979]. \t  \u001b[92m1.0115897831405252\u001b[0m \t 1.0115897831405252\n",
      "15     \t [1.73875072 0.07913889]. \t  -2.2225003429877965 \t 1.0115897831405252\n",
      "16     \t [0.05067324 0.56896764]. \t  0.8366186269448552 \t 1.0115897831405252\n",
      "17     \t [-0.54685574  0.71234399]. \t  0.3720150607136654 \t 1.0115897831405252\n",
      "18     \t [-2.45443226 -1.76743731]. \t  -51.63710217674941 \t 1.0115897831405252\n",
      "19     \t [-0.3539167  0.6164445]. \t  0.6918380695327568 \t 1.0115897831405252\n",
      "20     \t [-2.93308739  1.92957439]. \t  -126.12625451200103 \t 1.0115897831405252\n",
      "21     \t [-1.46468038 -0.45383003]. \t  -2.2180126657296784 \t 1.0115897831405252\n",
      "22     \t [-1.49665459  0.0429097 ]. \t  -2.0979520264098555 \t 1.0115897831405252\n",
      "23     \t [ 0.10172959 -0.50067633]. \t  0.7611142455075717 \t 1.0115897831405252\n",
      "24     \t [2.40576203 0.22100924]. \t  -17.77628783602815 \t 1.0115897831405252\n",
      "25     \t [ 1.15768582 -0.75681001]. \t  -0.5363429178601061 \t 1.0115897831405252\n",
      "26     \t [ 1.24532974 -0.34713449]. \t  -1.5397272611429655 \t 1.0115897831405252\n",
      "27     \t [ 0.21042367 -0.87409273]. \t  0.7320410593980813 \t 1.0115897831405252\n",
      "28     \t [-0.1249432   0.69344902]. \t  \u001b[92m1.0232454309393322\u001b[0m \t 1.0232454309393322\n",
      "29     \t [ 0.73655129 -0.67690304]. \t  -0.11360890499210385 \t 1.0232454309393322\n",
      "30     \t [-1.66145743 -0.36139022]. \t  -2.1974947754210867 \t 1.0232454309393322\n",
      "31     \t [0.63498595 0.24208286]. \t  -1.226310123768032 \t 1.0232454309393322\n",
      "32     \t [1.76454506 1.75355096]. \t  -30.773058715602147 \t 1.0232454309393322\n",
      "33     \t [-0.70063536 -1.49000954]. \t  -13.376267448872674 \t 1.0232454309393322\n",
      "34     \t [-1.76588361 -0.05751031]. \t  -2.248865967433256 \t 1.0232454309393322\n",
      "35     \t [-0.042137   -0.70719929]. \t  0.9631051838337794 \t 1.0232454309393322\n",
      "36     \t [-1.20861921  1.21160752]. \t  -3.684685961525207 \t 1.0232454309393322\n",
      "37     \t [1.93528781 1.15383591]. \t  -7.0336234600326355 \t 1.0232454309393322\n",
      "38     \t [ 0.01025272 -0.79312933]. \t  0.9410913960677343 \t 1.0232454309393322\n",
      "39     \t [-0.21181931  1.31563517]. \t  -4.957014144297905 \t 1.0232454309393322\n",
      "40     \t [ 0.09041254 -0.71871263]. \t  \u001b[92m1.0313277428674157\u001b[0m \t 1.0313277428674157\n",
      "41     \t [0.33567907 0.84587171]. \t  0.10576419435501183 \t 1.0313277428674157\n",
      "42     \t [-0.12862961  0.64521919]. \t  0.9893678250213382 \t 1.0313277428674157\n",
      "43     \t [ 0.08158329 -0.62395067]. \t  0.9753683746920428 \t 1.0313277428674157\n",
      "44     \t [-0.10882415  0.73852438]. \t  1.025041203582134 \t 1.0313277428674157\n",
      "45     \t [ 0.220423   -0.67723323]. \t  0.9530106735638181 \t 1.0313277428674157\n",
      "46     \t [-1.26778253  0.79166584]. \t  -0.4487301246208374 \t 1.0313277428674157\n",
      "47     \t [ 1.86169714 -1.66893029]. \t  -19.299309473598427 \t 1.0313277428674157\n",
      "48     \t [2.22582705 0.85300953]. \t  -9.913042978039233 \t 1.0313277428674157\n",
      "49     \t [-2.73300823  0.36249973]. \t  -50.17615223964773 \t 1.0313277428674157\n",
      "50     \t [-1.93294282  0.71893414]. \t  -0.6269092246740491 \t 1.0313277428674157\n",
      "51     \t [-1.7739137   0.54186476]. \t  -0.38830279172706494 \t 1.0313277428674157\n",
      "52     \t [2.86596799 1.35194377]. \t  -85.81964848600569 \t 1.0313277428674157\n",
      "53     \t [ 2.50809148 -1.9411586 ]. \t  -61.890306381826484 \t 1.0313277428674157\n",
      "54     \t [0.68684727 1.576156  ]. \t  -17.28651740726995 \t 1.0313277428674157\n",
      "55     \t [ 1.6876843  -1.06361766]. \t  -0.8578843170988343 \t 1.0313277428674157\n",
      "56     \t [1.61908395 0.84616162]. \t  -2.6160829030643655 \t 1.0313277428674157\n",
      "57     \t [ 1.73465985 -0.76689056]. \t  0.19554113814734309 \t 1.0313277428674157\n",
      "58     \t [ 0.08015378 -0.71414736]. \t  1.0312291710560308 \t 1.0313277428674157\n",
      "59     \t [ 0.18133602 -0.76305834]. \t  0.9820322506520267 \t 1.0313277428674157\n",
      "60     \t [-0.95114137 -0.52735464]. \t  -1.8453259498234824 \t 1.0313277428674157\n",
      "61     \t [-0.16063737  0.72765648]. \t  1.011586740702901 \t 1.0313277428674157\n",
      "62     \t [-1.37697643  1.99735281]. \t  -47.26063798593281 \t 1.0313277428674157\n",
      "63     \t [ 0.83770616 -1.62121248]. \t  -17.64909715689385 \t 1.0313277428674157\n",
      "64     \t [0.65001593 1.09752317]. \t  -3.039336773075536 \t 1.0313277428674157\n",
      "65     \t [1.73840271 1.06707025]. \t  -4.595694977471223 \t 1.0313277428674157\n",
      "66     \t [ 0.19764621 -0.70416827]. \t  0.9860360228473105 \t 1.0313277428674157\n",
      "67     \t [0.12038614 0.56886193]. \t  0.7495233589753024 \t 1.0313277428674157\n",
      "68     \t [ 0.11798213 -0.78541251]. \t  0.9827544844772327 \t 1.0313277428674157\n",
      "69     \t [1.3353567  1.04700935]. \t  -4.165381765206539 \t 1.0313277428674157\n",
      "70     \t [0.00615685 0.23329082]. \t  0.20426231579659207 \t 1.0313277428674157\n",
      "71     \t [0.87655999 0.96016329]. \t  -2.5385368155372108 \t 1.0313277428674157\n",
      "72     \t [2.96244727 0.78664517]. \t  -100.06067106051304 \t 1.0313277428674157\n",
      "73     \t [-0.92118385 -1.14758204]. \t  -4.8125465261924765 \t 1.0313277428674157\n",
      "74     \t [0.01525143 0.69519879]. \t  0.9873515367409686 \t 1.0313277428674157\n",
      "75     \t [0.56516436 1.69716506]. \t  -23.698064532884224 \t 1.0313277428674157\n",
      "76     \t [-2.88492799 -0.52827373]. \t  -80.71660737594533 \t 1.0313277428674157\n",
      "77     \t [ 1.69156201 -0.93166013]. \t  -0.0266658478363333 \t 1.0313277428674157\n",
      "78     \t [-0.74782999  1.64496886]. \t  -18.872726333510375 \t 1.0313277428674157\n",
      "79     \t [-1.55413022 -0.38824081]. \t  -2.198501298752952 \t 1.0313277428674157\n",
      "80     \t [ 0.52780801 -0.00876636]. \t  -0.9536217085611318 \t 1.0313277428674157\n",
      "81     \t [-2.56522298 -1.49674733]. \t  -45.32165847174537 \t 1.0313277428674157\n",
      "82     \t [-0.1538417   0.74760567]. \t  1.0076321782521729 \t 1.0313277428674157\n",
      "83     \t [ 0.15159605 -1.21007528]. \t  -2.626738233352941 \t 1.0313277428674157\n",
      "84     \t [ 0.07384001 -0.71883278]. \t  1.0302132968791833 \t 1.0313277428674157\n",
      "85     \t [-1.54479312  1.77040155]. \t  -26.140109675074886 \t 1.0313277428674157\n",
      "86     \t [-1.25157199 -1.40329153]. \t  -11.784961769668035 \t 1.0313277428674157\n",
      "87     \t [-1.81141394 -0.5683477 ]. \t  -2.4458799782390996 \t 1.0313277428674157\n",
      "88     \t [-2.09444163  1.19667487]. \t  -5.242459497536333 \t 1.0313277428674157\n",
      "89     \t [-2.75848662  1.78661372]. \t  -78.76463662692397 \t 1.0313277428674157\n",
      "90     \t [-0.00709024 -0.69535786]. \t  0.9937826376602398 \t 1.0313277428674157\n",
      "91     \t [-0.06683585  0.6538729 ]. \t  1.0048797576315551 \t 1.0313277428674157\n",
      "92     \t [2.34358413 0.81141137]. \t  -14.850598955553561 \t 1.0313277428674157\n",
      "93     \t [-0.06449487  0.71157144]. \t  1.0291301921296678 \t 1.0313277428674157\n",
      "94     \t [1.74749468 0.86841508]. \t  -2.9000546789739157 \t 1.0313277428674157\n",
      "95     \t [ 1.83828023 -0.60114099]. \t  -0.37116675153433265 \t 1.0313277428674157\n",
      "96     \t [2.30985615 1.83555002]. \t  -48.35912842854276 \t 1.0313277428674157\n",
      "97     \t [0.88287554 1.78244895]. \t  -31.241494035851236 \t 1.0313277428674157\n",
      "98     \t [-2.91923265 -1.74870989]. \t  -118.15330446754632 \t 1.0313277428674157\n",
      "99     \t [ 0.16676387 -0.67378003]. \t  0.994266858801935 \t 1.0313277428674157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [-1.05347936  1.81735804]. \t  -30.816346862556998 \t 1.0313277428674157\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\n",
    "\n",
    "np.random.seed(run_num_20)\n",
    "surrogate_loser_20 = GaussianProcess(cov_func)\n",
    "\n",
    "loser_20 = GPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
    "loser_20.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.67302105 -1.32372098]. \t  -5.793449752432556 \t -0.8736935954900025\n",
      "init   \t [-0.38364588  1.07704989]. \t  -0.8736935954900025 \t -0.8736935954900025\n",
      "init   \t [-1.22804817 -1.40334817]. \t  -11.759316761133794 \t -0.8736935954900025\n",
      "init   \t [-2.86513005 -0.31910203]. \t  -76.26436708944966 \t -0.8736935954900025\n",
      "init   \t [-1.56790715 -0.64937523]. \t  -2.1371114994016214 \t -0.8736935954900025\n",
      "1      \t [-0.76667053 -0.12653924]. \t  -1.727288288003642 \t -0.8736935954900025\n",
      "2      \t [1.45153667 2.        ]. \t  -53.126220386596856 \t -0.8736935954900025\n",
      "3      \t [ 2.84441635 -2.        ]. \t  -113.74685326845638 \t -0.8736935954900025\n",
      "4      \t [-1.64233822  2.        ]. \t  -46.76746427203114 \t -0.8736935954900025\n",
      "5      \t [3.         0.59730217]. \t  -109.77396598320101 \t -0.8736935954900025\n",
      "6      \t [0.68619289 0.15367273]. \t  -1.465868954993594 \t -0.8736935954900025\n",
      "7      \t [-3. -2.]. \t  -162.89999999999998 \t -0.8736935954900025\n",
      "8      \t [-3.  2.]. \t  -150.89999999999998 \t -0.8736935954900025\n",
      "9      \t [-0.26825687  2.        ]. \t  -47.740582635259855 \t -0.8736935954900025\n",
      "10     \t [-0.05049908 -2.        ]. \t  -48.1111851393283 \t -0.8736935954900025\n",
      "11     \t [-1.37551101  0.68398891]. \t  \u001b[92m-0.3715717434846746\u001b[0m \t -0.3715717434846746\n",
      "12     \t [ 1.39900419 -0.68270703]. \t  \u001b[92m-0.33306225530014266\u001b[0m \t -0.33306225530014266\n",
      "13     \t [3. 2.]. \t  -162.89999999999998 \t -0.33306225530014266\n",
      "14     \t [ 1.29401388 -2.        ]. \t  -47.78675471709944 \t -0.33306225530014266\n",
      "15     \t [ 0.70561704 -0.60947271]. \t  \u001b[92m-0.14817359032216992\u001b[0m \t -0.14817359032216992\n",
      "16     \t [0.67255096 1.07468494]. \t  -2.849098599842765 \t -0.14817359032216992\n",
      "17     \t [1.46195528 0.45626442]. \t  -2.218399204390969 \t -0.14817359032216992\n",
      "18     \t [-0.85752584  0.63921391]. \t  -0.4236693874508637 \t -0.14817359032216992\n",
      "19     \t [-0.87123618 -0.85449247]. \t  -1.9284056726963317 \t -0.14817359032216992\n",
      "20     \t [-1.45965017 -0.00992095]. \t  -2.2275597514432715 \t -0.14817359032216992\n",
      "21     \t [ 3.         -0.64153967]. \t  -106.00665822954537 \t -0.14817359032216992\n",
      "22     \t [0.02612437 0.6532801 ]. \t  \u001b[92m0.9587568463029928\u001b[0m \t 0.9587568463029928\n",
      "23     \t [ 1.30070942 -0.12713496]. \t  -2.141704818680438 \t 0.9587568463029928\n",
      "24     \t [ 1.05556564 -0.93041744]. \t  -0.8636154217664247 \t 0.9587568463029928\n",
      "25     \t [-0.03454824 -0.62459221]. \t  0.9253516330364147 \t 0.9587568463029928\n",
      "26     \t [-1.07237132  1.21971048]. \t  -3.9238710590981847 \t 0.9587568463029928\n",
      "27     \t [-3.          0.69043467]. \t  -105.83086754587542 \t 0.9587568463029928\n",
      "28     \t [1.01256197 0.69805556]. \t  -1.9603325727318337 \t 0.9587568463029928\n",
      "29     \t [ 0.09097513 -0.19478645]. \t  0.13076722905397561 \t 0.9587568463029928\n",
      "30     \t [ 1.92337749 -0.14675948]. \t  -2.5674313556040693 \t 0.9587568463029928\n",
      "31     \t [-1.20720645 -0.56966473]. \t  -2.211894018987744 \t 0.9587568463029928\n",
      "32     \t [-1.83162966 -2.        ]. \t  -54.033440077358996 \t 0.9587568463029928\n",
      "33     \t [1.84096441 1.18437836]. \t  -6.851841695153997 \t 0.9587568463029928\n",
      "34     \t [1.45889491 1.05837872]. \t  -4.296858524586104 \t 0.9587568463029928\n",
      "35     \t [0.32755089 0.78835248]. \t  0.27732898134864237 \t 0.9587568463029928\n",
      "36     \t [-1.99088718  0.27314865]. \t  -2.7994698594930445 \t 0.9587568463029928\n",
      "37     \t [-1.90747936  1.16761225]. \t  -2.5631165149250235 \t 0.9587568463029928\n",
      "38     \t [-1.68865982  0.92603491]. \t  -0.006949934977043715 \t 0.9587568463029928\n",
      "39     \t [ 2.03140028 -1.24412762]. \t  -5.034201220345844 \t 0.9587568463029928\n",
      "40     \t [ 1.8667403 -0.8087434]. \t  -0.12853866545138337 \t 0.9587568463029928\n",
      "41     \t [ 0.33622352 -0.9227722 ]. \t  0.3901950077772145 \t 0.9587568463029928\n",
      "42     \t [-1.05601758 -2.        ]. \t  -52.423424926597384 \t 0.9587568463029928\n",
      "43     \t [ 1.6802926  -1.10070184]. \t  -1.2312723767640326 \t 0.9587568463029928\n",
      "44     \t [1.98855257 0.68025606]. \t  -3.949430540664051 \t 0.9587568463029928\n",
      "45     \t [-2.21025569 -1.21024767]. \t  -13.683740340768335 \t 0.9587568463029928\n",
      "46     \t [-1.69587572 -1.10458423]. \t  -5.011051823091371 \t 0.9587568463029928\n",
      "47     \t [0.56100462 2.        ]. \t  -50.18329504272944 \t 0.9587568463029928\n",
      "48     \t [-2.03158035 -0.4222338 ]. \t  -4.444053609267336 \t 0.9587568463029928\n",
      "49     \t [-1.9539024   0.79476758]. \t  -0.7276193612598353 \t 0.9587568463029928\n",
      "50     \t [ 2.08973373 -2.        ]. \t  -49.00059082178099 \t 0.9587568463029928\n",
      "51     \t [-3.         -1.12852963]. \t  -113.67928759420286 \t 0.9587568463029928\n",
      "52     \t [-0.24797314 -1.2676971 ]. \t  -4.454746075127395 \t 0.9587568463029928\n",
      "53     \t [-2.28010799  2.        ]. \t  -54.31493159664009 \t 0.9587568463029928\n",
      "54     \t [2.20858371 2.        ]. \t  -60.64923779974848 \t 0.9587568463029928\n",
      "55     \t [-0.30707925 -0.56279714]. \t  0.33404381210568856 \t 0.9587568463029928\n",
      "56     \t [ 3.        -1.3572009]. \t  -111.03219194473783 \t 0.9587568463029928\n",
      "57     \t [2.74573902 1.31191756]. \t  -62.198692392798414 \t 0.9587568463029928\n",
      "58     \t [0.18661715 0.46314085]. \t  0.45075677140415676 \t 0.9587568463029928\n",
      "59     \t [-0.40230285  0.81565013]. \t  0.6250669097103231 \t 0.9587568463029928\n",
      "60     \t [-0.03243257  0.77937017]. \t  \u001b[92m0.9749174257634772\u001b[0m \t 0.9749174257634772\n",
      "61     \t [ 0.08574745 -0.67288085]. \t  \u001b[92m1.0194775038217088\u001b[0m \t 1.0194775038217088\n",
      "62     \t [ 0.07675542 -0.7052946 ]. \t  \u001b[92m1.0306162206546288\u001b[0m \t 1.0306162206546288\n",
      "63     \t [ 0.07006081 -0.75863871]. \t  1.0107465771401076 \t 1.0306162206546288\n",
      "64     \t [-0.06503597  0.74132976]. \t  1.0215032693945976 \t 1.0306162206546288\n",
      "65     \t [ 0.0224089  -0.72738768]. \t  1.0109062796742727 \t 1.0306162206546288\n",
      "66     \t [ 0.0184893  -0.77461787]. \t  0.9729286944262636 \t 1.0306162206546288\n",
      "67     \t [-2.79365716  1.40593604]. \t  -65.55947929630221 \t 1.0306162206546288\n",
      "68     \t [ 0.16023779 -0.7602874 ]. \t  0.9961419429202709 \t 1.0306162206546288\n",
      "69     \t [-0.08628971  0.75742897]. \t  1.0139650028724891 \t 1.0306162206546288\n",
      "70     \t [-0.05964763  0.75047688]. \t  1.0145745503717862 \t 1.0306162206546288\n",
      "71     \t [-0.12795775  0.73798133]. \t  1.0215366883283992 \t 1.0306162206546288\n",
      "72     \t [-0.12988261  0.68633578]. \t  1.0189102949372086 \t 1.0306162206546288\n",
      "73     \t [ 2.42949833 -0.01913198]. \t  -18.945288583216055 \t 1.0306162206546288\n",
      "74     \t [-0.92729627  2.        ]. \t  -48.24412896093454 \t 1.0306162206546288\n",
      "75     \t [-0.05014364  0.69999651]. \t  1.0246557160051248 \t 1.0306162206546288\n",
      "76     \t [-0.07347356  0.64824608]. \t  1.0006391423586327 \t 1.0306162206546288\n",
      "77     \t [-0.09525024  0.68471976]. \t  1.0252183984260343 \t 1.0306162206546288\n",
      "78     \t [-0.0940335   0.74133466]. \t  1.0246735103926512 \t 1.0306162206546288\n",
      "79     \t [-0.11543967  0.67314161]. \t  1.0159830506394976 \t 1.0306162206546288\n",
      "80     \t [ 0.10546219 -0.67884072]. \t  1.021223447580109 \t 1.0306162206546288\n",
      "81     \t [ 0.07800499 -0.73374925]. \t  1.027080121143681 \t 1.0306162206546288\n",
      "82     \t [-2.41481682 -2.        ]. \t  -70.84281013274334 \t 1.0306162206546288\n",
      "83     \t [ 0.12233339 -0.77045734]. \t  0.9998128128678788 \t 1.0306162206546288\n",
      "84     \t [-0.02261079  0.69639499]. \t  1.0127975119574564 \t 1.0306162206546288\n",
      "85     \t [-0.02207133  0.67223675]. \t  1.0036355108590762 \t 1.0306162206546288\n",
      "86     \t [ 0.14103335 -0.74304422]. \t  1.0151968168017653 \t 1.0306162206546288\n",
      "87     \t [-0.02599986 -0.73076664]. \t  0.9736677495111222 \t 1.0306162206546288\n",
      "88     \t [-0.06635785  0.720965  ]. \t  1.0287022598941806 \t 1.0306162206546288\n",
      "89     \t [-0.06584207  0.6975384 ]. \t  1.0279035459139225 \t 1.0306162206546288\n",
      "90     \t [-0.04287408  0.70374889]. \t  1.022737158840082 \t 1.0306162206546288\n",
      "91     \t [-0.04926054  0.75311248]. \t  1.009352932694486 \t 1.0306162206546288\n",
      "92     \t [-0.05262211  0.69034316]. \t  1.0230718921647926 \t 1.0306162206546288\n",
      "93     \t [-0.08199595  0.73142269]. \t  1.0282810152398565 \t 1.0306162206546288\n",
      "94     \t [-0.08327806  0.69974498]. \t  1.0302042900450217 \t 1.0306162206546288\n",
      "95     \t [ 0.08071161 -0.69912615]. \t  1.0299553772729426 \t 1.0306162206546288\n",
      "96     \t [ 0.09584415 -0.73768547]. \t  1.026327945292202 \t 1.0306162206546288\n",
      "97     \t [-0.06519773  0.68620583]. \t  1.0243817429957822 \t 1.0306162206546288\n",
      "98     \t [ 0.06408592 -0.74103533]. \t  1.021440963942526 \t 1.0306162206546288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [ 0.18698037 -0.65952202]. \t  0.9691076949452115 \t 1.0306162206546288\n",
      "100    \t [-0.09590253  0.63470195]. \t  0.986502566117976 \t 1.0306162206546288\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \n",
    "\n",
    "np.random.seed(run_num_1)\n",
    "surrogate_winner_1 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_1 = GPGO(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_1.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 2.11191296 -1.7043856 ]. \t  -24.175973303775706 \t -2.2733752808550527\n",
      "init   \t [ 2.36959058 -0.25402581]. \t  -14.417173913696448 \t -2.2733752808550527\n",
      "init   \t [-2.23393359  0.3034315 ]. \t  -8.078384314238003 \t -2.2733752808550527\n",
      "init   \t [ 2.04282552 -0.2595178 ]. \t  -3.564649730949853 \t -2.2733752808550527\n",
      "init   \t [1.17546336 0.73855239]. \t  -2.2733752808550527 \t -2.2733752808550527\n",
      "1      \t [ 0.97492642 -0.44471358]. \t  \u001b[92m-1.12278926632448\u001b[0m \t -1.12278926632448\n",
      "2      \t [-3. -2.]. \t  -162.89999999999998 \t -1.12278926632448\n",
      "3      \t [-3.  2.]. \t  -150.89999999999998 \t -1.12278926632448\n",
      "4      \t [3. 2.]. \t  -162.89999999999998 \t -1.12278926632448\n",
      "5      \t [-0.20181051  2.        ]. \t  -47.75582809840052 \t -1.12278926632448\n",
      "6      \t [-0.93572987 -0.30938931]. \t  -2.059406706508965 \t -1.12278926632448\n",
      "7      \t [-0.17465626 -2.        ]. \t  -48.469387088831745 \t -1.12278926632448\n",
      "8      \t [-1.03532884  0.67712556]. \t  \u001b[92m-0.5911361244015984\u001b[0m \t -0.5911361244015984\n",
      "9      \t [ 3. -2.]. \t  -150.89999999999998 \t -0.5911361244015984\n",
      "10     \t [ 1.2234401 -2.       ]. \t  -47.95327337797588 \t -0.5911361244015984\n",
      "11     \t [0.10828952 0.43119432]. \t  \u001b[92m0.5121244024848266\u001b[0m \t 0.5121244024848266\n",
      "12     \t [ 1.77594872 -0.99146209]. \t  -0.35651639162104487 \t 0.5121244024848266\n",
      "13     \t [-3.         -0.16328284]. \t  -109.28604666551222 \t 0.5121244024848266\n",
      "14     \t [-1.59106641  0.25007547]. \t  -1.443473598470021 \t 0.5121244024848266\n",
      "15     \t [-1.80166805  1.03415064]. \t  -0.6918106557846768 \t 0.5121244024848266\n",
      "16     \t [-1.28684828 -2.        ]. \t  -52.95256501083494 \t 0.5121244024848266\n",
      "17     \t [-0.12628052 -0.51460828]. \t  \u001b[92m0.6505252397691996\u001b[0m \t 0.6505252397691996\n",
      "18     \t [1.15014536 2.        ]. \t  -52.688462800351665 \t 0.6505252397691996\n",
      "19     \t [1.01720918 0.15888388]. \t  -2.3229841363704984 \t 0.6505252397691996\n",
      "20     \t [2.08988596 0.62999448]. \t  -5.5422089951534605 \t 0.6505252397691996\n",
      "21     \t [-1.32027757  2.        ]. \t  -47.716610888527164 \t 0.6505252397691996\n",
      "22     \t [-0.77856504 -1.08760002]. \t  -3.4393178439660375 \t 0.6505252397691996\n",
      "23     \t [-1.90822276  0.66236864]. \t  -0.565669802794281 \t 0.6505252397691996\n",
      "24     \t [-1.65270147 -0.6930101 ]. \t  -2.1979286568770138 \t 0.6505252397691996\n",
      "25     \t [-0.38118493  1.07749753]. \t  -0.874856925821048 \t 0.6505252397691996\n",
      "26     \t [3.         0.46187727]. \t  -109.61434916075208 \t 0.6505252397691996\n",
      "27     \t [1.67634402 0.56355785]. \t  -2.132015439160532 \t 0.6505252397691996\n",
      "28     \t [-1.16744482  1.16044671]. \t  -2.907138188281387 \t 0.6505252397691996\n",
      "29     \t [0.39964898 1.04359877]. \t  -1.3918846834058385 \t 0.6505252397691996\n",
      "30     \t [ 0.30054976 -0.98958893]. \t  0.0341281906453889 \t 0.6505252397691996\n",
      "31     \t [ 0.41411341 -0.51430011]. \t  0.36526393827856607 \t 0.6505252397691996\n",
      "32     \t [1.86923639 1.28023491]. \t  -9.139844212948056 \t 0.6505252397691996\n",
      "33     \t [-0.18492258 -0.89488587]. \t  0.33819992304174684 \t 0.6505252397691996\n",
      "34     \t [ 2.24740137 -0.94058717]. \t  -7.058813869747913 \t 0.6505252397691996\n",
      "35     \t [-3.          0.91024697]. \t  -105.60103830678722 \t 0.6505252397691996\n",
      "36     \t [-0.35571025  0.70766034]. \t  \u001b[92m0.778545769329088\u001b[0m \t 0.778545769329088\n",
      "37     \t [-1.88912675 -0.22540479]. \t  -2.91295632092314 \t 0.778545769329088\n",
      "38     \t [0.3929975  0.68551611]. \t  0.15805458597755528 \t 0.778545769329088\n",
      "39     \t [-1.24785677 -0.72766614]. \t  -2.306749099113076 \t 0.778545769329088\n",
      "40     \t [-0.45905735  0.13321051]. \t  -0.6219239729420665 \t 0.778545769329088\n",
      "41     \t [-1.89239042 -1.49378467]. \t  -16.519537371465475 \t 0.778545769329088\n",
      "42     \t [-1.55961781  0.88496701]. \t  -0.04247894350298187 \t 0.778545769329088\n",
      "43     \t [-2.07379067  2.        ]. \t  -48.728445074354134 \t 0.778545769329088\n",
      "44     \t [1.70612353 0.9427063 ]. \t  -3.2839677627263906 \t 0.778545769329088\n",
      "45     \t [ 1.01950305 -1.05220408]. \t  -1.6648752092166563 \t 0.778545769329088\n",
      "46     \t [ 3.         -0.91714906]. \t  -105.61411993932717 \t 0.778545769329088\n",
      "47     \t [2.01912601 2.        ]. \t  -56.028947307217194 \t 0.778545769329088\n",
      "48     \t [-2.5218049  -1.07169612]. \t  -29.625109872625096 \t 0.778545769329088\n",
      "49     \t [-0.52364109 -0.69663264]. \t  -0.3114322812332766 \t 0.778545769329088\n",
      "50     \t [1.3260565  1.35245802]. \t  -10.212678238154464 \t 0.778545769329088\n",
      "51     \t [-2.2009076 -2.       ]. \t  -60.38974992480275 \t 0.778545769329088\n",
      "52     \t [ 0.45256564 -1.58534188]. \t  -15.230212249870924 \t 0.778545769329088\n",
      "53     \t [ 0.10787702 -0.14060837]. \t  0.04642181136260565 \t 0.778545769329088\n",
      "54     \t [-0.03755591  0.77369526]. \t  \u001b[92m0.9845279468952898\u001b[0m \t 0.9845279468952898\n",
      "55     \t [ 1.87363936 -1.26735468]. \t  -4.103176046869763 \t 0.9845279468952898\n",
      "56     \t [3.         1.27861445]. \t  -116.88742599892936 \t 0.9845279468952898\n",
      "57     \t [ 0.09328605 -0.71237746]. \t  \u001b[92m1.0315806437782526\u001b[0m \t 1.0315806437782526\n",
      "58     \t [-3.         -1.17807114]. \t  -114.5873353886588 \t 1.0315806437782526\n",
      "59     \t [-2.22569204 -0.61315032]. \t  -9.228723338444246 \t 1.0315806437782526\n",
      "60     \t [ 0.0969617  -0.70053591]. \t  1.0301619989318676 \t 1.0315806437782526\n",
      "61     \t [-2.45629869  1.47282407]. \t  -27.426093864970156 \t 1.0315806437782526\n",
      "62     \t [-0.08947093  0.70632981]. \t  1.0313053701753931 \t 1.0315806437782526\n",
      "63     \t [-0.10930699  0.67436329]. \t  1.0180352535016546 \t 1.0315806437782526\n",
      "64     \t [ 0.52655971 -2.        ]. \t  -47.90160659852617 \t 1.0315806437782526\n",
      "65     \t [-0.04232305  0.71929383]. \t  1.0220757210015656 \t 1.0315806437782526\n",
      "66     \t [ 0.10161662 -0.70202859]. \t  1.0300527418990735 \t 1.0315806437782526\n",
      "67     \t [-0.01124561  0.68657253]. \t  1.0039391057093279 \t 1.0315806437782526\n",
      "68     \t [ 0.06677001 -0.7164581 ]. \t  1.029337828440158 \t 1.0315806437782526\n",
      "69     \t [-0.19469708  0.7285727 ]. \t  0.989423531725616 \t 1.0315806437782526\n",
      "70     \t [ 0.14573581 -0.67930471]. \t  1.0090445145119125 \t 1.0315806437782526\n",
      "71     \t [-0.07682707  0.72414949]. \t  1.0297178070657131 \t 1.0315806437782526\n",
      "72     \t [0.42550208 2.        ]. \t  -49.50835287501195 \t 1.0315806437782526\n",
      "73     \t [ 0.12626149 -0.69759875]. \t  1.0241308278284031 \t 1.0315806437782526\n",
      "74     \t [ 1.94617152 -2.        ]. \t  -47.24379233393633 \t 1.0315806437782526\n",
      "75     \t [2.45937213 1.52443158]. \t  -37.18265023210004 \t 1.0315806437782526\n",
      "76     \t [ 0.06857827 -0.70862561]. \t  1.0298123200269587 \t 1.0315806437782526\n",
      "77     \t [-0.03911843  0.71549453]. \t  1.0213034008135675 \t 1.0315806437782526\n",
      "78     \t [ 0.09210819 -0.72867003]. \t  1.0294976622550407 \t 1.0315806437782526\n",
      "79     \t [-0.14488767  0.71673742]. \t  1.0200468822373678 \t 1.0315806437782526\n",
      "80     \t [-0.1454317   0.71451916]. \t  1.0198043075277636 \t 1.0315806437782526\n",
      "81     \t [-0.22368726  0.77168793]. \t  0.9412058701859468 \t 1.0315806437782526\n",
      "82     \t [-0.10912298  0.69520934]. \t  1.0274158050999542 \t 1.0315806437782526\n",
      "83     \t [ 0.0739977  -0.66507655]. \t  1.014069622039975 \t 1.0315806437782526\n",
      "84     \t [ 0.03684087 -0.65771816]. \t  1.000631062611163 \t 1.0315806437782526\n",
      "85     \t [-0.14948655  0.77956943]. \t  0.9817734421065205 \t 1.0315806437782526\n",
      "86     \t [ 0.11602253 -0.7556258 ]. \t  1.0140572599701765 \t 1.0315806437782526\n",
      "87     \t [ 0.04660734 -0.67570226]. \t  1.015270186389554 \t 1.0315806437782526\n",
      "88     \t [-0.01838188 -0.72483851]. \t  0.9827459871388802 \t 1.0315806437782526\n",
      "89     \t [ 0.17680693 -0.7072905 ]. \t  1.0020528318299777 \t 1.0315806437782526\n",
      "90     \t [ 0.09947784 -0.72893548]. \t  1.0292043697695994 \t 1.0315806437782526\n",
      "91     \t [-0.07996677  0.74268103]. \t  1.0232568806643565 \t 1.0315806437782526\n",
      "92     \t [ 0.05667761 -0.78638038]. \t  0.9756737580354016 \t 1.0315806437782526\n",
      "93     \t [-0.04653804  0.61643624]. \t  0.9624283225127114 \t 1.0315806437782526\n",
      "94     \t [-0.07339503  0.69789113]. \t  1.0290647048062758 \t 1.0315806437782526\n",
      "95     \t [-0.12593051  0.66522748]. \t  1.0076529562894543 \t 1.0315806437782526\n",
      "96     \t [0.01636192 0.64722741]. \t  0.9620327353746673 \t 1.0315806437782526\n",
      "97     \t [-0.11354262  0.77887082]. \t  0.991727493645305 \t 1.0315806437782526\n",
      "98     \t [ 0.0495422  -0.74982513]. \t  1.0118487768358115 \t 1.0315806437782526\n",
      "99     \t [ 0.0954932  -0.72268722]. \t  1.0307252578819246 \t 1.0315806437782526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [-0.10764922  0.71043188]. \t  1.030316643689446 \t 1.0315806437782526\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \n",
    "\n",
    "np.random.seed(run_num_2)\n",
    "surrogate_winner_2 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_2 = GPGO(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_2.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.50945364 -1.5629949 ]. \t  -13.89572751148896 \t -0.027690663940903623\n",
      "init   \t [-0.08327233 -0.00067529]. \t  -0.027690663940903623 \t -0.027690663940903623\n",
      "init   \t [ 0.94265999 -1.05646116]. \t  -1.6525931811945593 \t -0.027690663940903623\n",
      "init   \t [ 0.67676934 -1.52139039]. \t  -12.56543686771716 \t -0.027690663940903623\n",
      "init   \t [ 1.27321378 -0.50975783]. \t  -0.9673942177133168 \t -0.027690663940903623\n",
      "1      \t [-3.  2.]. \t  -150.89999999999998 \t -0.027690663940903623\n",
      "2      \t [-3. -2.]. \t  -162.89999999999998 \t -0.027690663940903623\n",
      "3      \t [3. 2.]. \t  -162.89999999999998 \t -0.027690663940903623\n",
      "4      \t [-0.02541473  2.        ]. \t  -47.95175329775765 \t -0.027690663940903623\n",
      "5      \t [ 3. -2.]. \t  -150.89999999999998 \t -0.027690663940903623\n",
      "6      \t [-1.52877665 -0.02241103]. \t  -2.1654648849678164 \t -0.027690663940903623\n",
      "7      \t [-0.92575723 -1.17907963]. \t  -5.357065220986615 \t -0.027690663940903623\n",
      "8      \t [ 3.         -0.03158972]. \t  -108.80124318851033 \t -0.027690663940903623\n",
      "9      \t [-3.          0.01407054]. \t  -108.85699660210928 \t -0.027690663940903623\n",
      "10     \t [-0.997133    0.87831917]. \t  -0.6476441612132321 \t -0.027690663940903623\n",
      "11     \t [1.00979026 0.92671885]. \t  -2.699414960830802 \t -0.027690663940903623\n",
      "12     \t [-0.85815745 -2.        ]. \t  -51.656278926308026 \t -0.027690663940903623\n",
      "13     \t [-0.88886393 -0.24016642]. \t  -2.0099031465334662 \t -0.027690663940903623\n",
      "14     \t [0.71771451 0.25565829]. \t  -1.487931030858459 \t -0.027690663940903623\n",
      "15     \t [1.25145784 2.        ]. \t  -52.89707958714462 \t -0.027690663940903623\n",
      "16     \t [0.00120629 0.83975555]. \t  \u001b[92m0.830570459747225\u001b[0m \t 0.830570459747225\n",
      "17     \t [-1.35503346  2.        ]. \t  -47.6180004641028 \t 0.830570459747225\n",
      "18     \t [-1.65083123 -0.89721338]. \t  -2.904353488225276 \t 0.830570459747225\n",
      "19     \t [1.72479321 0.57354466]. \t  -2.1967994461295075 \t 0.830570459747225\n",
      "20     \t [-0.48835619  0.54236566]. \t  0.25634377209690096 \t 0.830570459747225\n",
      "21     \t [-0.01100751 -0.80544867]. \t  \u001b[92m0.9021460301877675\u001b[0m \t 0.9021460301877675\n",
      "22     \t [-1.33753936 -0.6493947 ]. \t  -2.2365857318926863 \t 0.9021460301877675\n",
      "23     \t [-1.71798831  0.79291151]. \t  0.21327512074243138 \t 0.9021460301877675\n",
      "24     \t [-1.39929143  0.58017976]. \t  -0.5782056442941141 \t 0.9021460301877675\n",
      "25     \t [1.36248802 0.34810374]. \t  -2.369383786676928 \t 0.9021460301877675\n",
      "26     \t [ 1.22096165 -2.        ]. \t  -47.958487699280894 \t 0.9021460301877675\n",
      "27     \t [ 1.59987509 -0.9546255 ]. \t  -0.21934629343720247 \t 0.9021460301877675\n",
      "28     \t [-0.28440618 -1.16052053]. \t  -2.5083780075302586 \t 0.9021460301877675\n",
      "29     \t [1.7029911  1.11813755]. \t  -5.224275794206819 \t 0.9021460301877675\n",
      "30     \t [-1.82681497 -2.        ]. \t  -54.0036830484909 \t 0.9021460301877675\n",
      "31     \t [-0.43401103 -0.66645017]. \t  0.017100188217460066 \t 0.9021460301877675\n",
      "32     \t [3.         0.93418698]. \t  -111.25819790380177 \t 0.9021460301877675\n",
      "33     \t [-0.48148858  1.13694814]. \t  -1.7843676920932308 \t 0.9021460301877675\n",
      "34     \t [-3.          0.97459362]. \t  -105.78561704062982 \t 0.9021460301877675\n",
      "35     \t [-1.94840719  0.41368098]. \t  -1.784088492823331 \t 0.9021460301877675\n",
      "36     \t [ 1.87873548 -0.23781517]. \t  -1.9536370690607239 \t 0.9021460301877675\n",
      "37     \t [0.14989243 0.54359044]. \t  0.6624088106593312 \t 0.9021460301877675\n",
      "38     \t [ 2.37188526 -1.06684986]. \t  -13.489542730199233 \t 0.9021460301877675\n",
      "39     \t [ 1.9188056  -0.83508578]. \t  -0.4502121623676393 \t 0.9021460301877675\n",
      "40     \t [-3.         -1.03196333]. \t  -112.27255632795844 \t 0.9021460301877675\n",
      "41     \t [ 0.48770196 -0.47982946]. \t  0.10583143818796492 \t 0.9021460301877675\n",
      "42     \t [2.05659083 2.        ]. \t  -56.68528925421835 \t 0.9021460301877675\n",
      "43     \t [-2.03600634 -0.43721398]. \t  -4.5112198249586895 \t 0.9021460301877675\n",
      "44     \t [ 2.15339567 -2.        ]. \t  -50.32274186732338 \t 0.9021460301877675\n",
      "45     \t [ 0.36177147 -0.94262605]. \t  0.2488594971974047 \t 0.9021460301877675\n",
      "46     \t [-2.13620661  2.        ]. \t  -49.92638799568335 \t 0.9021460301877675\n",
      "47     \t [-2.16329506  1.1617467 ]. \t  -6.266195427202392 \t 0.9021460301877675\n",
      "48     \t [0.37841443 0.80205885]. \t  0.08364451279350404 \t 0.9021460301877675\n",
      "49     \t [ 3.         -0.94098919]. \t  -105.67135209753326 \t 0.9021460301877675\n",
      "50     \t [ 1.94111135 -1.164649  ]. \t  -2.761851244726122 \t 0.9021460301877675\n",
      "51     \t [ 0.04344454 -2.        ]. \t  -47.92065315515655 \t 0.9021460301877675\n",
      "52     \t [-1.74859988  1.34933477]. \t  -5.743683443439975 \t 0.9021460301877675\n",
      "53     \t [0.56370067 1.42653428]. \t  -10.298729589743354 \t 0.9021460301877675\n",
      "54     \t [-2.28604209 -1.34642985]. \t  -20.09906762338394 \t 0.9021460301877675\n",
      "55     \t [2.31564575 1.22850762]. \t  -18.379685946929285 \t 0.9021460301877675\n",
      "56     \t [2.26954814 0.49565956]. \t  -10.824232771484997 \t 0.9021460301877675\n",
      "57     \t [-0.41718642  0.80802588]. \t  0.6092534023412759 \t 0.9021460301877675\n",
      "58     \t [-2.13247347 -0.88134785]. \t  -7.295124624318773 \t 0.9021460301877675\n",
      "59     \t [ 0.15498966 -0.61094109]. \t  \u001b[92m0.9355462983181055\u001b[0m \t 0.9355462983181055\n",
      "60     \t [-0.65391701  2.        ]. \t  -48.04467712287688 \t 0.9355462983181055\n",
      "61     \t [ 2.33371053 -0.39336788]. \t  -11.902120673470641 \t 0.9355462983181055\n",
      "62     \t [-2.39959758 -2.        ]. \t  -69.84215986002386 \t 0.9355462983181055\n",
      "63     \t [ 0.09994298 -0.71359569]. \t  \u001b[92m1.031233725895604\u001b[0m \t 1.031233725895604\n",
      "64     \t [ 0.11927908 -0.75834639]. \t  1.0114154108183109 \t 1.031233725895604\n",
      "65     \t [ 0.03702586 -0.61907415]. \t  0.9629224069987365 \t 1.031233725895604\n",
      "66     \t [ 0.10349437 -0.73264326]. \t  1.027813632483017 \t 1.031233725895604\n",
      "67     \t [ 0.04731073 -0.71393476]. \t  1.0244574905965738 \t 1.031233725895604\n",
      "68     \t [ 0.1048316 -0.6797354]. \t  1.0217884890094384 \t 1.031233725895604\n",
      "69     \t [-0.08723506  0.71048944]. \t  \u001b[92m1.0315692557205012\u001b[0m \t 1.0315692557205012\n",
      "70     \t [-0.085218    0.69780762]. \t  1.029845235826047 \t 1.0315692557205012\n",
      "71     \t [ 0.10059076 -0.66003813]. \t  1.0095708731123838 \t 1.0315692557205012\n",
      "72     \t [-0.12230103  0.70812995]. \t  1.0272351804027715 \t 1.0315692557205012\n",
      "73     \t [ 0.15402638 -0.71335775]. \t  1.0158415408650785 \t 1.0315692557205012\n",
      "74     \t [-0.06260491  0.6893381 ]. \t  1.0250479541201325 \t 1.0315692557205012\n",
      "75     \t [ 0.11106851 -0.69269113]. \t  1.0262815034298751 \t 1.0315692557205012\n",
      "76     \t [ 0.08545607 -0.71516949]. \t  1.031490472369654 \t 1.0315692557205012\n",
      "77     \t [ 0.06959487 -0.72321734]. \t  1.0288836810393729 \t 1.0315692557205012\n",
      "78     \t [ 0.03616725 -0.66591163]. \t  1.006058597212544 \t 1.0315692557205012\n",
      "79     \t [ 0.0737485  -0.72012428]. \t  1.0300340737648084 \t 1.0315692557205012\n",
      "80     \t [ 0.19858824 -0.76662394]. \t  0.9669652615957235 \t 1.0315692557205012\n",
      "81     \t [ 0.06440268 -0.70772616]. \t  1.0290216729926325 \t 1.0315692557205012\n",
      "82     \t [ 0.11596409 -0.70682466]. \t  1.0285539144321263 \t 1.0315692557205012\n",
      "83     \t [-0.10220584  0.71049719]. \t  1.0309691983736442 \t 1.0315692557205012\n",
      "84     \t [-2.39255494  0.62236624]. \t  -14.171062920669923 \t 1.0315692557205012\n",
      "85     \t [-0.08656489  0.7071939 ]. \t  1.0313619598470116 \t 1.0315692557205012\n",
      "86     \t [-0.13256151  0.71824672]. \t  1.024559826667381 \t 1.0315692557205012\n",
      "87     \t [-0.11804741  0.74811918]. \t  1.0187317520262313 \t 1.0315692557205012\n",
      "88     \t [ 0.03049975 -0.76975237]. \t  0.9855192814944557 \t 1.0315692557205012\n",
      "89     \t [-0.06220009  0.64971793]. \t  1.0007156876888748 \t 1.0315692557205012\n",
      "90     \t [-0.11584158  0.68578687]. \t  1.0226154053739913 \t 1.0315692557205012\n",
      "91     \t [ 0.14185278 -0.73013735]. \t  1.019548165158369 \t 1.0315692557205012\n",
      "92     \t [ 0.13358461 -0.68689723]. \t  1.0178716455831334 \t 1.0315692557205012\n",
      "93     \t [-0.08594972  0.73266868]. \t  1.0281197613941342 \t 1.0315692557205012\n",
      "94     \t [1.45217399 1.53965223]. \t  -17.453746001642045 \t 1.0315692557205012\n",
      "95     \t [ 0.10471117 -0.73865511]. \t  1.0254181572077463 \t 1.0315692557205012\n",
      "96     \t [ 0.07029535 -0.73294308]. \t  1.0262709670870378 \t 1.0315692557205012\n",
      "97     \t [-0.00314662 -0.69410427]. \t  0.9964485436635415 \t 1.0315692557205012\n",
      "98     \t [-0.16152445  0.7881505 ]. \t  0.9656289819946076 \t 1.0315692557205012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [ 0.08929223 -0.66959678]. \t  1.0173638855459055 \t 1.0315692557205012\n",
      "100    \t [ 0.07361829 -0.74484367]. \t  1.0212084440540428 \t 1.0315692557205012\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \n",
    "\n",
    "np.random.seed(run_num_3)\n",
    "surrogate_winner_3 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_3 = GPGO(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_3.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [2.03616628 1.28517986]. \t  -11.164343885550666 \t -1.5094648555070385\n",
      "init   \t [0.85324299 0.66889047]. \t  -1.5094648555070385 \t -1.5094648555070385\n",
      "init   \t [-2.76516057 -1.73731045]. \t  -85.9872978949554 \t -1.5094648555070385\n",
      "init   \t [-1.34099616 -0.73455799]. \t  -2.3318567753236477 \t -1.5094648555070385\n",
      "init   \t [-1.83844553 -1.21162847]. \t  -7.376051913381579 \t -1.5094648555070385\n",
      "1      \t [-0.71911804 -2.        ]. \t  -50.991266000780776 \t -1.5094648555070385\n",
      "2      \t [ 3. -2.]. \t  -150.89999999999998 \t -1.5094648555070385\n",
      "3      \t [-3.  2.]. \t  -150.89999999999998 \t -1.5094648555070385\n",
      "4      \t [-0.05694125  2.        ]. \t  -47.89906465192911 \t -1.5094648555070385\n",
      "5      \t [3. 2.]. \t  -162.89999999999998 \t -1.5094648555070385\n",
      "6      \t [1.98719812 0.14525787]. \t  -3.780937362027664 \t -1.5094648555070385\n",
      "7      \t [-2.41621685 -0.09401105]. \t  -18.296854391772293 \t -1.5094648555070385\n",
      "8      \t [ 0.83763526 -1.06233497]. \t  -1.578339771653447 \t -1.5094648555070385\n",
      "9      \t [1.22698583 2.        ]. \t  -52.85367890083489 \t -1.5094648555070385\n",
      "10     \t [-0.952259    0.76559959]. \t  \u001b[92m-0.4495789620609363\u001b[0m \t -0.4495789620609363\n",
      "11     \t [ 0.05440254 -0.15579215]. \t  \u001b[92m0.09138373530934304\u001b[0m \t 0.09138373530934304\n",
      "12     \t [ 0.94574825 -2.        ]. \t  -48.244739288058376 \t 0.09138373530934304\n",
      "13     \t [1.64975546 0.74507506]. \t  -2.292553039224233 \t 0.09138373530934304\n",
      "14     \t [3.         0.22167317]. \t  -109.37812208893236 \t 0.09138373530934304\n",
      "15     \t [ 1.28420982 -0.41131503]. \t  -1.2898443205239318 \t 0.09138373530934304\n",
      "16     \t [-0.09318807  0.76085832]. \t  \u001b[92m1.011420761024877\u001b[0m \t 1.011420761024877\n",
      "17     \t [-1.28474412  2.        ]. \t  -47.810518865617276 \t 1.011420761024877\n",
      "18     \t [-1.60673566  0.13445385]. \t  -1.7787420446414919 \t 1.011420761024877\n",
      "19     \t [ 1.92805468 -0.84132732]. \t  -0.5238690428393766 \t 1.011420761024877\n",
      "20     \t [-1.86257961  0.8774033 ]. \t  -0.17726269046216714 \t 1.011420761024877\n",
      "21     \t [-0.04547067 -1.0123189 ]. \t  -0.15590850686998473 \t 1.011420761024877\n",
      "22     \t [-3.          0.53445091]. \t  -106.48045191291713 \t 1.011420761024877\n",
      "23     \t [-1.46540432  0.90582164]. \t  -0.2901164806090406 \t 1.011420761024877\n",
      "24     \t [ 1.85535126 -0.46316799]. \t  -0.9485135641502054 \t 1.011420761024877\n",
      "25     \t [ 1.51870735 -1.13462968]. \t  -1.9010074792406721 \t 1.011420761024877\n",
      "26     \t [0.51935225 0.22875174]. \t  -0.853113737857589 \t 1.011420761024877\n",
      "27     \t [ 0.36213218 -0.65757576]. \t  0.7306578618029385 \t 1.011420761024877\n",
      "28     \t [-2.0355777  -0.70309127]. \t  -4.664257096436493 \t 1.011420761024877\n",
      "29     \t [-0.63961011  0.22693057]. \t  -0.9672344421906743 \t 1.011420761024877\n",
      "30     \t [-1.79869812  0.55803778]. \t  -0.3868342062219813 \t 1.011420761024877\n",
      "31     \t [-1.77649763 -2.        ]. \t  -53.738523548009034 \t 1.011420761024877\n",
      "32     \t [-3.         -0.78309265]. \t  -110.3005657766947 \t 1.011420761024877\n",
      "33     \t [2.1512072  0.78950013]. \t  -7.332181670905723 \t 1.011420761024877\n",
      "34     \t [-0.6485497   1.24619984]. \t  -3.962874917481378 \t 1.011420761024877\n",
      "35     \t [-0.37357466 -0.58272448]. \t  0.16111734032231373 \t 1.011420761024877\n",
      "36     \t [-1.89763226 -0.17770805]. \t  -2.95274976956095 \t 1.011420761024877\n",
      "37     \t [0.28294603 1.14014426]. \t  -2.1890896525404955 \t 1.011420761024877\n",
      "38     \t [ 0.13077739 -2.        ]. \t  -47.80624353737959 \t 1.011420761024877\n",
      "39     \t [ 3.         -0.94397246]. \t  -105.67986942399571 \t 1.011420761024877\n",
      "40     \t [-2.08157546  2.        ]. \t  -48.85858069725274 \t 1.011420761024877\n",
      "41     \t [ 1.61656795 -0.79943211]. \t  0.1543212424862238 \t 1.011420761024877\n",
      "42     \t [1.5081756  0.15368463]. \t  -2.295729613167402 \t 1.011420761024877\n",
      "43     \t [ 2.00983528 -2.        ]. \t  -47.84283954651462 \t 1.011420761024877\n",
      "44     \t [-0.8159622  -1.16146107]. \t  -4.661519885694138 \t 1.011420761024877\n",
      "45     \t [2.01692865 2.        ]. \t  -55.993685078395245 \t 1.011420761024877\n",
      "46     \t [-0.24438944  0.96925943]. \t  0.23287826426458139 \t 1.011420761024877\n",
      "47     \t [0.29718691 0.77499641]. \t  0.39205515252510204 \t 1.011420761024877\n",
      "48     \t [1.21527865 1.22905697]. \t  -6.9795734821170825 \t 1.011420761024877\n",
      "49     \t [-1.73749614  1.33594758]. \t  -5.389068223224738 \t 1.011420761024877\n",
      "50     \t [3.         1.16922003]. \t  -114.4149397118258 \t 1.011420761024877\n",
      "51     \t [-0.07220283 -0.76045811]. \t  0.8997754138491278 \t 1.011420761024877\n",
      "52     \t [-3. -2.]. \t  -162.89999999999998 \t 1.011420761024877\n",
      "53     \t [-0.52765675 -0.89449311]. \t  -0.7903602156030243 \t 1.011420761024877\n",
      "54     \t [ 0.28380649 -0.88358552]. \t  0.6268102997657534 \t 1.011420761024877\n",
      "55     \t [ 2.25167643 -1.45211308]. \t  -15.822487967230192 \t 1.011420761024877\n",
      "56     \t [-2.35415872 -2.        ]. \t  -67.11669667191782 \t 1.011420761024877\n",
      "57     \t [-0.52725576  0.7307583 ]. \t  0.4238091443453379 \t 1.011420761024877\n",
      "58     \t [-1.07191387 -0.29440405]. \t  -2.3281447141357567 \t 1.011420761024877\n",
      "59     \t [-3.          1.33503074]. \t  -110.47215342186125 \t 1.011420761024877\n",
      "60     \t [ 0.09233474 -0.71664137]. \t  \u001b[92m1.0314834083004647\u001b[0m \t 1.0314834083004647\n",
      "61     \t [-0.08123464  0.53490983]. \t  0.834184259347123 \t 1.0314834083004647\n",
      "62     \t [-2.3345975   1.52098474]. \t  -21.990740089784623 \t 1.0314834083004647\n",
      "63     \t [ 0.11051655 -0.71978481]. \t  1.0296961544635548 \t 1.0314834083004647\n",
      "64     \t [-2.31549536  0.69288864]. \t  -9.850788766831464 \t 1.0314834083004647\n",
      "65     \t [ 0.13467201 -0.69974139]. \t  1.0219486694389102 \t 1.0314834083004647\n",
      "66     \t [-0.09232882  0.71408579]. \t  \u001b[92m1.0315911448048054\u001b[0m \t 1.0315911448048054\n",
      "67     \t [ 0.54095666 -1.47600233]. \t  -10.471165167264209 \t 1.0315911448048054\n",
      "68     \t [0.56255938 2.        ]. \t  -50.19125053910112 \t 1.0315911448048054\n",
      "69     \t [-0.0056359   0.79323412]. \t  0.9375519280472033 \t 1.0315911448048054\n",
      "70     \t [-0.10213541  0.77484598]. \t  0.9973311513465152 \t 1.0315911448048054\n",
      "71     \t [-0.11849741  0.75910219]. \t  1.0109504357802688 \t 1.0315911448048054\n",
      "72     \t [ 0.03922539 -0.66383044]. \t  1.0058096589952072 \t 1.0315911448048054\n",
      "73     \t [-0.08313312  0.69158761]. \t  1.0280648648783632 \t 1.0315911448048054\n",
      "74     \t [ 0.14837875 -0.73984504]. \t  1.0137506856494132 \t 1.0315911448048054\n",
      "75     \t [-0.10952005  0.62324488]. \t  0.9707930722758658 \t 1.0315911448048054\n",
      "76     \t [ 0.0557432  -0.78220626]. \t  0.9811550532313957 \t 1.0315911448048054\n",
      "77     \t [ 0.10923723 -0.73311742]. \t  1.0270377307435008 \t 1.0315911448048054\n",
      "78     \t [-0.1173455   0.72929013]. \t  1.0268350814887446 \t 1.0315911448048054\n",
      "79     \t [ 0.03316704 -0.59763309]. \t  0.93381706222063 \t 1.0315911448048054\n",
      "80     \t [-0.03387082  0.63694917]. \t  0.9814210691020944 \t 1.0315911448048054\n",
      "81     \t [ 0.1993543  -0.76977286]. \t  0.9635226805885811 \t 1.0315911448048054\n",
      "82     \t [-0.10751678  0.71896054]. \t  1.0301979823774454 \t 1.0315911448048054\n",
      "83     \t [ 0.05631866 -0.79282924]. \t  0.9658556061999762 \t 1.0315911448048054\n",
      "84     \t [-0.21210618  0.66861551]. \t  0.9548652242659537 \t 1.0315911448048054\n",
      "85     \t [ 0.09170983 -0.72981419]. \t  1.0291781765082813 \t 1.0315911448048054\n",
      "86     \t [ 0.11237996 -0.74540384]. \t  1.0212083043592424 \t 1.0315911448048054\n",
      "87     \t [ 2.51291455 -0.39880294]. \t  -23.91772237836115 \t 1.0315911448048054\n",
      "88     \t [-0.19608454  0.68731846]. \t  0.9810159246491004 \t 1.0315911448048054\n",
      "89     \t [ 0.110558   -0.75169999]. \t  1.0175997962103287 \t 1.0315911448048054\n",
      "90     \t [-0.07011924  0.72966752]. \t  1.0273447797271005 \t 1.0315911448048054\n",
      "91     \t [-0.03042105  0.63866413]. \t  0.9817932379415851 \t 1.0315911448048054\n",
      "92     \t [ 0.05954598 -0.74577091]. \t  1.0176289618414656 \t 1.0315911448048054\n",
      "93     \t [-0.15916225  0.79206651]. \t  0.9611870351203415 \t 1.0315911448048054\n",
      "94     \t [ 0.12227245 -0.73587509]. \t  1.023750285512886 \t 1.0315911448048054\n",
      "95     \t [2.52278652 1.58637646]. \t  -45.59667544660737 \t 1.0315911448048054\n",
      "96     \t [-0.09172914  0.72794418]. \t  1.0296885407238963 \t 1.0315911448048054\n",
      "97     \t [-0.11987977  0.68803664]. \t  1.0225983354337922 \t 1.0315911448048054\n",
      "98     \t [ 0.20635385 -0.76318797]. \t  0.9637452681914052 \t 1.0315911448048054\n",
      "99     \t [ 0.08934995 -0.76137829]. \t  1.0108227472392115 \t 1.0315911448048054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [-0.05820621  0.6874555 ]. \t  1.023482298761602 \t 1.0315911448048054\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \n",
    "\n",
    "np.random.seed(run_num_4)\n",
    "surrogate_winner_4 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_4 = GPGO(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_4.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.34411965 0.16704251]. \t  -0.3937629170174044 \t 0.3730261027784846\n",
      "init   \t [ 0.3625427  -0.47180763]. \t  0.3730261027784846 \t 0.3730261027784846\n",
      "init   \t [-2.28543493  1.38399646]. \t  -14.95148366216595 \t 0.3730261027784846\n",
      "init   \t [1.43844967 1.72943026]. \t  -28.545251683089745 \t 0.3730261027784846\n",
      "init   \t [2.15596804 0.8324185 ]. \t  -7.6401968083208285 \t 0.3730261027784846\n",
      "1      \t [ 3.         -0.48938968]. \t  -106.70326723364347 \t 0.3730261027784846\n",
      "2      \t [-3. -2.]. \t  -162.89999999999998 \t 0.3730261027784846\n",
      "3      \t [3. 2.]. \t  -162.89999999999998 \t 0.3730261027784846\n",
      "4      \t [-0.02731921 -2.        ]. \t  -48.057622602209975 \t 0.3730261027784846\n",
      "5      \t [-0.63362659  2.        ]. \t  -48.02175340488427 \t 0.3730261027784846\n",
      "6      \t [1.37784905 0.30857138]. \t  -2.3864638737087893 \t 0.3730261027784846\n",
      "7      \t [-1.15584898 -0.14430308]. \t  -2.4758233105903367 \t 0.3730261027784846\n",
      "8      \t [-3.          0.34464933]. \t  -107.44735714209837 \t 0.3730261027784846\n",
      "9      \t [-3.  2.]. \t  -150.89999999999998 \t 0.3730261027784846\n",
      "10     \t [-1.51077872  0.97949027]. \t  -0.5176238484155105 \t 0.3730261027784846\n",
      "11     \t [ 1.78786593 -2.        ]. \t  -46.640120203788214 \t 0.3730261027784846\n",
      "12     \t [-0.64111426 -0.83634835]. \t  -1.0078368551591463 \t 0.3730261027784846\n",
      "13     \t [ 3. -2.]. \t  -150.89999999999998 \t 0.3730261027784846\n",
      "14     \t [-1.6726103  2.       ]. \t  -46.70792669016903 \t 0.3730261027784846\n",
      "15     \t [-0.41026749 -0.16107928]. \t  -0.5803635837664964 \t 0.3730261027784846\n",
      "16     \t [-1.32653345 -2.        ]. \t  -53.00545650507712 \t 0.3730261027784846\n",
      "17     \t [0.57395448 1.30403618]. \t  -6.615077261956079 \t 0.3730261027784846\n",
      "18     \t [-1.93656747  0.85946803]. \t  -0.6110188707597675 \t 0.3730261027784846\n",
      "19     \t [ 1.12767353 -1.1424164 ]. \t  -2.6807310674113305 \t 0.3730261027784846\n",
      "20     \t [1.32871797 0.91331205]. \t  -3.0108011520258584 \t 0.3730261027784846\n",
      "21     \t [ 0.97324671 -2.        ]. \t  -48.24149264003055 \t 0.3730261027784846\n",
      "22     \t [0.55836665 2.        ]. \t  -50.16980282516968 \t 0.3730261027784846\n",
      "23     \t [-0.18305326  0.87762588]. \t  \u001b[92m0.7368727561591512\u001b[0m \t 0.7368727561591512\n",
      "24     \t [ 1.22516941 -0.5760976 ]. \t  -0.8071761753284433 \t 0.7368727561591512\n",
      "25     \t [-1.7276357  -0.85562582]. \t  -2.7877819266669883 \t 0.7368727561591512\n",
      "26     \t [-1.27210408 -0.73076287]. \t  -2.3204833874392765 \t 0.7368727561591512\n",
      "27     \t [3.         0.57921518]. \t  -109.74589941044076 \t 0.7368727561591512\n",
      "28     \t [ 1.94684503 -0.03491825]. \t  -3.069687643189758 \t 0.7368727561591512\n",
      "29     \t [0.2454006  0.79876637]. \t  0.494432939786008 \t 0.7368727561591512\n",
      "30     \t [-1.85710934 -0.37560102]. \t  -2.7038471770723547 \t 0.7368727561591512\n",
      "31     \t [ 1.73876264 -1.02502128]. \t  -0.5404858296844666 \t 0.7368727561591512\n",
      "32     \t [1.80976728 0.60694745]. \t  -2.452968356805783 \t 0.7368727561591512\n",
      "33     \t [ 0.01696544 -1.02490675]. \t  -0.19567279451020647 \t 0.7368727561591512\n",
      "34     \t [-3.         -0.82902258]. \t  -110.52736061977308 \t 0.7368727561591512\n",
      "35     \t [-0.14544875 -0.65601428]. \t  \u001b[92m0.8014970377961239\u001b[0m \t 0.8014970377961239\n",
      "36     \t [-0.73150447  0.53910908]. \t  -0.3711404762801771 \t 0.8014970377961239\n",
      "37     \t [-1.86493321  1.15077206]. \t  -2.104817681920373 \t 0.8014970377961239\n",
      "38     \t [ 1.76042265 -0.59021204]. \t  -0.20176398202189116 \t 0.8014970377961239\n",
      "39     \t [-0.75375927  1.0923557 ]. \t  -1.7548284353756833 \t 0.8014970377961239\n",
      "40     \t [-0.74662813 -1.40403425]. \t  -10.342303256493167 \t 0.8014970377961239\n",
      "41     \t [1.8816082  1.23917836]. \t  -8.252887020674766 \t 0.8014970377961239\n",
      "42     \t [-1.56606678  0.52135835]. \t  -0.48784891653759055 \t 0.8014970377961239\n",
      "43     \t [-3.          1.17782327]. \t  -107.51550582398853 \t 0.8014970377961239\n",
      "44     \t [0.7851224  0.02067029]. \t  -1.760325213565251 \t 0.8014970377961239\n",
      "45     \t [2.11645682 2.        ]. \t  -57.973663844964165 \t 0.8014970377961239\n",
      "46     \t [ 1.52418774 -0.91050687]. \t  -0.18345359817851592 \t 0.8014970377961239\n",
      "47     \t [-2.14018257 -2.        ]. \t  -58.57608583554815 \t 0.8014970377961239\n",
      "48     \t [-0.18483061  0.60797998]. \t  \u001b[92m0.9101861564911968\u001b[0m \t 0.9101861564911968\n",
      "49     \t [ 1.58029117 -0.22017215]. \t  -1.5515665510917407 \t 0.9101861564911968\n",
      "50     \t [ 0.56015897 -0.8285001 ]. \t  0.2664422987050129 \t 0.9101861564911968\n",
      "51     \t [-1.6223714  -1.41712966]. \t  -12.45651923284455 \t 0.9101861564911968\n",
      "52     \t [ 3.         -1.27331575]. \t  -109.10960362590664 \t 0.9101861564911968\n",
      "53     \t [-0.55137585  0.80784819]. \t  0.3209237295517684 \t 0.9101861564911968\n",
      "54     \t [-2.38803515 -1.37307094]. \t  -26.291490268149325 \t 0.9101861564911968\n",
      "55     \t [-2.05671938  0.27995223]. \t  -3.709582814146791 \t 0.9101861564911968\n",
      "56     \t [-2.25458406  2.        ]. \t  -53.34293433972321 \t 0.9101861564911968\n",
      "57     \t [-2.34826782 -0.53829803]. \t  -14.535032449853405 \t 0.9101861564911968\n",
      "58     \t [ 0.09512114 -0.73382571]. \t  \u001b[92m1.0278528321721259\u001b[0m \t 1.0278528321721259\n",
      "59     \t [3.         1.32695295]. \t  -118.23936694406004 \t 1.0278528321721259\n",
      "60     \t [ 2.36001916 -1.71825864]. \t  -33.72919748840944 \t 1.0278528321721259\n",
      "61     \t [ 0.06318579 -0.65283948]. \t  1.003527733150153 \t 1.0278528321721259\n",
      "62     \t [ 2.1839062  -0.90953587]. \t  -4.9143452270770664 \t 1.0278528321721259\n",
      "63     \t [ 0.04573697 -0.65544324]. \t  1.0017983408771742 \t 1.0278528321721259\n",
      "64     \t [-0.03678273  0.6976524 ]. \t  1.0195479700933727 \t 1.0278528321721259\n",
      "65     \t [ 0.48120348 -1.37726311]. \t  -6.959823656781848 \t 1.0278528321721259\n",
      "66     \t [-0.11246904  0.73273229]. \t  1.0267024521375465 \t 1.0278528321721259\n",
      "67     \t [ 0.04715387 -0.25728553]. \t  0.2505042538450258 \t 1.0278528321721259\n",
      "68     \t [ 0.16327688 -0.77495252]. \t  0.98093832892372 \t 1.0278528321721259\n",
      "69     \t [ 0.12406971 -0.72043381]. \t  1.0268594446376824 \t 1.0278528321721259\n",
      "70     \t [ 0.13811303 -0.77305072]. \t  0.9931205080482071 \t 1.0278528321721259\n",
      "71     \t [ 0.06077242 -0.70429676]. \t  \u001b[92m1.0279943788501766\u001b[0m \t 1.0279943788501766\n",
      "72     \t [-0.07066127  0.71522012]. \t  \u001b[92m1.030085943008215\u001b[0m \t 1.030085943008215\n",
      "73     \t [-0.14215162  1.50645305]. \t  -11.388945295907464 \t 1.030085943008215\n",
      "74     \t [ 0.11164137 -0.70772281]. \t  1.0294785050646538 \t 1.030085943008215\n",
      "75     \t [-0.09228119  0.68807201]. \t  1.0267638394339305 \t 1.030085943008215\n",
      "76     \t [ 0.12855414 -0.74218975]. \t  1.0195378209226618 \t 1.030085943008215\n",
      "77     \t [ 0.02409282 -0.77408123]. \t  0.9769646955024199 \t 1.030085943008215\n",
      "78     \t [2.44598695 1.59023433]. \t  -39.501901294836046 \t 1.030085943008215\n",
      "79     \t [ 0.09076367 -0.75331402]. \t  1.0173484092952003 \t 1.030085943008215\n",
      "80     \t [0.71657014 0.72339842]. \t  -1.0658817522030721 \t 1.030085943008215\n",
      "81     \t [-0.03200929  0.71293139]. \t  1.0184506061868657 \t 1.030085943008215\n",
      "82     \t [ 0.22905631 -0.77038019]. \t  0.9373676735889238 \t 1.030085943008215\n",
      "83     \t [-0.03688361  0.76079804]. \t  0.9977768884675268 \t 1.030085943008215\n",
      "84     \t [ 0.12717713 -0.73308303]. \t  1.0234850032632017 \t 1.030085943008215\n",
      "85     \t [0.05849927 0.60288997]. \t  0.8765124250264613 \t 1.030085943008215\n",
      "86     \t [-0.12453321  0.7621438 ]. \t  1.0072265558644813 \t 1.030085943008215\n",
      "87     \t [-0.08958776  0.6173575 ]. \t  0.9668188769323965 \t 1.030085943008215\n",
      "88     \t [ 0.12482589 -0.76821946]. \t  1.0015601282587703 \t 1.030085943008215\n",
      "89     \t [ 0.02387163 -0.7017928 ]. \t  1.0142499873722364 \t 1.030085943008215\n",
      "90     \t [-0.08873309  0.69035279]. \t  1.0277002363114316 \t 1.030085943008215\n",
      "91     \t [-0.07069201  0.74170017]. \t  1.0224475233291124 \t 1.030085943008215\n",
      "92     \t [ 0.0695542 -0.7578362]. \t  1.0113173565445601 \t 1.030085943008215\n",
      "93     \t [-0.77875192 -2.        ]. \t  -51.285317801763 \t 1.030085943008215\n",
      "94     \t [-0.00860384 -0.66272869]. \t  0.9792198694899512 \t 1.030085943008215\n",
      "95     \t [-0.08123135  0.72039093]. \t  \u001b[92m1.030777142806461\u001b[0m \t 1.030777142806461\n",
      "96     \t [ 0.15057748 -0.75667605]. \t  1.003260742552102 \t 1.030777142806461\n",
      "97     \t [ 0.07577561 -0.69760554]. \t  1.0292503691137052 \t 1.030777142806461\n",
      "98     \t [ 0.02788698 -0.72116171]. \t  1.015389659961843 \t 1.030777142806461\n",
      "99     \t [-0.10038734  0.70478925]. \t  1.0306115507814448 \t 1.030777142806461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [ 0.03880227 -0.68946529]. \t  1.0183070655329003 \t 1.030777142806461\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \n",
    "\n",
    "np.random.seed(run_num_5)\n",
    "surrogate_winner_5 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_5 = GPGO(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_5.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 2.35716091 -0.67208078]. \t  -11.995980169715628 \t 0.3096187466378422\n",
      "init   \t [ 1.92737474 -1.8332135 ]. \t  -31.167927894156623 \t 0.3096187466378422\n",
      "init   \t [-2.35405992  0.38020826]. \t  -13.013381745677465 \t 0.3096187466378422\n",
      "init   \t [ 0.17890417 -0.32477029]. \t  0.3096187466378422 \t 0.3096187466378422\n",
      "init   \t [-0.9875529   0.49007773]. \t  -0.9989196280666852 \t 0.3096187466378422\n",
      "1      \t [0.43239624 1.49654422]. \t  -12.429189685911842 \t 0.3096187466378422\n",
      "2      \t [-1.18419132 -2.        ]. \t  -52.7672303105879 \t 0.3096187466378422\n",
      "3      \t [3. 2.]. \t  -162.89999999999998 \t 0.3096187466378422\n",
      "4      \t [-3.  2.]. \t  -150.89999999999998 \t 0.3096187466378422\n",
      "5      \t [-3.         -0.99565032]. \t  -111.85253060754643 \t 0.3096187466378422\n",
      "6      \t [1.28305979 0.21890806]. \t  -2.479273086948992 \t 0.3096187466378422\n",
      "7      \t [-0.76521988  2.        ]. \t  -48.15867962787903 \t 0.3096187466378422\n",
      "8      \t [ 0.41743864 -2.        ]. \t  -47.80014043253761 \t 0.3096187466378422\n",
      "9      \t [ 3. -2.]. \t  -150.89999999999998 \t 0.3096187466378422\n",
      "10     \t [ 1.33965258 -0.85840625]. \t  -0.41614426526813286 \t 0.3096187466378422\n",
      "11     \t [-1.19632865 -0.39381915]. \t  -2.347470514431805 \t 0.3096187466378422\n",
      "12     \t [3.       0.148361]. \t  -109.25897699260688 \t 0.3096187466378422\n",
      "13     \t [0.23958604 0.48757491]. \t  \u001b[92m0.3852906005597786\u001b[0m \t 0.3852906005597786\n",
      "14     \t [1.21917481 2.        ]. \t  -52.83891822427928 \t 0.3852906005597786\n",
      "15     \t [-1.63002984  0.23999416]. \t  -1.4469164922329822 \t 0.3852906005597786\n",
      "16     \t [-0.54199805 -0.12947136]. \t  -1.0065218616542575 \t 0.3852906005597786\n",
      "17     \t [ 1.73165581 -0.51820112]. \t  -0.41641742941792637 \t 0.3852906005597786\n",
      "18     \t [-0.46185227 -1.00871319]. \t  -1.2980260791417875 \t 0.3852906005597786\n",
      "19     \t [0.84229623 0.82296042]. \t  -1.718742147820094 \t 0.3852906005597786\n",
      "20     \t [-1.72816806  1.00567263]. \t  -0.4028437864587011 \t 0.3852906005597786\n",
      "21     \t [0.78808514 0.11275314]. \t  -1.792773043083108 \t 0.3852906005597786\n",
      "22     \t [-3. -2.]. \t  -162.89999999999998 \t 0.3852906005597786\n",
      "23     \t [ 1.82112537 -1.05436511]. \t  -0.9038275387518782 \t 0.3852906005597786\n",
      "24     \t [-0.39816214  1.06828181]. \t  -0.8020156017655777 \t 0.3852906005597786\n",
      "25     \t [-1.48952037  0.77601865]. \t  -0.06378980512603394 \t 0.3852906005597786\n",
      "26     \t [ 0.23893385 -0.89033335]. \t  \u001b[92m0.6484776590043779\u001b[0m \t 0.6484776590043779\n",
      "27     \t [-1.7221979  2.       ]. \t  -46.6430198034958 \t 0.6484776590043779\n",
      "28     \t [-0.09153975  0.81328573]. \t  \u001b[92m0.9368338011350864\u001b[0m \t 0.9368338011350864\n",
      "29     \t [ 1.26803044 -1.5770524 ]. \t  -15.182462466486136 \t 0.9368338011350864\n",
      "30     \t [-1.96605352 -0.45055316]. \t  -3.574784251641804 \t 0.9368338011350864\n",
      "31     \t [-1.31534111  1.18329261]. \t  -3.045655124774623 \t 0.9368338011350864\n",
      "32     \t [-0.39474777  0.58590928]. \t  0.5594809627934751 \t 0.9368338011350864\n",
      "33     \t [-0.12538513 -0.70447105]. \t  0.8492464622922238 \t 0.9368338011350864\n",
      "34     \t [1.86533457 1.06277226]. \t  -5.102867671151009 \t 0.9368338011350864\n",
      "35     \t [1.53848148 0.82064474]. \t  -2.5057971898631 \t 0.9368338011350864\n",
      "36     \t [-1.57472982 -1.16123935]. \t  -5.796840701281288 \t 0.9368338011350864\n",
      "37     \t [-3.          0.81407082]. \t  -105.56368741265636 \t 0.9368338011350864\n",
      "38     \t [2.01679646 0.52438132]. \t  -4.2180958292851605 \t 0.9368338011350864\n",
      "39     \t [-1.14827173 -0.98640522]. \t  -3.4148815221164948 \t 0.9368338011350864\n",
      "40     \t [ 3.         -1.03591941]. \t  -106.1061492600808 \t 0.9368338011350864\n",
      "41     \t [2.07486103 2.        ]. \t  -57.04547550478926 \t 0.9368338011350864\n",
      "42     \t [-2.02035169 -2.        ]. \t  -56.048766261668604 \t 0.9368338011350864\n",
      "43     \t [-3.         -0.09866356]. \t  -109.15743172239584 \t 0.9368338011350864\n",
      "44     \t [-1.90910672  0.64436377]. \t  -0.6198538910870152 \t 0.9368338011350864\n",
      "45     \t [0.16181592 2.        ]. \t  -48.426935591300506 \t 0.9368338011350864\n",
      "46     \t [ 0.63013991 -0.74396421]. \t  0.17929189109720733 \t 0.9368338011350864\n",
      "47     \t [ 1.97991773 -0.08995209]. \t  -3.2793366758823366 \t 0.9368338011350864\n",
      "48     \t [3.         1.12984095]. \t  -113.70158387600722 \t 0.9368338011350864\n",
      "49     \t [0.21949657 0.90486129]. \t  0.20704094449662813 \t 0.9368338011350864\n",
      "50     \t [ 1.84956653 -0.75626143]. \t  -0.07459202524564434 \t 0.9368338011350864\n",
      "51     \t [ 0.70890662 -1.1640185 ]. \t  -2.6206385788984363 \t 0.9368338011350864\n",
      "52     \t [-2.23417315 -1.19549785]. \t  -14.223823986470126 \t 0.9368338011350864\n",
      "53     \t [-0.34609808 -2.        ]. \t  -49.14177339712986 \t 0.9368338011350864\n",
      "54     \t [1.3525287  1.34003943]. \t  -9.858218081714059 \t 0.9368338011350864\n",
      "55     \t [-2.33984026  1.47073505]. \t  -20.2768928347095 \t 0.9368338011350864\n",
      "56     \t [-0.79943226  0.85750746]. \t  -0.3216378948787767 \t 0.9368338011350864\n",
      "57     \t [-1.76000108 -0.82914876]. \t  -2.747834504910625 \t 0.9368338011350864\n",
      "58     \t [-0.07017875  0.5854494 ]. \t  0.9225277923953198 \t 0.9368338011350864\n",
      "59     \t [-0.67747095 -0.71964382]. \t  -0.9145461000417081 \t 0.9368338011350864\n",
      "60     \t [ 2.37293494 -1.32895238]. \t  -17.709577756807562 \t 0.9368338011350864\n",
      "61     \t [-2.34154554  2.        ]. \t  -57.05985849210384 \t 0.9368338011350864\n",
      "62     \t [-0.0730052   0.29450317]. \t  0.31707946258980296 \t 0.9368338011350864\n",
      "63     \t [ 0.0956585  -0.69524756]. \t  \u001b[92m1.0289733901281577\u001b[0m \t 1.0289733901281577\n",
      "64     \t [ 0.10751039 -0.76243053]. \t  1.0095763465060452 \t 1.0289733901281577\n",
      "65     \t [ 0.05697019 -0.7018927 ]. \t  1.0268107683849246 \t 1.0289733901281577\n",
      "66     \t [ 0.10330858 -0.7375435 ]. \t  1.0260091277969863 \t 1.0289733901281577\n",
      "67     \t [-0.09146119  0.72716606]. \t  \u001b[92m1.0298826680597983\u001b[0m \t 1.0298826680597983\n",
      "68     \t [ 0.02178274 -0.72941493]. \t  1.0098833572917092 \t 1.0298826680597983\n",
      "69     \t [ 0.17982542 -0.74177273]. \t  0.9961346323815455 \t 1.0298826680597983\n",
      "70     \t [-2.35462662 -0.67516387]. \t  -15.031153443617832 \t 1.0298826680597983\n",
      "71     \t [0.02734031 0.66484689]. \t  0.9653880065181868 \t 1.0298826680597983\n",
      "72     \t [ 0.07338176 -0.72174713]. \t  1.029733979787307 \t 1.0298826680597983\n",
      "73     \t [ 1.3134302 -2.       ]. \t  -47.73528481961637 \t 1.0298826680597983\n",
      "74     \t [2.47487885 1.54880213]. \t  -39.56667019078279 \t 1.0298826680597983\n",
      "75     \t [-0.09132607  0.71848897]. \t  \u001b[92m1.0313476741930565\u001b[0m \t 1.0313476741930565\n",
      "76     \t [ 0.02838609 -0.70910016]. \t  1.0168749880505419 \t 1.0313476741930565\n",
      "77     \t [ 0.06831014 -1.40496481]. \t  -7.612478710636584 \t 1.0313476741930565\n",
      "78     \t [ 0.07198829 -0.77600202]. \t  0.9934278541230914 \t 1.0313476741930565\n",
      "79     \t [-0.12503452  0.73869518]. \t  1.0219966586321196 \t 1.0313476741930565\n",
      "80     \t [-0.0764014   0.63493977]. \t  0.9877122294136832 \t 1.0313476741930565\n",
      "81     \t [ 0.09813932 -0.77747488]. \t  0.994316483839714 \t 1.0313476741930565\n",
      "82     \t [-0.05841427  0.74487173]. \t  1.0178596471757142 \t 1.0313476741930565\n",
      "83     \t [ 0.11282827 -0.83983276]. \t  0.8755519520326741 \t 1.0313476741930565\n",
      "84     \t [ 0.12328468 -0.75104233]. \t  1.015862436449463 \t 1.0313476741930565\n",
      "85     \t [ 0.05099416 -0.75559275]. \t  1.0080245835025443 \t 1.0313476741930565\n",
      "86     \t [-0.15564337  0.73427727]. \t  1.0124785941154635 \t 1.0313476741930565\n",
      "87     \t [-0.07117444  0.76825386]. \t  1.0019164202505766 \t 1.0313476741930565\n",
      "88     \t [ 0.0761683  -0.74729115]. \t  1.0201212415192182 \t 1.0313476741930565\n",
      "89     \t [-0.02235716  0.64756703]. \t  0.98645674260795 \t 1.0313476741930565\n",
      "90     \t [-0.07576331  0.67476563]. \t  1.0202420240654777 \t 1.0313476741930565\n",
      "91     \t [-0.08412768  0.70005181]. \t  1.030294723353283 \t 1.0313476741930565\n",
      "92     \t [ 0.07360925 -0.73708529]. \t  1.0251468743168475 \t 1.0313476741930565\n",
      "93     \t [ 0.07773174 -0.6954181 ]. \t  1.0288887494943386 \t 1.0313476741930565\n",
      "94     \t [-0.22361871  0.65386405]. \t  0.9304015131927529 \t 1.0313476741930565\n",
      "95     \t [-0.15055507  0.73211726]. \t  1.015448960855217 \t 1.0313476741930565\n",
      "96     \t [ 0.03827306 -0.83674457]. \t  0.8659435262977604 \t 1.0313476741930565\n",
      "97     \t [ 0.06025277 -0.67013201]. \t  1.015510738762342 \t 1.0313476741930565\n",
      "98     \t [-0.05815953  0.65261413]. \t  1.0024894616829225 \t 1.0313476741930565\n",
      "99     \t [ 0.08653033 -0.72048151]. \t  1.0310528391415699 \t 1.0313476741930565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [-0.06527537  0.72940338]. \t  1.0265031767122283 \t 1.0313476741930565\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \n",
    "\n",
    "np.random.seed(run_num_6)\n",
    "surrogate_winner_6 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_6 = GPGO(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_6.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-2.08401759 -0.79057356]. \t  -5.778591021444629 \t -0.6139922708386365\n",
      "init   \t [-2.62778151 -0.16055863]. \t  -37.56206986588009 \t -0.6139922708386365\n",
      "init   \t [2.01152031 1.70798819]. \t  -29.692842569565112 \t -0.6139922708386365\n",
      "init   \t [1.36193391 1.07398489]. \t  -4.49224583043401 \t -0.6139922708386365\n",
      "init   \t [-1.3847696   0.57611717]. \t  -0.6139922708386365 \t -0.6139922708386365\n",
      "1      \t [-1.05230314 -1.36736553]. \t  -10.25001383186009 \t -0.6139922708386365\n",
      "2      \t [-0.00964284  0.81457353]. \t  \u001b[92m0.9005146019176441\u001b[0m \t 0.9005146019176441\n",
      "3      \t [ 1.5859012  -0.38367148]. \t  -0.9690503568956699 \t 0.9005146019176441\n",
      "4      \t [-2.61517988 -2.        ]. \t  -88.9933710490474 \t 0.9005146019176441\n",
      "5      \t [-1.2452265 -0.3975157]. \t  -2.3587799876310926 \t 0.9005146019176441\n",
      "6      \t [-1.18764405  2.        ]. \t  -48.024141670281416 \t 0.9005146019176441\n",
      "7      \t [ 3. -2.]. \t  -150.89999999999998 \t 0.9005146019176441\n",
      "8      \t [ 0.47199289 -2.        ]. \t  -47.846586422006695 \t 0.9005146019176441\n",
      "9      \t [3.         0.43395726]. \t  -109.59045220159354 \t 0.9005146019176441\n",
      "10     \t [ 8.07319526e-01 -6.62548704e-04]. \t  -1.806737461560502 \t 0.9005146019176441\n",
      "11     \t [0.70745732 2.        ]. \t  -50.93264700457717 \t 0.9005146019176441\n",
      "12     \t [-3.  2.]. \t  -150.89999999999998 \t 0.9005146019176441\n",
      "13     \t [-0.74367587  0.70045223]. \t  -0.10572009973489616 \t 0.9005146019176441\n",
      "14     \t [3. 2.]. \t  -162.89999999999998 \t 0.9005146019176441\n",
      "15     \t [ 1.16742973 -1.01801556]. \t  -1.3569651680385828 \t 0.9005146019176441\n",
      "16     \t [ 0.05319393 -0.79263135]. \t  \u001b[92m0.9650545160095907\u001b[0m \t 0.9650545160095907\n",
      "17     \t [-0.0932326  0.0934669]. \t  0.008742308895549358 \t 0.9650545160095907\n",
      "18     \t [ 0.80093423 -0.65062456]. \t  -0.2922094685407658 \t 0.9650545160095907\n",
      "19     \t [-3.         -0.98922466]. \t  -111.78377350029938 \t 0.9650545160095907\n",
      "20     \t [-1.83310264 -0.17586191]. \t  -2.5789938025167833 \t 0.9650545160095907\n",
      "21     \t [-1.58305964 -0.92510623]. \t  -3.0527629372250407 \t 0.9650545160095907\n",
      "22     \t [-1.45912163 -2.        ]. \t  -53.132346687132625 \t 0.9650545160095907\n",
      "23     \t [-0.51819625 -0.85180408]. \t  -0.5740683798859676 \t 0.9650545160095907\n",
      "24     \t [ 2.30400628 -0.80868635]. \t  -9.151521898333451 \t 0.9650545160095907\n",
      "25     \t [ 1.71783161 -0.82719925]. \t  0.2026795510097431 \t 0.9650545160095907\n",
      "26     \t [0.5046589  0.61580999]. \t  -0.2571415156140865 \t 0.9650545160095907\n",
      "27     \t [ 1.59659191 -2.        ]. \t  -46.878877105656436 \t 0.9650545160095907\n",
      "28     \t [1.56502138 2.        ]. \t  -53.227038176094126 \t 0.9650545160095907\n",
      "29     \t [1.82737877 0.77598561]. \t  -2.8121285617902543 \t 0.9650545160095907\n",
      "30     \t [-0.29774856  2.        ]. \t  -47.742846868862635 \t 0.9650545160095907\n",
      "31     \t [-0.22770538 -1.31805991]. \t  -5.625395454014356 \t 0.9650545160095907\n",
      "32     \t [-1.40802649  1.1650533 ]. \t  -2.573383558747908 \t 0.9650545160095907\n",
      "33     \t [-3.         0.8495651]. \t  -105.54801621464706 \t 0.9650545160095907\n",
      "34     \t [ 3.         -0.71123703]. \t  -105.76642617936406 \t 0.9650545160095907\n",
      "35     \t [ 2.02381573 -0.43152811]. \t  -2.5782384104761897 \t 0.9650545160095907\n",
      "36     \t [ 0.54070343 -1.07265983]. \t  -1.111400088840794 \t 0.9650545160095907\n",
      "37     \t [-1.9950668   1.51655688]. \t  -12.604574062350414 \t 0.9650545160095907\n",
      "38     \t [1.36363114 0.47520389]. \t  -2.2686759827491474 \t 0.9650545160095907\n",
      "39     \t [-0.69052736  0.23528773]. \t  -1.094328871608738 \t 0.9650545160095907\n",
      "40     \t [-1.8126694  0.9742445]. \t  -0.33655359956459674 \t 0.9650545160095907\n",
      "41     \t [-0.07052865  0.54558347]. \t  0.8548702597499218 \t 0.9650545160095907\n",
      "42     \t [-0.60160113 -2.        ]. \t  -50.39162388334953 \t 0.9650545160095907\n",
      "43     \t [-1.79262721 -0.55642975]. \t  -2.372136499212395 \t 0.9650545160095907\n",
      "44     \t [ 1.99350472 -1.334541  ]. \t  -6.555028118894761 \t 0.9650545160095907\n",
      "45     \t [-1.9792804  2.       ]. \t  -47.52366447802827 \t 0.9650545160095907\n",
      "46     \t [ 0.26738514 -0.47310799]. \t  0.5460582390642241 \t 0.9650545160095907\n",
      "47     \t [2.54509058 1.19628028]. \t  -33.90458450052139 \t 0.9650545160095907\n",
      "48     \t [-0.49528995  1.07107301]. \t  -1.0047616337039065 \t 0.9650545160095907\n",
      "49     \t [-0.27785437 -0.41138551]. \t  0.15163190976067775 \t 0.9650545160095907\n",
      "50     \t [-1.94542724 -1.42021118]. \t  -14.097159277287634 \t 0.9650545160095907\n",
      "51     \t [1.85052375 1.16029733]. \t  -6.469379501649265 \t 0.9650545160095907\n",
      "52     \t [1.80150974 0.15582521]. \t  -2.4432718489071608 \t 0.9650545160095907\n",
      "53     \t [-0.14281944  0.67596604]. \t  \u001b[92m1.0084023353919713\u001b[0m \t 1.0084023353919713\n",
      "54     \t [0.63964403 1.12599567]. \t  -3.386580532410358 \t 1.0084023353919713\n",
      "55     \t [ 2.27989868 -2.        ]. \t  -54.3065788331139 \t 1.0084023353919713\n",
      "56     \t [-0.26977842  0.78393724]. \t  0.8788691606449611 \t 1.0084023353919713\n",
      "57     \t [-3. -2.]. \t  -162.89999999999998 \t 1.0084023353919713\n",
      "58     \t [-2.15429641  0.55405864]. \t  -4.608375577545059 \t 1.0084023353919713\n",
      "59     \t [-0.0313657  -0.71504101]. \t  0.9731297624829728 \t 1.0084023353919713\n",
      "60     \t [0.05256744 0.70688924]. \t  0.9518029462796742 \t 1.0084023353919713\n",
      "61     \t [3.         1.24407917]. \t  -116.0232150347549 \t 1.0084023353919713\n",
      "62     \t [ 0.08310935 -0.6525533 ]. \t  1.0046974625477945 \t 1.0084023353919713\n",
      "63     \t [-2.38130265  1.17820982]. \t  -15.285930886519827 \t 1.0084023353919713\n",
      "64     \t [ 0.04860617 -0.67971751]. \t  \u001b[92m1.017828773215976\u001b[0m \t 1.017828773215976\n",
      "65     \t [-0.06367517  0.7171748 ]. \t  \u001b[92m1.0286601111732052\u001b[0m \t 1.0286601111732052\n",
      "66     \t [ 3.         -1.39821124]. \t  -112.173404387191 \t 1.0286601111732052\n",
      "67     \t [-0.15360379  0.73510871]. \t  1.013179909471423 \t 1.0286601111732052\n",
      "68     \t [ 0.01250711 -0.67354761]. \t  0.9992112543687086 \t 1.0286601111732052\n",
      "69     \t [-0.03279413  0.73833571]. \t  1.011763348373984 \t 1.0286601111732052\n",
      "70     \t [2.2808634  0.44674604]. \t  -11.286829051495937 \t 1.0286601111732052\n",
      "71     \t [-0.13065547  0.73170691]. \t  1.0229171904355483 \t 1.0286601111732052\n",
      "72     \t [ 0.14354744 -0.67941049]. \t  1.0100942472670713 \t 1.0286601111732052\n",
      "73     \t [-0.0807091   0.68418869]. \t  1.0251866057617562 \t 1.0286601111732052\n",
      "74     \t [-0.12861235  0.76103901]. \t  1.007209410211846 \t 1.0286601111732052\n",
      "75     \t [-0.08284664  0.67073178]. \t  1.0181648224499238 \t 1.0286601111732052\n",
      "76     \t [-0.07838646  0.71556216]. \t  \u001b[92m1.0310130354763518\u001b[0m \t 1.0310130354763518\n",
      "77     \t [-0.06370488  0.74977617]. \t  1.0161081168709163 \t 1.0310130354763518\n",
      "78     \t [ 0.02693554 -0.65939068]. \t  0.9978538484312321 \t 1.0310130354763518\n",
      "79     \t [ 0.00499081 -0.716116  ]. \t  1.002816742609933 \t 1.0310130354763518\n",
      "80     \t [-0.06946621  0.70281974]. \t  1.0294227298725986 \t 1.0310130354763518\n",
      "81     \t [-1.41662557  0.11022408]. \t  -2.0597777017482555 \t 1.0310130354763518\n",
      "82     \t [0.19859106 1.60967973]. \t  -16.964451387555183 \t 1.0310130354763518\n",
      "83     \t [ 0.32045313 -0.69788474]. \t  0.8339911149805208 \t 1.0310130354763518\n",
      "84     \t [-0.07793599  0.7379979 ]. \t  1.0253266859270622 \t 1.0310130354763518\n",
      "85     \t [ 0.09323304 -0.72998351]. \t  1.0291241299146272 \t 1.0310130354763518\n",
      "86     \t [-0.06060283  0.73351145]. \t  1.024002491216475 \t 1.0310130354763518\n",
      "87     \t [ 0.06959593 -0.66813931]. \t  1.0156871386181832 \t 1.0310130354763518\n",
      "88     \t [ 0.07266376 -0.65112554]. \t  1.0031260330210485 \t 1.0310130354763518\n",
      "89     \t [-0.10675964  0.76184129]. \t  1.0101577010141976 \t 1.0310130354763518\n",
      "90     \t [ 0.14444052 -0.72783072]. \t  1.019049737264862 \t 1.0310130354763518\n",
      "91     \t [ 0.13740367 -0.72827715]. \t  1.0216015444179603 \t 1.0310130354763518\n",
      "92     \t [ 0.07265731 -0.64316472]. \t  0.9958549624668895 \t 1.0310130354763518\n",
      "93     \t [-0.12102407  0.77257722]. \t  0.9978231067562445 \t 1.0310130354763518\n",
      "94     \t [-0.0611344   0.73580274]. \t  1.023204797147767 \t 1.0310130354763518\n",
      "95     \t [-0.10513998  0.68225359]. \t  1.0230013445885682 \t 1.0310130354763518\n",
      "96     \t [ 0.14167522 -0.70925916]. \t  1.021003152472187 \t 1.0310130354763518\n",
      "97     \t [ 0.08334437 -0.71869783]. \t  \u001b[92m1.031122987142766\u001b[0m \t 1.031122987142766\n",
      "98     \t [ 0.24611558 -0.77694393]. \t  0.9135909147120276 \t 1.031122987142766\n",
      "99     \t [ 0.05646049 -0.69024312]. \t  1.024020514989738 \t 1.031122987142766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [-0.13987521  0.65554574]. \t  0.9944899204930115 \t 1.031122987142766\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \n",
    "\n",
    "np.random.seed(run_num_7)\n",
    "surrogate_winner_7 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_7 = GPGO(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_7.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-2.34300075 -1.83113994]. \t  -49.66879811482005 \t 0.22190529745233212\n",
      "init   \t [ 0.59815551 -0.80051718]. \t  0.22190529745233212 \t 0.22190529745233212\n",
      "init   \t [ 1.77967069 -0.5352155 ]. \t  -0.42351602320787596 \t 0.22190529745233212\n",
      "init   \t [ 2.28365006 -0.80863347]. \t  -8.27265764318726 \t 0.22190529745233212\n",
      "init   \t [-1.02538042  0.27486766]. \t  -1.710375599764969 \t 0.22190529745233212\n",
      "1      \t [1.23713758 0.92444566]. \t  -3.0445560047266316 \t 0.22190529745233212\n",
      "2      \t [-2.57792912  2.        ]. \t  -74.516635188269 \t 0.22190529745233212\n",
      "3      \t [3. 2.]. \t  -162.89999999999998 \t 0.22190529745233212\n",
      "4      \t [0.00863862 2.        ]. \t  -48.01757573666605 \t 0.22190529745233212\n",
      "5      \t [ 1.29269883 -2.        ]. \t  -47.7901586055758 \t 0.22190529745233212\n",
      "6      \t [0.48364077 0.17371832]. \t  -0.7919498321032964 \t 0.22190529745233212\n",
      "7      \t [-0.66402969 -1.04854103]. \t  -2.5175948050340646 \t 0.22190529745233212\n",
      "8      \t [-3.         -0.05392546]. \t  -109.05017839310463 \t 0.22190529745233212\n",
      "9      \t [ 3. -2.]. \t  -150.89999999999998 \t 0.22190529745233212\n",
      "10     \t [ 3.         -0.07587344]. \t  -108.64948513199808 \t 0.22190529745233212\n",
      "11     \t [-0.29279183 -2.        ]. \t  -48.91326873097791 \t 0.22190529745233212\n",
      "12     \t [-0.3824056  -0.31532675]. \t  -0.30347625401047895 \t 0.22190529745233212\n",
      "13     \t [1.19578367 2.        ]. \t  -52.792009054700266 \t 0.22190529745233212\n",
      "14     \t [-1.17886534  1.41318702]. \t  -8.697017456215702 \t 0.22190529745233212\n",
      "15     \t [-0.30693591  0.87693071]. \t  \u001b[92m0.621222574622359\u001b[0m \t 0.621222574622359\n",
      "16     \t [ 1.62138753 -1.02828361]. \t  -0.6338551584269632 \t 0.621222574622359\n",
      "17     \t [0.64330662 0.92383117]. \t  -1.4133940985032671 \t 0.621222574622359\n",
      "18     \t [-1.47186358 -0.75567303]. \t  -2.3313160458856164 \t 0.621222574622359\n",
      "19     \t [-1.40547217 -2.        ]. \t  -53.08740445862319 \t 0.621222574622359\n",
      "20     \t [-0.99690972 -0.57232193]. \t  -1.9178605513056888 \t 0.621222574622359\n",
      "21     \t [-0.86441878  0.85204963]. \t  -0.42319320816085304 \t 0.621222574622359\n",
      "22     \t [ 0.03056041 -0.90269254]. \t  \u001b[92m0.6273210312391099\u001b[0m \t 0.6273210312391099\n",
      "23     \t [-1.16784821  2.        ]. \t  -48.05915029424553 \t 0.6273210312391099\n",
      "24     \t [-1.77531564  0.83668649]. \t  0.142724714785739 \t 0.6273210312391099\n",
      "25     \t [-3. -2.]. \t  -162.89999999999998 \t 0.6273210312391099\n",
      "26     \t [-3.          1.16171187]. \t  -107.3019714417806 \t 0.6273210312391099\n",
      "27     \t [-1.49350846  0.97196785]. \t  -0.5126759375272846 \t 0.6273210312391099\n",
      "28     \t [-1.75074603  0.18756878]. \t  -1.6657898865423049 \t 0.6273210312391099\n",
      "29     \t [1.11094668 0.3196963 ]. \t  -2.3527682096616367 \t 0.6273210312391099\n",
      "30     \t [ 0.58556772 -1.44092358]. \t  -9.232726301536086 \t 0.6273210312391099\n",
      "31     \t [-1.80388608 -1.27383242]. \t  -8.604343344970001 \t 0.6273210312391099\n",
      "32     \t [ 1.46211129 -0.72347288]. \t  -0.15492730177540348 \t 0.6273210312391099\n",
      "33     \t [-0.22586604  0.41219493]. \t  0.45860839923109303 \t 0.6273210312391099\n",
      "34     \t [-2.24043536 -0.82129746]. \t  -10.286088509352306 \t 0.6273210312391099\n",
      "35     \t [2.1854659  0.80576723]. \t  -8.368286093937401 \t 0.6273210312391099\n",
      "36     \t [1.79049965 0.58254973]. \t  -2.3696866187019943 \t 0.6273210312391099\n",
      "37     \t [ 1.86064115 -0.82817304]. \t  -0.10700767923576882 \t 0.6273210312391099\n",
      "38     \t [1.94884181 1.37788359]. \t  -12.67086805231962 \t 0.6273210312391099\n",
      "39     \t [ 1.04558854 -1.06176197]. \t  -1.762689880441991 \t 0.6273210312391099\n",
      "40     \t [-1.90761467  1.39406081]. \t  -7.484396488601124 \t 0.6273210312391099\n",
      "41     \t [-1.86515791 -0.50113588]. \t  -2.7168819787133893 \t 0.6273210312391099\n",
      "42     \t [0.59546686 1.40609405]. \t  -9.733741190277996 \t 0.6273210312391099\n",
      "43     \t [ 2.1177256 -2.       ]. \t  -49.533580092037276 \t 0.6273210312391099\n",
      "44     \t [ 0.96829953 -0.31218168]. \t  -1.5249297641891302 \t 0.6273210312391099\n",
      "45     \t [3.         1.13107193]. \t  -113.72259757623208 \t 0.6273210312391099\n",
      "46     \t [-3.         -1.03410363]. \t  -112.29904115155254 \t 0.6273210312391099\n",
      "47     \t [1.85031057 0.96447795]. \t  -3.9812687017487955 \t 0.6273210312391099\n",
      "48     \t [ 2.21985644 -1.31392339]. \t  -10.703229144103918 \t 0.6273210312391099\n",
      "49     \t [-1.88488268  2.        ]. \t  -46.882663956714964 \t 0.6273210312391099\n",
      "50     \t [-0.44983319  1.25022655]. \t  -3.6842233246133818 \t 0.6273210312391099\n",
      "51     \t [ 3.         -1.08556488]. \t  -106.48448895043761 \t 0.6273210312391099\n",
      "52     \t [ 0.07705222 -0.54585898]. \t  \u001b[92m0.8551080472474074\u001b[0m \t 0.8551080472474074\n",
      "53     \t [-0.34538761 -0.73810296]. \t  0.28918992023521006 \t 0.8551080472474074\n",
      "54     \t [2.10118159 2.        ]. \t  -57.61457076064411 \t 0.8551080472474074\n",
      "55     \t [-2.22535174  0.16882165]. \t  -8.304340637098875 \t 0.8551080472474074\n",
      "56     \t [2.11984985 0.04944483]. \t  -5.911787653803974 \t 0.8551080472474074\n",
      "57     \t [ 0.42190756 -2.        ]. \t  -47.803548069753965 \t 0.8551080472474074\n",
      "58     \t [-3.  2.]. \t  -150.89999999999998 \t 0.8551080472474074\n",
      "59     \t [0.0403276  0.74373729]. \t  \u001b[92m0.9522095240770714\u001b[0m \t 0.9522095240770714\n",
      "60     \t [-2.24670474  0.82419847]. \t  -6.8316214498362315 \t 0.9522095240770714\n",
      "61     \t [ 0.05761367 -0.07671538]. \t  0.014568094995513635 \t 0.9522095240770714\n",
      "62     \t [ 0.15190846 -0.77143515]. \t  \u001b[92m0.9898116633825058\u001b[0m \t 0.9898116633825058\n",
      "63     \t [-0.06909856  0.6900005 ]. \t  \u001b[92m1.0263427260641873\u001b[0m \t 1.0263427260641873\n",
      "64     \t [1.4448865  1.43026944]. \t  -12.854011748692418 \t 1.0263427260641873\n",
      "65     \t [ 0.12977229 -0.70618505]. \t  1.0248670705541123 \t 1.0263427260641873\n",
      "66     \t [-2.39415791 -1.35508285]. \t  -26.0937294980474 \t 1.0263427260641873\n",
      "67     \t [-0.07115337  0.72827174]. \t  \u001b[92m1.0279298529947083\u001b[0m \t 1.0279298529947083\n",
      "68     \t [-0.18570646  0.75973137]. \t  0.9817891524030812 \t 1.0279298529947083\n",
      "69     \t [ 0.13521378 -0.78050923]. \t  0.9854105523089883 \t 1.0279298529947083\n",
      "70     \t [-0.10090831  0.68788269]. \t  1.0260238212684538 \t 1.0279298529947083\n",
      "71     \t [-0.0080796  0.627892 ]. \t  0.9600783753499393 \t 1.0279298529947083\n",
      "72     \t [ 0.07726422 -0.71695271]. \t  \u001b[92m1.0308041479058596\u001b[0m \t 1.0308041479058596\n",
      "73     \t [ 0.08011127 -0.70715668]. \t  \u001b[92m1.0310663446730914\u001b[0m \t 1.0310663446730914\n",
      "74     \t [-0.05737943  0.67728259]. \t  1.0188963497578687 \t 1.0310663446730914\n",
      "75     \t [-0.038947   0.6775947]. \t  1.0136477082114401 \t 1.0310663446730914\n",
      "76     \t [-0.07275789  0.71351819]. \t  1.0304661977224532 \t 1.0310663446730914\n",
      "77     \t [-0.09233718  0.66540565]. \t  1.0143859852868706 \t 1.0310663446730914\n",
      "78     \t [ 0.08417634 -0.7027735 ]. \t  1.0307702915855181 \t 1.0310663446730914\n",
      "79     \t [0.01139192 0.56812451]. \t  0.8673605683912371 \t 1.0310663446730914\n",
      "80     \t [-0.08963745  0.72320194]. \t  1.0307020143344252 \t 1.0310663446730914\n",
      "81     \t [-0.11817055  0.64699661]. \t  0.9945065866602228 \t 1.0310663446730914\n",
      "82     \t [ 0.10699137 -0.73527896]. \t  1.0265497086313446 \t 1.0310663446730914\n",
      "83     \t [-0.02977083  0.69411298]. \t  1.0157947558398073 \t 1.0310663446730914\n",
      "84     \t [-0.04694722 -0.72494311]. \t  0.9545502859143851 \t 1.0310663446730914\n",
      "85     \t [-3.          0.57883291]. \t  -106.27233854739899 \t 1.0310663446730914\n",
      "86     \t [-0.16738034  0.74026374]. \t  1.004269544002237 \t 1.0310663446730914\n",
      "87     \t [-0.0904781   0.69866612]. \t  1.030046211263251 \t 1.0310663446730914\n",
      "88     \t [ 0.02327437 -0.67607974]. \t  1.0062019641857898 \t 1.0310663446730914\n",
      "89     \t [ 0.09801468 -0.80329368]. \t  0.9560746187438809 \t 1.0310663446730914\n",
      "90     \t [ 0.19950374 -0.63976139]. \t  0.9388237282520706 \t 1.0310663446730914\n",
      "91     \t [ 0.06811639 -0.76754929]. \t  1.0019904981599754 \t 1.0310663446730914\n",
      "92     \t [-1.58879516  0.634448  ]. \t  -0.10750434865658087 \t 1.0310663446730914\n",
      "93     \t [ 0.03034614 -0.69582801]. \t  1.0164324061590713 \t 1.0310663446730914\n",
      "94     \t [ 0.08735952 -0.75641523]. \t  1.0148449194699498 \t 1.0310663446730914\n",
      "95     \t [-0.03050008  0.79978974]. \t  0.9426507225442184 \t 1.0310663446730914\n",
      "96     \t [ 0.14260352 -0.71250097]. \t  1.0208931725289152 \t 1.0310663446730914\n",
      "97     \t [-0.09471464  0.71957531]. \t  \u001b[92m1.0311738829857382\u001b[0m \t 1.0311738829857382\n",
      "98     \t [-0.03164633  0.69848895]. \t  1.0175138390276508 \t 1.0311738829857382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [ 0.13339977 -0.65600779]. \t  0.9975858441558766 \t 1.0311738829857382\n",
      "100    \t [-0.15128977  0.71869372]. \t  1.0171810691847512 \t 1.0311738829857382\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \n",
    "\n",
    "np.random.seed(run_num_8)\n",
    "surrogate_winner_8 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_8 = GPGO(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_8.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 1.03367135 -0.0476864 ]. \t  -2.224696248130898 \t -2.224696248130898\n",
      "init   \t [ 1.95297104 -1.87421445]. \t  -34.846635035334884 \t -2.224696248130898\n",
      "init   \t [1.84829978 0.26246968]. \t  -2.674972725698239 \t -2.224696248130898\n",
      "init   \t [-1.21426501 -1.81321712]. \t  -34.68894768245752 \t -2.224696248130898\n",
      "init   \t [ 2.9437644  -1.97269707]. \t  -133.08463964627106 \t -2.224696248130898\n",
      "1      \t [ 0.76602233 -2.        ]. \t  -48.159386811538894 \t -2.224696248130898\n",
      "2      \t [-0.92514378  2.        ]. \t  -48.24391685467946 \t -2.224696248130898\n",
      "3      \t [3. 2.]. \t  -162.89999999999998 \t -2.224696248130898\n",
      "4      \t [-3.          0.06108447]. \t  -108.70187702306416 \t -2.224696248130898\n",
      "5      \t [-3. -2.]. \t  -162.89999999999998 \t -2.224696248130898\n",
      "6      \t [-3.  2.]. \t  -150.89999999999998 \t -2.224696248130898\n",
      "7      \t [-0.72352039 -0.02490101]. \t  \u001b[92m-1.5818122163746298\u001b[0m \t -1.5818122163746298\n",
      "8      \t [0.67628529 1.5049881 ]. \t  -13.900608325515151 \t -1.5818122163746298\n",
      "9      \t [ 1.83226835 -0.69385358]. \t  \u001b[92m-0.10296614856341524\u001b[0m \t -0.10296614856341524\n",
      "10     \t [ 3.         -0.03791615]. \t  -108.7805092861148 \t -0.10296614856341524\n",
      "11     \t [-0.29195542  0.82191121]. \t  \u001b[92m0.7908062860833716\u001b[0m \t 0.7908062860833716\n",
      "12     \t [-1.31948627  0.61851071]. \t  -0.5968113162165585 \t 0.7908062860833716\n",
      "13     \t [-1.53976554 -0.50738201]. \t  -2.1381627980313587 \t 0.7908062860833716\n",
      "14     \t [1.16214219 0.72384461]. \t  -2.236467323512718 \t 0.7908062860833716\n",
      "15     \t [ 1.32560178 -0.98563301]. \t  -0.9357025998677309 \t 0.7908062860833716\n",
      "16     \t [ 1.52696464 -0.18650432]. \t  -1.7160961169238227 \t 0.7908062860833716\n",
      "17     \t [-0.28214838 -1.02780368]. \t  -0.8335202324504272 \t 0.7908062860833716\n",
      "18     \t [1.44669777 2.        ]. \t  -53.122292303436666 \t 0.7908062860833716\n",
      "19     \t [ 0.14177413 -0.36388728]. \t  0.4315578821099847 \t 0.7908062860833716\n",
      "20     \t [-0.96934162 -0.85811052]. \t  -2.236203081923639 \t 0.7908062860833716\n",
      "21     \t [0.31858995 0.54007351]. \t  0.26963586938363393 \t 0.7908062860833716\n",
      "22     \t [0.07993882 2.        ]. \t  -48.18535284665576 \t 0.7908062860833716\n",
      "23     \t [-0.42599497 -2.        ]. \t  -49.51071169508757 \t 0.7908062860833716\n",
      "24     \t [-1.33585633  0.03242141]. \t  -2.297372401031351 \t 0.7908062860833716\n",
      "25     \t [-0.88479424  0.6888944 ]. \t  -0.3974008327664702 \t 0.7908062860833716\n",
      "26     \t [ 1.73262047 -1.12302982]. \t  -1.4726857321939433 \t 0.7908062860833716\n",
      "27     \t [ 0.39430402 -0.88676792]. \t  0.4492612696082255 \t 0.7908062860833716\n",
      "28     \t [-1.6780297  1.4575366]. \t  -9.16385176948158 \t 0.7908062860833716\n",
      "29     \t [1.90620576 0.96521995]. \t  -4.384762872986542 \t 0.7908062860833716\n",
      "30     \t [-1.27301238  1.1326034 ]. \t  -2.3950606780538437 \t 0.7908062860833716\n",
      "31     \t [1.67516002 0.68411936]. \t  -2.203953329347984 \t 0.7908062860833716\n",
      "32     \t [0.33122232 0.9652662 ]. \t  -0.4793090457436788 \t 0.7908062860833716\n",
      "33     \t [-2.10628142  0.9067939 ]. \t  -3.024952334174872 \t 0.7908062860833716\n",
      "34     \t [-1.75075616  0.93938996]. \t  -0.0704107111943918 \t 0.7908062860833716\n",
      "35     \t [-1.79195863 -1.24980081]. \t  -7.978680412109334 \t 0.7908062860833716\n",
      "36     \t [3.         0.93521167]. \t  -111.26699818025381 \t 0.7908062860833716\n",
      "37     \t [-0.10872144 -0.72084027]. \t  \u001b[92m0.8731023504380421\u001b[0m \t 0.8731023504380421\n",
      "38     \t [-1.97372197  0.37398672]. \t  -2.2001620355522618 \t 0.8731023504380421\n",
      "39     \t [-1.45772352 -1.02573356]. \t  -3.9303955791142062 \t 0.8731023504380421\n",
      "40     \t [-1.97657327 -2.        ]. \t  -55.4045935135269 \t 0.8731023504380421\n",
      "41     \t [1.43954575 1.18381238]. \t  -6.191677790671801 \t 0.8731023504380421\n",
      "42     \t [ 3.         -1.06296468]. \t  -106.29817130588131 \t 0.8731023504380421\n",
      "43     \t [-3.         -0.97665651]. \t  -111.6539176133299 \t 0.8731023504380421\n",
      "44     \t [-0.4481961   1.15883333]. \t  -2.0439816078446844 \t 0.8731023504380421\n",
      "45     \t [ 1.25843605 -1.42850968]. \t  -9.08840981408159 \t 0.8731023504380421\n",
      "46     \t [-1.89635268  2.        ]. \t  -46.93623921511396 \t 0.8731023504380421\n",
      "47     \t [-2.01752522 -0.70964125]. \t  -4.399944873215132 \t 0.8731023504380421\n",
      "48     \t [ 0.49667326 -0.60527521]. \t  0.3652338497907468 \t 0.8731023504380421\n",
      "49     \t [-3.          1.06267395]. \t  -106.29593088597292 \t 0.8731023504380421\n",
      "50     \t [2.19365708 2.        ]. \t  -60.15117596857441 \t 0.8731023504380421\n",
      "51     \t [-0.17527108  0.34230718]. \t  0.35286653142251223 \t 0.8731023504380421\n",
      "52     \t [-1.80444447  0.66722985]. \t  -0.07502803223911758 \t 0.8731023504380421\n",
      "53     \t [ 2.27466255 -1.2509188 ]. \t  -11.338793826082256 \t 0.8731023504380421\n",
      "54     \t [ 0.76749036 -0.9280778 ]. \t  -0.5055978031013474 \t 0.8731023504380421\n",
      "55     \t [2.24249564 0.56452974]. \t  -9.796814516269336 \t 0.8731023504380421\n",
      "56     \t [-2.31495003  1.5171015 ]. \t  -20.89877204412903 \t 0.8731023504380421\n",
      "57     \t [ 0.27688523 -1.48799969]. \t  -10.635609944627694 \t 0.8731023504380421\n",
      "58     \t [ 2.24464629 -0.36824265]. \t  -8.183032989790203 \t 0.8731023504380421\n",
      "59     \t [-0.0383609   0.73554851]. \t  \u001b[92m1.0156002409715617\u001b[0m \t 1.0156002409715617\n",
      "60     \t [-0.01593685  0.64333572]. \t  0.9795709095305197 \t 1.0156002409715617\n",
      "61     \t [-2.47578614 -1.4118027 ]. \t  -33.796746492246974 \t 1.0156002409715617\n",
      "62     \t [ 0.00192194 -0.6861539 ]. \t  0.9978950861411218 \t 1.0156002409715617\n",
      "63     \t [ 1.17269897 -0.60344555]. \t  -0.7624143715707585 \t 1.0156002409715617\n",
      "64     \t [ 0.09228544 -0.76739043]. \t  1.0053003040597863 \t 1.0156002409715617\n",
      "65     \t [-0.14770911  0.68968289]. \t  1.013227452929242 \t 1.0156002409715617\n",
      "66     \t [-0.04279427  0.72775553]. \t  \u001b[92m1.0203141087123824\u001b[0m \t 1.0203141087123824\n",
      "67     \t [ 0.06467297 -0.74848532]. \t  1.0172023672966992 \t 1.0203141087123824\n",
      "68     \t [ 0.05089228 -0.71828301]. \t  \u001b[92m1.0251939258075264\u001b[0m \t 1.0251939258075264\n",
      "69     \t [ 0.06753901 -0.69761427]. \t  \u001b[92m1.0282025530519927\u001b[0m \t 1.0282025530519927\n",
      "70     \t [ 0.10255728 -0.6773897 ]. \t  1.0208601431513076 \t 1.0282025530519927\n",
      "71     \t [ 0.08424966 -0.76789529]. \t  1.0042506364002377 \t 1.0282025530519927\n",
      "72     \t [-0.07251252  0.71548943]. \t  \u001b[92m1.0303388553351212\u001b[0m \t 1.0303388553351212\n",
      "73     \t [-0.03509181  0.74876284]. \t  1.006641239778269 \t 1.0303388553351212\n",
      "74     \t [-0.10778482  0.70869599]. \t  1.0301790602735847 \t 1.0303388553351212\n",
      "75     \t [ 0.06224883 -0.7383571 ]. \t  1.022331956023745 \t 1.0303388553351212\n",
      "76     \t [ 0.06661747 -0.74995036]. \t  1.0166617856697249 \t 1.0303388553351212\n",
      "77     \t [ 0.14315419 -0.75447747]. \t  1.0077385472538316 \t 1.0303388553351212\n",
      "78     \t [ 0.08695447 -0.73569461]. \t  1.0270424130835423 \t 1.0303388553351212\n",
      "79     \t [-0.0318707   0.72272942]. \t  1.0169771834616002 \t 1.0303388553351212\n",
      "80     \t [ 0.0330298  -0.66552848]. \t  1.0045921165358331 \t 1.0303388553351212\n",
      "81     \t [2.52470487 1.44786187]. \t  -39.34872851132065 \t 1.0303388553351212\n",
      "82     \t [ 2.11559084 -0.87055568]. \t  -3.145778469567827 \t 1.0303388553351212\n",
      "83     \t [-0.16283287  0.67009196]. \t  0.9941304293850635 \t 1.0303388553351212\n",
      "84     \t [-0.05341373  0.66216713]. \t  1.0088277231369058 \t 1.0303388553351212\n",
      "85     \t [-0.10541176  0.7164148 ]. \t  \u001b[92m1.0306285470306398\u001b[0m \t 1.0306285470306398\n",
      "86     \t [ 0.04238948 -0.73160403]. \t  1.0188629321178577 \t 1.0306285470306398\n",
      "87     \t [-0.11607401  0.72955139]. \t  1.0270106246213615 \t 1.0306285470306398\n",
      "88     \t [-0.04044029  0.70552715]. \t  1.0219757527843771 \t 1.0306285470306398\n",
      "89     \t [-1.8601433  -0.17711388]. \t  -2.71499299529371 \t 1.0306285470306398\n",
      "90     \t [-0.0906985   0.71026283]. \t  \u001b[92m1.0315767832763452\u001b[0m \t 1.0315767832763452\n",
      "91     \t [-0.05383315  0.75773411]. \t  1.0072174071942486 \t 1.0315767832763452\n",
      "92     \t [-0.1896049   0.68192527]. \t  0.9833010436589807 \t 1.0315767832763452\n",
      "93     \t [-0.04066144 -0.68396866]. \t  0.9614372001248181 \t 1.0315767832763452\n",
      "94     \t [ 0.10148629 -0.7355633 ]. \t  1.0269326067512825 \t 1.0315767832763452\n",
      "95     \t [0.00805363 0.74944552]. \t  0.9784927514748618 \t 1.0315767832763452\n",
      "96     \t [ 0.20728251 -0.76577746]. \t  0.9608481581420207 \t 1.0315767832763452\n",
      "97     \t [ 0.04258749 -0.75153199]. \t  1.00796165743588 \t 1.0315767832763452\n",
      "98     \t [ 0.21041181 -0.72761345]. \t  0.9766307890225449 \t 1.0315767832763452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [-0.02767238 -0.79678692]. \t  0.9021301783540562 \t 1.0315767832763452\n",
      "100    \t [-0.07283438  0.74969383]. \t  1.018046881891072 \t 1.0315767832763452\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \n",
    "\n",
    "np.random.seed(run_num_9)\n",
    "surrogate_winner_9 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_9 = GPGO(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_9.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.92153751 -1.53997223]. \t  -13.677695110590259 \t -6.372423095293032\n",
      "init   \t [ 2.70169719 -0.07123439]. \t  -46.72852427361676 \t -6.372423095293032\n",
      "init   \t [ 2.23484721 -1.15066928]. \t  -8.26768569212749 \t -6.372423095293032\n",
      "init   \t [-2.75574225 -0.41122215]. \t  -55.82511776655432 \t -6.372423095293032\n",
      "init   \t [-1.60120682  1.3669629 ]. \t  -6.372423095293032 \t -6.372423095293032\n",
      "1      \t [ 3. -2.]. \t  -150.89999999999998 \t -6.372423095293032\n",
      "2      \t [0.45379356 2.        ]. \t  -49.64515858612178 \t -6.372423095293032\n",
      "3      \t [-3.  2.]. \t  -150.89999999999998 \t -6.372423095293032\n",
      "4      \t [-0.72026425  0.27159627]. \t  \u001b[92m-1.0875684123192142\u001b[0m \t -1.0875684123192142\n",
      "5      \t [ 1.39935109 -0.48633642]. \t  \u001b[92m-0.8803189277308552\u001b[0m \t -0.8803189277308552\n",
      "6      \t [-1.31516632 -2.        ]. \t  -52.9912404524689 \t -0.8803189277308552\n",
      "7      \t [3. 2.]. \t  -162.89999999999998 \t -0.8803189277308552\n",
      "8      \t [-3. -2.]. \t  -162.89999999999998 \t -0.8803189277308552\n",
      "9      \t [-0.88462623  2.        ]. \t  -48.2346985656513 \t -0.8803189277308552\n",
      "10     \t [-1.65590946  0.40622009]. \t  \u001b[92m-0.8271799414705111\u001b[0m \t -0.8271799414705111\n",
      "11     \t [-0.38304875 -0.90880732]. \t  \u001b[92m-0.3157891056563036\u001b[0m \t -0.3157891056563036\n",
      "12     \t [-0.14321549 -2.        ]. \t  -48.3675931026828 \t -0.3157891056563036\n",
      "13     \t [ 0.29918409 -0.04104832]. \t  -0.32244832358432474 \t -0.3157891056563036\n",
      "14     \t [1.13960814 0.88378414]. \t  -2.7062134218088385 \t -0.3157891056563036\n",
      "15     \t [-1.23768823 -0.63762414]. \t  -2.221908314594473 \t -0.3157891056563036\n",
      "16     \t [ 1.6549754  -1.13111306]. \t  -1.608980060257141 \t -0.3157891056563036\n",
      "17     \t [0.28655068 0.86100369]. \t  \u001b[92m0.2058523112825834\u001b[0m \t 0.2058523112825834\n",
      "18     \t [-1.16689315  0.93964074]. \t  -0.8846139977437472 \t 0.2058523112825834\n",
      "19     \t [0.808591   0.47558876]. \t  -1.4951910157086348 \t 0.2058523112825834\n",
      "20     \t [ 2.0244644  -0.68559807]. \t  -1.6827492670688993 \t 0.2058523112825834\n",
      "21     \t [ 0.57352956 -0.81943681]. \t  \u001b[92m0.25196395976313224\u001b[0m \t 0.25196395976313224\n",
      "22     \t [-3.          0.54331101]. \t  -106.43786065914574 \t 0.25196395976313224\n",
      "23     \t [1.49326662 2.        ]. \t  -53.160026431454675 \t 0.25196395976313224\n",
      "24     \t [-0.49436658 -0.43791126]. \t  -0.4535459783814767 \t 0.25196395976313224\n",
      "25     \t [ 1.5919393 -2.       ]. \t  -46.891370253192 \t 0.25196395976313224\n",
      "26     \t [ 3.         -0.76523149]. \t  -105.6336007565767 \t 0.25196395976313224\n",
      "27     \t [1.87663238 0.46871339]. \t  -2.794935880253514 \t 0.25196395976313224\n",
      "28     \t [-1.90231363 -1.01261446]. \t  -4.80155969781064 \t 0.25196395976313224\n",
      "29     \t [-1.84474774 -0.43402351]. \t  -2.618407314537243 \t 0.25196395976313224\n",
      "30     \t [-0.1395246   0.49281405]. \t  \u001b[92m0.7272123833453262\u001b[0m \t 0.7272123833453262\n",
      "31     \t [ 0.08762695 -0.81891973]. \t  \u001b[92m0.9247114375620126\u001b[0m \t 0.9247114375620126\n",
      "32     \t [3.         0.85701609]. \t  -110.69096534531647 \t 0.9247114375620126\n",
      "33     \t [1.72255038 0.07832606]. \t  -2.1983608410736677 \t 0.9247114375620126\n",
      "34     \t [-1.48084327  0.79490955]. \t  -0.08061368648611744 \t 0.9247114375620126\n",
      "35     \t [-1.73313171  2.        ]. \t  -46.63524743188587 \t 0.9247114375620126\n",
      "36     \t [-0.27245925  1.10453285]. \t  -1.0581185114164953 \t 0.9247114375620126\n",
      "37     \t [-1.38143806 -1.15016739]. \t  -5.599666331643941 \t 0.9247114375620126\n",
      "38     \t [ 0.76801797 -0.3233407 ]. \t  -1.0743659526199616 \t 0.9247114375620126\n",
      "39     \t [ 0.22840571 -1.21054726]. \t  -2.654691846734538 \t 0.9247114375620126\n",
      "40     \t [-1.39276029  0.11245437]. \t  -2.0837525108668125 \t 0.9247114375620126\n",
      "41     \t [ 1.82882168 -0.8988193 ]. \t  -0.09371234665482386 \t 0.9247114375620126\n",
      "42     \t [1.85293554 1.25345478]. \t  -8.381517807882835 \t 0.9247114375620126\n",
      "43     \t [-2.07553626 -2.        ]. \t  -57.059320591764234 \t 0.9247114375620126\n",
      "44     \t [-2.18052356  1.1515082 ]. \t  -6.59175515722735 \t 0.9247114375620126\n",
      "45     \t [-1.85326289  0.91939777]. \t  -0.24418979038135913 \t 0.9247114375620126\n",
      "46     \t [-0.54811422  0.7778254 ]. \t  0.3610124846496361 \t 0.9247114375620126\n",
      "47     \t [1.61989793 0.8553257 ]. \t  -2.6591712173763007 \t 0.9247114375620126\n",
      "48     \t [-3.         -1.10559499]. \t  -113.30388697027202 \t 0.9247114375620126\n",
      "49     \t [0.28953854 0.51544716]. \t  0.31037747624388523 \t 0.9247114375620126\n",
      "50     \t [2.19114125 2.        ]. \t  -60.06985351671052 \t 0.9247114375620126\n",
      "51     \t [ 2.27416983 -2.        ]. \t  -54.08044843577849 \t 0.9247114375620126\n",
      "52     \t [-0.15820145 -0.6969406 ]. \t  0.7901274195038603 \t 0.9247114375620126\n",
      "53     \t [0.67128845 1.35788446]. \t  -8.541842845096914 \t 0.9247114375620126\n",
      "54     \t [-2.33368878  2.        ]. \t  -56.674832032214 \t 0.9247114375620126\n",
      "55     \t [ 0.55213103 -2.        ]. \t  -47.929417413251045 \t 0.9247114375620126\n",
      "56     \t [-2.21252193  0.11771213]. \t  -8.0450311609722 \t 0.9247114375620126\n",
      "57     \t [ 1.02743658 -0.9608872 ]. \t  -1.00398294566608 \t 0.9247114375620126\n",
      "58     \t [-0.18140427  2.        ]. \t  -47.766559279303095 \t 0.9247114375620126\n",
      "59     \t [2.39436412 1.2424525 ]. \t  -23.051844580228668 \t 0.9247114375620126\n",
      "60     \t [-2.3886518 -1.4779026]. \t  -30.249376578146215 \t 0.9247114375620126\n",
      "61     \t [-2.28250589 -0.76173453]. \t  -11.740515258614835 \t 0.9247114375620126\n",
      "62     \t [-0.64403542 -1.50108816]. \t  -13.58413750166988 \t 0.9247114375620126\n",
      "63     \t [ 0.08592451 -0.66043845]. \t  \u001b[92m1.011037589915902\u001b[0m \t 1.011037589915902\n",
      "64     \t [ 0.09382532 -0.7233581 ]. \t  \u001b[92m1.0306573759631283\u001b[0m \t 1.0306573759631283\n",
      "65     \t [ 0.0857069  -0.67467037]. \t  1.0205191039180272 \t 1.0306573759631283\n",
      "66     \t [ 0.08295052 -0.71989234]. \t  \u001b[92m1.030960088084549\u001b[0m \t 1.030960088084549\n",
      "67     \t [-0.07469313  0.73406655]. \t  1.0265403427586806 \t 1.030960088084549\n",
      "68     \t [ 0.02974282 -0.64772351]. \t  0.9898366779763101 \t 1.030960088084549\n",
      "69     \t [-0.03516927  0.68469337]. \t  1.015243357459488 \t 1.030960088084549\n",
      "70     \t [-0.10383197  0.72795002]. \t  1.0291251135049915 \t 1.030960088084549\n",
      "71     \t [ 0.10140772 -0.6467153 ]. \t  0.9979311876007763 \t 1.030960088084549\n",
      "72     \t [ 0.00777054 -0.67980816]. \t  0.9993071767070072 \t 1.030960088084549\n",
      "73     \t [ 0.1219485  -0.68518732]. \t  1.0208096838683376 \t 1.030960088084549\n",
      "74     \t [-0.09162737  0.73139263]. \t  1.0286992376157782 \t 1.030960088084549\n",
      "75     \t [ 0.093846   -0.71487684]. \t  \u001b[92m1.031534390512444\u001b[0m \t 1.031534390512444\n",
      "76     \t [-3.         1.3452997]. \t  -110.72673292729792 \t 1.031534390512444\n",
      "77     \t [-0.07376069  0.64221667]. \t  0.9950044616455976 \t 1.031534390512444\n",
      "78     \t [ 0.05732768 -0.67566713]. \t  1.0180513167083585 \t 1.031534390512444\n",
      "79     \t [ 0.09438796 -0.74467525]. \t  1.022919489946487 \t 1.031534390512444\n",
      "80     \t [-0.12954066  0.70631475]. \t  1.0249581098885139 \t 1.031534390512444\n",
      "81     \t [-0.07466978  0.71370389]. \t  1.0307036030553347 \t 1.031534390512444\n",
      "82     \t [-0.14516135  0.69947822]. \t  1.0177187082707597 \t 1.031534390512444\n",
      "83     \t [-0.08660136  0.71882997]. \t  1.0312526795436454 \t 1.031534390512444\n",
      "84     \t [2.13128028 0.83128984]. \t  -6.998634705655495 \t 1.031534390512444\n",
      "85     \t [ 0.04172827 -0.63428967]. \t  0.9813462942871956 \t 1.031534390512444\n",
      "86     \t [0.03718396 0.65886134]. \t  0.9526021950493216 \t 1.031534390512444\n",
      "87     \t [ 3.         -1.46973588]. \t  -114.51483321491222 \t 1.031534390512444\n",
      "88     \t [-1.74288906 -1.56380158]. \t  -18.98139005336179 \t 1.031534390512444\n",
      "89     \t [-0.05417418  0.70453941]. \t  1.026394017143232 \t 1.031534390512444\n",
      "90     \t [ 0.15918679 -0.79316828]. \t  0.9595595536982205 \t 1.031534390512444\n",
      "91     \t [ 0.14605935 -0.70820585]. \t  1.0190495677580609 \t 1.031534390512444\n",
      "92     \t [-0.08528855  0.74941536]. \t  1.0197412292363608 \t 1.031534390512444\n",
      "93     \t [-0.0118176   0.67293661]. \t  0.998499038014879 \t 1.031534390512444\n",
      "94     \t [ 0.09643124 -0.68814999]. \t  1.0265462168985602 \t 1.031534390512444\n",
      "95     \t [-0.07070598  0.72509085]. \t  1.0286697216492098 \t 1.031534390512444\n",
      "96     \t [ 5.71171847e-04 -6.89196202e-01]. \t  0.9978906253216937 \t 1.031534390512444\n",
      "97     \t [-0.07077703  0.72250646]. \t  1.0292132207127727 \t 1.031534390512444\n",
      "98     \t [ 0.07147418 -0.68806703]. \t  1.025977018538004 \t 1.031534390512444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [ 0.06971814 -0.70234968]. \t  1.0293937880310824 \t 1.031534390512444\n",
      "100    \t [ 0.05548015 -0.72502622]. \t  1.0252978990250967 \t 1.031534390512444\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \n",
    "\n",
    "np.random.seed(run_num_10)\n",
    "surrogate_winner_10 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_10 = GPGO(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_10.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [2.88621398 1.2849914 ]. \t  -88.29186392936245 \t -1.3768038348803564\n",
      "init   \t [ 0.87536168 -0.30524795]. \t  -1.3768038348803564 \t -1.3768038348803564\n",
      "init   \t [-1.78613413 -0.03237292]. \t  -2.2646252293989613 \t -1.3768038348803564\n",
      "init   \t [-2.16865001 -0.18991586]. \t  -7.310805476408371 \t -1.3768038348803564\n",
      "init   \t [-2.31759223 -1.98830205]. \t  -63.86369356519016 \t -1.3768038348803564\n",
      "1      \t [-2.41975323  1.9068438 ]. \t  -52.06302626145653 \t -1.3768038348803564\n",
      "2      \t [ 2.46944753 -2.        ]. \t  -64.95193904542583 \t -1.3768038348803564\n",
      "3      \t [0.00710129 1.18524954]. \t  -2.2833898750277606 \t -1.3768038348803564\n",
      "4      \t [-0.25866964 -0.95743214]. \t  \u001b[92m-0.20047337973023238\u001b[0m \t -0.20047337973023238\n",
      "5      \t [ 0.30219213 -2.        ]. \t  -47.74363727335353 \t -0.20047337973023238\n",
      "6      \t [-0.31534536 -0.02035613]. \t  -0.38209440098869246 \t -0.20047337973023238\n",
      "7      \t [0.7114791 2.       ]. \t  -50.95289878921656 \t -0.20047337973023238\n",
      "8      \t [-0.85936796  1.41691177]. \t  -8.817223826943337 \t -0.20047337973023238\n",
      "9      \t [-1.11225582 -0.78163216]. \t  -2.2842361091787398 \t -0.20047337973023238\n",
      "10     \t [-3.          0.47614378]. \t  -106.77031203806169 \t -0.20047337973023238\n",
      "11     \t [ 3.         -0.50680299]. \t  -106.61608012113841 \t -0.20047337973023238\n",
      "12     \t [0.57090696 0.42341749]. \t  -0.7453616689201548 \t -0.20047337973023238\n",
      "13     \t [ 0.29021637 -0.44526545]. \t  \u001b[92m0.44283434858706805\u001b[0m \t 0.44283434858706805\n",
      "14     \t [-3.         -1.05850881]. \t  -112.61531411410708 \t 0.44283434858706805\n",
      "15     \t [-0.98560259 -2.        ]. \t  -52.180761723168075 \t 0.44283434858706805\n",
      "16     \t [-0.61545634  2.        ]. \t  -48.00104298308445 \t 0.44283434858706805\n",
      "17     \t [-0.39975149  0.79167765]. \t  \u001b[92m0.6652697791279314\u001b[0m \t 0.6652697791279314\n",
      "18     \t [1.48442708 0.68811048]. \t  -2.2081847601597593 \t 0.6652697791279314\n",
      "19     \t [ 1.29477846 -1.23746759]. \t  -4.02661488332102 \t 0.6652697791279314\n",
      "20     \t [1.97493658 2.        ]. \t  -55.38293319606125 \t 0.6652697791279314\n",
      "21     \t [0.9537438  0.94928032]. \t  -2.700806467363384 \t 0.6652697791279314\n",
      "22     \t [-1.75857969 -0.48230365]. \t  -2.279167882405741 \t 0.6652697791279314\n",
      "23     \t [-0.38430209 -0.58846451]. \t  0.13332395825971477 \t 0.6652697791279314\n",
      "24     \t [ 0.75491934 -1.02978695]. \t  -1.1383148303494695 \t 0.6652697791279314\n",
      "25     \t [-1.64392883  1.11766604]. \t  -1.4595822486609606 \t 0.6652697791279314\n",
      "26     \t [ 1.63009958 -0.40476926]. \t  -0.8474154489679517 \t 0.6652697791279314\n",
      "27     \t [0.04552106 0.64767422]. \t  \u001b[92m0.9363048645124022\u001b[0m \t 0.9363048645124022\n",
      "28     \t [1.41410219 0.15811256]. \t  -2.392881564383037 \t 0.9363048645124022\n",
      "29     \t [ 1.36894344 -0.77658278]. \t  -0.2942147772097323 \t 0.9363048645124022\n",
      "30     \t [-1.26162109  0.88704616]. \t  -0.6006461374365024 \t 0.9363048645124022\n",
      "31     \t [-1.68597488  2.        ]. \t  -46.68607513585108 \t 0.9363048645124022\n",
      "32     \t [ 1.40028193 -2.        ]. \t  -47.48161316990168 \t 0.9363048645124022\n",
      "33     \t [3. 2.]. \t  -162.89999999999998 \t 0.9363048645124022\n",
      "34     \t [2.21946781 0.38003046]. \t  -8.939900138959219 \t 0.9363048645124022\n",
      "35     \t [-1.61355875 -1.24615977]. \t  -7.507366555385742 \t 0.9363048645124022\n",
      "36     \t [-1.8336864   0.66511003]. \t  -0.1727134255592756 \t 0.9363048645124022\n",
      "37     \t [0.10953523 0.01217474]. \t  -0.048430901640493594 \t 0.9363048645124022\n",
      "38     \t [ 1.9770702  -1.09885716]. \t  -2.286696461691018 \t 0.9363048645124022\n",
      "39     \t [-3.  2.]. \t  -150.89999999999998 \t 0.9363048645124022\n",
      "40     \t [ 3.         -1.59463495]. \t  -119.80921318888453 \t 0.9363048645124022\n",
      "41     \t [-3. -2.]. \t  -162.89999999999998 \t 0.9363048645124022\n",
      "42     \t [ 0.2114125  -0.95428621]. \t  0.35255872827237345 \t 0.9363048645124022\n",
      "43     \t [3.         0.48571679]. \t  -109.63610166702412 \t 0.9363048645124022\n",
      "44     \t [1.97511328 0.9769986 ]. \t  -5.191096535594501 \t 0.9363048645124022\n",
      "45     \t [1.84000586 0.39153386]. \t  -2.608428282278885 \t 0.9363048645124022\n",
      "46     \t [ 1.82954905 -0.80079234]. \t  0.023880435946970313 \t 0.9363048645124022\n",
      "47     \t [1.50357383 1.44191134]. \t  -13.303835601334566 \t 0.9363048645124022\n",
      "48     \t [-1.61683845  0.74745971]. \t  0.13423940185061478 \t 0.9363048645124022\n",
      "49     \t [-1.68068302 -2.        ]. \t  -53.417106935804185 \t 0.9363048645124022\n",
      "50     \t [-2.13814896 -1.08313683]. \t  -9.374569819379571 \t 0.9363048645124022\n",
      "51     \t [-2.34259415  1.24608685]. \t  -14.311132641848666 \t 0.9363048645124022\n",
      "52     \t [-0.13719986  0.88800768]. \t  0.7142191502294667 \t 0.9363048645124022\n",
      "53     \t [ 0.46625306 -0.77971649]. \t  0.5431778042523788 \t 0.9363048645124022\n",
      "54     \t [-1.10362523  0.3140806 ]. \t  -1.6566170254740926 \t 0.9363048645124022\n",
      "55     \t [-0.03938892 -0.67342488]. \t  \u001b[92m0.9586250248306616\u001b[0m \t 0.9586250248306616\n",
      "56     \t [ 2.12293553 -0.27432693]. \t  -5.026042638531379 \t 0.9586250248306616\n",
      "57     \t [1.38271062 2.        ]. \t  -53.066328059110354 \t 0.9586250248306616\n",
      "58     \t [-3.         -0.29733868]. \t  -109.46964039159474 \t 0.9586250248306616\n",
      "59     \t [ 1.69840635 -1.02181083]. \t  -0.514027682576513 \t 0.9586250248306616\n",
      "60     \t [ 0.08312439 -0.71175745]. \t  \u001b[92m1.0314517284121414\u001b[0m \t 1.0314517284121414\n",
      "61     \t [-0.96954551 -1.32311598]. \t  -8.7205004076378 \t 1.0314517284121414\n",
      "62     \t [ 0.06457844 -0.68620854]. \t  1.0242778815501388 \t 1.0314517284121414\n",
      "63     \t [-1.91471861  0.95698802]. \t  -0.7236532970686975 \t 1.0314517284121414\n",
      "64     \t [ 0.06768677 -0.68029834]. \t  1.0222316248605867 \t 1.0314517284121414\n",
      "65     \t [-0.25581208 -1.63375342]. \t  -18.491644309343837 \t 1.0314517284121414\n",
      "66     \t [-0.03633275  0.70075523]. \t  1.0198639028837384 \t 1.0314517284121414\n",
      "67     \t [ 0.08603904 -0.74699737]. \t  1.021316661498546 \t 1.0314517284121414\n",
      "68     \t [-0.01397254  0.66131533]. \t  0.992753280984679 \t 1.0314517284121414\n",
      "69     \t [ 0.1348449  -0.70089154]. \t  1.0221650496831711 \t 1.0314517284121414\n",
      "70     \t [-0.10503922  0.70853482]. \t  1.0305298362377648 \t 1.0314517284121414\n",
      "71     \t [-0.13984709  0.71297986]. \t  1.0220018079023947 \t 1.0314517284121414\n",
      "72     \t [ 0.11171088 -0.68380377]. \t  1.0225951735430754 \t 1.0314517284121414\n",
      "73     \t [-0.12824035  0.76122997]. \t  1.0071419241415576 \t 1.0314517284121414\n",
      "74     \t [ 0.05158998 -0.64665951]. \t  0.9959443514241106 \t 1.0314517284121414\n",
      "75     \t [ 1.95298553 -2.        ]. \t  -47.29613522842968 \t 1.0314517284121414\n",
      "76     \t [-0.17601805  0.72894135]. \t  1.0024506434824374 \t 1.0314517284121414\n",
      "77     \t [0.01547192 2.        ]. \t  -48.03190124505989 \t 1.0314517284121414\n",
      "78     \t [ 0.07871155 -0.69548682]. \t  1.0289788319058193 \t 1.0314517284121414\n",
      "79     \t [-0.04455392  0.74657926]. \t  1.0121609706222172 \t 1.0314517284121414\n",
      "80     \t [0.04672565 0.72896228]. \t  0.9532752950976812 \t 1.0314517284121414\n",
      "81     \t [ 0.14273911 -0.72463087]. \t  1.0202862699315622 \t 1.0314517284121414\n",
      "82     \t [ 0.11755592 -0.74652031]. \t  1.019750717260814 \t 1.0314517284121414\n",
      "83     \t [-0.12326444  0.6701318 ]. \t  1.0119378110618265 \t 1.0314517284121414\n",
      "84     \t [0.01544787 0.70035126]. \t  0.9878650185223002 \t 1.0314517284121414\n",
      "85     \t [-0.01082548  0.66142879]. \t  0.9910605697198752 \t 1.0314517284121414\n",
      "86     \t [ 0.09698781 -0.75146528]. \t  1.0186975781210232 \t 1.0314517284121414\n",
      "87     \t [2.31544533 1.53918531]. \t  -28.98912526881315 \t 1.0314517284121414\n",
      "88     \t [-0.03274106  0.6454268 ]. \t  0.9890080482764565 \t 1.0314517284121414\n",
      "89     \t [-3.          1.27948627]. \t  -109.23339140022819 \t 1.0314517284121414\n",
      "90     \t [ 0.10766946 -0.68317188]. \t  1.0230383823155738 \t 1.0314517284121414\n",
      "91     \t [-0.17796631  0.64035102]. \t  0.9570044094303681 \t 1.0314517284121414\n",
      "92     \t [-0.15046428  0.70093471]. \t  1.0156780164608714 \t 1.0314517284121414\n",
      "93     \t [-0.09455132  0.70951885]. \t  1.0314470315780095 \t 1.0314517284121414\n",
      "94     \t [ 0.13896094 -0.772967  ]. \t  0.9929444737170331 \t 1.0314517284121414\n",
      "95     \t [-0.13329113  0.75491326]. \t  1.010677432116075 \t 1.0314517284121414\n",
      "96     \t [ 0.10438486 -0.68809474]. \t  1.025676336807496 \t 1.0314517284121414\n",
      "97     \t [-0.02086663  0.65418548]. \t  0.991149550703761 \t 1.0314517284121414\n",
      "98     \t [ 0.01667205 -0.78467639]. \t  0.9584087692266591 \t 1.0314517284121414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [-0.20269951  0.76156311]. \t  0.9679559422021096 \t 1.0314517284121414\n",
      "100    \t [-0.18111638  0.74223761]. \t  0.9950967232450331 \t 1.0314517284121414\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \n",
    "\n",
    "np.random.seed(run_num_11)\n",
    "surrogate_winner_11 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_11 = GPGO(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_11.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [-1.8508833   0.48843508]. \t  -0.8285352707628978 \t 0.04378866326980524\n",
      "init   \t [-0.37363357  1.14143433]. \t  -1.670326523786612 \t 0.04378866326980524\n",
      "init   \t [ 1.67985485 -0.90962958]. \t  0.04378866326980524 \t 0.04378866326980524\n",
      "init   \t [-1.34121447  1.20748871]. \t  -3.3921424222614553 \t 0.04378866326980524\n",
      "init   \t [2.74883612 1.50373054]. \t  -69.67081810821485 \t 0.04378866326980524\n",
      "1      \t [-0.4963354 -1.6324534]. \t  -19.420418027285123 \t 0.04378866326980524\n",
      "2      \t [ 3. -2.]. \t  -150.89999999999998 \t 0.04378866326980524\n",
      "3      \t [ 0.8496065  -0.19087199]. \t  -1.6159225896876896 \t 0.04378866326980524\n",
      "4      \t [-3. -2.]. \t  -162.89999999999998 \t 0.04378866326980524\n",
      "5      \t [-3.  2.]. \t  -150.89999999999998 \t 0.04378866326980524\n",
      "6      \t [ 0.81008806 -2.        ]. \t  -48.194624831593316 \t 0.04378866326980524\n",
      "7      \t [-0.91227066 -0.15067665]. \t  -2.1152969782404956 \t 0.04378866326980524\n",
      "8      \t [0.76760679 2.        ]. \t  -51.23120400520492 \t 0.04378866326980524\n",
      "9      \t [ 2.08605059 -0.05888034]. \t  -4.97121089705273 \t 0.04378866326980524\n",
      "10     \t [-0.71298154  2.        ]. \t  -48.10852917799106 \t 0.04378866326980524\n",
      "11     \t [-0.99834307  0.62460107]. \t  -0.655394969897493 \t 0.04378866326980524\n",
      "12     \t [-3.         -0.02695753]. \t  -108.97796785907073 \t 0.04378866326980524\n",
      "13     \t [0.61849097 0.77015009]. \t  -0.7525144006615724 \t 0.04378866326980524\n",
      "14     \t [-1.42414431 -0.92844287]. \t  -3.101750325031716 \t 0.04378866326980524\n",
      "15     \t [ 3.         -0.16888294]. \t  -108.28251926415987 \t 0.04378866326980524\n",
      "16     \t [ 1.52422987 -0.18043789]. \t  -1.7371467702100687 \t 0.04378866326980524\n",
      "17     \t [0.05566424 0.26036075]. \t  \u001b[92m0.22590351915871545\u001b[0m \t 0.22590351915871545\n",
      "18     \t [ 0.14346986 -0.82332515]. \t  \u001b[92m0.9101319226621625\u001b[0m \t 0.9101319226621625\n",
      "19     \t [-1.34667965 -2.        ]. \t  -53.02897033842665 \t 0.9101319226621625\n",
      "20     \t [-1.56157152 -0.18768508]. \t  -2.2572863531304326 \t 0.9101319226621625\n",
      "21     \t [-0.85723594 -0.87893621]. \t  -1.9882132772142045 \t 0.9101319226621625\n",
      "22     \t [1.7996568  0.84909411]. \t  -2.9747233982495698 \t 0.9101319226621625\n",
      "23     \t [ 0.97201642 -0.91335357]. \t  -0.7447835698661623 \t 0.9101319226621625\n",
      "24     \t [1.91878847 2.        ]. \t  -54.734201966403425 \t 0.9101319226621625\n",
      "25     \t [-1.54339355  0.73251805]. \t  0.007376236881332376 \t 0.9101319226621625\n",
      "26     \t [ 0.30485773 -0.46535613]. \t  0.4666244247719262 \t 0.9101319226621625\n",
      "27     \t [1.65673533 0.43044153]. \t  -2.1603166627649824 \t 0.9101319226621625\n",
      "28     \t [-1.77879526  2.        ]. \t  -46.633767498463094 \t 0.9101319226621625\n",
      "29     \t [-0.19195891  0.73895011]. \t  \u001b[92m0.9888084490089132\u001b[0m \t 0.9888084490089132\n",
      "30     \t [ 1.80568307 -0.58933701]. \t  -0.30024982051655025 \t 0.9888084490089132\n",
      "31     \t [ 1.81667769 -2.        ]. \t  -46.67697211042776 \t 0.9888084490089132\n",
      "32     \t [-3.          0.92015997]. \t  -105.62030803502384 \t 0.9888084490089132\n",
      "33     \t [ 1.48375903 -0.734369  ]. \t  -0.1012678773869774 \t 0.9888084490089132\n",
      "34     \t [1.27366415 1.18932035]. \t  -6.2454453498726465 \t 0.9888084490089132\n",
      "35     \t [ 0.40987299 -0.9608961 ]. \t  0.06274520396393307 \t 0.9888084490089132\n",
      "36     \t [-2.21583095 -0.81502248]. \t  -9.383095715771512 \t 0.9888084490089132\n",
      "37     \t [-1.84978822 -0.59339638]. \t  -2.6388793632040377 \t 0.9888084490089132\n",
      "38     \t [-2.06211581  1.17898278]. \t  -4.404286888580991 \t 0.9888084490089132\n",
      "39     \t [-1.84077933  0.88800832]. \t  -0.1091827424072428 \t 0.9888084490089132\n",
      "40     \t [3. 2.]. \t  -162.89999999999998 \t 0.9888084490089132\n",
      "41     \t [3.         0.81014573]. \t  -110.42820108512818 \t 0.9888084490089132\n",
      "42     \t [-1.99718824 -1.49228524]. \t  -17.606927319376315 \t 0.9888084490089132\n",
      "43     \t [0.34819105 1.26415693]. \t  -4.7180742644163525 \t 0.9888084490089132\n",
      "44     \t [ 3.         -1.13489328]. \t  -106.97898751181839 \t 0.9888084490089132\n",
      "45     \t [-0.46831363  0.29286169]. \t  -0.3289781296010192 \t 0.9888084490089132\n",
      "46     \t [-0.04462926 -2.        ]. \t  -48.097217266280005 \t 0.9888084490089132\n",
      "47     \t [ 1.33802271 -1.23615617]. \t  -3.916880039204138 \t 0.9888084490089132\n",
      "48     \t [0.11536938 0.76990404]. \t  0.8238956211954525 \t 0.9888084490089132\n",
      "49     \t [-0.23498769 -0.99434847]. \t  -0.40361306820629844 \t 0.9888084490089132\n",
      "50     \t [0.02087152 2.        ]. \t  -48.04348511632391 \t 0.9888084490089132\n",
      "51     \t [ 0.42164637 -0.72615942]. \t  0.6565601698532755 \t 0.9888084490089132\n",
      "52     \t [-3.         -1.14142523]. \t  -113.90255820152478 \t 0.9888084490089132\n",
      "53     \t [-1.38557638 -1.3503677 ]. \t  -10.175494910150277 \t 0.9888084490089132\n",
      "54     \t [1.91663909 1.3630017 ]. \t  -11.866066052263658 \t 0.9888084490089132\n",
      "55     \t [-1.90696804 -1.02786437]. \t  -5.004123976242962 \t 0.9888084490089132\n",
      "56     \t [2.20642904 0.76979158]. \t  -8.895590484266686 \t 0.9888084490089132\n",
      "57     \t [-1.75788571  1.21919238]. \t  -2.8926016061751745 \t 0.9888084490089132\n",
      "58     \t [-2.15873782 -2.        ]. \t  -59.087196223866485 \t 0.9888084490089132\n",
      "59     \t [-0.19612392 -0.42196753]. \t  0.3518815068362384 \t 0.9888084490089132\n",
      "60     \t [-0.77040083  0.90499356]. \t  -0.4138792247311871 \t 0.9888084490089132\n",
      "61     \t [ 2.27893961 -1.49152498]. \t  -18.324797129363407 \t 0.9888084490089132\n",
      "62     \t [ 2.23779724 -0.85289445]. \t  -6.527023233568501 \t 0.9888084490089132\n",
      "63     \t [-0.09044564  0.7084933 ]. \t  \u001b[92m1.0314834131699484\u001b[0m \t 1.0314834131699484\n",
      "64     \t [-0.0865348  0.7752376]. \t  0.9964509453370798 \t 1.0314834131699484\n",
      "65     \t [-2.37029412  2.        ]. \t  -58.559855084707884 \t 1.0314834131699484\n",
      "66     \t [-2.13397199 -0.03066845]. \t  -6.206720017821081 \t 1.0314834131699484\n",
      "67     \t [-0.08457948  0.70545605]. \t  1.0311379498151096 \t 1.0314834131699484\n",
      "68     \t [2.44609582 2.        ]. \t  -73.04736117003604 \t 1.0314834131699484\n",
      "69     \t [-0.13867496  0.79162517]. \t  0.9694482944236767 \t 1.0314834131699484\n",
      "70     \t [-0.11306796  0.66206414]. \t  1.0088500317207132 \t 1.0314834131699484\n",
      "71     \t [ 0.11759502 -0.72417143]. \t  1.0278591218358313 \t 1.0314834131699484\n",
      "72     \t [-0.12862303  0.64136678]. \t  0.9854579277732111 \t 1.0314834131699484\n",
      "73     \t [ 0.13194743 -0.704813  ]. \t  1.0239505876481712 \t 1.0314834131699484\n",
      "74     \t [ 0.04484817 -0.65688445]. \t  1.0026525843097844 \t 1.0314834131699484\n",
      "75     \t [0.01188445 0.74485114]. \t  0.9785693670059552 \t 1.0314834131699484\n",
      "76     \t [-0.06441297  0.68540437]. \t  1.023935734143922 \t 1.0314834131699484\n",
      "77     \t [1.22316566 0.65193557]. \t  -2.2200975133821172 \t 1.0314834131699484\n",
      "78     \t [-0.04128534  0.6903162 ]. \t  1.0194859680642627 \t 1.0314834131699484\n",
      "79     \t [ 2.39719813 -2.        ]. \t  -60.09975501908376 \t 1.0314834131699484\n",
      "80     \t [ 0.09446225 -0.70516045]. \t  1.03105532289926 \t 1.0314834131699484\n",
      "81     \t [ 0.10384912 -0.69131629]. \t  1.0269474369543101 \t 1.0314834131699484\n",
      "82     \t [ 0.12586264 -0.7102052 ]. \t  1.0264712153678441 \t 1.0314834131699484\n",
      "83     \t [-0.11103632  0.66997309]. \t  1.0149340882002496 \t 1.0314834131699484\n",
      "84     \t [-0.21630051  0.73482283]. \t  0.9699728372416072 \t 1.0314834131699484\n",
      "85     \t [-0.10951032  0.74157292]. \t  1.0235691166331853 \t 1.0314834131699484\n",
      "86     \t [ 0.06160626 -0.64407192]. \t  0.9955111272987446 \t 1.0314834131699484\n",
      "87     \t [-0.09232509  0.68691372]. \t  1.0263064686741878 \t 1.0314834131699484\n",
      "88     \t [-0.06318254  0.73615156]. \t  1.0235483864064832 \t 1.0314834131699484\n",
      "89     \t [-0.06113801  0.64181728]. \t  0.9932916237335067 \t 1.0314834131699484\n",
      "90     \t [-0.10843656  0.67973082]. \t  1.0211978572635392 \t 1.0314834131699484\n",
      "91     \t [-0.0569265   0.65406956]. \t  1.0034461004914834 \t 1.0314834131699484\n",
      "92     \t [-2.37158514  0.5594793 ]. \t  -13.186842150922894 \t 1.0314834131699484\n",
      "93     \t [ 0.11773236 -0.63696164]. \t  0.9843951563871932 \t 1.0314834131699484\n",
      "94     \t [0.0519855  0.67265601]. \t  0.9451991209937454 \t 1.0314834131699484\n",
      "95     \t [-0.15715929  0.64580562]. \t  0.9764613336746035 \t 1.0314834131699484\n",
      "96     \t [-0.0655674   0.74082954]. \t  1.0218798630638017 \t 1.0314834131699484\n",
      "97     \t [-0.08794656  0.69562159]. \t  1.029326407411714 \t 1.0314834131699484\n",
      "98     \t [-0.14551484  0.70720115]. \t  1.0191483118805045 \t 1.0314834131699484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [0.06859599 0.69280602]. \t  0.9320979371328928 \t 1.0314834131699484\n",
      "100    \t [ 0.11069461 -0.71794019]. \t  1.0298202693130918 \t 1.0314834131699484\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\n",
    "\n",
    "np.random.seed(run_num_12)\n",
    "surrogate_winner_12 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_12 = GPGO(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_12.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.96386586 -0.34559739]. \t  -1.4171064314549415 \t -1.4171064314549415\n",
      "init   \t [ 1.00861535 -1.08022631]. \t  -1.9362798876488938 \t -1.4171064314549415\n",
      "init   \t [1.84607002 0.54727386]. \t  -2.606716488173805 \t -1.4171064314549415\n",
      "init   \t [-1.96683689 -0.95845653]. \t  -4.9306493500188235 \t -1.4171064314549415\n",
      "init   \t [ 2.49191994 -0.14873796]. \t  -23.22037861975103 \t -1.4171064314549415\n",
      "1      \t [1.19147958 1.42688924]. \t  -12.537421623154366 \t -1.4171064314549415\n",
      "2      \t [-3.  2.]. \t  -150.89999999999998 \t -1.4171064314549415\n",
      "3      \t [3. 2.]. \t  -162.89999999999998 \t -1.4171064314549415\n",
      "4      \t [-0.46503271 -2.        ]. \t  -49.70024883544387 \t -1.4171064314549415\n",
      "5      \t [-3. -2.]. \t  -162.89999999999998 \t -1.4171064314549415\n",
      "6      \t [-1.27861674  0.02262279]. \t  -2.35219010686557 \t -1.4171064314549415\n",
      "7      \t [ 3. -2.]. \t  -150.89999999999998 \t -1.4171064314549415\n",
      "8      \t [-0.37593842  2.        ]. \t  -47.77243734545759 \t -1.4171064314549415\n",
      "9      \t [-2.66720311 -0.05017511]. \t  -42.31100853154064 \t -1.4171064314549415\n",
      "10     \t [-0.98255145 -0.7701759 ]. \t  -1.9957898091866133 \t -1.4171064314549415\n",
      "11     \t [0.10772045 0.61828579]. \t  \u001b[92m0.831830910576891\u001b[0m \t 0.831830910576891\n",
      "12     \t [ 1.01867305 -2.        ]. \t  -48.22459792802556 \t 0.831830910576891\n",
      "13     \t [ 0.16774778 -0.67060994]. \t  \u001b[92m0.9914782164217719\u001b[0m \t 0.9914782164217719\n",
      "14     \t [-1.65905911 -0.52170349]. \t  -2.124231329824284 \t 0.9914782164217719\n",
      "15     \t [-0.45798949  0.12479137]. \t  -0.6312255768224874 \t 0.9914782164217719\n",
      "16     \t [-1.07889416  1.01660749]. \t  -1.3780617264095134 \t 0.9914782164217719\n",
      "17     \t [1.07038945 0.69740407]. \t  -2.0748265994421935 \t 0.9914782164217719\n",
      "18     \t [-1.51461071 -2.        ]. \t  -53.178098232911026 \t 0.9914782164217719\n",
      "19     \t [-0.65430496  0.72874348]. \t  0.11923769617852542 \t 0.9914782164217719\n",
      "20     \t [3.         0.44843064]. \t  -109.60268058219826 \t 0.9914782164217719\n",
      "21     \t [ 1.77218574 -0.2982286 ]. \t  -1.3223503008944903 \t 0.9914782164217719\n",
      "22     \t [-1.53105113  2.        ]. \t  -47.06865880998877 \t 0.9914782164217719\n",
      "23     \t [-1.75784845  0.7477917 ]. \t  0.15693873178552742 \t 0.9914782164217719\n",
      "24     \t [-1.35514397  0.66651375]. \t  -0.43717978051917017 \t 0.9914782164217719\n",
      "25     \t [ 0.5446653  -1.04080017]. \t  -0.8044329833925123 \t 0.9914782164217719\n",
      "26     \t [0.2724731  1.22612784]. \t  -3.646776638275109 \t 0.9914782164217719\n",
      "27     \t [-3.          0.77811904]. \t  -105.61013791878183 \t 0.9914782164217719\n",
      "28     \t [0.94908737 2.        ]. \t  -52.04096385788266 \t 0.9914782164217719\n",
      "29     \t [-0.17610937  0.94456435]. \t  0.429009570024822 \t 0.9914782164217719\n",
      "30     \t [ 0.72131387 -0.73297456]. \t  -0.03648802250940031 \t 0.9914782164217719\n",
      "31     \t [ 2.13142587 -1.00851368]. \t  -4.004378985581272 \t 0.9914782164217719\n",
      "32     \t [ 3.         -0.83009572]. \t  -105.55268591682334 \t 0.9914782164217719\n",
      "33     \t [ 1.73187238 -1.01900414]. \t  -0.4943346036704505 \t 0.9914782164217719\n",
      "34     \t [ 0.42944565 -0.05032576]. \t  -0.6366425887809808 \t 0.9914782164217719\n",
      "35     \t [-3.         -0.85832912]. \t  -110.69914960740145 \t 0.9914782164217719\n",
      "36     \t [-1.93273607  0.24851758]. \t  -2.3015136136670886 \t 0.9914782164217719\n",
      "37     \t [ 1.94537475 -2.        ]. \t  -47.23784288392176 \t 0.9914782164217719\n",
      "38     \t [1.94959269 2.        ]. \t  -55.06811442630793 \t 0.9914782164217719\n",
      "39     \t [-0.52801776 -0.54295256]. \t  -0.4143207236555242 \t 0.9914782164217719\n",
      "40     \t [-0.40197101 -1.11342339]. \t  -2.229158723569087 \t 0.9914782164217719\n",
      "41     \t [-1.58705583 -1.10375745]. \t  -4.894221818195691 \t 0.9914782164217719\n",
      "42     \t [1.76538326 0.16692752]. \t  -2.345728372988205 \t 0.9914782164217719\n",
      "43     \t [1.72881298 1.11814359]. \t  -5.280162201508585 \t 0.9914782164217719\n",
      "44     \t [ 1.99807813 -0.67624983]. \t  -1.3649456608957031 \t 0.9914782164217719\n",
      "45     \t [0.43925835 0.84512182]. \t  -0.2508116626140223 \t 0.9914782164217719\n",
      "46     \t [-1.7079149   1.25371973]. \t  -3.526689760807957 \t 0.9914782164217719\n",
      "47     \t [-2.19336641 -2.        ]. \t  -60.14174232984928 \t 0.9914782164217719\n",
      "48     \t [-0.07719519 -0.83158289]. \t  0.7653088082632873 \t 0.9914782164217719\n",
      "49     \t [-0.18649458  0.70432826]. \t  \u001b[92m0.9946972405981427\u001b[0m \t 0.9946972405981427\n",
      "50     \t [1.47508362 0.8774174 ]. \t  -2.7806026936508097 \t 0.9946972405981427\n",
      "51     \t [-2.22712973  2.        ]. \t  -52.39785587046929 \t 0.9946972405981427\n",
      "52     \t [ 1.61227976 -1.45536087]. \t  -9.188976717853393 \t 0.9946972405981427\n",
      "53     \t [-2.35220538  1.29397789]. \t  -15.776481063653645 \t 0.9946972405981427\n",
      "54     \t [-2.09809654 -0.41772923]. \t  -5.648723441402429 \t 0.9946972405981427\n",
      "55     \t [-0.03294564 -0.10669208]. \t  0.03716026262240283 \t 0.9946972405981427\n",
      "56     \t [ 0.26293929 -2.        ]. \t  -47.74074199547655 \t 0.9946972405981427\n",
      "57     \t [2.63674986 1.25221372]. \t  -45.18679759554726 \t 0.9946972405981427\n",
      "58     \t [-2.13037065  0.79428064]. \t  -3.4358437154348325 \t 0.9946972405981427\n",
      "59     \t [-0.76891218  1.48873729]. \t  -11.338306326115855 \t 0.9946972405981427\n",
      "60     \t [-2.24714183 -1.49186581]. \t  -23.83511979369686 \t 0.9946972405981427\n",
      "61     \t [-0.06578586  0.56366539]. \t  0.886903469397021 \t 0.9946972405981427\n",
      "62     \t [ 0.02133008 -0.68431779]. \t  \u001b[92m1.0087552138783002\u001b[0m \t 1.0087552138783002\n",
      "63     \t [-0.07132216  0.72356582]. \t  \u001b[92m1.0290952313928472\u001b[0m \t 1.0290952313928472\n",
      "64     \t [-0.11105098  0.71018308]. \t  \u001b[92m1.0297799634409517\u001b[0m \t 1.0297799634409517\n",
      "65     \t [ 0.085432   -0.72810695]. \t  1.0294872512829503 \t 1.0297799634409517\n",
      "66     \t [-0.11022014  0.72352491]. \t  1.0292556307425258 \t 1.0297799634409517\n",
      "67     \t [ 0.18543354 -0.74719283]. \t  0.9898874230783501 \t 1.0297799634409517\n",
      "68     \t [-0.05817655  0.70963798]. \t  1.0277188484819024 \t 1.0297799634409517\n",
      "69     \t [2.18107954 0.30249078]. \t  -7.717087088918668 \t 1.0297799634409517\n",
      "70     \t [2.4685756 2.       ]. \t  -74.76099809078848 \t 1.0297799634409517\n",
      "71     \t [ 0.03119986 -0.73392246]. \t  1.0130336676696419 \t 1.0297799634409517\n",
      "72     \t [ 0.10641497 -0.71398764]. \t  \u001b[92m1.030568737177604\u001b[0m \t 1.030568737177604\n",
      "73     \t [ 0.0021417  -1.43715522]. \t  -8.799038430923504 \t 1.030568737177604\n",
      "74     \t [-0.11627907  0.68244688]. \t  1.020957374268722 \t 1.030568737177604\n",
      "75     \t [ 0.12096372 -0.66880551]. \t  1.011712065433126 \t 1.030568737177604\n",
      "76     \t [ 0.10984698 -0.68123863]. \t  1.0217124954818504 \t 1.030568737177604\n",
      "77     \t [0.01546701 0.69972656]. \t  0.9877893273473349 \t 1.030568737177604\n",
      "78     \t [-0.12337853  0.69370472]. \t  1.023774834583822 \t 1.030568737177604\n",
      "79     \t [-0.21677903  0.71135547]. \t  0.9706920341256687 \t 1.030568737177604\n",
      "80     \t [ 0.2970512  -0.77763658]. \t  0.8502982286372175 \t 1.030568737177604\n",
      "81     \t [ 0.14108165 -0.70229806]. \t  1.020110837002599 \t 1.030568737177604\n",
      "82     \t [ 0.04847264 -0.72810003]. \t  1.022274948873657 \t 1.030568737177604\n",
      "83     \t [-0.09277166  0.68289379]. \t  1.02455128573471 \t 1.030568737177604\n",
      "84     \t [ 0.1659095  -0.69209509]. \t  1.0045407361778937 \t 1.030568737177604\n",
      "85     \t [ 0.13348618 -0.71277534]. \t  1.0242771569527562 \t 1.030568737177604\n",
      "86     \t [-0.16306019  0.73387324]. \t  1.0088387859084897 \t 1.030568737177604\n",
      "87     \t [-0.04162307 -0.8076146 ]. \t  0.8667512755071656 \t 1.030568737177604\n",
      "88     \t [ 0.01057243 -0.61570061]. \t  0.9475827928134873 \t 1.030568737177604\n",
      "89     \t [ 0.07051163 -0.77132614]. \t  0.998494307759796 \t 1.030568737177604\n",
      "90     \t [ 0.18525374 -0.70202758]. \t  0.9950324176944494 \t 1.030568737177604\n",
      "91     \t [ 0.19432355 -0.65468405]. \t  0.9587650478843628 \t 1.030568737177604\n",
      "92     \t [ 0.15922002 -0.66799884]. \t  0.9947308165418531 \t 1.030568737177604\n",
      "93     \t [ 0.17184825 -0.76048645]. \t  0.9898354699733483 \t 1.030568737177604\n",
      "94     \t [-0.03748651  0.63418233]. \t  0.9798871210839935 \t 1.030568737177604\n",
      "95     \t [-0.03114076  0.70534177]. \t  1.0180630077564015 \t 1.030568737177604\n",
      "96     \t [ 0.17726538 -0.79107269]. \t  0.9533024221330384 \t 1.030568737177604\n",
      "97     \t [-0.14164689  0.73490618]. \t  1.0182565863827424 \t 1.030568737177604\n",
      "98     \t [-0.02596802 -0.70846336]. \t  0.9788914656423491 \t 1.030568737177604\n",
      "99     \t [-0.00105995  0.74974025]. \t  0.9853593605269463 \t 1.030568737177604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [ 0.09657195 -0.77545075]. \t  0.9966985036488061 \t 1.030568737177604\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \n",
    "\n",
    "np.random.seed(run_num_13)\n",
    "surrogate_winner_13 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_13 = GPGO(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_13.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.8853063  0.02859875]. \t  -2.0275945291834825 \t -2.0275945291834825\n",
      "init   \t [0.17004828 1.58514082]. \t  -15.586853032333524 \t -2.0275945291834825\n",
      "init   \t [1.19994714 0.85718841]. \t  -2.6498170273522446 \t -2.0275945291834825\n",
      "init   \t [ 1.30403029 -1.10872215]. \t  -2.0500441913631415 \t -2.0275945291834825\n",
      "init   \t [-1.94907286 -0.17263405]. \t  -3.38472265204694 \t -2.0275945291834825\n",
      "1      \t [2.67891777 0.76172675]. \t  -44.822107585935996 \t -2.0275945291834825\n",
      "2      \t [-3.  2.]. \t  -150.89999999999998 \t -2.0275945291834825\n",
      "3      \t [-3. -2.]. \t  -162.89999999999998 \t -2.0275945291834825\n",
      "4      \t [ 3. -2.]. \t  -150.89999999999998 \t -2.0275945291834825\n",
      "5      \t [ 0.00637183 -2.        ]. \t  -47.98741874161033 \t -2.0275945291834825\n",
      "6      \t [1.73952498 2.        ]. \t  -53.59004057915284 \t -2.0275945291834825\n",
      "7      \t [-0.89033531  0.35174239]. \t  \u001b[92m-1.2704190483055382\u001b[0m \t -1.2704190483055382\n",
      "8      \t [-0.98119602 -0.72669976]. \t  -1.9181775952345061 \t -1.2704190483055382\n",
      "9      \t [-0.99931921  2.        ]. \t  -48.23360398014646 \t -1.2704190483055382\n",
      "10     \t [ 1.83412984 -0.2678513 ]. \t  -1.6232985846673993 \t -1.2704190483055382\n",
      "11     \t [-3.          0.07901522]. \t  -108.63813664035709 \t -1.2704190483055382\n",
      "12     \t [-1.38698724 -0.18157802]. \t  -2.4207560223606066 \t -1.2704190483055382\n",
      "13     \t [0.05445374 0.67881216]. \t  \u001b[92m0.945042791734045\u001b[0m \t 0.945042791734045\n",
      "14     \t [-1.31053672 -1.8142776 ]. \t  -34.91404087462806 \t 0.945042791734045\n",
      "15     \t [ 0.1258146 -0.7501771]. \t  \u001b[92m1.0158327247848264\u001b[0m \t 1.0158327247848264\n",
      "16     \t [3. 2.]. \t  -162.89999999999998 \t 1.0158327247848264\n",
      "17     \t [ 3.         -0.19973521]. \t  -108.14758394163024 \t 1.0158327247848264\n",
      "18     \t [1.76457914 0.44746673]. \t  -2.3066764313358292 \t 1.0158327247848264\n",
      "19     \t [-1.65173721  0.81536098]. \t  0.18703400532838765 \t 1.0158327247848264\n",
      "20     \t [ 1.27023141 -0.56943691]. \t  -0.7873089389950242 \t 1.0158327247848264\n",
      "21     \t [ 1.38347162 -2.        ]. \t  -47.533174179418225 \t 1.0158327247848264\n",
      "22     \t [-0.10098857 -0.11943373]. \t  0.0036056371757980654 \t 1.0158327247848264\n",
      "23     \t [0.83193242 2.        ]. \t  -51.53688381068541 \t 1.0158327247848264\n",
      "24     \t [-0.86000299  1.06010381]. \t  -1.5894595407311298 \t 1.0158327247848264\n",
      "25     \t [-1.59977033  0.40697175]. \t  -0.8661539420743285 \t 1.0158327247848264\n",
      "26     \t [-1.8142056  -0.89590118]. \t  -3.292882192835835 \t 1.0158327247848264\n",
      "27     \t [ 1.79729006 -1.00142125]. \t  -0.4554497744597021 \t 1.0158327247848264\n",
      "28     \t [-0.31074543 -0.80564332]. \t  0.29380325708404365 \t 1.0158327247848264\n",
      "29     \t [-1.3015054   0.80520557]. \t  -0.410231332365129 \t 1.0158327247848264\n",
      "30     \t [-0.39330065  0.83433632]. \t  0.6045699137902858 \t 1.0158327247848264\n",
      "31     \t [0.73493239 0.62934583]. \t  -1.1061094638987758 \t 1.0158327247848264\n",
      "32     \t [1.89648732 1.07738862]. \t  -5.519580701846124 \t 1.0158327247848264\n",
      "33     \t [ 1.64835988 -0.74589355]. \t  0.16547407837522277 \t 1.0158327247848264\n",
      "34     \t [-3.         -0.92945787]. \t  -111.2180429685195 \t 1.0158327247848264\n",
      "35     \t [-0.64972048 -1.38028828]. \t  -9.134504659995049 \t 1.0158327247848264\n",
      "36     \t [-1.92557289  2.        ]. \t  -47.101122398215196 \t 1.0158327247848264\n",
      "37     \t [0.42804973 1.06706   ]. \t  -1.7525372201527136 \t 1.0158327247848264\n",
      "38     \t [ 0.40638828 -1.18584825]. \t  -2.407970200516305 \t 1.0158327247848264\n",
      "39     \t [ 0.66078339 -0.78697817]. \t  0.08913690418613518 \t 1.0158327247848264\n",
      "40     \t [ 2.08639402 -2.        ]. \t  -48.94175136190079 \t 1.0158327247848264\n",
      "41     \t [-1.62425172 -0.64657416]. \t  -2.134360693130424 \t 1.0158327247848264\n",
      "42     \t [-1.51043225  1.24290603]. \t  -3.642845159390956 \t 1.0158327247848264\n",
      "43     \t [-0.57164972 -0.24223631]. \t  -1.0120454338916878 \t 1.0158327247848264\n",
      "44     \t [-3.          1.08687913]. \t  -106.49607536385736 \t 1.0158327247848264\n",
      "45     \t [-2.0966838 -2.       ]. \t  -57.51295433024014 \t 1.0158327247848264\n",
      "46     \t [2.19261886 0.29194856]. \t  -8.060543624967305 \t 1.0158327247848264\n",
      "47     \t [ 0.43003613 -0.31539413]. \t  -0.17606870870764185 \t 1.0158327247848264\n",
      "48     \t [-0.21675493  0.4626736 ]. \t  0.589925852226553 \t 1.0158327247848264\n",
      "49     \t [-1.51580915 -1.23822296]. \t  -7.2944134739482935 \t 1.0158327247848264\n",
      "50     \t [ 3.         -1.14529256]. \t  -107.09951770589092 \t 1.0158327247848264\n",
      "51     \t [ 0.1118282  -0.91243526]. \t  0.6100152171866555 \t 1.0158327247848264\n",
      "52     \t [1.37035615 1.36484842]. \t  -10.61276374704628 \t 1.0158327247848264\n",
      "53     \t [-2.19702788  1.40281113]. \t  -12.40391429589004 \t 1.0158327247848264\n",
      "54     \t [-2.17064876  0.62237776]. \t  -4.793213932870093 \t 1.0158327247848264\n",
      "55     \t [-0.16635451  2.        ]. \t  -47.776385073962295 \t 1.0158327247848264\n",
      "56     \t [ 2.2495337  -0.72394923]. \t  -7.034310782391891 \t 1.0158327247848264\n",
      "57     \t [2.34766041 1.60620812]. \t  -34.13695639794814 \t 1.0158327247848264\n",
      "58     \t [-2.38084783 -1.4852354 ]. \t  -30.08628129147898 \t 1.0158327247848264\n",
      "59     \t [3.         1.37588492]. \t  -119.79011178142466 \t 1.0158327247848264\n",
      "60     \t [ 0.0343443  -0.67264628]. \t  1.0093435037027159 \t 1.0158327247848264\n",
      "61     \t [ 2.40372152 -1.44642939]. \t  -22.96440391453139 \t 1.0158327247848264\n",
      "62     \t [-0.64660404 -2.        ]. \t  -50.622866656243914 \t 1.0158327247848264\n",
      "63     \t [ 0.10973649 -0.70285802]. \t  \u001b[92m1.0291211784171435\u001b[0m \t 1.0291211784171435\n",
      "64     \t [ 0.03644064 -0.6701516 ]. \t  1.0087508199506698 \t 1.0291211784171435\n",
      "65     \t [-0.60286432  0.71497138]. \t  0.2381404020174812 \t 1.0291211784171435\n",
      "66     \t [-0.07806535  0.73821471]. \t  1.0252441597952286 \t 1.0291211784171435\n",
      "67     \t [ 0.05905112 -0.7093035 ]. \t  1.0279238235465677 \t 1.0291211784171435\n",
      "68     \t [-0.17386691  0.73830112]. \t  1.0012252615748864 \t 1.0291211784171435\n",
      "69     \t [ 0.04879202 -0.67177442]. \t  1.0137722707444763 \t 1.0291211784171435\n",
      "70     \t [-0.07387444  0.66986448]. \t  1.0171994226210388 \t 1.0291211784171435\n",
      "71     \t [ 0.03616429 -0.74573787]. \t  1.0091412009749618 \t 1.0291211784171435\n",
      "72     \t [-0.04734652  0.68603764]. \t  1.0200790276973182 \t 1.0291211784171435\n",
      "73     \t [-0.0377271  -0.66764939]. \t  0.9573526751800742 \t 1.0291211784171435\n",
      "74     \t [ 0.08284298 -0.73147857]. \t  1.028327765098536 \t 1.0291211784171435\n",
      "75     \t [-0.10043073  0.74070384]. \t  1.024793156792192 \t 1.0291211784171435\n",
      "76     \t [ 0.03540383 -0.6746551 ]. \t  1.010832266917646 \t 1.0291211784171435\n",
      "77     \t [-0.04623304  0.75066406]. \t  1.0100378586054501 \t 1.0291211784171435\n",
      "78     \t [0.03889425 0.63191834]. \t  0.9288305572226963 \t 1.0291211784171435\n",
      "79     \t [ 0.18509488 -0.75994715]. \t  0.9820361497389667 \t 1.0291211784171435\n",
      "80     \t [ 0.14347648 -0.70125623]. \t  1.0188871972565954 \t 1.0291211784171435\n",
      "81     \t [ 0.71483565 -1.99999998]. \t  -48.11042843566693 \t 1.0291211784171435\n",
      "82     \t [-0.16207522  0.74919676]. \t  1.002767045124525 \t 1.0291211784171435\n",
      "83     \t [ 0.04478357 -0.66761992]. \t  1.010097716812643 \t 1.0291211784171435\n",
      "84     \t [ 0.06852688 -0.68998734]. \t  1.026257038221168 \t 1.0291211784171435\n",
      "85     \t [ 0.04397873 -0.74516276]. \t  1.0128246442578859 \t 1.0291211784171435\n",
      "86     \t [-0.09111138  0.8269578 ]. \t  0.9070678346913537 \t 1.0291211784171435\n",
      "87     \t [ 0.03059767 -0.72307159]. \t  1.01629597954528 \t 1.0291211784171435\n",
      "88     \t [ 0.09151039 -0.58527871]. \t  0.9210490139124192 \t 1.0291211784171435\n",
      "89     \t [ 0.0270478  -0.66246532]. \t  1.0000407660712134 \t 1.0291211784171435\n",
      "90     \t [-0.10833044  0.70815117]. \t  \u001b[92m1.0300523289749726\u001b[0m \t 1.0300523289749726\n",
      "91     \t [-0.06141523  0.67073264]. \t  1.0160885895779297 \t 1.0300523289749726\n",
      "92     \t [ 0.0802498  -0.79238198]. \t  0.9725132531192399 \t 1.0300523289749726\n",
      "93     \t [0.01068168 0.72381375]. \t  0.9895260373729474 \t 1.0300523289749726\n",
      "94     \t [ 0.0209851  -0.68597151]. \t  1.0091665062952913 \t 1.0300523289749726\n",
      "95     \t [-0.06057937  0.73757203]. \t  1.022282075636786 \t 1.0300523289749726\n",
      "96     \t [-0.16434271  0.67349081]. \t  0.9955589022614688 \t 1.0300523289749726\n",
      "97     \t [0.02226864 0.74573658]. \t  0.9688112198614599 \t 1.0300523289749726\n",
      "98     \t [ 0.05822032 -0.677669  ]. \t  1.0192727466638984 \t 1.0300523289749726\n",
      "99     \t [ 0.03671246 -0.70191961]. \t  1.0201681132467058 \t 1.0300523289749726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [ 0.14629968 -0.66121154]. \t  0.9963044159779276 \t 1.0300523289749726\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \n",
    "\n",
    "np.random.seed(run_num_14)\n",
    "surrogate_winner_14 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_14 = GPGO(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_14.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 0.46729115 -0.97799468]. \t  -0.15324775214966774 \t -0.10477079024230418\n",
      "init   \t [-1.48483878  0.83961742]. \t  -0.10477079024230418 \t -0.10477079024230418\n",
      "init   \t [-0.31468582 -1.09222817]. \t  -1.6403350770531386 \t -0.10477079024230418\n",
      "init   \t [-0.58974261  1.52903094]. \t  -12.761482581115207 \t -0.10477079024230418\n",
      "init   \t [-0.3767192   1.51370072]. \t  -11.790964688915137 \t -0.10477079024230418\n",
      "1      \t [-2.27809435  0.26159861]. \t  -9.940079732156835 \t -0.10477079024230418\n",
      "2      \t [1.56117466 0.98543955]. \t  -3.5266592545259194 \t -0.10477079024230418\n",
      "3      \t [ 2.40359763 -2.        ]. \t  -60.486342987641656 \t -0.10477079024230418\n",
      "4      \t [3. 2.]. \t  -162.89999999999998 \t -0.10477079024230418\n",
      "5      \t [0.80081148 0.28880324]. \t  -1.714930095134656 \t -0.10477079024230418\n",
      "6      \t [-3. -2.]. \t  -162.89999999999998 \t -0.10477079024230418\n",
      "7      \t [-3.  2.]. \t  -150.89999999999998 \t -0.10477079024230418\n",
      "8      \t [ 3.         -0.25906523]. \t  -107.87236270862583 \t -0.10477079024230418\n",
      "9      \t [ 0.25698576 -2.        ]. \t  -47.741132059556634 \t -0.10477079024230418\n",
      "10     \t [-1.02708173 -0.09244993]. \t  -2.3350488257594972 \t -0.10477079024230418\n",
      "11     \t [0.92787208 2.        ]. \t  -51.95566908336883 \t -0.10477079024230418\n",
      "12     \t [-0.3192729   0.46781141]. \t  \u001b[92m0.446899456192132\u001b[0m \t 0.446899456192132\n",
      "13     \t [-1.19398644 -2.        ]. \t  -52.78823455868309 \t 0.446899456192132\n",
      "14     \t [ 1.54474989 -0.18358714]. \t  -1.7025858682633315 \t 0.446899456192132\n",
      "15     \t [-0.0426628  -0.46471309]. \t  \u001b[92m0.6501816781564844\u001b[0m \t 0.6501816781564844\n",
      "16     \t [1.46225581 0.44703051]. \t  -2.22442583108073 \t 0.6501816781564844\n",
      "17     \t [ 1.36077805 -0.9960265 ]. \t  -0.935850088237039 \t 0.6501816781564844\n",
      "18     \t [ 1.04581998 -0.54994522]. \t  -0.8799101740825739 \t 0.6501816781564844\n",
      "19     \t [0.75947492 0.99118389]. \t  -2.3562985522808755 \t 0.6501816781564844\n",
      "20     \t [-1.55008999  0.32851481]. \t  -1.2167862262410563 \t 0.6501816781564844\n",
      "21     \t [-3.         -0.18903419]. \t  -109.32927452064649 \t 0.6501816781564844\n",
      "22     \t [-1.35913057 -0.85022363]. \t  -2.6785175567608777 \t 0.6501816781564844\n",
      "23     \t [-2.0277402   0.75577796]. \t  -1.6028108654757602 \t 0.6501816781564844\n",
      "24     \t [-0.82278193 -0.71764274]. \t  -1.4402552003609586 \t 0.6501816781564844\n",
      "25     \t [-1.6435403  2.       ]. \t  -46.76485906724546 \t 0.6501816781564844\n",
      "26     \t [ 3.         -1.44180096]. \t  -113.54490868913967 \t 0.6501816781564844\n",
      "27     \t [ 1.40849426 -2.        ]. \t  -47.45610825348679 \t 0.6501816781564844\n",
      "28     \t [-1.66610831 -0.35525519]. \t  -2.20258871840572 \t 0.6501816781564844\n",
      "29     \t [2.35202373 0.73698871]. \t  -15.034666981737002 \t 0.6501816781564844\n",
      "30     \t [0.22373837 0.6960338 ]. \t  0.6482900726778009 \t 0.6501816781564844\n",
      "31     \t [-0.26865246  2.        ]. \t  -47.74057785307134 \t 0.6501816781564844\n",
      "32     \t [-0.58506959  0.91663204]. \t  -0.0632202770912893 \t 0.6501816781564844\n",
      "33     \t [-3.          0.81210034]. \t  -105.5654688638399 \t 0.6501816781564844\n",
      "34     \t [ 1.82361583 -0.85220475]. \t  0.01220013717701407 \t 0.6501816781564844\n",
      "35     \t [ 1.58034916 -0.72396409]. \t  0.05782821683556805 \t 0.6501816781564844\n",
      "36     \t [ 0.41058542 -0.27100809]. \t  -0.23276128934336032 \t 0.6501816781564844\n",
      "37     \t [ 1.78193693 -1.19959753]. \t  -2.5891062670350227 \t 0.6501816781564844\n",
      "38     \t [1.89983487 2.        ]. \t  -54.55304722368956 \t 0.6501816781564844\n",
      "39     \t [-2.10690184 -1.05042355]. \t  -8.202139145728623 \t 0.6501816781564844\n",
      "40     \t [2.00379206 0.2841598 ]. \t  -4.054847026135452 \t 0.6501816781564844\n",
      "41     \t [-1.83815267  0.57328664]. \t  -0.4624293661299139 \t 0.6501816781564844\n",
      "42     \t [-2.00428667 -2.        ]. \t  -55.797396288734326 \t 0.6501816781564844\n",
      "43     \t [-1.77409738  1.22762474]. \t  -3.0583700106571277 \t 0.6501816781564844\n",
      "44     \t [-0.82257248  0.47355942]. \t  -0.7629301636377862 \t 0.6501816781564844\n",
      "45     \t [3.         0.75668421]. \t  -110.1911188300753 \t 0.6501816781564844\n",
      "46     \t [0.11749565 1.00829843]. \t  -0.24106614889439545 \t 0.6501816781564844\n",
      "47     \t [-0.09591201  0.02962635]. \t  -0.030269677980299708 \t 0.6501816781564844\n",
      "48     \t [-0.05695408 -0.78336314]. \t  \u001b[92m0.8907588980487189\u001b[0m \t 0.8907588980487189\n",
      "49     \t [-1.62675203 -1.33169428]. \t  -9.708986170756976 \t 0.8907588980487189\n",
      "50     \t [2.1186176  1.26879868]. \t  -12.404174732225997 \t 0.8907588980487189\n",
      "51     \t [ 0.2371411  -0.71182168]. \t  \u001b[92m0.9502614575671464\u001b[0m \t 0.9502614575671464\n",
      "52     \t [-3.         -1.16517951]. \t  -114.33774667380781 \t 0.9502614575671464\n",
      "53     \t [ 3. -2.]. \t  -150.89999999999998 \t 0.9502614575671464\n",
      "54     \t [1.91502724 0.77789582]. \t  -3.400681713878566 \t 0.9502614575671464\n",
      "55     \t [-2.14727566 -0.51651317]. \t  -6.799306264546247 \t 0.9502614575671464\n",
      "56     \t [-0.01792837  0.66561569]. \t  \u001b[92m0.99767208306684\u001b[0m \t 0.99767208306684\n",
      "57     \t [-2.31812936  2.        ]. \t  -55.94263858919904 \t 0.99767208306684\n",
      "58     \t [-0.15856737  0.81172872]. \t  0.9284602660806704 \t 0.99767208306684\n",
      "59     \t [ 2.0110792  -0.40208542]. \t  -2.5285573180675707 \t 0.99767208306684\n",
      "60     \t [-0.38945843 -2.        ]. \t  -49.33847852968542 \t 0.99767208306684\n",
      "61     \t [-0.10131046  0.75271386]. \t  \u001b[92m1.0176928097344666\u001b[0m \t 1.0176928097344666\n",
      "62     \t [-2.47402973  1.43112017]. \t  -27.291584459242934 \t 1.0176928097344666\n",
      "63     \t [-1.80573152 -0.8428611 ]. \t  -2.970401037994308 \t 1.0176928097344666\n",
      "64     \t [1.36622852 1.57531766]. \t  -19.177068617948088 \t 1.0176928097344666\n",
      "65     \t [ 0.18379273 -0.70270731]. \t  0.9962629463927863 \t 1.0176928097344666\n",
      "66     \t [-0.03271201  0.73956222]. \t  1.0110966028979782 \t 1.0176928097344666\n",
      "67     \t [3.         1.41044948]. \t  -121.0042136034949 \t 1.0176928097344666\n",
      "68     \t [ 0.12903895 -0.76997359]. \t  0.9988416578412672 \t 1.0176928097344666\n",
      "69     \t [-0.10009227  0.72493665]. \t  \u001b[92m1.0300893793888526\u001b[0m \t 1.0300893793888526\n",
      "70     \t [-0.07402663  0.73514897]. \t  1.0260209705046393 \t 1.0300893793888526\n",
      "71     \t [ 0.13698567 -0.68208288]. \t  1.0142787020098176 \t 1.0300893793888526\n",
      "72     \t [ 2.28650762 -1.3321438 ]. \t  -13.59900508115462 \t 1.0300893793888526\n",
      "73     \t [ 0.8839173  -1.54401691]. \t  -13.835150274932655 \t 1.0300893793888526\n",
      "74     \t [-0.00290062  0.66320879]. \t  0.9874159999821929 \t 1.0300893793888526\n",
      "75     \t [ 0.09162577 -0.72688216]. \t  1.0299511910877637 \t 1.0300893793888526\n",
      "76     \t [ 0.02380719 -0.69091652]. \t  1.0121330705152023 \t 1.0300893793888526\n",
      "77     \t [-0.02021328  0.68307891]. \t  1.0077102100265933 \t 1.0300893793888526\n",
      "78     \t [-0.02485474  0.68660862]. \t  1.0113305847649379 \t 1.0300893793888526\n",
      "79     \t [-0.06723112  0.74187141]. \t  1.0216897918064785 \t 1.0300893793888526\n",
      "80     \t [ 0.16567632 -0.60911133]. \t  0.9261491906569486 \t 1.0300893793888526\n",
      "81     \t [-0.01691454  0.65238747]. \t  0.9877547598330986 \t 1.0300893793888526\n",
      "82     \t [ 0.08292578 -0.73176122]. \t  1.0282405710960256 \t 1.0300893793888526\n",
      "83     \t [ 0.08241327 -0.70106364]. \t  \u001b[92m1.030416262648203\u001b[0m \t 1.030416262648203\n",
      "84     \t [ 0.07376922 -0.73165722]. \t  1.0272776730444262 \t 1.030416262648203\n",
      "85     \t [ 0.12361132 -1.40725832]. \t  -7.652729019655116 \t 1.030416262648203\n",
      "86     \t [ 0.11850254 -0.745204  ]. \t  1.0203051662703129 \t 1.030416262648203\n",
      "87     \t [-1.21953166  1.17406243]. \t  -3.055237731696672 \t 1.030416262648203\n",
      "88     \t [-0.02676321  0.70680497]. \t  1.0160516426479713 \t 1.030416262648203\n",
      "89     \t [-0.07787019  0.62235007]. \t  0.9734971087939845 \t 1.030416262648203\n",
      "90     \t [-0.0163227  -0.61985871]. \t  0.9352010916841812 \t 1.030416262648203\n",
      "91     \t [ 0.15580586 -0.6322136 ]. \t  0.9623883327394138 \t 1.030416262648203\n",
      "92     \t [ 0.18274501 -0.73462961]. \t  0.9966983865727349 \t 1.030416262648203\n",
      "93     \t [ 0.13529705 -0.62838259]. \t  0.9682850347885088 \t 1.030416262648203\n",
      "94     \t [ 0.20572708 -0.69773916]. \t  0.9772930048723621 \t 1.030416262648203\n",
      "95     \t [-0.03547248  0.7133446 ]. \t  1.0199602048367433 \t 1.030416262648203\n",
      "96     \t [-0.04586117  0.71301874]. \t  1.0240142211374441 \t 1.030416262648203\n",
      "97     \t [-0.06796051  0.74240163]. \t  1.0215547773128852 \t 1.030416262648203\n",
      "98     \t [ 0.09413381 -0.55199501]. \t  0.8641106451767914 \t 1.030416262648203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [-0.0840706   0.66260516]. \t  1.012677182503316 \t 1.030416262648203\n",
      "100    \t [ 0.15804    -0.74048373]. \t  1.0090865339127204 \t 1.030416262648203\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \n",
    "\n",
    "np.random.seed(run_num_15)\n",
    "surrogate_winner_15 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_15 = GPGO(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_15.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [ 2.18368999 -0.73453599]. \t  -4.868217712547965 \t 0.15481417016151777\n",
      "init   \t [1.03290467 0.03164667]. \t  -2.310710708702905 \t 0.15481417016151777\n",
      "init   \t [ 1.68996256 -0.86527329]. \t  0.15481417016151777 \t 0.15481417016151777\n",
      "init   \t [-1.5977473   0.25519875]. \t  -1.420015353077075 \t 0.15481417016151777\n",
      "init   \t [2.25014619 0.87577168]. \t  -10.939300534748709 \t 0.15481417016151777\n",
      "1      \t [ 0.89723137 -1.49748315]. \t  -11.834084752726746 \t 0.15481417016151777\n",
      "2      \t [-3. -2.]. \t  -162.89999999999998 \t 0.15481417016151777\n",
      "3      \t [-3.  2.]. \t  -150.89999999999998 \t 0.15481417016151777\n",
      "4      \t [0.13739259 2.        ]. \t  -48.34954603472248 \t 0.15481417016151777\n",
      "5      \t [ 3. -2.]. \t  -150.89999999999998 \t 0.15481417016151777\n",
      "6      \t [-0.65986818 -0.78836112]. \t  -0.9503474674175262 \t 0.15481417016151777\n",
      "7      \t [3. 2.]. \t  -162.89999999999998 \t 0.15481417016151777\n",
      "8      \t [2.06522144 0.06602695]. \t  -4.840559187082348 \t 0.15481417016151777\n",
      "9      \t [-0.39254725  0.38907163]. \t  0.09884724505261971 \t 0.15481417016151777\n",
      "10     \t [-0.61484072 -2.        ]. \t  -50.45970280687105 \t 0.15481417016151777\n",
      "11     \t [1.28174883 1.21200831]. \t  -6.690635781400804 \t 0.15481417016151777\n",
      "12     \t [-3.         -0.00972527]. \t  -108.92879751053798 \t 0.15481417016151777\n",
      "13     \t [3.        0.0559054]. \t  -109.05525361593446 \t 0.15481417016151777\n",
      "14     \t [-1.19218507  2.        ]. \t  -48.015686424917966 \t 0.15481417016151777\n",
      "15     \t [-0.99913202 -0.09201908]. \t  -2.2902978201284285 \t 0.15481417016151777\n",
      "16     \t [ 0.13660214 -0.66147253]. \t  \u001b[92m1.0008449818197864\u001b[0m \t 1.0008449818197864\n",
      "17     \t [1.66722442 0.69849334]. \t  -2.217123215424947 \t 1.0008449818197864\n",
      "18     \t [-1.22661346  0.88776604]. \t  -0.6428848594507196 \t 1.0008449818197864\n",
      "19     \t [1.54871667 2.        ]. \t  -53.20990652440868 \t 1.0008449818197864\n",
      "20     \t [0.38151511 0.71646738]. \t  0.1871939829093482 \t 1.0008449818197864\n",
      "21     \t [-1.61302831 -0.97477128]. \t  -3.445349111314097 \t 1.0008449818197864\n",
      "22     \t [ 1.4215441 -2.       ]. \t  -47.41522321241314 \t 1.0008449818197864\n",
      "23     \t [-0.45632908  1.07137555]. \t  -0.9348096430893702 \t 1.0008449818197864\n",
      "24     \t [ 1.758347   -0.46138349]. \t  -0.6630447261133096 \t 1.0008449818197864\n",
      "25     \t [ 0.38465652 -0.06996596]. \t  -0.5005503921068047 \t 1.0008449818197864\n",
      "26     \t [-0.89703876  0.65858388]. \t  -0.4594169083956847 \t 1.0008449818197864\n",
      "27     \t [-1.51983526 -0.51868872]. \t  -2.1447190887419723 \t 1.0008449818197864\n",
      "28     \t [-1.58855617 -2.        ]. \t  -53.25479342614733 \t 1.0008449818197864\n",
      "29     \t [ 0.79075868 -0.75023511]. \t  -0.18414293520257818 \t 1.0008449818197864\n",
      "30     \t [-2.11905489  0.89312878]. \t  -3.260689324609995 \t 1.0008449818197864\n",
      "31     \t [-1.75574101  0.74783012]. \t  0.15957075466985093 \t 1.0008449818197864\n",
      "32     \t [ 0.10042489 -1.229777  ]. \t  -3.0160492295945094 \t 1.0008449818197864\n",
      "33     \t [-1.21908675 -1.08717335]. \t  -4.586135165288559 \t 1.0008449818197864\n",
      "34     \t [-3.          0.92796312]. \t  -105.63772839770877 \t 1.0008449818197864\n",
      "35     \t [-1.87414886  1.44862135]. \t  -9.092082022457316 \t 1.0008449818197864\n",
      "36     \t [ 0.40889425 -2.        ]. \t  -47.79384404248746 \t 1.0008449818197864\n",
      "37     \t [0.61700895 1.35970586]. \t  -8.35283057589799 \t 1.0008449818197864\n",
      "38     \t [-2.04269034 -0.31411206]. \t  -4.629815204829526 \t 1.0008449818197864\n",
      "39     \t [-3.         -1.02892155]. \t  -112.23525604281197 \t 1.0008449818197864\n",
      "40     \t [0.85709381 0.69020329]. \t  -1.5311158394839905 \t 1.0008449818197864\n",
      "41     \t [ 3.         -1.06874273]. \t  -106.34351181056611 \t 1.0008449818197864\n",
      "42     \t [ 0.36619211 -0.91122603]. \t  0.3977761435866008 \t 1.0008449818197864\n",
      "43     \t [-0.16493184 -0.85698331]. \t  0.5315816669826621 \t 1.0008449818197864\n",
      "44     \t [-1.99776295  0.32952548]. \t  -2.6593694910261787 \t 1.0008449818197864\n",
      "45     \t [-0.30415493 -0.33934797]. \t  -0.04796347343658841 \t 1.0008449818197864\n",
      "46     \t [3.         1.03733049]. \t  -112.33934683150824 \t 1.0008449818197864\n",
      "47     \t [ 2.04779787 -1.39059171]. \t  -8.800845476145591 \t 1.0008449818197864\n",
      "48     \t [2.04548953 1.44724256]. \t  -16.51882523015008 \t 1.0008449818197864\n",
      "49     \t [ 1.8891519  -0.96132635]. \t  -0.583723676243195 \t 1.0008449818197864\n",
      "50     \t [-1.76047528  1.04671568]. \t  -0.7252107520611482 \t 1.0008449818197864\n",
      "51     \t [ 1.5938971  -1.24656458]. \t  -3.530005997436418 \t 1.0008449818197864\n",
      "52     \t [-0.22759209  0.75342068]. \t  0.9515658784126415 \t 1.0008449818197864\n",
      "53     \t [-2.25449436 -1.50139727]. \t  -24.542344114353348 \t 1.0008449818197864\n",
      "54     \t [-2.07708386  2.        ]. \t  -48.78286379847307 \t 1.0008449818197864\n",
      "55     \t [0.84003764 2.        ]. \t  -51.574141784010145 \t 1.0008449818197864\n",
      "56     \t [ 2.18902997 -2.        ]. \t  -51.24606245203299 \t 1.0008449818197864\n",
      "57     \t [-2.11604896 -0.91442158]. \t  -7.118685101967028 \t 1.0008449818197864\n",
      "58     \t [0.09398082 1.0008851 ]. \t  -0.13632645807456203 \t 1.0008449818197864\n",
      "59     \t [-2.26179023 -2.        ]. \t  -62.65514119225294 \t 1.0008449818197864\n",
      "60     \t [-0.00605127  0.53140315]. \t  0.8136515418402728 \t 1.0008449818197864\n",
      "61     \t [2.31950658 2.        ]. \t  -65.28387679599723 \t 1.0008449818197864\n",
      "62     \t [ 0.03278619 -0.71193932]. \t  \u001b[92m1.0188563625605012\u001b[0m \t 1.0188563625605012\n",
      "63     \t [ 0.07626801 -0.72252307]. \t  \u001b[92m1.0299661746166873\u001b[0m \t 1.0299661746166873\n",
      "64     \t [-2.41939718  1.51396123]. \t  -26.49752753600717 \t 1.0299661746166873\n",
      "65     \t [-0.14542004  0.67163912]. \t  1.0044526058472143 \t 1.0299661746166873\n",
      "66     \t [ 0.05960052 -0.73579783]. \t  1.0228162021822316 \t 1.0299661746166873\n",
      "67     \t [ 0.08882332 -0.73133194]. \t  1.028674466113383 \t 1.0299661746166873\n",
      "68     \t [ 0.02433447 -0.65232126]. \t  0.9913186862146015 \t 1.0299661746166873\n",
      "69     \t [-0.62578261 -1.34968917]. \t  -8.096152704219227 \t 1.0299661746166873\n",
      "70     \t [ 0.0621604  -0.76622147]. \t  1.001861906618762 \t 1.0299661746166873\n",
      "71     \t [-0.09928531  0.6908335 ]. \t  1.0272929859729483 \t 1.0299661746166873\n",
      "72     \t [1.58009274 0.06785088]. \t  -2.1730449226770703 \t 1.0299661746166873\n",
      "73     \t [-0.09032216  0.67850905]. \t  1.022510902498736 \t 1.0299661746166873\n",
      "74     \t [-0.05550033  0.74622989]. \t  1.0161829676404368 \t 1.0299661746166873\n",
      "75     \t [-0.08970546  0.7860896 ]. \t  0.9828276582526966 \t 1.0299661746166873\n",
      "76     \t [-0.198985    0.75020787]. \t  0.9783902958364554 \t 1.0299661746166873\n",
      "77     \t [ 0.04812195 -0.7058883 ]. \t  1.0247052338831626 \t 1.0299661746166873\n",
      "78     \t [ 0.10311472 -0.68567768]. \t  1.024846729338926 \t 1.0299661746166873\n",
      "79     \t [ 0.13184844 -0.77981171]. \t  0.9871658962191121 \t 1.0299661746166873\n",
      "80     \t [-0.1217324   0.70178674]. \t  1.0263904187533048 \t 1.0299661746166873\n",
      "81     \t [-0.13900118  0.72493968]. \t  1.0216550490050202 \t 1.0299661746166873\n",
      "82     \t [-0.07057965  0.70209915]. \t  1.0294808415647447 \t 1.0299661746166873\n",
      "83     \t [-0.04217301  0.67482397]. \t  1.0133906083286162 \t 1.0299661746166873\n",
      "84     \t [-0.14310533  0.7568252 ]. \t  1.0060767793836138 \t 1.0299661746166873\n",
      "85     \t [ 0.08426762 -0.6981384 ]. \t  1.0298967861434378 \t 1.0299661746166873\n",
      "86     \t [-0.11594774  0.67005273]. \t  1.0138783508145226 \t 1.0299661746166873\n",
      "87     \t [ 0.05793112 -0.65764229]. \t  1.0064689591177698 \t 1.0299661746166873\n",
      "88     \t [ 0.09284274 -0.70684695]. \t  \u001b[92m1.0313017877637678\u001b[0m \t 1.0313017877637678\n",
      "89     \t [ 0.19392115 -0.6215183 ]. \t  0.9213315068237908 \t 1.0313017877637678\n",
      "90     \t [-0.56664723  2.        ]. \t  -47.94559022182414 \t 1.0313017877637678\n",
      "91     \t [ 0.07621319 -0.69517623]. \t  1.028699009498748 \t 1.0313017877637678\n",
      "92     \t [ 0.14280597 -0.7228651 ]. \t  1.0204947111000575 \t 1.0313017877637678\n",
      "93     \t [0.0241199  0.73063141]. \t  0.9754750928282886 \t 1.0313017877637678\n",
      "94     \t [ 0.00372845 -0.74159598]. \t  0.992723552577655 \t 1.0313017877637678\n",
      "95     \t [-0.24037388  0.75837825]. \t  0.9355398129272462 \t 1.0313017877637678\n",
      "96     \t [ 0.05356016 -0.64615384]. \t  0.9959353723601319 \t 1.0313017877637678\n",
      "97     \t [ 0.14962137 -0.75596655]. \t  1.0041705615565628 \t 1.0313017877637678\n",
      "98     \t [ 0.03959666 -0.77133242]. \t  0.9882109404283133 \t 1.0313017877637678\n",
      "99     \t [-0.14154495  0.765251  ]. \t  0.9997021131359523 \t 1.0313017877637678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [-0.01342689  0.66527557]. \t  0.9950286184368543 \t 1.0313017877637678\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \n",
    "\n",
    "np.random.seed(run_num_16)\n",
    "surrogate_winner_16 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_16 = GPGO(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_16.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.91094417 0.9220785 ]. \t  -2.3942909876906837 \t -0.5838248233399961\n",
      "init   \t [-1.83979583  0.07870895]. \t  -2.2368016331814964 \t -0.5838248233399961\n",
      "init   \t [1.0582996  1.21777949]. \t  -6.46789670014418 \t -0.5838248233399961\n",
      "init   \t [0.20107719 1.63213672]. \t  -18.215840822341526 \t -0.5838248233399961\n",
      "init   \t [-0.33911323 -0.97611116]. \t  -0.5838248233399961 \t -0.5838248233399961\n",
      "1      \t [ 1.26479002 -0.10508546]. \t  -2.2127872029340248 \t -0.5838248233399961\n",
      "2      \t [-3. -2.]. \t  -162.89999999999998 \t -0.5838248233399961\n",
      "3      \t [-3.          1.91694272]. \t  -142.4632747322682 \t -0.5838248233399961\n",
      "4      \t [ 3. -2.]. \t  -150.89999999999998 \t -0.5838248233399961\n",
      "5      \t [3. 2.]. \t  -162.89999999999998 \t -0.5838248233399961\n",
      "6      \t [ 0.50086148 -2.        ]. \t  -47.874831443734855 \t -0.5838248233399961\n",
      "7      \t [-0.7001164   0.17413242]. \t  -1.2558382684652716 \t -0.5838248233399961\n",
      "8      \t [ 3.0000000e+00 -2.5272473e-03]. \t  -108.89239271035548 \t -0.5838248233399961\n",
      "9      \t [-1.01032094 -2.        ]. \t  -52.270103355044064 \t -0.5838248233399961\n",
      "10     \t [ 0.38615238 -0.3462202 ]. \t  \u001b[92m0.004827127673154852\u001b[0m \t 0.004827127673154852\n",
      "11     \t [-1.10161526  2.        ]. \t  -48.15402738757592 \t 0.004827127673154852\n",
      "12     \t [-1.23627013 -0.55721991]. \t  -2.2306310925213197 \t 0.004827127673154852\n",
      "13     \t [-3.         -0.03720289]. \t  -109.00608012027988 \t 0.004827127673154852\n",
      "14     \t [-1.3959878  0.5116295]. \t  -0.7996423700924062 \t 0.004827127673154852\n",
      "15     \t [-1.30484381  0.00570608]. \t  -2.360433819084991 \t 0.004827127673154852\n",
      "16     \t [ 1.42025474 -1.1144774 ]. \t  -1.8795592223928161 \t 0.004827127673154852\n",
      "17     \t [-0.2894433   0.86002936]. \t  \u001b[92m0.698633770551676\u001b[0m \t 0.698633770551676\n",
      "18     \t [1.09007084 2.        ]. \t  -52.52731875512089 \t 0.698633770551676\n",
      "19     \t [ 0.95077803 -0.85866698]. \t  -0.5549397688201112 \t 0.698633770551676\n",
      "20     \t [1.71690173 0.71024352]. \t  -2.301007387389527 \t 0.698633770551676\n",
      "21     \t [-0.3013534 -0.4771071]. \t  0.21329695304458124 \t 0.698633770551676\n",
      "22     \t [ 1.53980341 -2.        ]. \t  -47.0419191999002 \t 0.698633770551676\n",
      "23     \t [-1.9240867  -0.78871693]. \t  -3.517044517292356 \t 0.698633770551676\n",
      "24     \t [1.35765301 0.71925526]. \t  -2.3033356633921307 \t 0.698633770551676\n",
      "25     \t [-0.03050523  0.42185404]. \t  0.5943113966815089 \t 0.698633770551676\n",
      "26     \t [ 1.71371327 -0.55280557]. \t  -0.2820590176292681 \t 0.698633770551676\n",
      "27     \t [ 1.38849352 -0.68876848]. \t  -0.3411269234118083 \t 0.698633770551676\n",
      "28     \t [1.7797727  0.15981431]. \t  -2.378763387788945 \t 0.698633770551676\n",
      "29     \t [ 0.19702147 -0.88158806]. \t  \u001b[92m0.7142128623523959\u001b[0m \t 0.7142128623523959\n",
      "30     \t [-1.48949317 -1.2307866 ]. \t  -7.130747920552574 \t 0.7142128623523959\n",
      "31     \t [-1.95738804  2.        ]. \t  -47.33136604326179 \t 0.7142128623523959\n",
      "32     \t [-2.11547792  1.01081114]. \t  -3.6695344794710816 \t 0.7142128623523959\n",
      "33     \t [-1.8286302   0.76126162]. \t  0.009228079450749771 \t 0.7142128623523959\n",
      "34     \t [-1.64386408  1.246061  ]. \t  -3.4359735012443333 \t 0.7142128623523959\n",
      "35     \t [0.21291403 0.9507761 ]. \t  -0.03226241035167604 \t 0.7142128623523959\n",
      "36     \t [-1.67677144 -0.57273087]. \t  -2.132998778786625 \t 0.7142128623523959\n",
      "37     \t [-1.94098027 -2.        ]. \t  -54.96958477543308 \t 0.7142128623523959\n",
      "38     \t [ 3.         -1.03072009]. \t  -106.0729423245338 \t 0.7142128623523959\n",
      "39     \t [2.01342395 2.        ]. \t  -55.93815155558003 \t 0.7142128623523959\n",
      "40     \t [0.46813213 0.37711332]. \t  -0.46782585541642185 \t 0.7142128623523959\n",
      "41     \t [ 2.08139817 -1.28927751]. \t  -6.738030265767504 \t 0.7142128623523959\n",
      "42     \t [ 1.79615654 -0.96517795]. \t  -0.2517638608515946 \t 0.7142128623523959\n",
      "43     \t [-3.          0.88377818]. \t  -105.56465305897837 \t 0.7142128623523959\n",
      "44     \t [3.         1.01484109]. \t  -112.06770961797321 \t 0.7142128623523959\n",
      "45     \t [-3.         -1.03351657]. \t  -112.2917568874714 \t 0.7142128623523959\n",
      "46     \t [-0.86353381  1.1034412 ]. \t  -2.06010256733269 \t 0.7142128623523959\n",
      "47     \t [-0.72240907  0.71784484]. \t  -0.04529439225757714 \t 0.7142128623523959\n",
      "48     \t [-0.27879224  2.        ]. \t  -47.740785983991124 \t 0.7142128623523959\n",
      "49     \t [ 0.40415419 -1.25029947]. \t  -3.615464770637784 \t 0.7142128623523959\n",
      "50     \t [-0.21622949 -2.        ]. \t  -48.61492312164814 \t 0.7142128623523959\n",
      "51     \t [1.9931529  1.34339514]. \t  -12.133924066051252 \t 0.7142128623523959\n",
      "52     \t [ 2.20818132 -2.        ]. \t  -51.802731318357594 \t 0.7142128623523959\n",
      "53     \t [-0.85932462 -0.95682126]. \t  -2.4556679041965186 \t 0.7142128623523959\n",
      "54     \t [-2.25722827 -1.45294388]. \t  -22.61530705390514 \t 0.7142128623523959\n",
      "55     \t [0.01242543 0.00276633]. \t  -0.0006212777444998334 \t 0.7142128623523959\n",
      "56     \t [ 2.27736566 -0.20449738]. \t  -10.134737722779809 \t 0.7142128623523959\n",
      "57     \t [1.60390792 1.12043107]. \t  -5.1468121062921295 \t 0.7142128623523959\n",
      "58     \t [2.33038051 0.59679752]. \t  -13.650087698676293 \t 0.7142128623523959\n",
      "59     \t [-2.27160028  0.52065221]. \t  -8.550680910395732 \t 0.7142128623523959\n",
      "60     \t [-0.12396793 -0.77783324]. \t  \u001b[92m0.7984755727840764\u001b[0m \t 0.7984755727840764\n",
      "61     \t [-2.2413047  -0.32971434]. \t  -9.707256240145892 \t 0.7984755727840764\n",
      "62     \t [-1.47538517  0.90564107]. \t  -0.2685848593513205 \t 0.7984755727840764\n",
      "63     \t [ 0.08614889 -0.68789609]. \t  \u001b[92m1.0268177517533694\u001b[0m \t 1.0268177517533694\n",
      "64     \t [-0.10242893  0.68477657]. \t  1.0245408367710964 \t 1.0268177517533694\n",
      "65     \t [-0.02298673  0.78180799]. \t  0.9663753612545007 \t 1.0268177517533694\n",
      "66     \t [ 2.27087859 -0.89502643]. \t  -7.824533793607104 \t 1.0268177517533694\n",
      "67     \t [ 0.04042525 -0.72759594]. \t  1.0194255878076877 \t 1.0268177517533694\n",
      "68     \t [ 0.12973602 -0.7429373 ]. \t  1.0188556915261475 \t 1.0268177517533694\n",
      "69     \t [-0.04286081  0.72760904]. \t  1.020383852408896 \t 1.0268177517533694\n",
      "70     \t [-0.07582861  0.61519002]. \t  0.9646293645185917 \t 1.0268177517533694\n",
      "71     \t [ 0.08540776 -0.72181112]. \t  \u001b[92m1.0308160422333652\u001b[0m \t 1.0308160422333652\n",
      "72     \t [ 0.09373717 -0.70804217]. \t  \u001b[92m1.031378137274174\u001b[0m \t 1.031378137274174\n",
      "73     \t [ 0.04792353 -0.68948231]. \t  1.0214434159976231 \t 1.031378137274174\n",
      "74     \t [ 0.08829868 -0.7673092 ]. \t  1.0051774920049557 \t 1.031378137274174\n",
      "75     \t [-0.0530046   0.73180566]. \t  1.0225154527564195 \t 1.031378137274174\n",
      "76     \t [ 0.01919796 -0.73580994]. \t  1.0057908582980695 \t 1.031378137274174\n",
      "77     \t [ 0.10214771 -0.73454199]. \t  1.027265991243087 \t 1.031378137274174\n",
      "78     \t [-2.50369422  2.        ]. \t  -67.65376461887226 \t 1.031378137274174\n",
      "79     \t [-0.02188545  0.72858482]. \t  1.010226600381735 \t 1.031378137274174\n",
      "80     \t [-0.06732074  0.7959643 ]. \t  0.9641474796299497 \t 1.031378137274174\n",
      "81     \t [ 0.04599281 -0.72531562]. \t  1.0221861020059875 \t 1.031378137274174\n",
      "82     \t [ 0.03754518 -0.69962846]. \t  1.020190602030062 \t 1.031378137274174\n",
      "83     \t [-0.23678852  0.77842116]. \t  0.9216962980500462 \t 1.031378137274174\n",
      "84     \t [ 0.0796401  -0.74730139]. \t  1.0205593765067005 \t 1.031378137274174\n",
      "85     \t [-0.02179903  0.7307205 ]. \t  1.0094176055883282 \t 1.031378137274174\n",
      "86     \t [-0.1395741   0.71285374]. \t  1.0221003274231344 \t 1.031378137274174\n",
      "87     \t [-0.11429671  0.67099207]. \t  1.0148868089511396 \t 1.031378137274174\n",
      "88     \t [-0.14987441  0.73560062]. \t  1.0146945867223347 \t 1.031378137274174\n",
      "89     \t [-0.09705982  0.74401072]. \t  1.0232459564339664 \t 1.031378137274174\n",
      "90     \t [ 0.02709242 -0.6395198 ]. \t  0.9812567711388189 \t 1.031378137274174\n",
      "91     \t [ 0.11423977 -0.71729589]. \t  1.0292551972175115 \t 1.031378137274174\n",
      "92     \t [ 0.08119554 -0.66495178]. \t  1.0143299728683146 \t 1.031378137274174\n",
      "93     \t [ 0.06687804 -0.69502302]. \t  1.0274848064671054 \t 1.031378137274174\n",
      "94     \t [-0.15386224  0.66314115]. \t  0.9939932566681845 \t 1.031378137274174\n",
      "95     \t [-0.07647362  0.71121475]. \t  1.0309322767494524 \t 1.031378137274174\n",
      "96     \t [ 0.09006237 -0.7279951 ]. \t  1.0296635787397268 \t 1.031378137274174\n",
      "97     \t [-0.11834675  0.72745327]. \t  1.0270711330344058 \t 1.031378137274174\n",
      "98     \t [ 0.11279228 -0.7048859 ]. \t  1.0289171629059273 \t 1.031378137274174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [-0.0351234   0.67644611]. \t  1.0116296209424083 \t 1.031378137274174\n",
      "100    \t [-0.03699392 -0.71698577]. \t  0.9672139196380971 \t 1.031378137274174\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \n",
    "\n",
    "np.random.seed(run_num_17)\n",
    "surrogate_winner_17 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_17 = GPGO(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_17.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [2.24057642 1.87416265]. \t  -48.82879577505094 \t -0.03190064766213015\n",
      "init   \t [2.21516724 0.12342277]. \t  -8.660623551418242 \t -0.03190064766213015\n",
      "init   \t [-1.60363003 -1.95440478]. \t  -48.28332373939171 \t -0.03190064766213015\n",
      "init   \t [-0.41718709 -0.39059456]. \t  -0.2801229037700297 \t -0.03190064766213015\n",
      "init   \t [ 0.13604803 -0.08643282]. \t  -0.03190064766213015 \t -0.03190064766213015\n",
      "1      \t [-1.53420698  1.16050289]. \t  -2.215000045970245 \t -0.03190064766213015\n",
      "2      \t [ 3. -2.]. \t  -150.89999999999998 \t -0.03190064766213015\n",
      "3      \t [-3.  2.]. \t  -150.89999999999998 \t -0.03190064766213015\n",
      "4      \t [-0.28808016  1.99852056]. \t  -47.57645813240742 \t -0.03190064766213015\n",
      "5      \t [-1.95021129 -0.00522661]. \t  -3.185012877620029 \t -0.03190064766213015\n",
      "6      \t [ 0.46624948 -2.        ]. \t  -47.84123842137153 \t -0.03190064766213015\n",
      "7      \t [-0.93720417  0.49447592]. \t  -0.9168193897033141 \t -0.03190064766213015\n",
      "8      \t [-3.         -1.07264016]. \t  -112.81081773461445 \t -0.03190064766213015\n",
      "9      \t [ 1.19569604 -0.43106585]. \t  -1.2798516622650529 \t -0.03190064766213015\n",
      "10     \t [1.18697706 0.60668431]. \t  -2.1890712739121208 \t -0.03190064766213015\n",
      "11     \t [3.         0.76521692]. \t  -110.22493071292011 \t -0.03190064766213015\n",
      "12     \t [1.11152396 2.        ]. \t  -52.58812370973458 \t -0.03190064766213015\n",
      "13     \t [1.56057043 0.04620081]. \t  -2.164648457663574 \t -0.03190064766213015\n",
      "14     \t [ 2.19897682 -0.61238151]. \t  -5.64352709134482 \t -0.03190064766213015\n",
      "15     \t [-1.63588985  0.49519171]. \t  -0.5031132422275759 \t -0.03190064766213015\n",
      "16     \t [-1.33841165 -0.56477534]. \t  -2.2297342987266444 \t -0.03190064766213015\n",
      "17     \t [-1.3215406  2.       ]. \t  -47.713137764021454 \t -0.03190064766213015\n",
      "18     \t [0.10785172 0.91009474]. \t  \u001b[92m0.4245491793631141\u001b[0m \t 0.4245491793631141\n",
      "19     \t [-3.          0.40949123]. \t  -107.1132644779214 \t 0.4245491793631141\n",
      "20     \t [ 0.18000862 -0.9163559 ]. \t  \u001b[92m0.5759266455540218\u001b[0m \t 0.5759266455540218\n",
      "21     \t [-0.9482666   1.10960568]. \t  -2.2277416658418687 \t 0.5759266455540218\n",
      "22     \t [ 1.293335   -1.25795986]. \t  -4.435124364971531 \t 0.5759266455540218\n",
      "23     \t [-0.66695909 -2.        ]. \t  -50.72705373464943 \t 0.5759266455540218\n",
      "24     \t [ 0.8276479  -0.93087396]. \t  -0.6286991819162702 \t 0.5759266455540218\n",
      "25     \t [ 1.71124017 -0.58247316]. \t  -0.18241445501510845 \t 0.5759266455540218\n",
      "26     \t [0.54018703 0.58020751]. \t  -0.4168441075815732 \t 0.5759266455540218\n",
      "27     \t [-3. -2.]. \t  -162.89999999999998 \t 0.5759266455540218\n",
      "28     \t [ 1.53811564 -2.        ]. \t  -47.04705035880494 \t 0.5759266455540218\n",
      "29     \t [ 3.         -0.39954565]. \t  -107.16475167113464 \t 0.5759266455540218\n",
      "30     \t [3. 2.]. \t  -162.89999999999998 \t 0.5759266455540218\n",
      "31     \t [1.91617012 0.9286377 ]. \t  -4.180448507624487 \t 0.5759266455540218\n",
      "32     \t [ 0.38151419 -0.53705937]. \t  0.48710276041202394 \t 0.5759266455540218\n",
      "33     \t [-0.7988265  -1.07420987]. \t  -3.3525747482408126 \t 0.5759266455540218\n",
      "34     \t [-0.2654783   0.69196134]. \t  \u001b[92m0.9103043672220468\u001b[0m \t 0.9103043672220468\n",
      "35     \t [-1.91310822 -0.83731761]. \t  -3.615482973321792 \t 0.9103043672220468\n",
      "36     \t [-1.34264881  0.83802015]. \t  -0.37763037923230947 \t 0.9103043672220468\n",
      "37     \t [ 1.91976618 -1.10253409]. \t  -1.8361177742244388 \t 0.9103043672220468\n",
      "38     \t [1.55627692 1.34560091]. \t  -10.0703647256865 \t 0.9103043672220468\n",
      "39     \t [0.72335647 0.04068078]. \t  -1.5886001588207461 \t 0.9103043672220468\n",
      "40     \t [-1.6490112  -0.16907361]. \t  -2.218991882418549 \t 0.9103043672220468\n",
      "41     \t [-1.48845501 -1.07916388]. \t  -4.552190483568246 \t 0.9103043672220468\n",
      "42     \t [-1.93931199  0.86153791]. \t  -0.6362952373890105 \t 0.9103043672220468\n",
      "43     \t [0.6772032 1.104168 ]. \t  -3.241565837432608 \t 0.9103043672220468\n",
      "44     \t [-0.40410637  0.23326491]. \t  -0.298586607319202 \t 0.9103043672220468\n",
      "45     \t [-0.19448145 -0.80735799]. \t  0.6024722227490926 \t 0.9103043672220468\n",
      "46     \t [-2.13409371  2.        ]. \t  -49.879761538156764 \t 0.9103043672220468\n",
      "47     \t [-0.45274692  0.8821253 ]. \t  0.35536599124615137 \t 0.9103043672220468\n",
      "48     \t [1.77992616 0.55785179]. \t  -2.3298011878120652 \t 0.9103043672220468\n",
      "49     \t [-2.16821016 -1.53018691]. \t  -22.90772247650657 \t 0.9103043672220468\n",
      "50     \t [-0.14291318 -1.30882186]. \t  -5.153489816258426 \t 0.9103043672220468\n",
      "51     \t [ 3.         -1.27547175]. \t  -109.15255181133277 \t 0.9103043672220468\n",
      "52     \t [-3.          1.24387344]. \t  -108.55506812291547 \t 0.9103043672220468\n",
      "53     \t [0.37898421 2.        ]. \t  -49.29015069596764 \t 0.9103043672220468\n",
      "54     \t [ 2.23743719 -2.        ]. \t  -52.74090273196789 \t 0.9103043672220468\n",
      "55     \t [-2.19260083  1.39109484]. \t  -11.920188186676754 \t 0.9103043672220468\n",
      "56     \t [-2.25257218 -2.        ]. \t  -62.280513272190966 \t 0.9103043672220468\n",
      "57     \t [-2.54622555 -0.43951998]. \t  -28.996437496267863 \t 0.9103043672220468\n",
      "58     \t [ 1.57292632 -0.9413111 ]. \t  -0.20564826118578272 \t 0.9103043672220468\n",
      "59     \t [0.02452446 0.62839655]. \t  \u001b[92m0.937984892532778\u001b[0m \t 0.937984892532778\n",
      "60     \t [2.67388353 1.41106752]. \t  -54.74268928049757 \t 0.937984892532778\n",
      "61     \t [-0.09629965  0.69006861]. \t  \u001b[92m1.0272724384237262\u001b[0m \t 1.0272724384237262\n",
      "62     \t [-0.05901598  0.73203414]. \t  1.0241478598824227 \t 1.0272724384237262\n",
      "63     \t [ 2.00494473 -0.26383321]. \t  -3.009430004725619 \t 1.0272724384237262\n",
      "64     \t [ 0.015409   -0.61509105]. \t  0.9493207650741342 \t 1.0272724384237262\n",
      "65     \t [-0.12937272  1.42190373]. \t  -8.146032156453993 \t 1.0272724384237262\n",
      "66     \t [ 0.09509055 -0.72198457]. \t  \u001b[92m1.0308482697280197\u001b[0m \t 1.0308482697280197\n",
      "67     \t [-0.02403427  0.76712132]. \t  0.9848159341319289 \t 1.0308482697280197\n",
      "68     \t [ 0.07799498 -0.70064146]. \t  1.0300599340127754 \t 1.0308482697280197\n",
      "69     \t [-2.38523635  0.64414163]. \t  -13.661306179429031 \t 1.0308482697280197\n",
      "70     \t [-0.1815013   0.67248752]. \t  0.9834292453672202 \t 1.0308482697280197\n",
      "71     \t [-0.1383879   0.73831046]. \t  1.0181993823297213 \t 1.0308482697280197\n",
      "72     \t [-0.14608265  0.75079228]. \t  1.009045090694558 \t 1.0308482697280197\n",
      "73     \t [-0.08337134  0.76605439]. \t  1.0060009309835718 \t 1.0308482697280197\n",
      "74     \t [-0.03198806  0.78172963]. \t  0.9715413459063215 \t 1.0308482697280197\n",
      "75     \t [ 0.07270489 -0.73392779]. \t  1.0262994730829837 \t 1.0308482697280197\n",
      "76     \t [ 0.14282019 -0.72757887]. \t  1.0197428025267619 \t 1.0308482697280197\n",
      "77     \t [-0.08235883  0.77246383]. \t  0.9991799824804014 \t 1.0308482697280197\n",
      "78     \t [-0.10825192  0.72346019]. \t  1.0295406464530767 \t 1.0308482697280197\n",
      "79     \t [ 0.1127602  -0.72244505]. \t  1.0290192675317917 \t 1.0308482697280197\n",
      "80     \t [ 0.01241953 -0.67602737]. \t  1.0003874861534219 \t 1.0308482697280197\n",
      "81     \t [ 0.16903441 -0.76217647]. \t  0.9900625382366236 \t 1.0308482697280197\n",
      "82     \t [ 0.12624751 -0.68008645]. \t  1.017017879676936 \t 1.0308482697280197\n",
      "83     \t [-0.0404761   0.80129089]. \t  0.9451532635763712 \t 1.0308482697280197\n",
      "84     \t [-0.10453142  0.75226579]. \t  1.017805185297324 \t 1.0308482697280197\n",
      "85     \t [ 0.06698596 -0.57100429]. \t  0.8993029302465412 \t 1.0308482697280197\n",
      "86     \t [-0.14010047  0.74619977]. \t  1.0139255386908996 \t 1.0308482697280197\n",
      "87     \t [-0.10723064  0.78561347]. \t  0.983592737327985 \t 1.0308482697280197\n",
      "88     \t [ 0.05744837 -0.73310553]. \t  1.0233291893939658 \t 1.0308482697280197\n",
      "89     \t [-0.13362881  0.5783635 ]. \t  0.8969718372950797 \t 1.0308482697280197\n",
      "90     \t [ 0.17111524 -0.74581868]. \t  0.9996370674163528 \t 1.0308482697280197\n",
      "91     \t [-0.12943676  0.71544088]. \t  1.0256145115461834 \t 1.0308482697280197\n",
      "92     \t [-0.14441738  0.76971169]. \t  0.9944521674156495 \t 1.0308482697280197\n",
      "93     \t [ 0.1361291  -0.75260594]. \t  1.011401884160891 \t 1.0308482697280197\n",
      "94     \t [-0.15838128  0.80760848]. \t  0.9361900185398137 \t 1.0308482697280197\n",
      "95     \t [-0.05916414  0.72790883]. \t  1.0255258444857596 \t 1.0308482697280197\n",
      "96     \t [ 0.10613013 -0.70535729]. \t  1.0300467661997539 \t 1.0308482697280197\n",
      "97     \t [ 0.03132798 -0.69297216]. \t  1.016219159100094 \t 1.0308482697280197\n",
      "98     \t [-0.09975938  0.77867868]. \t  0.9928472329215722 \t 1.0308482697280197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [ 0.09358643 -0.70357855]. \t  \u001b[92m1.0308735017886335\u001b[0m \t 1.0308735017886335\n",
      "100    \t [-0.24219574  0.76559529]. \t  0.9282695736757078 \t 1.0308735017886335\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \n",
    "\n",
    "np.random.seed(run_num_18)\n",
    "surrogate_winner_18 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_18 = GPGO(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_18.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.35667346 1.04596796]. \t  -1.260209694806778 \t -1.260209694806778\n",
      "init   \t [-1.53281067  1.08675788]. \t  -1.318361654110369 \t -1.260209694806778\n",
      "init   \t [-0.87316751  1.1867896 ]. \t  -3.241714295197782 \t -1.260209694806778\n",
      "init   \t [-0.92399987 -0.22450226]. \t  -2.107788234046822 \t -1.260209694806778\n",
      "init   \t [-1.18017846 -0.23154432]. \t  -2.468366013151972 \t -1.260209694806778\n",
      "1      \t [-2.85648493  1.5828355 ]. \t  -84.46980217554298 \t -1.260209694806778\n",
      "2      \t [ 1.44338403 -0.25129926]. \t  -1.6334476499240238 \t -1.260209694806778\n",
      "3      \t [1.87353356 1.99603034]. \t  -53.878984725289456 \t -1.260209694806778\n",
      "4      \t [ 3. -2.]. \t  -150.89999999999998 \t -1.260209694806778\n",
      "5      \t [-3. -2.]. \t  -162.89999999999998 \t -1.260209694806778\n",
      "6      \t [ 0.10130591 -2.        ]. \t  -47.83821889927305 \t -1.260209694806778\n",
      "7      \t [3.         0.49691732]. \t  -109.64693609417934 \t -1.260209694806778\n",
      "8      \t [ 0.57660503 -0.09396231]. \t  \u001b[92m-1.0208298919926224\u001b[0m \t -1.0208298919926224\n",
      "9      \t [0.17265394 2.        ]. \t  -48.46268819566878 \t -1.0208298919926224\n",
      "10     \t [1.12702744 0.55853277]. \t  -2.1466771805224374 \t -1.0208298919926224\n",
      "11     \t [-1.10519551  0.4833659 ]. \t  -1.1097430139516367 \t -1.0208298919926224\n",
      "12     \t [-3.         -0.00621784]. \t  -108.9184988672432 \t -1.0208298919926224\n",
      "13     \t [-1.53278018  2.        ]. \t  -47.06335131178365 \t -1.0208298919926224\n",
      "14     \t [-1.10355811 -1.49954987]. \t  -15.24479666240683 \t -1.0208298919926224\n",
      "15     \t [ 1.21770731 -1.24894258]. \t  -4.373026182383376 \t -1.0208298919926224\n",
      "16     \t [3. 2.]. \t  -162.89999999999998 \t -1.0208298919926224\n",
      "17     \t [ 0.32629545 -0.92175176]. \t  \u001b[92m0.40933674002547926\u001b[0m \t 0.40933674002547926\n",
      "18     \t [ 0.98896861 -0.70534138]. \t  -0.5177117755330223 \t 0.40933674002547926\n",
      "19     \t [-0.62940908 -0.92868258]. \t  -1.3857797132791825 \t 0.40933674002547926\n",
      "20     \t [ 2.2170097  -0.70311966]. \t  -5.949709794383948 \t 0.40933674002547926\n",
      "21     \t [1.04246399 1.41781167]. \t  -11.895373302738685 \t 0.40933674002547926\n",
      "22     \t [0.02504463 0.50085329]. \t  \u001b[92m0.7366533294816914\u001b[0m \t 0.7366533294816914\n",
      "23     \t [ 1.70274    -0.76743298]. \t  0.20657657862438117 \t 0.7366533294816914\n",
      "24     \t [-1.408316   -0.93175346]. \t  -3.127679575744962 \t 0.7366533294816914\n",
      "25     \t [0.57989791 0.64892816]. \t  -0.5215314011715402 \t 0.7366533294816914\n",
      "26     \t [ 0.03211542 -0.49097974]. \t  \u001b[92m0.7434472512695377\u001b[0m \t 0.7434472512695377\n",
      "27     \t [ 1.55414389 -2.        ]. \t  -46.99886800023903 \t 0.7434472512695377\n",
      "28     \t [-1.06160681 -0.77556595]. \t  -2.1824362288644865 \t 0.7434472512695377\n",
      "29     \t [ 3.         -0.65329956]. \t  -105.96153409695987 \t 0.7434472512695377\n",
      "30     \t [2.00545384 0.04100006]. \t  -3.8796588268375514 \t 0.7434472512695377\n",
      "31     \t [-1.92228038  0.54134534]. \t  -1.055656503729152 \t 0.7434472512695377\n",
      "32     \t [-1.8144294 -2.       ]. \t  -53.9308511635711 \t 0.7434472512695377\n",
      "33     \t [-1.61496046  0.64818299]. \t  -0.04012386602131468 \t 0.7434472512695377\n",
      "34     \t [-0.43941123  0.79115592]. \t  0.5877729003855242 \t 0.7434472512695377\n",
      "35     \t [-1.93620572 -0.28472581]. \t  -3.29761734933031 \t 0.7434472512695377\n",
      "36     \t [ 1.92154907 -0.44946146]. \t  -1.4104976371870284 \t 0.7434472512695377\n",
      "37     \t [1.96321319 0.99514508]. \t  -5.22145797115201 \t 0.7434472512695377\n",
      "38     \t [1.75806552 0.57184757]. \t  -2.2690346526743266 \t 0.7434472512695377\n",
      "39     \t [ 1.88721752 -1.16761619]. \t  -2.445311794999049 \t 0.7434472512695377\n",
      "40     \t [-0.39949437  0.30129267]. \t  -0.13573757636447326 \t 0.7434472512695377\n",
      "41     \t [-0.14465141 -0.72133615]. \t  \u001b[92m0.8112253840934263\u001b[0m \t 0.8112253840934263\n",
      "42     \t [-2.37217738  2.        ]. \t  -58.663263071746925 \t 0.8112253840934263\n",
      "43     \t [-1.79871565  0.15468811]. \t  -1.8766972803573927 \t 0.8112253840934263\n",
      "44     \t [-3.         -1.05185779]. \t  -112.52648052804601 \t 0.8112253840934263\n",
      "45     \t [1.57567631 1.15805822]. \t  -5.742275519750267 \t 0.8112253840934263\n",
      "46     \t [-0.72483001 -2.        ]. \t  -51.019865497512264 \t 0.8112253840934263\n",
      "47     \t [-2.04077367  1.14863552]. \t  -3.654943790135028 \t 0.8112253840934263\n",
      "48     \t [-0.00087432  0.78067406]. \t  \u001b[92m0.9527605435427025\u001b[0m \t 0.9527605435427025\n",
      "49     \t [ 0.86417781 -2.        ]. \t  -48.226490218551675 \t 0.9527605435427025\n",
      "50     \t [-0.64181569  2.        ]. \t  -48.03104057877466 \t 0.9527605435427025\n",
      "51     \t [-2.20268078 -1.27422116]. \t  -14.900659755878385 \t 0.9527605435427025\n",
      "52     \t [ 0.2804653  -0.70596414]. \t  0.8961764287385725 \t 0.9527605435427025\n",
      "53     \t [-3.         0.8402435]. \t  -105.54902856173396 \t 0.9527605435427025\n",
      "54     \t [3.         1.28717164]. \t  -117.11436006362928 \t 0.9527605435427025\n",
      "55     \t [-0.21663503  1.2482407 ]. \t  -3.3910628543430272 \t 0.9527605435427025\n",
      "56     \t [ 2.30800777 -2.        ]. \t  -55.48739870515163 \t 0.9527605435427025\n",
      "57     \t [ 0.72257982 -0.94339192]. \t  -0.4901363040999789 \t 0.9527605435427025\n",
      "58     \t [ 0.06967906 -0.72186237]. \t  \u001b[92m1.0291491128838983\u001b[0m \t 1.0291491128838983\n",
      "59     \t [1.16076049 2.        ]. \t  -52.713988923207154 \t 1.0291491128838983\n",
      "60     \t [-2.07652679 -0.82090077]. \t  -5.752203918652382 \t 1.0291491128838983\n",
      "61     \t [ 2.69427368 -1.36351488]. \t  -48.59884244995952 \t 1.0291491128838983\n",
      "62     \t [ 0.10229388 -0.73582615]. \t  1.0267747846972068 \t 1.0291491128838983\n",
      "63     \t [-3.  2.]. \t  -150.89999999999998 \t 1.0291491128838983\n",
      "64     \t [ 0.15228052 -0.72168986]. \t  1.0165303907066483 \t 1.0291491128838983\n",
      "65     \t [-0.12249562  0.71006115]. \t  1.027360253443147 \t 1.0291491128838983\n",
      "66     \t [ 0.09739594 -0.75421105]. \t  1.0167492540614325 \t 1.0291491128838983\n",
      "67     \t [2.25680935 0.63396596]. \t  -10.406852499525218 \t 1.0291491128838983\n",
      "68     \t [ 0.09402617 -0.70589595]. \t  \u001b[92m1.0311612112101831\u001b[0m \t 1.0311612112101831\n",
      "69     \t [ 0.11153612 -0.73848484]. \t  1.0247007967131199 \t 1.0311612112101831\n",
      "70     \t [-2.40294043 -2.        ]. \t  -70.05803533547545 \t 1.0311612112101831\n",
      "71     \t [-0.08883925  0.73583908]. \t  1.027056944179995 \t 1.0311612112101831\n",
      "72     \t [ 0.13561631 -0.6931938 ]. \t  1.0196312693632121 \t 1.0311612112101831\n",
      "73     \t [ 0.12124396 -0.69646195]. \t  1.0252012450146901 \t 1.0311612112101831\n",
      "74     \t [-0.06971186  0.72472452]. \t  1.0285871479445268 \t 1.0311612112101831\n",
      "75     \t [ 0.1195457  -0.74145363]. \t  1.0219991871473828 \t 1.0311612112101831\n",
      "76     \t [ 0.10445858 -0.7159081 ]. \t  1.0307585196960298 \t 1.0311612112101831\n",
      "77     \t [-0.31944373 -1.4424934 ]. \t  -9.84301225730052 \t 1.0311612112101831\n",
      "78     \t [-0.002504   -0.79646379]. \t  0.9257759662954214 \t 1.0311612112101831\n",
      "79     \t [-2.42714527 -0.34392915]. \t  -19.250765932229257 \t 1.0311612112101831\n",
      "80     \t [ 0.05964272 -0.61807646]. \t  0.966982776939202 \t 1.0311612112101831\n",
      "81     \t [ 0.03021228 -0.72808237]. \t  1.0147226614328921 \t 1.0311612112101831\n",
      "82     \t [-0.22293118  0.72994282]. \t  0.9647719929726405 \t 1.0311612112101831\n",
      "83     \t [ 0.05473727 -0.79108892]. \t  0.9680118334604724 \t 1.0311612112101831\n",
      "84     \t [2.42178646 1.63867279]. \t  -40.54270050267634 \t 1.0311612112101831\n",
      "85     \t [-0.13656418  0.72499834]. \t  1.0225118800098472 \t 1.0311612112101831\n",
      "86     \t [ 0.12433889 -0.6620437 ]. \t  1.005751200715869 \t 1.0311612112101831\n",
      "87     \t [-0.02264477  0.67166502]. \t  1.0036075283799184 \t 1.0311612112101831\n",
      "88     \t [-0.1268877   0.75029219]. \t  1.015498925750298 \t 1.0311612112101831\n",
      "89     \t [ 0.03255992 -0.72519054]. \t  1.0166903998406578 \t 1.0311612112101831\n",
      "90     \t [-0.01135319  0.74885594]. \t  0.9932069881541561 \t 1.0311612112101831\n",
      "91     \t [-0.09334623  0.64079282]. \t  0.9931623441447289 \t 1.0311612112101831\n",
      "92     \t [0.99437182 0.96647571]. \t  -2.9389361651749386 \t 1.0311612112101831\n",
      "93     \t [-0.03125773  0.71274283]. \t  1.0181163947936305 \t 1.0311612112101831\n",
      "94     \t [-1.06195398  0.8395533 ]. \t  -0.5945609203739042 \t 1.0311612112101831\n",
      "95     \t [-0.0901548  0.6430284]. \t  0.9956600762517612 \t 1.0311612112101831\n",
      "96     \t [ 0.06235045 -0.75365511]. \t  1.0129783042698357 \t 1.0311612112101831\n",
      "97     \t [ 0.18522322 -0.66690982]. \t  0.9765529094220969 \t 1.0311612112101831\n",
      "98     \t [ 0.16708223 -0.79828431]. \t  0.9479837159369844 \t 1.0311612112101831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     \t [-0.19005316  0.79932048]. \t  0.9329691197710107 \t 1.0311612112101831\n",
      "100    \t [-0.18421002  0.67506483]. \t  0.9831799249273057 \t 1.0311612112101831\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \n",
    "\n",
    "np.random.seed(run_num_19)\n",
    "surrogate_winner_19 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_19 = GPGO(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_19.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [0.42310371 0.25811502]. \t  -0.5111512856704337 \t 0.9146183273252478\n",
      "init   \t [-0.069349   -0.65408899]. \t  0.9146183273252478 \t 0.9146183273252478\n",
      "init   \t [-0.74479093  0.12814348]. \t  -1.4695211123708676 \t 0.9146183273252478\n",
      "init   \t [-2.59136227  0.33811624]. \t  -31.819735064764004 \t 0.9146183273252478\n",
      "init   \t [-1.57261342 -1.35697367]. \t  -10.421507729581124 \t 0.9146183273252478\n",
      "1      \t [ 3. -2.]. \t  -150.89999999999998 \t 0.9146183273252478\n",
      "2      \t [3. 2.]. \t  -162.89999999999998 \t 0.9146183273252478\n",
      "3      \t [-0.57997508  2.        ]. \t  -47.960614932144644 \t 0.9146183273252478\n",
      "4      \t [-3. -2.]. \t  -162.89999999999998 \t 0.9146183273252478\n",
      "5      \t [-0.40777323 -2.        ]. \t  -49.42413268345838 \t 0.9146183273252478\n",
      "6      \t [-3.  2.]. \t  -150.89999999999998 \t 0.9146183273252478\n",
      "7      \t [ 1.69566631 -0.19763819]. \t  -1.5781968711763217 \t 0.9146183273252478\n",
      "8      \t [-1.51818397 -0.47066517]. \t  -2.1696129768610284 \t 0.9146183273252478\n",
      "9      \t [0.93333625 2.        ]. \t  -51.97791342982089 \t 0.9146183273252478\n",
      "10     \t [3.00000000e+00 3.73154143e-04]. \t  -108.90111890545401 \t 0.9146183273252478\n",
      "11     \t [ 1.07525667 -0.90410441]. \t  -0.7635531809998088 \t 0.9146183273252478\n",
      "12     \t [ 1.01884136 -0.22554935]. \t  -1.8392532980058545 \t 0.9146183273252478\n",
      "13     \t [ 1.18187135 -2.        ]. \t  -48.03466247663923 \t 0.9146183273252478\n",
      "14     \t [-0.92637998 -0.88188471]. \t  -2.2222775192865694 \t 0.9146183273252478\n",
      "15     \t [-3.         -0.39336047]. \t  -109.55692010940986 \t 0.9146183273252478\n",
      "16     \t [-1.63162904  0.79732729]. \t  0.17252290170486995 \t 0.9146183273252478\n",
      "17     \t [1.42198527 0.80525817]. \t  -2.491008649375112 \t 0.9146183273252478\n",
      "18     \t [-0.10512925  1.00901234]. \t  -0.011610803099130866 \t 0.9146183273252478\n",
      "19     \t [-1.67622227  0.31778167]. \t  -1.158356029847496 \t 0.9146183273252478\n",
      "20     \t [0.72697084 0.92142585]. \t  -1.7337473844985638 \t 0.9146183273252478\n",
      "21     \t [-0.89695054  0.89550137]. \t  -0.5938349249398956 \t 0.9146183273252478\n",
      "22     \t [ 0.45622589 -1.20457754]. \t  -2.8126936486641747 \t 0.9146183273252478\n",
      "23     \t [-0.17473229  0.57005659]. \t  0.8568801762049173 \t 0.9146183273252478\n",
      "24     \t [1.44701235 0.33238864]. \t  -2.3163904818252026 \t 0.9146183273252478\n",
      "25     \t [ 0.48135799 -0.66948467]. \t  0.49330788510223333 \t 0.9146183273252478\n",
      "26     \t [-1.54756708 -2.        ]. \t  -53.2087532899458 \t 0.9146183273252478\n",
      "27     \t [ 1.91818238 -0.90709216]. \t  -0.5686416574984068 \t 0.9146183273252478\n",
      "28     \t [ 1.63949488 -0.69406669]. \t  0.08389394949984741 \t 0.9146183273252478\n",
      "29     \t [-1.63878877  2.        ]. \t  -46.77526396725655 \t 0.9146183273252478\n",
      "30     \t [1.82133509 2.        ]. \t  -53.97080879729434 \t 0.9146183273252478\n",
      "31     \t [ 3.        -0.9277262]. \t  -105.63716981658509 \t 0.9146183273252478\n",
      "32     \t [2.19017175 0.88029702]. \t  -8.88870230496328 \t 0.9146183273252478\n",
      "33     \t [-3.          0.88264525]. \t  -105.5635680732849 \t 0.9146183273252478\n",
      "34     \t [-0.27660932 -0.24918326]. \t  -0.12988540096829684 \t 0.9146183273252478\n",
      "35     \t [ 1.70476445 -1.28995416]. \t  -4.2904746121754815 \t 0.9146183273252478\n",
      "36     \t [1.89773391 0.55890016]. \t  -2.9400326161352663 \t 0.9146183273252478\n",
      "37     \t [0.21346871 0.75819829]. \t  0.6377831313588918 \t 0.9146183273252478\n",
      "38     \t [-1.96395953  0.59112475]. \t  -1.2437206382436705 \t 0.9146183273252478\n",
      "39     \t [-2.01063445 -0.6028378 ]. \t  -4.160008306914914 \t 0.9146183273252478\n",
      "40     \t [-1.62516832 -0.86400742]. \t  -2.704173856122145 \t 0.9146183273252478\n",
      "41     \t [-1.9772897 -0.0639439]. \t  -3.56969501870345 \t 0.9146183273252478\n",
      "42     \t [-0.41919133 -1.20480146]. \t  -3.766656156848471 \t 0.9146183273252478\n",
      "43     \t [ 2.06022782 -2.        ]. \t  -48.51389431708152 \t 0.9146183273252478\n",
      "44     \t [-2.12483325 -1.40147238]. \t  -16.48287857984394 \t 0.9146183273252478\n",
      "45     \t [-1.14281968  1.37111715]. \t  -7.43493619322951 \t 0.9146183273252478\n",
      "46     \t [-0.54335609 -0.59667817]. \t  -0.4136006490722757 \t 0.9146183273252478\n",
      "47     \t [3.         0.97681231]. \t  -111.65549042773819 \t 0.9146183273252478\n",
      "48     \t [1.80490366 1.18637665]. \t  -6.703976394820856 \t 0.9146183273252478\n",
      "49     \t [-2.06874079  1.40969845]. \t  -9.7156043574265 \t 0.9146183273252478\n",
      "50     \t [-1.76657718  1.07116587]. \t  -0.9462285938365778 \t 0.9146183273252478\n",
      "51     \t [ 0.18172085 -0.85925453]. \t  0.7991500622242607 \t 0.9146183273252478\n",
      "52     \t [0.18850374 2.        ]. \t  -48.51650553133455 \t 0.9146183273252478\n",
      "53     \t [ 0.32370976 -2.        ]. \t  -47.749056951717264 \t 0.9146183273252478\n",
      "54     \t [-3.         -1.29647618]. \t  -117.36705999565524 \t 0.9146183273252478\n",
      "55     \t [ 2.12052549 -0.43483081]. \t  -4.296609436819249 \t 0.9146183273252478\n",
      "56     \t [0.40413763 1.27611091]. \t  -5.208145811577156 \t 0.9146183273252478\n",
      "57     \t [-2.28009141  2.        ]. \t  -54.31426993962437 \t 0.9146183273252478\n",
      "58     \t [ 0.21727809 -0.29083398]. \t  0.18871763884513262 \t 0.9146183273252478\n",
      "59     \t [-2.196878 -2.      ]. \t  -60.25638359391911 \t 0.9146183273252478\n",
      "60     \t [2.39759367 1.61531523]. \t  -37.587104260249916 \t 0.9146183273252478\n",
      "61     \t [ 2.39682059 -1.48848733]. \t  -24.076259745278914 \t 0.9146183273252478\n",
      "62     \t [-0.06971822  0.70487698]. \t  \u001b[92m1.0297101717431836\u001b[0m \t 1.0297101717431836\n",
      "63     \t [2.28954997 0.23535852]. \t  -11.607160317645434 \t 1.0297101717431836\n",
      "64     \t [ 0.0694307  -0.72503385]. \t  1.0284692491085041 \t 1.0297101717431836\n",
      "65     \t [-0.05766768  0.74905013]. \t  1.0149957930183453 \t 1.0297101717431836\n",
      "66     \t [0.0183836  0.44213154]. \t  0.6195914432280437 \t 1.0297101717431836\n",
      "67     \t [ 0.07553535 -0.72477145]. \t  1.0294327346451144 \t 1.0297101717431836\n",
      "68     \t [ 0.03668774 -0.65576391]. \t  0.9990930704019131 \t 1.0297101717431836\n",
      "69     \t [-2.25759113  1.03611455]. \t  -7.944476362292581 \t 1.0297101717431836\n",
      "70     \t [-0.06191727  0.73763594]. \t  1.0225866241922155 \t 1.0297101717431836\n",
      "71     \t [-0.01923759  0.67950007]. \t  1.0057305612526195 \t 1.0297101717431836\n",
      "72     \t [ 0.16523795 -0.71224059]. \t  1.0098212069320487 \t 1.0297101717431836\n",
      "73     \t [-0.07141801  0.64926151]. \t  1.0013978601241142 \t 1.0297101717431836\n",
      "74     \t [-0.09696589  0.73713169]. \t  1.0265310224562267 \t 1.0297101717431836\n",
      "75     \t [ 0.11372088 -0.74825596]. \t  1.019367266724171 \t 1.0297101717431836\n",
      "76     \t [ 0.03877641 -0.77628266]. \t  0.9819725941040793 \t 1.0297101717431836\n",
      "77     \t [ 0.11718712 -0.80899641]. \t  0.9448175182718249 \t 1.0297101717431836\n",
      "78     \t [-0.10309159  0.80403535]. \t  0.9547973992748012 \t 1.0297101717431836\n",
      "79     \t [-0.03263462  0.69274719]. \t  1.016733593672802 \t 1.0297101717431836\n",
      "80     \t [-0.04929418  0.65538876]. \t  1.0027379934780705 \t 1.0297101717431836\n",
      "81     \t [-0.0792509   0.70902467]. \t  \u001b[92m1.0311212728789847\u001b[0m \t 1.0311212728789847\n",
      "82     \t [-1.07811987e-04  6.81601059e-01]. \t  0.9950551335103035 \t 1.0311212728789847\n",
      "83     \t [-0.1193868   0.71898881]. \t  1.0281020795011726 \t 1.0311212728789847\n",
      "84     \t [ 0.07826195 -0.73821965]. \t  1.0252649087283676 \t 1.0311212728789847\n",
      "85     \t [ 0.10058659 -0.7177746 ]. \t  1.0310182711162967 \t 1.0311212728789847\n",
      "86     \t [-0.02045704 -0.70032104]. \t  0.9836350628695739 \t 1.0311212728789847\n",
      "87     \t [1.26089186 1.47984336]. \t  -14.680367856477046 \t 1.0311212728789847\n",
      "88     \t [-0.07891748  0.73531143]. \t  1.0265780289640363 \t 1.0311212728789847\n",
      "89     \t [ 0.1723977  -0.76922524]. \t  0.9819340107581157 \t 1.0311212728789847\n",
      "90     \t [-0.05215112  0.6713741 ]. \t  1.0144445452005888 \t 1.0311212728789847\n",
      "91     \t [ 0.05521062 -0.72994414]. \t  1.023819118246494 \t 1.0311212728789847\n",
      "92     \t [0.02157391 0.69335193]. \t  0.9816961410604975 \t 1.0311212728789847\n",
      "93     \t [-0.09910077  0.727064  ]. \t  1.0296940934114451 \t 1.0311212728789847\n",
      "94     \t [-0.04518916  0.72486025]. \t  1.022011146758731 \t 1.0311212728789847\n",
      "95     \t [2.40387016 2.        ]. \t  -70.11846425102279 \t 1.0311212728789847\n",
      "96     \t [-0.03991944 -0.76838742]. \t  0.9302549451543414 \t 1.0311212728789847\n",
      "97     \t [ 0.13685291 -0.75261082]. \t  1.0111684378861594 \t 1.0311212728789847\n",
      "98     \t [ 0.05174668 -0.66823151]. \t  1.0124482059323714 \t 1.0311212728789847\n",
      "99     \t [ 0.16676265 -0.673779  ]. \t  0.9942669343742494 \t 1.0311212728789847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    \t [-1.19412875  0.61462943]. \t  -0.7260914837089678 \t 1.0311212728789847\n"
     ]
    }
   ],
   "source": [
    "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \n",
    "\n",
    "np.random.seed(run_num_20)\n",
    "surrogate_winner_20 = tStudentProcess(cov_func, nu = df)\n",
    "\n",
    "winner_20 = GPGO(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
    "winner_20.run(max_iter = max_iter, init_evals = n_init) # run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.970904200660691, -6.924108928566968)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 1\n",
    "\n",
    "loser_output_1 = np.append(np.max(loser_1.GP.y[0:n_init]),loser_1.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_1 = np.append(np.max(winner_1.GP.y[0:n_init]),winner_1.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_1 = np.log(y_global_orig - loser_output_1)\n",
    "regret_winner_1 = np.log(y_global_orig - winner_output_1)\n",
    "\n",
    "train_regret_loser_1 = min_max_array(regret_loser_1)\n",
    "train_regret_winner_1 = min_max_array(regret_winner_1)\n",
    "\n",
    "min_train_regret_loser_1 = min(train_regret_loser_1)\n",
    "min_train_regret_winner_1 = min(train_regret_winner_1)\n",
    "\n",
    "min_train_regret_loser_1, min_train_regret_winner_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-6.699693111723787, -10.852496652835953)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 2\n",
    "\n",
    "loser_output_2 = np.append(np.max(loser_2.GP.y[0:n_init]),loser_2.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_2 = np.append(np.max(winner_2.GP.y[0:n_init]),winner_2.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_2 = np.log(y_global_orig - loser_output_2)\n",
    "regret_winner_2 = np.log(y_global_orig - winner_output_2)\n",
    "\n",
    "train_regret_loser_2 = min_max_array(regret_loser_2)\n",
    "train_regret_winner_2 = min_max_array(regret_winner_2)\n",
    "\n",
    "min_train_regret_loser_2 = min(train_regret_loser_2)\n",
    "min_train_regret_winner_2 = min(train_regret_winner_2)\n",
    "\n",
    "min_train_regret_loser_2, min_train_regret_winner_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-8.216499649930707, -10.389806613574194)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 3\n",
    "\n",
    "loser_output_3 = np.append(np.max(loser_3.GP.y[0:n_init]),loser_3.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_3 = np.append(np.max(winner_3.GP.y[0:n_init]),winner_3.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_3 = np.log(y_global_orig - loser_output_3)\n",
    "regret_winner_3 = np.log(y_global_orig - winner_output_3)\n",
    "\n",
    "train_regret_loser_3 = min_max_array(regret_loser_3)\n",
    "train_regret_winner_3 = min_max_array(regret_winner_3)\n",
    "\n",
    "min_train_regret_loser_3 = min(train_regret_loser_3)\n",
    "min_train_regret_winner_3 = min(train_regret_winner_3)\n",
    "\n",
    "min_train_regret_loser_3, min_train_regret_winner_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.948454147079031, -11.634506243531657)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 4\n",
    "\n",
    "loser_output_4 = np.append(np.max(loser_4.GP.y[0:n_init]),loser_4.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_4 = np.append(np.max(winner_4.GP.y[0:n_init]),winner_4.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_4 = np.log(y_global_orig - loser_output_4)\n",
    "regret_winner_4 = np.log(y_global_orig - winner_output_4)\n",
    "\n",
    "train_regret_loser_4 = min_max_array(regret_loser_4)\n",
    "train_regret_winner_4 = min_max_array(regret_winner_4)\n",
    "\n",
    "min_train_regret_loser_4 = min(train_regret_loser_4)\n",
    "min_train_regret_winner_4 = min(train_regret_winner_4)\n",
    "\n",
    "min_train_regret_loser_4, min_train_regret_winner_4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-8.655941354427242, -7.102727891736948)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 5\n",
    "\n",
    "loser_output_5 = np.append(np.max(loser_5.GP.y[0:n_init]),loser_5.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_5 = np.append(np.max(winner_5.GP.y[0:n_init]),winner_5.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_5 = np.log(y_global_orig - loser_output_5)\n",
    "regret_winner_5 = np.log(y_global_orig - winner_output_5)\n",
    "\n",
    "train_regret_loser_5 = min_max_array(regret_loser_5)\n",
    "train_regret_winner_5 = min_max_array(regret_winner_5)\n",
    "\n",
    "min_train_regret_loser_5 = min(train_regret_loser_5)\n",
    "min_train_regret_winner_5 = min(train_regret_winner_5)\n",
    "\n",
    "min_train_regret_loser_5, min_train_regret_winner_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.679629852384771, -8.284789420811906)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 6\n",
    "\n",
    "loser_output_6 = np.append(np.max(loser_6.GP.y[0:n_init]),loser_6.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_6 = np.append(np.max(winner_6.GP.y[0:n_init]),winner_6.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_6 = np.log(y_global_orig - loser_output_6)\n",
    "regret_winner_6 = np.log(y_global_orig - winner_output_6)\n",
    "\n",
    "train_regret_loser_6 = min_max_array(regret_loser_6)\n",
    "train_regret_winner_6 = min_max_array(regret_winner_6)\n",
    "\n",
    "min_train_regret_loser_6 = min(train_regret_loser_6)\n",
    "min_train_regret_winner_6 = min(train_regret_winner_6)\n",
    "\n",
    "min_train_regret_loser_6, min_train_regret_winner_6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-7.032257683360543, -7.647967113070212)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 7\n",
    "\n",
    "loser_output_7 = np.append(np.max(loser_7.GP.y[0:n_init]),loser_7.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_7 = np.append(np.max(winner_7.GP.y[0:n_init]),winner_7.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_7 = np.log(y_global_orig - loser_output_7)\n",
    "regret_winner_7 = np.log(y_global_orig - winner_output_7)\n",
    "\n",
    "train_regret_loser_7 = min_max_array(regret_loser_7)\n",
    "train_regret_winner_7 = min_max_array(regret_winner_7)\n",
    "\n",
    "min_train_regret_loser_7 = min(train_regret_loser_7)\n",
    "min_train_regret_winner_7 = min(train_regret_winner_7)\n",
    "\n",
    "min_train_regret_loser_7, min_train_regret_winner_7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-10.031280240575756, -7.760796568047024)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 8\n",
    "\n",
    "loser_output_8 = np.append(np.max(loser_8.GP.y[0:n_init]),loser_8.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_8 = np.append(np.max(winner_8.GP.y[0:n_init]),winner_8.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_8 = np.log(y_global_orig - loser_output_8)\n",
    "regret_winner_8 = np.log(y_global_orig - winner_output_8)\n",
    "\n",
    "train_regret_loser_8 = min_max_array(regret_loser_8)\n",
    "train_regret_winner_8 = min_max_array(regret_winner_8)\n",
    "\n",
    "min_train_regret_loser_8 = min(train_regret_loser_8)\n",
    "min_train_regret_winner_8 = min(train_regret_winner_8)\n",
    "\n",
    "min_train_regret_loser_8, min_train_regret_winner_8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-7.347088372812169, -10.670637691783321)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 9\n",
    "\n",
    "loser_output_9 = np.append(np.max(loser_9.GP.y[0:n_init]),loser_9.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_9 = np.append(np.max(winner_9.GP.y[0:n_init]),winner_9.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_9 = np.log(y_global_orig - loser_output_9)\n",
    "regret_winner_9 = np.log(y_global_orig - winner_output_9)\n",
    "\n",
    "train_regret_loser_9 = min_max_array(regret_loser_9)\n",
    "train_regret_winner_9 = min_max_array(regret_winner_9)\n",
    "\n",
    "min_train_regret_loser_9 = min(train_regret_loser_9)\n",
    "min_train_regret_winner_9 = min(train_regret_winner_9)\n",
    "\n",
    "min_train_regret_loser_9, min_train_regret_winner_9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-8.676652568596023, -9.63179024509429)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 10\n",
    "\n",
    "loser_output_10 = np.append(np.max(loser_10.GP.y[0:n_init]),loser_10.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_10 = np.append(np.max(winner_10.GP.y[0:n_init]),winner_10.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_10 = np.log(y_global_orig - loser_output_10)\n",
    "regret_winner_10 = np.log(y_global_orig - winner_output_10)\n",
    "\n",
    "train_regret_loser_10 = min_max_array(regret_loser_10)\n",
    "train_regret_winner_10 = min_max_array(regret_winner_10)\n",
    "\n",
    "min_train_regret_loser_10 = min(train_regret_loser_10)\n",
    "min_train_regret_winner_10 = min(train_regret_winner_10)\n",
    "\n",
    "min_train_regret_loser_10, min_train_regret_winner_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.80460924679021, -8.816464912753915)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 11\n",
    "\n",
    "loser_output_11 = np.append(np.max(loser_11.GP.y[0:n_init]),loser_11.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_11 = np.append(np.max(winner_11.GP.y[0:n_init]),winner_11.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_11 = np.log(y_global_orig - loser_output_11)\n",
    "regret_winner_11 = np.log(y_global_orig - winner_output_11)\n",
    "\n",
    "train_regret_loser_11 = min_max_array(regret_loser_11)\n",
    "train_regret_winner_11 = min_max_array(regret_winner_11)\n",
    "\n",
    "min_train_regret_loser_11 = min(train_regret_loser_11)\n",
    "min_train_regret_winner_11 = min(train_regret_winner_11)\n",
    "\n",
    "min_train_regret_loser_11, min_train_regret_winner_11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.982088148036611, -9.05687424024136)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 12\n",
    "\n",
    "loser_output_12 = np.append(np.max(loser_12.GP.y[0:n_init]),loser_12.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_12 = np.append(np.max(winner_12.GP.y[0:n_init]),winner_12.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_12 = np.log(y_global_orig - loser_output_12)\n",
    "regret_winner_12 = np.log(y_global_orig - winner_output_12)\n",
    "\n",
    "train_regret_loser_12 = min_max_array(regret_loser_12)\n",
    "train_regret_winner_12 = min_max_array(regret_winner_12)\n",
    "\n",
    "min_train_regret_loser_12 = min(train_regret_loser_12)\n",
    "min_train_regret_winner_12 = min(train_regret_winner_12)\n",
    "\n",
    "min_train_regret_loser_12, min_train_regret_winner_12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-8.825641965209888, -6.876971186554067)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 13\n",
    "\n",
    "loser_output_13 = np.append(np.max(loser_13.GP.y[0:n_init]),loser_13.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_13 = np.append(np.max(winner_13.GP.y[0:n_init]),winner_13.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_13 = np.log(y_global_orig - loser_output_13)\n",
    "regret_winner_13 = np.log(y_global_orig - winner_output_13)\n",
    "\n",
    "train_regret_loser_13 = min_max_array(regret_loser_13)\n",
    "train_regret_winner_13 = min_max_array(regret_winner_13)\n",
    "\n",
    "min_train_regret_loser_13 = min(train_regret_loser_13)\n",
    "min_train_regret_winner_13 = min(train_regret_winner_13)\n",
    "\n",
    "min_train_regret_loser_13, min_train_regret_winner_13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-7.04607920243526, -6.4710040425314554)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 14\n",
    "\n",
    "loser_output_14 = np.append(np.max(loser_14.GP.y[0:n_init]),loser_14.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_14 = np.append(np.max(winner_14.GP.y[0:n_init]),winner_14.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_14 = np.log(y_global_orig - loser_output_14)\n",
    "regret_winner_14 = np.log(y_global_orig - winner_output_14)\n",
    "\n",
    "train_regret_loser_14 = min_max_array(regret_loser_14)\n",
    "train_regret_winner_14 = min_max_array(regret_winner_14)\n",
    "\n",
    "min_train_regret_loser_14 = min(train_regret_loser_14)\n",
    "min_train_regret_winner_14 = min(train_regret_winner_14)\n",
    "\n",
    "min_train_regret_loser_14, min_train_regret_winner_14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-9.278743571125627, -6.739078598380956)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 15\n",
    "\n",
    "loser_output_15 = np.append(np.max(loser_15.GP.y[0:n_init]),loser_15.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_15 = np.append(np.max(winner_15.GP.y[0:n_init]),winner_15.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_15 = np.log(y_global_orig - loser_output_15)\n",
    "regret_winner_15 = np.log(y_global_orig - winner_output_15)\n",
    "\n",
    "train_regret_loser_15 = min_max_array(regret_loser_15)\n",
    "train_regret_winner_15 = min_max_array(regret_winner_15)\n",
    "\n",
    "min_train_regret_loser_15 = min(train_regret_loser_15)\n",
    "min_train_regret_winner_15 = min(train_regret_winner_15)\n",
    "\n",
    "min_train_regret_loser_15, min_train_regret_winner_15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-7.607771789777762, -8.117705122832717)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 16\n",
    "\n",
    "loser_output_16 = np.append(np.max(loser_16.GP.y[0:n_init]),loser_16.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_16 = np.append(np.max(winner_16.GP.y[0:n_init]),winner_16.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_16 = np.log(y_global_orig - loser_output_16)\n",
    "regret_winner_16 = np.log(y_global_orig - winner_output_16)\n",
    "\n",
    "train_regret_loser_16 = min_max_array(regret_loser_16)\n",
    "train_regret_winner_16 = min_max_array(regret_winner_16)\n",
    "\n",
    "min_train_regret_loser_16 = min(train_regret_loser_16)\n",
    "min_train_regret_winner_16 = min(train_regret_winner_16)\n",
    "\n",
    "min_train_regret_loser_16, min_train_regret_winner_16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-10.339251078685901, -8.413451719485034)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 17\n",
    "\n",
    "loser_output_17 = np.append(np.max(loser_17.GP.y[0:n_init]),loser_17.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_17 = np.append(np.max(winner_17.GP.y[0:n_init]),winner_17.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_17 = np.log(y_global_orig - loser_output_17)\n",
    "regret_winner_17 = np.log(y_global_orig - winner_output_17)\n",
    "\n",
    "train_regret_loser_17 = min_max_array(regret_loser_17)\n",
    "train_regret_winner_17 = min_max_array(regret_winner_17)\n",
    "\n",
    "min_train_regret_loser_17 = min(train_regret_loser_17)\n",
    "min_train_regret_winner_17 = min(train_regret_winner_17)\n",
    "\n",
    "min_train_regret_loser_17, min_train_regret_winner_17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-6.403375308397165, -7.227274536943739)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 18\n",
    "\n",
    "loser_output_18 = np.append(np.max(loser_18.GP.y[0:n_init]),loser_18.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_18 = np.append(np.max(winner_18.GP.y[0:n_init]),winner_18.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_18 = np.log(y_global_orig - loser_output_18)\n",
    "regret_winner_18 = np.log(y_global_orig - winner_output_18)\n",
    "\n",
    "train_regret_loser_18 = min_max_array(regret_loser_18)\n",
    "train_regret_winner_18 = min_max_array(regret_winner_18)\n",
    "\n",
    "min_train_regret_loser_18 = min(train_regret_loser_18)\n",
    "min_train_regret_winner_18 = min(train_regret_winner_18)\n",
    "\n",
    "min_train_regret_loser_18, min_train_regret_winner_18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.741692009347558, -7.731492377252899)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 19\n",
    "\n",
    "loser_output_19 = np.append(np.max(loser_19.GP.y[0:n_init]),loser_19.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_19 = np.append(np.max(winner_19.GP.y[0:n_init]),winner_19.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_19 = np.log(y_global_orig - loser_output_19)\n",
    "regret_winner_19 = np.log(y_global_orig - winner_output_19)\n",
    "\n",
    "train_regret_loser_19 = min_max_array(regret_loser_19)\n",
    "train_regret_winner_19 = min_max_array(regret_winner_19)\n",
    "\n",
    "min_train_regret_loser_19 = min(train_regret_loser_19)\n",
    "min_train_regret_winner_19 = min(train_regret_winner_19)\n",
    "\n",
    "min_train_regret_loser_19, min_train_regret_winner_19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-8.208763597837319, -7.6443798076131735)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training regret minimization: run number = 20\n",
    "\n",
    "loser_output_20 = np.append(np.max(loser_20.GP.y[0:n_init]),loser_20.GP.y[n_init:(n_init+max_iter)]) \n",
    "winner_output_20 = np.append(np.max(winner_20.GP.y[0:n_init]),winner_20.GP.y[n_init:(n_init+max_iter)]) \n",
    "\n",
    "regret_loser_20 = np.log(y_global_orig - loser_output_20)\n",
    "regret_winner_20 = np.log(y_global_orig - winner_output_20)\n",
    "\n",
    "train_regret_loser_20 = min_max_array(regret_loser_20)\n",
    "train_regret_winner_20 = min_max_array(regret_winner_20)\n",
    "\n",
    "min_train_regret_loser_20 = min(train_regret_loser_20)\n",
    "min_train_regret_winner_20 = min(train_regret_winner_20)\n",
    "\n",
    "min_train_regret_loser_20, min_train_regret_winner_20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration1 :\n",
    "\n",
    "slice1 = 0\n",
    "\n",
    "loser1 = [train_regret_loser_1[slice1],\n",
    "       train_regret_loser_2[slice1],\n",
    "       train_regret_loser_3[slice1],\n",
    "       train_regret_loser_4[slice1],\n",
    "       train_regret_loser_5[slice1],\n",
    "       train_regret_loser_6[slice1],\n",
    "       train_regret_loser_7[slice1],\n",
    "       train_regret_loser_8[slice1],\n",
    "       train_regret_loser_9[slice1],\n",
    "       train_regret_loser_10[slice1],\n",
    "       train_regret_loser_11[slice1],\n",
    "       train_regret_loser_12[slice1],\n",
    "       train_regret_loser_13[slice1],\n",
    "       train_regret_loser_14[slice1],\n",
    "       train_regret_loser_15[slice1],\n",
    "       train_regret_loser_16[slice1],\n",
    "       train_regret_loser_17[slice1],\n",
    "       train_regret_loser_18[slice1],\n",
    "       train_regret_loser_19[slice1],\n",
    "       train_regret_loser_20[slice1]]\n",
    "\n",
    "winner1 = [train_regret_winner_1[slice1],\n",
    "       train_regret_winner_2[slice1],\n",
    "       train_regret_winner_3[slice1],\n",
    "       train_regret_winner_4[slice1],\n",
    "       train_regret_winner_5[slice1],\n",
    "       train_regret_winner_6[slice1],\n",
    "       train_regret_winner_7[slice1],\n",
    "       train_regret_winner_8[slice1],\n",
    "       train_regret_winner_9[slice1],\n",
    "       train_regret_winner_10[slice1],\n",
    "       train_regret_winner_11[slice1],\n",
    "       train_regret_winner_12[slice1],\n",
    "       train_regret_winner_13[slice1],\n",
    "       train_regret_winner_14[slice1],\n",
    "       train_regret_winner_15[slice1],\n",
    "       train_regret_winner_16[slice1],\n",
    "       train_regret_winner_17[slice1],\n",
    "       train_regret_winner_18[slice1],\n",
    "       train_regret_winner_19[slice1],\n",
    "       train_regret_winner_20[slice1]]\n",
    "\n",
    "loser1_results = pd.DataFrame(loser1).sort_values(by=[0], ascending=False)\n",
    "winner1_results = pd.DataFrame(winner1).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser1 = np.asarray(loser1_results[4:5][0])[0]\n",
    "median_loser1 = np.asarray(loser1_results[9:10][0])[0]\n",
    "upper_loser1 = np.asarray(loser1_results[14:15][0])[0]\n",
    "\n",
    "lower_winner1 = np.asarray(winner1_results[4:5][0])[0]\n",
    "median_winner1 = np.asarray(winner1_results[9:10][0])[0]\n",
    "upper_winner1 = np.asarray(winner1_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration11 :\n",
    "\n",
    "slice11 = 10\n",
    "\n",
    "loser11 = [train_regret_loser_1[slice11],\n",
    "       train_regret_loser_2[slice11],\n",
    "       train_regret_loser_3[slice11],\n",
    "       train_regret_loser_4[slice11],\n",
    "       train_regret_loser_5[slice11],\n",
    "       train_regret_loser_6[slice11],\n",
    "       train_regret_loser_7[slice11],\n",
    "       train_regret_loser_8[slice11],\n",
    "       train_regret_loser_9[slice11],\n",
    "       train_regret_loser_10[slice11],\n",
    "       train_regret_loser_11[slice11],\n",
    "       train_regret_loser_12[slice11],\n",
    "       train_regret_loser_13[slice11],\n",
    "       train_regret_loser_14[slice11],\n",
    "       train_regret_loser_15[slice11],\n",
    "       train_regret_loser_16[slice11],\n",
    "       train_regret_loser_17[slice11],\n",
    "       train_regret_loser_18[slice11],\n",
    "       train_regret_loser_19[slice11],\n",
    "       train_regret_loser_20[slice11]]\n",
    "\n",
    "winner11 = [train_regret_winner_1[slice11],\n",
    "       train_regret_winner_2[slice11],\n",
    "       train_regret_winner_3[slice11],\n",
    "       train_regret_winner_4[slice11],\n",
    "       train_regret_winner_5[slice11],\n",
    "       train_regret_winner_6[slice11],\n",
    "       train_regret_winner_7[slice11],\n",
    "       train_regret_winner_8[slice11],\n",
    "       train_regret_winner_9[slice11],\n",
    "       train_regret_winner_10[slice11],\n",
    "       train_regret_winner_11[slice11],\n",
    "       train_regret_winner_12[slice11],\n",
    "       train_regret_winner_13[slice11],\n",
    "       train_regret_winner_14[slice11],\n",
    "       train_regret_winner_15[slice11],\n",
    "       train_regret_winner_16[slice11],\n",
    "       train_regret_winner_17[slice11],\n",
    "       train_regret_winner_18[slice11],\n",
    "       train_regret_winner_19[slice11],\n",
    "       train_regret_winner_20[slice11]]\n",
    "\n",
    "loser11_results = pd.DataFrame(loser11).sort_values(by=[0], ascending=False)\n",
    "winner11_results = pd.DataFrame(winner11).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser11 = np.asarray(loser11_results[4:5][0])[0]\n",
    "median_loser11 = np.asarray(loser11_results[9:10][0])[0]\n",
    "upper_loser11 = np.asarray(loser11_results[14:15][0])[0]\n",
    "\n",
    "lower_winner11 = np.asarray(winner11_results[4:5][0])[0]\n",
    "median_winner11 = np.asarray(winner11_results[9:10][0])[0]\n",
    "upper_winner11 = np.asarray(winner11_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration21 :\n",
    "\n",
    "slice21 = 20\n",
    "\n",
    "loser21 = [train_regret_loser_1[slice21],\n",
    "       train_regret_loser_2[slice21],\n",
    "       train_regret_loser_3[slice21],\n",
    "       train_regret_loser_4[slice21],\n",
    "       train_regret_loser_5[slice21],\n",
    "       train_regret_loser_6[slice21],\n",
    "       train_regret_loser_7[slice21],\n",
    "       train_regret_loser_8[slice21],\n",
    "       train_regret_loser_9[slice21],\n",
    "       train_regret_loser_10[slice21],\n",
    "       train_regret_loser_11[slice21],\n",
    "       train_regret_loser_12[slice21],\n",
    "       train_regret_loser_13[slice21],\n",
    "       train_regret_loser_14[slice21],\n",
    "       train_regret_loser_15[slice21],\n",
    "       train_regret_loser_16[slice21],\n",
    "       train_regret_loser_17[slice21],\n",
    "       train_regret_loser_18[slice21],\n",
    "       train_regret_loser_19[slice21],\n",
    "       train_regret_loser_20[slice21]]\n",
    "\n",
    "winner21 = [train_regret_winner_1[slice21],\n",
    "       train_regret_winner_2[slice21],\n",
    "       train_regret_winner_3[slice21],\n",
    "       train_regret_winner_4[slice21],\n",
    "       train_regret_winner_5[slice21],\n",
    "       train_regret_winner_6[slice21],\n",
    "       train_regret_winner_7[slice21],\n",
    "       train_regret_winner_8[slice21],\n",
    "       train_regret_winner_9[slice21],\n",
    "       train_regret_winner_10[slice21],\n",
    "       train_regret_winner_11[slice21],\n",
    "       train_regret_winner_12[slice21],\n",
    "       train_regret_winner_13[slice21],\n",
    "       train_regret_winner_14[slice21],\n",
    "       train_regret_winner_15[slice21],\n",
    "       train_regret_winner_16[slice21],\n",
    "       train_regret_winner_17[slice21],\n",
    "       train_regret_winner_18[slice21],\n",
    "       train_regret_winner_19[slice21],\n",
    "       train_regret_winner_20[slice21]]\n",
    "\n",
    "loser21_results = pd.DataFrame(loser21).sort_values(by=[0], ascending=False)\n",
    "winner21_results = pd.DataFrame(winner21).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser21 = np.asarray(loser21_results[4:5][0])[0]\n",
    "median_loser21 = np.asarray(loser21_results[9:10][0])[0]\n",
    "upper_loser21 = np.asarray(loser21_results[14:15][0])[0]\n",
    "\n",
    "lower_winner21 = np.asarray(winner21_results[4:5][0])[0]\n",
    "median_winner21 = np.asarray(winner21_results[9:10][0])[0]\n",
    "upper_winner21 = np.asarray(winner21_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration31 :\n",
    "\n",
    "slice31 = 30\n",
    "\n",
    "loser31 = [train_regret_loser_1[slice31],\n",
    "       train_regret_loser_2[slice31],\n",
    "       train_regret_loser_3[slice31],\n",
    "       train_regret_loser_4[slice31],\n",
    "       train_regret_loser_5[slice31],\n",
    "       train_regret_loser_6[slice31],\n",
    "       train_regret_loser_7[slice31],\n",
    "       train_regret_loser_8[slice31],\n",
    "       train_regret_loser_9[slice31],\n",
    "       train_regret_loser_10[slice31],\n",
    "       train_regret_loser_11[slice31],\n",
    "       train_regret_loser_12[slice31],\n",
    "       train_regret_loser_13[slice31],\n",
    "       train_regret_loser_14[slice31],\n",
    "       train_regret_loser_15[slice31],\n",
    "       train_regret_loser_16[slice31],\n",
    "       train_regret_loser_17[slice31],\n",
    "       train_regret_loser_18[slice31],\n",
    "       train_regret_loser_19[slice31],\n",
    "       train_regret_loser_20[slice31]]\n",
    "\n",
    "winner31 = [train_regret_winner_1[slice31],\n",
    "       train_regret_winner_2[slice31],\n",
    "       train_regret_winner_3[slice31],\n",
    "       train_regret_winner_4[slice31],\n",
    "       train_regret_winner_5[slice31],\n",
    "       train_regret_winner_6[slice31],\n",
    "       train_regret_winner_7[slice31],\n",
    "       train_regret_winner_8[slice31],\n",
    "       train_regret_winner_9[slice31],\n",
    "       train_regret_winner_10[slice31],\n",
    "       train_regret_winner_11[slice31],\n",
    "       train_regret_winner_12[slice31],\n",
    "       train_regret_winner_13[slice31],\n",
    "       train_regret_winner_14[slice31],\n",
    "       train_regret_winner_15[slice31],\n",
    "       train_regret_winner_16[slice31],\n",
    "       train_regret_winner_17[slice31],\n",
    "       train_regret_winner_18[slice31],\n",
    "       train_regret_winner_19[slice31],\n",
    "       train_regret_winner_20[slice31]]\n",
    "\n",
    "loser31_results = pd.DataFrame(loser31).sort_values(by=[0], ascending=False)\n",
    "winner31_results = pd.DataFrame(winner31).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser31 = np.asarray(loser31_results[4:5][0])[0]\n",
    "median_loser31 = np.asarray(loser31_results[9:10][0])[0]\n",
    "upper_loser31 = np.asarray(loser31_results[14:15][0])[0]\n",
    "\n",
    "lower_winner31 = np.asarray(winner31_results[4:5][0])[0]\n",
    "median_winner31 = np.asarray(winner31_results[9:10][0])[0]\n",
    "upper_winner31 = np.asarray(winner31_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration41 :\n",
    "\n",
    "slice41 = 40\n",
    "\n",
    "loser41 = [train_regret_loser_1[slice41],\n",
    "       train_regret_loser_2[slice41],\n",
    "       train_regret_loser_3[slice41],\n",
    "       train_regret_loser_4[slice41],\n",
    "       train_regret_loser_5[slice41],\n",
    "       train_regret_loser_6[slice41],\n",
    "       train_regret_loser_7[slice41],\n",
    "       train_regret_loser_8[slice41],\n",
    "       train_regret_loser_9[slice41],\n",
    "       train_regret_loser_10[slice41],\n",
    "       train_regret_loser_11[slice41],\n",
    "       train_regret_loser_12[slice41],\n",
    "       train_regret_loser_13[slice41],\n",
    "       train_regret_loser_14[slice41],\n",
    "       train_regret_loser_15[slice41],\n",
    "       train_regret_loser_16[slice41],\n",
    "       train_regret_loser_17[slice41],\n",
    "       train_regret_loser_18[slice41],\n",
    "       train_regret_loser_19[slice41],\n",
    "       train_regret_loser_20[slice41]]\n",
    "\n",
    "winner41 = [train_regret_winner_1[slice41],\n",
    "       train_regret_winner_2[slice41],\n",
    "       train_regret_winner_3[slice41],\n",
    "       train_regret_winner_4[slice41],\n",
    "       train_regret_winner_5[slice41],\n",
    "       train_regret_winner_6[slice41],\n",
    "       train_regret_winner_7[slice41],\n",
    "       train_regret_winner_8[slice41],\n",
    "       train_regret_winner_9[slice41],\n",
    "       train_regret_winner_10[slice41],\n",
    "       train_regret_winner_11[slice41],\n",
    "       train_regret_winner_12[slice41],\n",
    "       train_regret_winner_13[slice41],\n",
    "       train_regret_winner_14[slice41],\n",
    "       train_regret_winner_15[slice41],\n",
    "       train_regret_winner_16[slice41],\n",
    "       train_regret_winner_17[slice41],\n",
    "       train_regret_winner_18[slice41],\n",
    "       train_regret_winner_19[slice41],\n",
    "       train_regret_winner_20[slice41]]\n",
    "\n",
    "loser41_results = pd.DataFrame(loser41).sort_values(by=[0], ascending=False)\n",
    "winner41_results = pd.DataFrame(winner41).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser41 = np.asarray(loser41_results[4:5][0])[0]\n",
    "median_loser41 = np.asarray(loser41_results[9:10][0])[0]\n",
    "upper_loser41 = np.asarray(loser41_results[14:15][0])[0]\n",
    "\n",
    "lower_winner41 = np.asarray(winner41_results[4:5][0])[0]\n",
    "median_winner41 = np.asarray(winner41_results[9:10][0])[0]\n",
    "upper_winner41 = np.asarray(winner41_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration51 :\n",
    "\n",
    "slice51 = 50\n",
    "\n",
    "loser51 = [train_regret_loser_1[slice51],\n",
    "       train_regret_loser_2[slice51],\n",
    "       train_regret_loser_3[slice51],\n",
    "       train_regret_loser_4[slice51],\n",
    "       train_regret_loser_5[slice51],\n",
    "       train_regret_loser_6[slice51],\n",
    "       train_regret_loser_7[slice51],\n",
    "       train_regret_loser_8[slice51],\n",
    "       train_regret_loser_9[slice51],\n",
    "       train_regret_loser_10[slice51],\n",
    "       train_regret_loser_11[slice51],\n",
    "       train_regret_loser_12[slice51],\n",
    "       train_regret_loser_13[slice51],\n",
    "       train_regret_loser_14[slice51],\n",
    "       train_regret_loser_15[slice51],\n",
    "       train_regret_loser_16[slice51],\n",
    "       train_regret_loser_17[slice51],\n",
    "       train_regret_loser_18[slice51],\n",
    "       train_regret_loser_19[slice51],\n",
    "       train_regret_loser_20[slice51]]\n",
    "\n",
    "winner51 = [train_regret_winner_1[slice51],\n",
    "       train_regret_winner_2[slice51],\n",
    "       train_regret_winner_3[slice51],\n",
    "       train_regret_winner_4[slice51],\n",
    "       train_regret_winner_5[slice51],\n",
    "       train_regret_winner_6[slice51],\n",
    "       train_regret_winner_7[slice51],\n",
    "       train_regret_winner_8[slice51],\n",
    "       train_regret_winner_9[slice51],\n",
    "       train_regret_winner_10[slice51],\n",
    "       train_regret_winner_11[slice51],\n",
    "       train_regret_winner_12[slice51],\n",
    "       train_regret_winner_13[slice51],\n",
    "       train_regret_winner_14[slice51],\n",
    "       train_regret_winner_15[slice51],\n",
    "       train_regret_winner_16[slice51],\n",
    "       train_regret_winner_17[slice51],\n",
    "       train_regret_winner_18[slice51],\n",
    "       train_regret_winner_19[slice51],\n",
    "       train_regret_winner_20[slice51]]\n",
    "\n",
    "loser51_results = pd.DataFrame(loser51).sort_values(by=[0], ascending=False)\n",
    "winner51_results = pd.DataFrame(winner51).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser51 = np.asarray(loser51_results[4:5][0])[0]\n",
    "median_loser51 = np.asarray(loser51_results[9:10][0])[0]\n",
    "upper_loser51 = np.asarray(loser51_results[14:15][0])[0]\n",
    "\n",
    "lower_winner51 = np.asarray(winner51_results[4:5][0])[0]\n",
    "median_winner51 = np.asarray(winner51_results[9:10][0])[0]\n",
    "upper_winner51 = np.asarray(winner51_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration61 :\n",
    "\n",
    "slice61 = 60\n",
    "\n",
    "loser61 = [train_regret_loser_1[slice61],\n",
    "       train_regret_loser_2[slice61],\n",
    "       train_regret_loser_3[slice61],\n",
    "       train_regret_loser_4[slice61],\n",
    "       train_regret_loser_5[slice61],\n",
    "       train_regret_loser_6[slice61],\n",
    "       train_regret_loser_7[slice61],\n",
    "       train_regret_loser_8[slice61],\n",
    "       train_regret_loser_9[slice61],\n",
    "       train_regret_loser_10[slice61],\n",
    "       train_regret_loser_11[slice61],\n",
    "       train_regret_loser_12[slice61],\n",
    "       train_regret_loser_13[slice61],\n",
    "       train_regret_loser_14[slice61],\n",
    "       train_regret_loser_15[slice61],\n",
    "       train_regret_loser_16[slice61],\n",
    "       train_regret_loser_17[slice61],\n",
    "       train_regret_loser_18[slice61],\n",
    "       train_regret_loser_19[slice61],\n",
    "       train_regret_loser_20[slice61]]\n",
    "\n",
    "winner61 = [train_regret_winner_1[slice61],\n",
    "       train_regret_winner_2[slice61],\n",
    "       train_regret_winner_3[slice61],\n",
    "       train_regret_winner_4[slice61],\n",
    "       train_regret_winner_5[slice61],\n",
    "       train_regret_winner_6[slice61],\n",
    "       train_regret_winner_7[slice61],\n",
    "       train_regret_winner_8[slice61],\n",
    "       train_regret_winner_9[slice61],\n",
    "       train_regret_winner_10[slice61],\n",
    "       train_regret_winner_11[slice61],\n",
    "       train_regret_winner_12[slice61],\n",
    "       train_regret_winner_13[slice61],\n",
    "       train_regret_winner_14[slice61],\n",
    "       train_regret_winner_15[slice61],\n",
    "       train_regret_winner_16[slice61],\n",
    "       train_regret_winner_17[slice61],\n",
    "       train_regret_winner_18[slice61],\n",
    "       train_regret_winner_19[slice61],\n",
    "       train_regret_winner_20[slice61]]\n",
    "\n",
    "loser61_results = pd.DataFrame(loser61).sort_values(by=[0], ascending=False)\n",
    "winner61_results = pd.DataFrame(winner61).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser61 = np.asarray(loser61_results[4:5][0])[0]\n",
    "median_loser61 = np.asarray(loser61_results[9:10][0])[0]\n",
    "upper_loser61 = np.asarray(loser61_results[14:15][0])[0]\n",
    "\n",
    "lower_winner61 = np.asarray(winner61_results[4:5][0])[0]\n",
    "median_winner61 = np.asarray(winner61_results[9:10][0])[0]\n",
    "upper_winner61 = np.asarray(winner61_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration71 :\n",
    "\n",
    "slice71 = 70\n",
    "\n",
    "loser71 = [train_regret_loser_1[slice71],\n",
    "       train_regret_loser_2[slice71],\n",
    "       train_regret_loser_3[slice71],\n",
    "       train_regret_loser_4[slice71],\n",
    "       train_regret_loser_5[slice71],\n",
    "       train_regret_loser_6[slice71],\n",
    "       train_regret_loser_7[slice71],\n",
    "       train_regret_loser_8[slice71],\n",
    "       train_regret_loser_9[slice71],\n",
    "       train_regret_loser_10[slice71],\n",
    "       train_regret_loser_11[slice71],\n",
    "       train_regret_loser_12[slice71],\n",
    "       train_regret_loser_13[slice71],\n",
    "       train_regret_loser_14[slice71],\n",
    "       train_regret_loser_15[slice71],\n",
    "       train_regret_loser_16[slice71],\n",
    "       train_regret_loser_17[slice71],\n",
    "       train_regret_loser_18[slice71],\n",
    "       train_regret_loser_19[slice71],\n",
    "       train_regret_loser_20[slice71]]\n",
    "\n",
    "winner71 = [train_regret_winner_1[slice71],\n",
    "       train_regret_winner_2[slice71],\n",
    "       train_regret_winner_3[slice71],\n",
    "       train_regret_winner_4[slice71],\n",
    "       train_regret_winner_5[slice71],\n",
    "       train_regret_winner_6[slice71],\n",
    "       train_regret_winner_7[slice71],\n",
    "       train_regret_winner_8[slice71],\n",
    "       train_regret_winner_9[slice71],\n",
    "       train_regret_winner_10[slice71],\n",
    "       train_regret_winner_11[slice71],\n",
    "       train_regret_winner_12[slice71],\n",
    "       train_regret_winner_13[slice71],\n",
    "       train_regret_winner_14[slice71],\n",
    "       train_regret_winner_15[slice71],\n",
    "       train_regret_winner_16[slice71],\n",
    "       train_regret_winner_17[slice71],\n",
    "       train_regret_winner_18[slice71],\n",
    "       train_regret_winner_19[slice71],\n",
    "       train_regret_winner_20[slice71]]\n",
    "\n",
    "loser71_results = pd.DataFrame(loser71).sort_values(by=[0], ascending=False)\n",
    "winner71_results = pd.DataFrame(winner71).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser71 = np.asarray(loser71_results[4:5][0])[0]\n",
    "median_loser71 = np.asarray(loser71_results[9:10][0])[0]\n",
    "upper_loser71 = np.asarray(loser71_results[14:15][0])[0]\n",
    "\n",
    "lower_winner71 = np.asarray(winner71_results[4:5][0])[0]\n",
    "median_winner71 = np.asarray(winner71_results[9:10][0])[0]\n",
    "upper_winner71 = np.asarray(winner71_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration81 :\n",
    "\n",
    "slice81 = 80\n",
    "\n",
    "loser81 = [train_regret_loser_1[slice81],\n",
    "       train_regret_loser_2[slice81],\n",
    "       train_regret_loser_3[slice81],\n",
    "       train_regret_loser_4[slice81],\n",
    "       train_regret_loser_5[slice81],\n",
    "       train_regret_loser_6[slice81],\n",
    "       train_regret_loser_7[slice81],\n",
    "       train_regret_loser_8[slice81],\n",
    "       train_regret_loser_9[slice81],\n",
    "       train_regret_loser_10[slice81],\n",
    "       train_regret_loser_11[slice81],\n",
    "       train_regret_loser_12[slice81],\n",
    "       train_regret_loser_13[slice81],\n",
    "       train_regret_loser_14[slice81],\n",
    "       train_regret_loser_15[slice81],\n",
    "       train_regret_loser_16[slice81],\n",
    "       train_regret_loser_17[slice81],\n",
    "       train_regret_loser_18[slice81],\n",
    "       train_regret_loser_19[slice81],\n",
    "       train_regret_loser_20[slice81]]\n",
    "\n",
    "winner81 = [train_regret_winner_1[slice81],\n",
    "       train_regret_winner_2[slice81],\n",
    "       train_regret_winner_3[slice81],\n",
    "       train_regret_winner_4[slice81],\n",
    "       train_regret_winner_5[slice81],\n",
    "       train_regret_winner_6[slice81],\n",
    "       train_regret_winner_7[slice81],\n",
    "       train_regret_winner_8[slice81],\n",
    "       train_regret_winner_9[slice81],\n",
    "       train_regret_winner_10[slice81],\n",
    "       train_regret_winner_11[slice81],\n",
    "       train_regret_winner_12[slice81],\n",
    "       train_regret_winner_13[slice81],\n",
    "       train_regret_winner_14[slice81],\n",
    "       train_regret_winner_15[slice81],\n",
    "       train_regret_winner_16[slice81],\n",
    "       train_regret_winner_17[slice81],\n",
    "       train_regret_winner_18[slice81],\n",
    "       train_regret_winner_19[slice81],\n",
    "       train_regret_winner_20[slice81]]\n",
    "\n",
    "loser81_results = pd.DataFrame(loser81).sort_values(by=[0], ascending=False)\n",
    "winner81_results = pd.DataFrame(winner81).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser81 = np.asarray(loser81_results[4:5][0])[0]\n",
    "median_loser81 = np.asarray(loser81_results[9:10][0])[0]\n",
    "upper_loser81 = np.asarray(loser81_results[14:15][0])[0]\n",
    "\n",
    "lower_winner81 = np.asarray(winner81_results[4:5][0])[0]\n",
    "median_winner81 = np.asarray(winner81_results[9:10][0])[0]\n",
    "upper_winner81 = np.asarray(winner81_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration91 :\n",
    "\n",
    "slice91 = 90\n",
    "\n",
    "loser91 = [train_regret_loser_1[slice91],\n",
    "       train_regret_loser_2[slice91],\n",
    "       train_regret_loser_3[slice91],\n",
    "       train_regret_loser_4[slice91],\n",
    "       train_regret_loser_5[slice91],\n",
    "       train_regret_loser_6[slice91],\n",
    "       train_regret_loser_7[slice91],\n",
    "       train_regret_loser_8[slice91],\n",
    "       train_regret_loser_9[slice91],\n",
    "       train_regret_loser_10[slice91],\n",
    "       train_regret_loser_11[slice91],\n",
    "       train_regret_loser_12[slice91],\n",
    "       train_regret_loser_13[slice91],\n",
    "       train_regret_loser_14[slice91],\n",
    "       train_regret_loser_15[slice91],\n",
    "       train_regret_loser_16[slice91],\n",
    "       train_regret_loser_17[slice91],\n",
    "       train_regret_loser_18[slice91],\n",
    "       train_regret_loser_19[slice91],\n",
    "       train_regret_loser_20[slice91]]\n",
    "\n",
    "winner91 = [train_regret_winner_1[slice91],\n",
    "       train_regret_winner_2[slice91],\n",
    "       train_regret_winner_3[slice91],\n",
    "       train_regret_winner_4[slice91],\n",
    "       train_regret_winner_5[slice91],\n",
    "       train_regret_winner_6[slice91],\n",
    "       train_regret_winner_7[slice91],\n",
    "       train_regret_winner_8[slice91],\n",
    "       train_regret_winner_9[slice91],\n",
    "       train_regret_winner_10[slice91],\n",
    "       train_regret_winner_11[slice91],\n",
    "       train_regret_winner_12[slice91],\n",
    "       train_regret_winner_13[slice91],\n",
    "       train_regret_winner_14[slice91],\n",
    "       train_regret_winner_15[slice91],\n",
    "       train_regret_winner_16[slice91],\n",
    "       train_regret_winner_17[slice91],\n",
    "       train_regret_winner_18[slice91],\n",
    "       train_regret_winner_19[slice91],\n",
    "       train_regret_winner_20[slice91]]\n",
    "\n",
    "loser91_results = pd.DataFrame(loser91).sort_values(by=[0], ascending=False)\n",
    "winner91_results = pd.DataFrame(winner91).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser91 = np.asarray(loser91_results[4:5][0])[0]\n",
    "median_loser91 = np.asarray(loser91_results[9:10][0])[0]\n",
    "upper_loser91 = np.asarray(loser91_results[14:15][0])[0]\n",
    "\n",
    "lower_winner91 = np.asarray(winner91_results[4:5][0])[0]\n",
    "median_winner91 = np.asarray(winner91_results[9:10][0])[0]\n",
    "upper_winner91 = np.asarray(winner91_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration101 :\n",
    "\n",
    "slice101 = 100\n",
    "\n",
    "loser101 = [train_regret_loser_1[slice101],\n",
    "       train_regret_loser_2[slice101],\n",
    "       train_regret_loser_3[slice101],\n",
    "       train_regret_loser_4[slice101],\n",
    "       train_regret_loser_5[slice101],\n",
    "       train_regret_loser_6[slice101],\n",
    "       train_regret_loser_7[slice101],\n",
    "       train_regret_loser_8[slice101],\n",
    "       train_regret_loser_9[slice101],\n",
    "       train_regret_loser_10[slice101],\n",
    "       train_regret_loser_11[slice101],\n",
    "       train_regret_loser_12[slice101],\n",
    "       train_regret_loser_13[slice101],\n",
    "       train_regret_loser_14[slice101],\n",
    "       train_regret_loser_15[slice101],\n",
    "       train_regret_loser_16[slice101],\n",
    "       train_regret_loser_17[slice101],\n",
    "       train_regret_loser_18[slice101],\n",
    "       train_regret_loser_19[slice101],\n",
    "       train_regret_loser_20[slice101]]\n",
    "\n",
    "winner101 = [train_regret_winner_1[slice101],\n",
    "       train_regret_winner_2[slice101],\n",
    "       train_regret_winner_3[slice101],\n",
    "       train_regret_winner_4[slice101],\n",
    "       train_regret_winner_5[slice101],\n",
    "       train_regret_winner_6[slice101],\n",
    "       train_regret_winner_7[slice101],\n",
    "       train_regret_winner_8[slice101],\n",
    "       train_regret_winner_9[slice101],\n",
    "       train_regret_winner_10[slice101],\n",
    "       train_regret_winner_11[slice101],\n",
    "       train_regret_winner_12[slice101],\n",
    "       train_regret_winner_13[slice101],\n",
    "       train_regret_winner_14[slice101],\n",
    "       train_regret_winner_15[slice101],\n",
    "       train_regret_winner_16[slice101],\n",
    "       train_regret_winner_17[slice101],\n",
    "       train_regret_winner_18[slice101],\n",
    "       train_regret_winner_19[slice101],\n",
    "       train_regret_winner_20[slice101]]\n",
    "\n",
    "loser101_results = pd.DataFrame(loser101).sort_values(by=[0], ascending=False)\n",
    "winner101_results = pd.DataFrame(winner101).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser101 = np.asarray(loser101_results[4:5][0])[0]\n",
    "median_loser101 = np.asarray(loser101_results[9:10][0])[0]\n",
    "upper_loser101 = np.asarray(loser101_results[14:15][0])[0]\n",
    "\n",
    "lower_winner101 = np.asarray(winner101_results[4:5][0])[0]\n",
    "median_winner101 = np.asarray(winner101_results[9:10][0])[0]\n",
    "upper_winner101 = np.asarray(winner101_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration2 :\n",
    "\n",
    "slice2 = 1\n",
    "\n",
    "loser2 = [train_regret_loser_1[slice2],\n",
    "       train_regret_loser_2[slice2],\n",
    "       train_regret_loser_3[slice2],\n",
    "       train_regret_loser_4[slice2],\n",
    "       train_regret_loser_5[slice2],\n",
    "       train_regret_loser_6[slice2],\n",
    "       train_regret_loser_7[slice2],\n",
    "       train_regret_loser_8[slice2],\n",
    "       train_regret_loser_9[slice2],\n",
    "       train_regret_loser_10[slice2],\n",
    "       train_regret_loser_11[slice2],\n",
    "       train_regret_loser_12[slice2],\n",
    "       train_regret_loser_13[slice2],\n",
    "       train_regret_loser_14[slice2],\n",
    "       train_regret_loser_15[slice2],\n",
    "       train_regret_loser_16[slice2],\n",
    "       train_regret_loser_17[slice2],\n",
    "       train_regret_loser_18[slice2],\n",
    "       train_regret_loser_19[slice2],\n",
    "       train_regret_loser_20[slice2]]\n",
    "\n",
    "winner2 = [train_regret_winner_1[slice2],\n",
    "       train_regret_winner_2[slice2],\n",
    "       train_regret_winner_3[slice2],\n",
    "       train_regret_winner_4[slice2],\n",
    "       train_regret_winner_5[slice2],\n",
    "       train_regret_winner_6[slice2],\n",
    "       train_regret_winner_7[slice2],\n",
    "       train_regret_winner_8[slice2],\n",
    "       train_regret_winner_9[slice2],\n",
    "       train_regret_winner_10[slice2],\n",
    "       train_regret_winner_11[slice2],\n",
    "       train_regret_winner_12[slice2],\n",
    "       train_regret_winner_13[slice2],\n",
    "       train_regret_winner_14[slice2],\n",
    "       train_regret_winner_15[slice2],\n",
    "       train_regret_winner_16[slice2],\n",
    "       train_regret_winner_17[slice2],\n",
    "       train_regret_winner_18[slice2],\n",
    "       train_regret_winner_19[slice2],\n",
    "       train_regret_winner_20[slice2]]\n",
    "\n",
    "loser2_results = pd.DataFrame(loser2).sort_values(by=[0], ascending=False)\n",
    "winner2_results = pd.DataFrame(winner2).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser2 = np.asarray(loser2_results[4:5][0])[0]\n",
    "median_loser2 = np.asarray(loser2_results[9:10][0])[0]\n",
    "upper_loser2 = np.asarray(loser2_results[14:15][0])[0]\n",
    "\n",
    "lower_winner2 = np.asarray(winner2_results[4:5][0])[0]\n",
    "median_winner2 = np.asarray(winner2_results[9:10][0])[0]\n",
    "upper_winner2 = np.asarray(winner2_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration12 :\n",
    "\n",
    "slice12 = 11\n",
    "\n",
    "loser12 = [train_regret_loser_1[slice12],\n",
    "       train_regret_loser_2[slice12],\n",
    "       train_regret_loser_3[slice12],\n",
    "       train_regret_loser_4[slice12],\n",
    "       train_regret_loser_5[slice12],\n",
    "       train_regret_loser_6[slice12],\n",
    "       train_regret_loser_7[slice12],\n",
    "       train_regret_loser_8[slice12],\n",
    "       train_regret_loser_9[slice12],\n",
    "       train_regret_loser_10[slice12],\n",
    "       train_regret_loser_11[slice12],\n",
    "       train_regret_loser_12[slice12],\n",
    "       train_regret_loser_13[slice12],\n",
    "       train_regret_loser_14[slice12],\n",
    "       train_regret_loser_15[slice12],\n",
    "       train_regret_loser_16[slice12],\n",
    "       train_regret_loser_17[slice12],\n",
    "       train_regret_loser_18[slice12],\n",
    "       train_regret_loser_19[slice12],\n",
    "       train_regret_loser_20[slice12]]\n",
    "\n",
    "winner12 = [train_regret_winner_1[slice12],\n",
    "       train_regret_winner_2[slice12],\n",
    "       train_regret_winner_3[slice12],\n",
    "       train_regret_winner_4[slice12],\n",
    "       train_regret_winner_5[slice12],\n",
    "       train_regret_winner_6[slice12],\n",
    "       train_regret_winner_7[slice12],\n",
    "       train_regret_winner_8[slice12],\n",
    "       train_regret_winner_9[slice12],\n",
    "       train_regret_winner_10[slice12],\n",
    "       train_regret_winner_11[slice12],\n",
    "       train_regret_winner_12[slice12],\n",
    "       train_regret_winner_13[slice12],\n",
    "       train_regret_winner_14[slice12],\n",
    "       train_regret_winner_15[slice12],\n",
    "       train_regret_winner_16[slice12],\n",
    "       train_regret_winner_17[slice12],\n",
    "       train_regret_winner_18[slice12],\n",
    "       train_regret_winner_19[slice12],\n",
    "       train_regret_winner_20[slice12]]\n",
    "\n",
    "loser12_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\n",
    "winner12_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser12 = np.asarray(loser12_results[4:5][0])[0]\n",
    "median_loser12 = np.asarray(loser12_results[9:10][0])[0]\n",
    "upper_loser12 = np.asarray(loser12_results[14:15][0])[0]\n",
    "\n",
    "lower_winner12 = np.asarray(winner12_results[4:5][0])[0]\n",
    "median_winner12 = np.asarray(winner12_results[9:10][0])[0]\n",
    "upper_winner12 = np.asarray(winner12_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration22 :\n",
    "\n",
    "slice22 = 21\n",
    "\n",
    "loser22 = [train_regret_loser_1[slice22],\n",
    "       train_regret_loser_2[slice22],\n",
    "       train_regret_loser_3[slice22],\n",
    "       train_regret_loser_4[slice22],\n",
    "       train_regret_loser_5[slice22],\n",
    "       train_regret_loser_6[slice22],\n",
    "       train_regret_loser_7[slice22],\n",
    "       train_regret_loser_8[slice22],\n",
    "       train_regret_loser_9[slice22],\n",
    "       train_regret_loser_10[slice22],\n",
    "       train_regret_loser_11[slice22],\n",
    "       train_regret_loser_12[slice22],\n",
    "       train_regret_loser_13[slice22],\n",
    "       train_regret_loser_14[slice22],\n",
    "       train_regret_loser_15[slice22],\n",
    "       train_regret_loser_16[slice22],\n",
    "       train_regret_loser_17[slice22],\n",
    "       train_regret_loser_18[slice22],\n",
    "       train_regret_loser_19[slice22],\n",
    "       train_regret_loser_20[slice22]]\n",
    "\n",
    "winner22 = [train_regret_winner_1[slice22],\n",
    "       train_regret_winner_2[slice22],\n",
    "       train_regret_winner_3[slice22],\n",
    "       train_regret_winner_4[slice22],\n",
    "       train_regret_winner_5[slice22],\n",
    "       train_regret_winner_6[slice22],\n",
    "       train_regret_winner_7[slice22],\n",
    "       train_regret_winner_8[slice22],\n",
    "       train_regret_winner_9[slice22],\n",
    "       train_regret_winner_10[slice22],\n",
    "       train_regret_winner_11[slice22],\n",
    "       train_regret_winner_12[slice22],\n",
    "       train_regret_winner_13[slice22],\n",
    "       train_regret_winner_14[slice22],\n",
    "       train_regret_winner_15[slice22],\n",
    "       train_regret_winner_16[slice22],\n",
    "       train_regret_winner_17[slice22],\n",
    "       train_regret_winner_18[slice22],\n",
    "       train_regret_winner_19[slice22],\n",
    "       train_regret_winner_20[slice22]]\n",
    "\n",
    "loser22_results = pd.DataFrame(loser22).sort_values(by=[0], ascending=False)\n",
    "winner22_results = pd.DataFrame(winner22).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser22 = np.asarray(loser22_results[4:5][0])[0]\n",
    "median_loser22 = np.asarray(loser22_results[9:10][0])[0]\n",
    "upper_loser22 = np.asarray(loser22_results[14:15][0])[0]\n",
    "\n",
    "lower_winner22 = np.asarray(winner22_results[4:5][0])[0]\n",
    "median_winner22 = np.asarray(winner22_results[9:10][0])[0]\n",
    "upper_winner22 = np.asarray(winner22_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration32 :\n",
    "\n",
    "slice32 = 31\n",
    "\n",
    "loser32 = [train_regret_loser_1[slice32],\n",
    "       train_regret_loser_2[slice32],\n",
    "       train_regret_loser_3[slice32],\n",
    "       train_regret_loser_4[slice32],\n",
    "       train_regret_loser_5[slice32],\n",
    "       train_regret_loser_6[slice32],\n",
    "       train_regret_loser_7[slice32],\n",
    "       train_regret_loser_8[slice32],\n",
    "       train_regret_loser_9[slice32],\n",
    "       train_regret_loser_10[slice32],\n",
    "       train_regret_loser_11[slice32],\n",
    "       train_regret_loser_12[slice32],\n",
    "       train_regret_loser_13[slice32],\n",
    "       train_regret_loser_14[slice32],\n",
    "       train_regret_loser_15[slice32],\n",
    "       train_regret_loser_16[slice32],\n",
    "       train_regret_loser_17[slice32],\n",
    "       train_regret_loser_18[slice32],\n",
    "       train_regret_loser_19[slice32],\n",
    "       train_regret_loser_20[slice32]]\n",
    "\n",
    "winner32 = [train_regret_winner_1[slice32],\n",
    "       train_regret_winner_2[slice32],\n",
    "       train_regret_winner_3[slice32],\n",
    "       train_regret_winner_4[slice32],\n",
    "       train_regret_winner_5[slice32],\n",
    "       train_regret_winner_6[slice32],\n",
    "       train_regret_winner_7[slice32],\n",
    "       train_regret_winner_8[slice32],\n",
    "       train_regret_winner_9[slice32],\n",
    "       train_regret_winner_10[slice32],\n",
    "       train_regret_winner_11[slice32],\n",
    "       train_regret_winner_12[slice32],\n",
    "       train_regret_winner_13[slice32],\n",
    "       train_regret_winner_14[slice32],\n",
    "       train_regret_winner_15[slice32],\n",
    "       train_regret_winner_16[slice32],\n",
    "       train_regret_winner_17[slice32],\n",
    "       train_regret_winner_18[slice32],\n",
    "       train_regret_winner_19[slice32],\n",
    "       train_regret_winner_20[slice32]]\n",
    "\n",
    "loser32_results = pd.DataFrame(loser32).sort_values(by=[0], ascending=False)\n",
    "winner32_results = pd.DataFrame(winner32).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser32 = np.asarray(loser32_results[4:5][0])[0]\n",
    "median_loser32 = np.asarray(loser32_results[9:10][0])[0]\n",
    "upper_loser32 = np.asarray(loser32_results[14:15][0])[0]\n",
    "\n",
    "lower_winner32 = np.asarray(winner32_results[4:5][0])[0]\n",
    "median_winner32 = np.asarray(winner32_results[9:10][0])[0]\n",
    "upper_winner32 = np.asarray(winner32_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration42 :\n",
    "\n",
    "slice42 = 41\n",
    "\n",
    "loser42 = [train_regret_loser_1[slice42],\n",
    "       train_regret_loser_2[slice42],\n",
    "       train_regret_loser_3[slice42],\n",
    "       train_regret_loser_4[slice42],\n",
    "       train_regret_loser_5[slice42],\n",
    "       train_regret_loser_6[slice42],\n",
    "       train_regret_loser_7[slice42],\n",
    "       train_regret_loser_8[slice42],\n",
    "       train_regret_loser_9[slice42],\n",
    "       train_regret_loser_10[slice42],\n",
    "       train_regret_loser_11[slice42],\n",
    "       train_regret_loser_12[slice42],\n",
    "       train_regret_loser_13[slice42],\n",
    "       train_regret_loser_14[slice42],\n",
    "       train_regret_loser_15[slice42],\n",
    "       train_regret_loser_16[slice42],\n",
    "       train_regret_loser_17[slice42],\n",
    "       train_regret_loser_18[slice42],\n",
    "       train_regret_loser_19[slice42],\n",
    "       train_regret_loser_20[slice42]]\n",
    "\n",
    "winner42 = [train_regret_winner_1[slice42],\n",
    "       train_regret_winner_2[slice42],\n",
    "       train_regret_winner_3[slice42],\n",
    "       train_regret_winner_4[slice42],\n",
    "       train_regret_winner_5[slice42],\n",
    "       train_regret_winner_6[slice42],\n",
    "       train_regret_winner_7[slice42],\n",
    "       train_regret_winner_8[slice42],\n",
    "       train_regret_winner_9[slice42],\n",
    "       train_regret_winner_10[slice42],\n",
    "       train_regret_winner_11[slice42],\n",
    "       train_regret_winner_12[slice42],\n",
    "       train_regret_winner_13[slice42],\n",
    "       train_regret_winner_14[slice42],\n",
    "       train_regret_winner_15[slice42],\n",
    "       train_regret_winner_16[slice42],\n",
    "       train_regret_winner_17[slice42],\n",
    "       train_regret_winner_18[slice42],\n",
    "       train_regret_winner_19[slice42],\n",
    "       train_regret_winner_20[slice42]]\n",
    "\n",
    "loser42_results = pd.DataFrame(loser42).sort_values(by=[0], ascending=False)\n",
    "winner42_results = pd.DataFrame(winner42).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser42 = np.asarray(loser42_results[4:5][0])[0]\n",
    "median_loser42 = np.asarray(loser42_results[9:10][0])[0]\n",
    "upper_loser42 = np.asarray(loser42_results[14:15][0])[0]\n",
    "\n",
    "lower_winner42 = np.asarray(winner42_results[4:5][0])[0]\n",
    "median_winner42 = np.asarray(winner42_results[9:10][0])[0]\n",
    "upper_winner42 = np.asarray(winner42_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration52 :\n",
    "\n",
    "slice52 = 51\n",
    "\n",
    "loser52 = [train_regret_loser_1[slice52],\n",
    "       train_regret_loser_2[slice52],\n",
    "       train_regret_loser_3[slice52],\n",
    "       train_regret_loser_4[slice52],\n",
    "       train_regret_loser_5[slice52],\n",
    "       train_regret_loser_6[slice52],\n",
    "       train_regret_loser_7[slice52],\n",
    "       train_regret_loser_8[slice52],\n",
    "       train_regret_loser_9[slice52],\n",
    "       train_regret_loser_10[slice52],\n",
    "       train_regret_loser_11[slice52],\n",
    "       train_regret_loser_12[slice52],\n",
    "       train_regret_loser_13[slice52],\n",
    "       train_regret_loser_14[slice52],\n",
    "       train_regret_loser_15[slice52],\n",
    "       train_regret_loser_16[slice52],\n",
    "       train_regret_loser_17[slice52],\n",
    "       train_regret_loser_18[slice52],\n",
    "       train_regret_loser_19[slice52],\n",
    "       train_regret_loser_20[slice52]]\n",
    "\n",
    "winner52 = [train_regret_winner_1[slice52],\n",
    "       train_regret_winner_2[slice52],\n",
    "       train_regret_winner_3[slice52],\n",
    "       train_regret_winner_4[slice52],\n",
    "       train_regret_winner_5[slice52],\n",
    "       train_regret_winner_6[slice52],\n",
    "       train_regret_winner_7[slice52],\n",
    "       train_regret_winner_8[slice52],\n",
    "       train_regret_winner_9[slice52],\n",
    "       train_regret_winner_10[slice52],\n",
    "       train_regret_winner_11[slice52],\n",
    "       train_regret_winner_12[slice52],\n",
    "       train_regret_winner_13[slice52],\n",
    "       train_regret_winner_14[slice52],\n",
    "       train_regret_winner_15[slice52],\n",
    "       train_regret_winner_16[slice52],\n",
    "       train_regret_winner_17[slice52],\n",
    "       train_regret_winner_18[slice52],\n",
    "       train_regret_winner_19[slice52],\n",
    "       train_regret_winner_20[slice52]]\n",
    "\n",
    "loser52_results = pd.DataFrame(loser52).sort_values(by=[0], ascending=False)\n",
    "winner52_results = pd.DataFrame(winner52).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser52 = np.asarray(loser52_results[4:5][0])[0]\n",
    "median_loser52 = np.asarray(loser52_results[9:10][0])[0]\n",
    "upper_loser52 = np.asarray(loser52_results[14:15][0])[0]\n",
    "\n",
    "lower_winner52 = np.asarray(winner52_results[4:5][0])[0]\n",
    "median_winner52 = np.asarray(winner52_results[9:10][0])[0]\n",
    "upper_winner52 = np.asarray(winner52_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration62 :\n",
    "\n",
    "slice62 = 61\n",
    "\n",
    "loser62 = [train_regret_loser_1[slice62],\n",
    "       train_regret_loser_2[slice62],\n",
    "       train_regret_loser_3[slice62],\n",
    "       train_regret_loser_4[slice62],\n",
    "       train_regret_loser_5[slice62],\n",
    "       train_regret_loser_6[slice62],\n",
    "       train_regret_loser_7[slice62],\n",
    "       train_regret_loser_8[slice62],\n",
    "       train_regret_loser_9[slice62],\n",
    "       train_regret_loser_10[slice62],\n",
    "       train_regret_loser_11[slice62],\n",
    "       train_regret_loser_12[slice62],\n",
    "       train_regret_loser_13[slice62],\n",
    "       train_regret_loser_14[slice62],\n",
    "       train_regret_loser_15[slice62],\n",
    "       train_regret_loser_16[slice62],\n",
    "       train_regret_loser_17[slice62],\n",
    "       train_regret_loser_18[slice62],\n",
    "       train_regret_loser_19[slice62],\n",
    "       train_regret_loser_20[slice62]]\n",
    "\n",
    "winner62 = [train_regret_winner_1[slice62],\n",
    "       train_regret_winner_2[slice62],\n",
    "       train_regret_winner_3[slice62],\n",
    "       train_regret_winner_4[slice62],\n",
    "       train_regret_winner_5[slice62],\n",
    "       train_regret_winner_6[slice62],\n",
    "       train_regret_winner_7[slice62],\n",
    "       train_regret_winner_8[slice62],\n",
    "       train_regret_winner_9[slice62],\n",
    "       train_regret_winner_10[slice62],\n",
    "       train_regret_winner_11[slice62],\n",
    "       train_regret_winner_12[slice62],\n",
    "       train_regret_winner_13[slice62],\n",
    "       train_regret_winner_14[slice62],\n",
    "       train_regret_winner_15[slice62],\n",
    "       train_regret_winner_16[slice62],\n",
    "       train_regret_winner_17[slice62],\n",
    "       train_regret_winner_18[slice62],\n",
    "       train_regret_winner_19[slice62],\n",
    "       train_regret_winner_20[slice62]]\n",
    "\n",
    "loser62_results = pd.DataFrame(loser62).sort_values(by=[0], ascending=False)\n",
    "winner62_results = pd.DataFrame(winner62).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser62 = np.asarray(loser62_results[4:5][0])[0]\n",
    "median_loser62 = np.asarray(loser62_results[9:10][0])[0]\n",
    "upper_loser62 = np.asarray(loser62_results[14:15][0])[0]\n",
    "\n",
    "lower_winner62 = np.asarray(winner62_results[4:5][0])[0]\n",
    "median_winner62 = np.asarray(winner62_results[9:10][0])[0]\n",
    "upper_winner62 = np.asarray(winner62_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration72 :\n",
    "\n",
    "slice72 = 71\n",
    "\n",
    "loser72 = [train_regret_loser_1[slice72],\n",
    "       train_regret_loser_2[slice72],\n",
    "       train_regret_loser_3[slice72],\n",
    "       train_regret_loser_4[slice72],\n",
    "       train_regret_loser_5[slice72],\n",
    "       train_regret_loser_6[slice72],\n",
    "       train_regret_loser_7[slice72],\n",
    "       train_regret_loser_8[slice72],\n",
    "       train_regret_loser_9[slice72],\n",
    "       train_regret_loser_10[slice72],\n",
    "       train_regret_loser_11[slice72],\n",
    "       train_regret_loser_12[slice72],\n",
    "       train_regret_loser_13[slice72],\n",
    "       train_regret_loser_14[slice72],\n",
    "       train_regret_loser_15[slice72],\n",
    "       train_regret_loser_16[slice72],\n",
    "       train_regret_loser_17[slice72],\n",
    "       train_regret_loser_18[slice72],\n",
    "       train_regret_loser_19[slice72],\n",
    "       train_regret_loser_20[slice72]]\n",
    "\n",
    "winner72 = [train_regret_winner_1[slice72],\n",
    "       train_regret_winner_2[slice72],\n",
    "       train_regret_winner_3[slice72],\n",
    "       train_regret_winner_4[slice72],\n",
    "       train_regret_winner_5[slice72],\n",
    "       train_regret_winner_6[slice72],\n",
    "       train_regret_winner_7[slice72],\n",
    "       train_regret_winner_8[slice72],\n",
    "       train_regret_winner_9[slice72],\n",
    "       train_regret_winner_10[slice72],\n",
    "       train_regret_winner_11[slice72],\n",
    "       train_regret_winner_12[slice72],\n",
    "       train_regret_winner_13[slice72],\n",
    "       train_regret_winner_14[slice72],\n",
    "       train_regret_winner_15[slice72],\n",
    "       train_regret_winner_16[slice72],\n",
    "       train_regret_winner_17[slice72],\n",
    "       train_regret_winner_18[slice72],\n",
    "       train_regret_winner_19[slice72],\n",
    "       train_regret_winner_20[slice72]]\n",
    "\n",
    "loser72_results = pd.DataFrame(loser72).sort_values(by=[0], ascending=False)\n",
    "winner72_results = pd.DataFrame(winner72).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser72 = np.asarray(loser72_results[4:5][0])[0]\n",
    "median_loser72 = np.asarray(loser72_results[9:10][0])[0]\n",
    "upper_loser72 = np.asarray(loser72_results[14:15][0])[0]\n",
    "\n",
    "lower_winner72 = np.asarray(winner72_results[4:5][0])[0]\n",
    "median_winner72 = np.asarray(winner72_results[9:10][0])[0]\n",
    "upper_winner72 = np.asarray(winner72_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration82 :\n",
    "\n",
    "slice82 = 81\n",
    "\n",
    "loser82 = [train_regret_loser_1[slice82],\n",
    "       train_regret_loser_2[slice82],\n",
    "       train_regret_loser_3[slice82],\n",
    "       train_regret_loser_4[slice82],\n",
    "       train_regret_loser_5[slice82],\n",
    "       train_regret_loser_6[slice82],\n",
    "       train_regret_loser_7[slice82],\n",
    "       train_regret_loser_8[slice82],\n",
    "       train_regret_loser_9[slice82],\n",
    "       train_regret_loser_10[slice82],\n",
    "       train_regret_loser_11[slice82],\n",
    "       train_regret_loser_12[slice82],\n",
    "       train_regret_loser_13[slice82],\n",
    "       train_regret_loser_14[slice82],\n",
    "       train_regret_loser_15[slice82],\n",
    "       train_regret_loser_16[slice82],\n",
    "       train_regret_loser_17[slice82],\n",
    "       train_regret_loser_18[slice82],\n",
    "       train_regret_loser_19[slice82],\n",
    "       train_regret_loser_20[slice82]]\n",
    "\n",
    "winner82 = [train_regret_winner_1[slice82],\n",
    "       train_regret_winner_2[slice82],\n",
    "       train_regret_winner_3[slice82],\n",
    "       train_regret_winner_4[slice82],\n",
    "       train_regret_winner_5[slice82],\n",
    "       train_regret_winner_6[slice82],\n",
    "       train_regret_winner_7[slice82],\n",
    "       train_regret_winner_8[slice82],\n",
    "       train_regret_winner_9[slice82],\n",
    "       train_regret_winner_10[slice82],\n",
    "       train_regret_winner_11[slice82],\n",
    "       train_regret_winner_12[slice82],\n",
    "       train_regret_winner_13[slice82],\n",
    "       train_regret_winner_14[slice82],\n",
    "       train_regret_winner_15[slice82],\n",
    "       train_regret_winner_16[slice82],\n",
    "       train_regret_winner_17[slice82],\n",
    "       train_regret_winner_18[slice82],\n",
    "       train_regret_winner_19[slice82],\n",
    "       train_regret_winner_20[slice82]]\n",
    "\n",
    "loser82_results = pd.DataFrame(loser82).sort_values(by=[0], ascending=False)\n",
    "winner82_results = pd.DataFrame(winner82).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser82 = np.asarray(loser82_results[4:5][0])[0]\n",
    "median_loser82 = np.asarray(loser82_results[9:10][0])[0]\n",
    "upper_loser82 = np.asarray(loser82_results[14:15][0])[0]\n",
    "\n",
    "lower_winner82 = np.asarray(winner82_results[4:5][0])[0]\n",
    "median_winner82 = np.asarray(winner82_results[9:10][0])[0]\n",
    "upper_winner82 = np.asarray(winner82_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration92 :\n",
    "\n",
    "slice92 = 91\n",
    "\n",
    "loser92 = [train_regret_loser_1[slice92],\n",
    "       train_regret_loser_2[slice92],\n",
    "       train_regret_loser_3[slice92],\n",
    "       train_regret_loser_4[slice92],\n",
    "       train_regret_loser_5[slice92],\n",
    "       train_regret_loser_6[slice92],\n",
    "       train_regret_loser_7[slice92],\n",
    "       train_regret_loser_8[slice92],\n",
    "       train_regret_loser_9[slice92],\n",
    "       train_regret_loser_10[slice92],\n",
    "       train_regret_loser_11[slice92],\n",
    "       train_regret_loser_12[slice92],\n",
    "       train_regret_loser_13[slice92],\n",
    "       train_regret_loser_14[slice92],\n",
    "       train_regret_loser_15[slice92],\n",
    "       train_regret_loser_16[slice92],\n",
    "       train_regret_loser_17[slice92],\n",
    "       train_regret_loser_18[slice92],\n",
    "       train_regret_loser_19[slice92],\n",
    "       train_regret_loser_20[slice92]]\n",
    "\n",
    "winner92 = [train_regret_winner_1[slice92],\n",
    "       train_regret_winner_2[slice92],\n",
    "       train_regret_winner_3[slice92],\n",
    "       train_regret_winner_4[slice92],\n",
    "       train_regret_winner_5[slice92],\n",
    "       train_regret_winner_6[slice92],\n",
    "       train_regret_winner_7[slice92],\n",
    "       train_regret_winner_8[slice92],\n",
    "       train_regret_winner_9[slice92],\n",
    "       train_regret_winner_10[slice92],\n",
    "       train_regret_winner_11[slice92],\n",
    "       train_regret_winner_12[slice92],\n",
    "       train_regret_winner_13[slice92],\n",
    "       train_regret_winner_14[slice92],\n",
    "       train_regret_winner_15[slice92],\n",
    "       train_regret_winner_16[slice92],\n",
    "       train_regret_winner_17[slice92],\n",
    "       train_regret_winner_18[slice92],\n",
    "       train_regret_winner_19[slice92],\n",
    "       train_regret_winner_20[slice92]]\n",
    "\n",
    "loser92_results = pd.DataFrame(loser92).sort_values(by=[0], ascending=False)\n",
    "winner92_results = pd.DataFrame(winner92).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser92 = np.asarray(loser92_results[4:5][0])[0]\n",
    "median_loser92 = np.asarray(loser92_results[9:10][0])[0]\n",
    "upper_loser92 = np.asarray(loser92_results[14:15][0])[0]\n",
    "\n",
    "lower_winner92 = np.asarray(winner92_results[4:5][0])[0]\n",
    "median_winner92 = np.asarray(winner92_results[9:10][0])[0]\n",
    "upper_winner92 = np.asarray(winner92_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration3 :\n",
    "\n",
    "slice3 = 2\n",
    "\n",
    "loser3 = [train_regret_loser_1[slice3],\n",
    "       train_regret_loser_2[slice3],\n",
    "       train_regret_loser_3[slice3],\n",
    "       train_regret_loser_4[slice3],\n",
    "       train_regret_loser_5[slice3],\n",
    "       train_regret_loser_6[slice3],\n",
    "       train_regret_loser_7[slice3],\n",
    "       train_regret_loser_8[slice3],\n",
    "       train_regret_loser_9[slice3],\n",
    "       train_regret_loser_10[slice3],\n",
    "       train_regret_loser_11[slice3],\n",
    "       train_regret_loser_12[slice3],\n",
    "       train_regret_loser_13[slice3],\n",
    "       train_regret_loser_14[slice3],\n",
    "       train_regret_loser_15[slice3],\n",
    "       train_regret_loser_16[slice3],\n",
    "       train_regret_loser_17[slice3],\n",
    "       train_regret_loser_18[slice3],\n",
    "       train_regret_loser_19[slice3],\n",
    "       train_regret_loser_20[slice3]]\n",
    "\n",
    "winner3 = [train_regret_winner_1[slice3],\n",
    "       train_regret_winner_2[slice3],\n",
    "       train_regret_winner_3[slice3],\n",
    "       train_regret_winner_4[slice3],\n",
    "       train_regret_winner_5[slice3],\n",
    "       train_regret_winner_6[slice3],\n",
    "       train_regret_winner_7[slice3],\n",
    "       train_regret_winner_8[slice3],\n",
    "       train_regret_winner_9[slice3],\n",
    "       train_regret_winner_10[slice3],\n",
    "       train_regret_winner_11[slice3],\n",
    "       train_regret_winner_12[slice3],\n",
    "       train_regret_winner_13[slice3],\n",
    "       train_regret_winner_14[slice3],\n",
    "       train_regret_winner_15[slice3],\n",
    "       train_regret_winner_16[slice3],\n",
    "       train_regret_winner_17[slice3],\n",
    "       train_regret_winner_18[slice3],\n",
    "       train_regret_winner_19[slice3],\n",
    "       train_regret_winner_20[slice3]]\n",
    "\n",
    "loser3_results = pd.DataFrame(loser3).sort_values(by=[0], ascending=False)\n",
    "winner3_results = pd.DataFrame(winner3).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser3 = np.asarray(loser3_results[4:5][0])[0]\n",
    "median_loser3 = np.asarray(loser3_results[9:10][0])[0]\n",
    "upper_loser3 = np.asarray(loser3_results[14:15][0])[0]\n",
    "\n",
    "lower_winner3 = np.asarray(winner3_results[4:5][0])[0]\n",
    "median_winner3 = np.asarray(winner3_results[9:10][0])[0]\n",
    "upper_winner3 = np.asarray(winner3_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration13 :\n",
    "\n",
    "slice13 = 12\n",
    "\n",
    "loser13 = [train_regret_loser_1[slice13],\n",
    "       train_regret_loser_2[slice13],\n",
    "       train_regret_loser_3[slice13],\n",
    "       train_regret_loser_4[slice13],\n",
    "       train_regret_loser_5[slice13],\n",
    "       train_regret_loser_6[slice13],\n",
    "       train_regret_loser_7[slice13],\n",
    "       train_regret_loser_8[slice13],\n",
    "       train_regret_loser_9[slice13],\n",
    "       train_regret_loser_10[slice13],\n",
    "       train_regret_loser_11[slice13],\n",
    "       train_regret_loser_12[slice13],\n",
    "       train_regret_loser_13[slice13],\n",
    "       train_regret_loser_14[slice13],\n",
    "       train_regret_loser_15[slice13],\n",
    "       train_regret_loser_16[slice13],\n",
    "       train_regret_loser_17[slice13],\n",
    "       train_regret_loser_18[slice13],\n",
    "       train_regret_loser_19[slice13],\n",
    "       train_regret_loser_20[slice13]]\n",
    "\n",
    "winner13 = [train_regret_winner_1[slice13],\n",
    "       train_regret_winner_2[slice13],\n",
    "       train_regret_winner_3[slice13],\n",
    "       train_regret_winner_4[slice13],\n",
    "       train_regret_winner_5[slice13],\n",
    "       train_regret_winner_6[slice13],\n",
    "       train_regret_winner_7[slice13],\n",
    "       train_regret_winner_8[slice13],\n",
    "       train_regret_winner_9[slice13],\n",
    "       train_regret_winner_10[slice13],\n",
    "       train_regret_winner_11[slice13],\n",
    "       train_regret_winner_12[slice13],\n",
    "       train_regret_winner_13[slice13],\n",
    "       train_regret_winner_14[slice13],\n",
    "       train_regret_winner_15[slice13],\n",
    "       train_regret_winner_16[slice13],\n",
    "       train_regret_winner_17[slice13],\n",
    "       train_regret_winner_18[slice13],\n",
    "       train_regret_winner_19[slice13],\n",
    "       train_regret_winner_20[slice13]]\n",
    "\n",
    "loser13_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\n",
    "winner13_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser13 = np.asarray(loser13_results[4:5][0])[0]\n",
    "median_loser13 = np.asarray(loser13_results[9:10][0])[0]\n",
    "upper_loser13 = np.asarray(loser13_results[14:15][0])[0]\n",
    "\n",
    "lower_winner13 = np.asarray(winner13_results[4:5][0])[0]\n",
    "median_winner13 = np.asarray(winner13_results[9:10][0])[0]\n",
    "upper_winner13 = np.asarray(winner13_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration23 :\n",
    "\n",
    "slice23 = 22\n",
    "\n",
    "loser23 = [train_regret_loser_1[slice23],\n",
    "       train_regret_loser_2[slice23],\n",
    "       train_regret_loser_3[slice23],\n",
    "       train_regret_loser_4[slice23],\n",
    "       train_regret_loser_5[slice23],\n",
    "       train_regret_loser_6[slice23],\n",
    "       train_regret_loser_7[slice23],\n",
    "       train_regret_loser_8[slice23],\n",
    "       train_regret_loser_9[slice23],\n",
    "       train_regret_loser_10[slice23],\n",
    "       train_regret_loser_11[slice23],\n",
    "       train_regret_loser_12[slice23],\n",
    "       train_regret_loser_13[slice23],\n",
    "       train_regret_loser_14[slice23],\n",
    "       train_regret_loser_15[slice23],\n",
    "       train_regret_loser_16[slice23],\n",
    "       train_regret_loser_17[slice23],\n",
    "       train_regret_loser_18[slice23],\n",
    "       train_regret_loser_19[slice23],\n",
    "       train_regret_loser_20[slice23]]\n",
    "\n",
    "winner23 = [train_regret_winner_1[slice23],\n",
    "       train_regret_winner_2[slice23],\n",
    "       train_regret_winner_3[slice23],\n",
    "       train_regret_winner_4[slice23],\n",
    "       train_regret_winner_5[slice23],\n",
    "       train_regret_winner_6[slice23],\n",
    "       train_regret_winner_7[slice23],\n",
    "       train_regret_winner_8[slice23],\n",
    "       train_regret_winner_9[slice23],\n",
    "       train_regret_winner_10[slice23],\n",
    "       train_regret_winner_11[slice23],\n",
    "       train_regret_winner_12[slice23],\n",
    "       train_regret_winner_13[slice23],\n",
    "       train_regret_winner_14[slice23],\n",
    "       train_regret_winner_15[slice23],\n",
    "       train_regret_winner_16[slice23],\n",
    "       train_regret_winner_17[slice23],\n",
    "       train_regret_winner_18[slice23],\n",
    "       train_regret_winner_19[slice23],\n",
    "       train_regret_winner_20[slice23]]\n",
    "\n",
    "loser23_results = pd.DataFrame(loser23).sort_values(by=[0], ascending=False)\n",
    "winner23_results = pd.DataFrame(winner23).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser23 = np.asarray(loser23_results[4:5][0])[0]\n",
    "median_loser23 = np.asarray(loser23_results[9:10][0])[0]\n",
    "upper_loser23 = np.asarray(loser23_results[14:15][0])[0]\n",
    "\n",
    "lower_winner23 = np.asarray(winner23_results[4:5][0])[0]\n",
    "median_winner23 = np.asarray(winner23_results[9:10][0])[0]\n",
    "upper_winner23 = np.asarray(winner23_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration33 :\n",
    "\n",
    "slice33 = 32\n",
    "\n",
    "loser33 = [train_regret_loser_1[slice33],\n",
    "       train_regret_loser_2[slice33],\n",
    "       train_regret_loser_3[slice33],\n",
    "       train_regret_loser_4[slice33],\n",
    "       train_regret_loser_5[slice33],\n",
    "       train_regret_loser_6[slice33],\n",
    "       train_regret_loser_7[slice33],\n",
    "       train_regret_loser_8[slice33],\n",
    "       train_regret_loser_9[slice33],\n",
    "       train_regret_loser_10[slice33],\n",
    "       train_regret_loser_11[slice33],\n",
    "       train_regret_loser_12[slice33],\n",
    "       train_regret_loser_13[slice33],\n",
    "       train_regret_loser_14[slice33],\n",
    "       train_regret_loser_15[slice33],\n",
    "       train_regret_loser_16[slice33],\n",
    "       train_regret_loser_17[slice33],\n",
    "       train_regret_loser_18[slice33],\n",
    "       train_regret_loser_19[slice33],\n",
    "       train_regret_loser_20[slice33]]\n",
    "\n",
    "winner33 = [train_regret_winner_1[slice33],\n",
    "       train_regret_winner_2[slice33],\n",
    "       train_regret_winner_3[slice33],\n",
    "       train_regret_winner_4[slice33],\n",
    "       train_regret_winner_5[slice33],\n",
    "       train_regret_winner_6[slice33],\n",
    "       train_regret_winner_7[slice33],\n",
    "       train_regret_winner_8[slice33],\n",
    "       train_regret_winner_9[slice33],\n",
    "       train_regret_winner_10[slice33],\n",
    "       train_regret_winner_11[slice33],\n",
    "       train_regret_winner_12[slice33],\n",
    "       train_regret_winner_13[slice33],\n",
    "       train_regret_winner_14[slice33],\n",
    "       train_regret_winner_15[slice33],\n",
    "       train_regret_winner_16[slice33],\n",
    "       train_regret_winner_17[slice33],\n",
    "       train_regret_winner_18[slice33],\n",
    "       train_regret_winner_19[slice33],\n",
    "       train_regret_winner_20[slice33]]\n",
    "\n",
    "loser33_results = pd.DataFrame(loser33).sort_values(by=[0], ascending=False)\n",
    "winner33_results = pd.DataFrame(winner33).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser33 = np.asarray(loser33_results[4:5][0])[0]\n",
    "median_loser33 = np.asarray(loser33_results[9:10][0])[0]\n",
    "upper_loser33 = np.asarray(loser33_results[14:15][0])[0]\n",
    "\n",
    "lower_winner33 = np.asarray(winner33_results[4:5][0])[0]\n",
    "median_winner33 = np.asarray(winner33_results[9:10][0])[0]\n",
    "upper_winner33 = np.asarray(winner33_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration43 :\n",
    "\n",
    "slice43 = 42\n",
    "\n",
    "loser43 = [train_regret_loser_1[slice43],\n",
    "       train_regret_loser_2[slice43],\n",
    "       train_regret_loser_3[slice43],\n",
    "       train_regret_loser_4[slice43],\n",
    "       train_regret_loser_5[slice43],\n",
    "       train_regret_loser_6[slice43],\n",
    "       train_regret_loser_7[slice43],\n",
    "       train_regret_loser_8[slice43],\n",
    "       train_regret_loser_9[slice43],\n",
    "       train_regret_loser_10[slice43],\n",
    "       train_regret_loser_11[slice43],\n",
    "       train_regret_loser_12[slice43],\n",
    "       train_regret_loser_13[slice43],\n",
    "       train_regret_loser_14[slice43],\n",
    "       train_regret_loser_15[slice43],\n",
    "       train_regret_loser_16[slice43],\n",
    "       train_regret_loser_17[slice43],\n",
    "       train_regret_loser_18[slice43],\n",
    "       train_regret_loser_19[slice43],\n",
    "       train_regret_loser_20[slice43]]\n",
    "\n",
    "winner43 = [train_regret_winner_1[slice43],\n",
    "       train_regret_winner_2[slice43],\n",
    "       train_regret_winner_3[slice43],\n",
    "       train_regret_winner_4[slice43],\n",
    "       train_regret_winner_5[slice43],\n",
    "       train_regret_winner_6[slice43],\n",
    "       train_regret_winner_7[slice43],\n",
    "       train_regret_winner_8[slice43],\n",
    "       train_regret_winner_9[slice43],\n",
    "       train_regret_winner_10[slice43],\n",
    "       train_regret_winner_11[slice43],\n",
    "       train_regret_winner_12[slice43],\n",
    "       train_regret_winner_13[slice43],\n",
    "       train_regret_winner_14[slice43],\n",
    "       train_regret_winner_15[slice43],\n",
    "       train_regret_winner_16[slice43],\n",
    "       train_regret_winner_17[slice43],\n",
    "       train_regret_winner_18[slice43],\n",
    "       train_regret_winner_19[slice43],\n",
    "       train_regret_winner_20[slice43]]\n",
    "\n",
    "loser43_results = pd.DataFrame(loser43).sort_values(by=[0], ascending=False)\n",
    "winner43_results = pd.DataFrame(winner43).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser43 = np.asarray(loser43_results[4:5][0])[0]\n",
    "median_loser43 = np.asarray(loser43_results[9:10][0])[0]\n",
    "upper_loser43 = np.asarray(loser43_results[14:15][0])[0]\n",
    "\n",
    "lower_winner43 = np.asarray(winner43_results[4:5][0])[0]\n",
    "median_winner43 = np.asarray(winner43_results[9:10][0])[0]\n",
    "upper_winner43 = np.asarray(winner43_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration53 :\n",
    "\n",
    "slice53 = 52\n",
    "\n",
    "loser53 = [train_regret_loser_1[slice53],\n",
    "       train_regret_loser_2[slice53],\n",
    "       train_regret_loser_3[slice53],\n",
    "       train_regret_loser_4[slice53],\n",
    "       train_regret_loser_5[slice53],\n",
    "       train_regret_loser_6[slice53],\n",
    "       train_regret_loser_7[slice53],\n",
    "       train_regret_loser_8[slice53],\n",
    "       train_regret_loser_9[slice53],\n",
    "       train_regret_loser_10[slice53],\n",
    "       train_regret_loser_11[slice53],\n",
    "       train_regret_loser_12[slice53],\n",
    "       train_regret_loser_13[slice53],\n",
    "       train_regret_loser_14[slice53],\n",
    "       train_regret_loser_15[slice53],\n",
    "       train_regret_loser_16[slice53],\n",
    "       train_regret_loser_17[slice53],\n",
    "       train_regret_loser_18[slice53],\n",
    "       train_regret_loser_19[slice53],\n",
    "       train_regret_loser_20[slice53]]\n",
    "\n",
    "winner53 = [train_regret_winner_1[slice53],\n",
    "       train_regret_winner_2[slice53],\n",
    "       train_regret_winner_3[slice53],\n",
    "       train_regret_winner_4[slice53],\n",
    "       train_regret_winner_5[slice53],\n",
    "       train_regret_winner_6[slice53],\n",
    "       train_regret_winner_7[slice53],\n",
    "       train_regret_winner_8[slice53],\n",
    "       train_regret_winner_9[slice53],\n",
    "       train_regret_winner_10[slice53],\n",
    "       train_regret_winner_11[slice53],\n",
    "       train_regret_winner_12[slice53],\n",
    "       train_regret_winner_13[slice53],\n",
    "       train_regret_winner_14[slice53],\n",
    "       train_regret_winner_15[slice53],\n",
    "       train_regret_winner_16[slice53],\n",
    "       train_regret_winner_17[slice53],\n",
    "       train_regret_winner_18[slice53],\n",
    "       train_regret_winner_19[slice53],\n",
    "       train_regret_winner_20[slice53]]\n",
    "\n",
    "loser53_results = pd.DataFrame(loser53).sort_values(by=[0], ascending=False)\n",
    "winner53_results = pd.DataFrame(winner53).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser53 = np.asarray(loser53_results[4:5][0])[0]\n",
    "median_loser53 = np.asarray(loser53_results[9:10][0])[0]\n",
    "upper_loser53 = np.asarray(loser53_results[14:15][0])[0]\n",
    "\n",
    "lower_winner53 = np.asarray(winner53_results[4:5][0])[0]\n",
    "median_winner53 = np.asarray(winner53_results[9:10][0])[0]\n",
    "upper_winner53 = np.asarray(winner53_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration63 :\n",
    "\n",
    "slice63 = 62\n",
    "\n",
    "loser63 = [train_regret_loser_1[slice63],\n",
    "       train_regret_loser_2[slice63],\n",
    "       train_regret_loser_3[slice63],\n",
    "       train_regret_loser_4[slice63],\n",
    "       train_regret_loser_5[slice63],\n",
    "       train_regret_loser_6[slice63],\n",
    "       train_regret_loser_7[slice63],\n",
    "       train_regret_loser_8[slice63],\n",
    "       train_regret_loser_9[slice63],\n",
    "       train_regret_loser_10[slice63],\n",
    "       train_regret_loser_11[slice63],\n",
    "       train_regret_loser_12[slice63],\n",
    "       train_regret_loser_13[slice63],\n",
    "       train_regret_loser_14[slice63],\n",
    "       train_regret_loser_15[slice63],\n",
    "       train_regret_loser_16[slice63],\n",
    "       train_regret_loser_17[slice63],\n",
    "       train_regret_loser_18[slice63],\n",
    "       train_regret_loser_19[slice63],\n",
    "       train_regret_loser_20[slice63]]\n",
    "\n",
    "winner63 = [train_regret_winner_1[slice63],\n",
    "       train_regret_winner_2[slice63],\n",
    "       train_regret_winner_3[slice63],\n",
    "       train_regret_winner_4[slice63],\n",
    "       train_regret_winner_5[slice63],\n",
    "       train_regret_winner_6[slice63],\n",
    "       train_regret_winner_7[slice63],\n",
    "       train_regret_winner_8[slice63],\n",
    "       train_regret_winner_9[slice63],\n",
    "       train_regret_winner_10[slice63],\n",
    "       train_regret_winner_11[slice63],\n",
    "       train_regret_winner_12[slice63],\n",
    "       train_regret_winner_13[slice63],\n",
    "       train_regret_winner_14[slice63],\n",
    "       train_regret_winner_15[slice63],\n",
    "       train_regret_winner_16[slice63],\n",
    "       train_regret_winner_17[slice63],\n",
    "       train_regret_winner_18[slice63],\n",
    "       train_regret_winner_19[slice63],\n",
    "       train_regret_winner_20[slice63]]\n",
    "\n",
    "loser63_results = pd.DataFrame(loser63).sort_values(by=[0], ascending=False)\n",
    "winner63_results = pd.DataFrame(winner63).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser63 = np.asarray(loser63_results[4:5][0])[0]\n",
    "median_loser63 = np.asarray(loser63_results[9:10][0])[0]\n",
    "upper_loser63 = np.asarray(loser63_results[14:15][0])[0]\n",
    "\n",
    "lower_winner63 = np.asarray(winner63_results[4:5][0])[0]\n",
    "median_winner63 = np.asarray(winner63_results[9:10][0])[0]\n",
    "upper_winner63 = np.asarray(winner63_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration73 :\n",
    "\n",
    "slice73 = 72\n",
    "\n",
    "loser73 = [train_regret_loser_1[slice73],\n",
    "       train_regret_loser_2[slice73],\n",
    "       train_regret_loser_3[slice73],\n",
    "       train_regret_loser_4[slice73],\n",
    "       train_regret_loser_5[slice73],\n",
    "       train_regret_loser_6[slice73],\n",
    "       train_regret_loser_7[slice73],\n",
    "       train_regret_loser_8[slice73],\n",
    "       train_regret_loser_9[slice73],\n",
    "       train_regret_loser_10[slice73],\n",
    "       train_regret_loser_11[slice73],\n",
    "       train_regret_loser_12[slice73],\n",
    "       train_regret_loser_13[slice73],\n",
    "       train_regret_loser_14[slice73],\n",
    "       train_regret_loser_15[slice73],\n",
    "       train_regret_loser_16[slice73],\n",
    "       train_regret_loser_17[slice73],\n",
    "       train_regret_loser_18[slice73],\n",
    "       train_regret_loser_19[slice73],\n",
    "       train_regret_loser_20[slice73]]\n",
    "\n",
    "winner73 = [train_regret_winner_1[slice73],\n",
    "       train_regret_winner_2[slice73],\n",
    "       train_regret_winner_3[slice73],\n",
    "       train_regret_winner_4[slice73],\n",
    "       train_regret_winner_5[slice73],\n",
    "       train_regret_winner_6[slice73],\n",
    "       train_regret_winner_7[slice73],\n",
    "       train_regret_winner_8[slice73],\n",
    "       train_regret_winner_9[slice73],\n",
    "       train_regret_winner_10[slice73],\n",
    "       train_regret_winner_11[slice73],\n",
    "       train_regret_winner_12[slice73],\n",
    "       train_regret_winner_13[slice73],\n",
    "       train_regret_winner_14[slice73],\n",
    "       train_regret_winner_15[slice73],\n",
    "       train_regret_winner_16[slice73],\n",
    "       train_regret_winner_17[slice73],\n",
    "       train_regret_winner_18[slice73],\n",
    "       train_regret_winner_19[slice73],\n",
    "       train_regret_winner_20[slice73]]\n",
    "\n",
    "loser73_results = pd.DataFrame(loser73).sort_values(by=[0], ascending=False)\n",
    "winner73_results = pd.DataFrame(winner73).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser73 = np.asarray(loser73_results[4:5][0])[0]\n",
    "median_loser73 = np.asarray(loser73_results[9:10][0])[0]\n",
    "upper_loser73 = np.asarray(loser73_results[14:15][0])[0]\n",
    "\n",
    "lower_winner73 = np.asarray(winner73_results[4:5][0])[0]\n",
    "median_winner73 = np.asarray(winner73_results[9:10][0])[0]\n",
    "upper_winner73 = np.asarray(winner73_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration83 :\n",
    "\n",
    "slice83 = 82\n",
    "\n",
    "loser83 = [train_regret_loser_1[slice83],\n",
    "       train_regret_loser_2[slice83],\n",
    "       train_regret_loser_3[slice83],\n",
    "       train_regret_loser_4[slice83],\n",
    "       train_regret_loser_5[slice83],\n",
    "       train_regret_loser_6[slice83],\n",
    "       train_regret_loser_7[slice83],\n",
    "       train_regret_loser_8[slice83],\n",
    "       train_regret_loser_9[slice83],\n",
    "       train_regret_loser_10[slice83],\n",
    "       train_regret_loser_11[slice83],\n",
    "       train_regret_loser_12[slice83],\n",
    "       train_regret_loser_13[slice83],\n",
    "       train_regret_loser_14[slice83],\n",
    "       train_regret_loser_15[slice83],\n",
    "       train_regret_loser_16[slice83],\n",
    "       train_regret_loser_17[slice83],\n",
    "       train_regret_loser_18[slice83],\n",
    "       train_regret_loser_19[slice83],\n",
    "       train_regret_loser_20[slice83]]\n",
    "\n",
    "winner83 = [train_regret_winner_1[slice83],\n",
    "       train_regret_winner_2[slice83],\n",
    "       train_regret_winner_3[slice83],\n",
    "       train_regret_winner_4[slice83],\n",
    "       train_regret_winner_5[slice83],\n",
    "       train_regret_winner_6[slice83],\n",
    "       train_regret_winner_7[slice83],\n",
    "       train_regret_winner_8[slice83],\n",
    "       train_regret_winner_9[slice83],\n",
    "       train_regret_winner_10[slice83],\n",
    "       train_regret_winner_11[slice83],\n",
    "       train_regret_winner_12[slice83],\n",
    "       train_regret_winner_13[slice83],\n",
    "       train_regret_winner_14[slice83],\n",
    "       train_regret_winner_15[slice83],\n",
    "       train_regret_winner_16[slice83],\n",
    "       train_regret_winner_17[slice83],\n",
    "       train_regret_winner_18[slice83],\n",
    "       train_regret_winner_19[slice83],\n",
    "       train_regret_winner_20[slice83]]\n",
    "\n",
    "loser83_results = pd.DataFrame(loser83).sort_values(by=[0], ascending=False)\n",
    "winner83_results = pd.DataFrame(winner83).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser83 = np.asarray(loser83_results[4:5][0])[0]\n",
    "median_loser83 = np.asarray(loser83_results[9:10][0])[0]\n",
    "upper_loser83 = np.asarray(loser83_results[14:15][0])[0]\n",
    "\n",
    "lower_winner83 = np.asarray(winner83_results[4:5][0])[0]\n",
    "median_winner83 = np.asarray(winner83_results[9:10][0])[0]\n",
    "upper_winner83 = np.asarray(winner83_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration93 :\n",
    "\n",
    "slice93 = 92\n",
    "\n",
    "loser93 = [train_regret_loser_1[slice93],\n",
    "       train_regret_loser_2[slice93],\n",
    "       train_regret_loser_3[slice93],\n",
    "       train_regret_loser_4[slice93],\n",
    "       train_regret_loser_5[slice93],\n",
    "       train_regret_loser_6[slice93],\n",
    "       train_regret_loser_7[slice93],\n",
    "       train_regret_loser_8[slice93],\n",
    "       train_regret_loser_9[slice93],\n",
    "       train_regret_loser_10[slice93],\n",
    "       train_regret_loser_11[slice93],\n",
    "       train_regret_loser_12[slice93],\n",
    "       train_regret_loser_13[slice93],\n",
    "       train_regret_loser_14[slice93],\n",
    "       train_regret_loser_15[slice93],\n",
    "       train_regret_loser_16[slice93],\n",
    "       train_regret_loser_17[slice93],\n",
    "       train_regret_loser_18[slice93],\n",
    "       train_regret_loser_19[slice93],\n",
    "       train_regret_loser_20[slice93]]\n",
    "\n",
    "winner93 = [train_regret_winner_1[slice93],\n",
    "       train_regret_winner_2[slice93],\n",
    "       train_regret_winner_3[slice93],\n",
    "       train_regret_winner_4[slice93],\n",
    "       train_regret_winner_5[slice93],\n",
    "       train_regret_winner_6[slice93],\n",
    "       train_regret_winner_7[slice93],\n",
    "       train_regret_winner_8[slice93],\n",
    "       train_regret_winner_9[slice93],\n",
    "       train_regret_winner_10[slice93],\n",
    "       train_regret_winner_11[slice93],\n",
    "       train_regret_winner_12[slice93],\n",
    "       train_regret_winner_13[slice93],\n",
    "       train_regret_winner_14[slice93],\n",
    "       train_regret_winner_15[slice93],\n",
    "       train_regret_winner_16[slice93],\n",
    "       train_regret_winner_17[slice93],\n",
    "       train_regret_winner_18[slice93],\n",
    "       train_regret_winner_19[slice93],\n",
    "       train_regret_winner_20[slice93]]\n",
    "\n",
    "loser93_results = pd.DataFrame(loser93).sort_values(by=[0], ascending=False)\n",
    "winner93_results = pd.DataFrame(winner93).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser93 = np.asarray(loser93_results[4:5][0])[0]\n",
    "median_loser93 = np.asarray(loser93_results[9:10][0])[0]\n",
    "upper_loser93 = np.asarray(loser93_results[14:15][0])[0]\n",
    "\n",
    "lower_winner93 = np.asarray(winner93_results[4:5][0])[0]\n",
    "median_winner93 = np.asarray(winner93_results[9:10][0])[0]\n",
    "upper_winner93 = np.asarray(winner93_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration4 :\n",
    "\n",
    "slice4 = 3\n",
    "\n",
    "loser4 = [train_regret_loser_1[slice4],\n",
    "       train_regret_loser_2[slice4],\n",
    "       train_regret_loser_3[slice4],\n",
    "       train_regret_loser_4[slice4],\n",
    "       train_regret_loser_5[slice4],\n",
    "       train_regret_loser_6[slice4],\n",
    "       train_regret_loser_7[slice4],\n",
    "       train_regret_loser_8[slice4],\n",
    "       train_regret_loser_9[slice4],\n",
    "       train_regret_loser_10[slice4],\n",
    "       train_regret_loser_11[slice4],\n",
    "       train_regret_loser_12[slice4],\n",
    "       train_regret_loser_13[slice4],\n",
    "       train_regret_loser_14[slice4],\n",
    "       train_regret_loser_15[slice4],\n",
    "       train_regret_loser_16[slice4],\n",
    "       train_regret_loser_17[slice4],\n",
    "       train_regret_loser_18[slice4],\n",
    "       train_regret_loser_19[slice4],\n",
    "       train_regret_loser_20[slice4]]\n",
    "\n",
    "winner4 = [train_regret_winner_1[slice4],\n",
    "       train_regret_winner_2[slice4],\n",
    "       train_regret_winner_3[slice4],\n",
    "       train_regret_winner_4[slice4],\n",
    "       train_regret_winner_5[slice4],\n",
    "       train_regret_winner_6[slice4],\n",
    "       train_regret_winner_7[slice4],\n",
    "       train_regret_winner_8[slice4],\n",
    "       train_regret_winner_9[slice4],\n",
    "       train_regret_winner_10[slice4],\n",
    "       train_regret_winner_11[slice4],\n",
    "       train_regret_winner_12[slice4],\n",
    "       train_regret_winner_13[slice4],\n",
    "       train_regret_winner_14[slice4],\n",
    "       train_regret_winner_15[slice4],\n",
    "       train_regret_winner_16[slice4],\n",
    "       train_regret_winner_17[slice4],\n",
    "       train_regret_winner_18[slice4],\n",
    "       train_regret_winner_19[slice4],\n",
    "       train_regret_winner_20[slice4]]\n",
    "\n",
    "loser4_results = pd.DataFrame(loser4).sort_values(by=[0], ascending=False)\n",
    "winner4_results = pd.DataFrame(winner4).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser4 = np.asarray(loser4_results[4:5][0])[0]\n",
    "median_loser4 = np.asarray(loser4_results[9:10][0])[0]\n",
    "upper_loser4 = np.asarray(loser4_results[14:15][0])[0]\n",
    "\n",
    "lower_winner4 = np.asarray(winner4_results[4:5][0])[0]\n",
    "median_winner4 = np.asarray(winner4_results[9:10][0])[0]\n",
    "upper_winner4 = np.asarray(winner4_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration14 :\n",
    "\n",
    "slice14 = 13\n",
    "\n",
    "loser14 = [train_regret_loser_1[slice14],\n",
    "       train_regret_loser_2[slice14],\n",
    "       train_regret_loser_3[slice14],\n",
    "       train_regret_loser_4[slice14],\n",
    "       train_regret_loser_5[slice14],\n",
    "       train_regret_loser_6[slice14],\n",
    "       train_regret_loser_7[slice14],\n",
    "       train_regret_loser_8[slice14],\n",
    "       train_regret_loser_9[slice14],\n",
    "       train_regret_loser_10[slice14],\n",
    "       train_regret_loser_11[slice14],\n",
    "       train_regret_loser_12[slice14],\n",
    "       train_regret_loser_13[slice14],\n",
    "       train_regret_loser_14[slice14],\n",
    "       train_regret_loser_15[slice14],\n",
    "       train_regret_loser_16[slice14],\n",
    "       train_regret_loser_17[slice14],\n",
    "       train_regret_loser_18[slice14],\n",
    "       train_regret_loser_19[slice14],\n",
    "       train_regret_loser_20[slice14]]\n",
    "\n",
    "winner14 = [train_regret_winner_1[slice14],\n",
    "       train_regret_winner_2[slice14],\n",
    "       train_regret_winner_3[slice14],\n",
    "       train_regret_winner_4[slice14],\n",
    "       train_regret_winner_5[slice14],\n",
    "       train_regret_winner_6[slice14],\n",
    "       train_regret_winner_7[slice14],\n",
    "       train_regret_winner_8[slice14],\n",
    "       train_regret_winner_9[slice14],\n",
    "       train_regret_winner_10[slice14],\n",
    "       train_regret_winner_11[slice14],\n",
    "       train_regret_winner_12[slice14],\n",
    "       train_regret_winner_13[slice14],\n",
    "       train_regret_winner_14[slice14],\n",
    "       train_regret_winner_15[slice14],\n",
    "       train_regret_winner_16[slice14],\n",
    "       train_regret_winner_17[slice14],\n",
    "       train_regret_winner_18[slice14],\n",
    "       train_regret_winner_19[slice14],\n",
    "       train_regret_winner_20[slice14]]\n",
    "\n",
    "loser14_results = pd.DataFrame(loser14).sort_values(by=[0], ascending=False)\n",
    "winner14_results = pd.DataFrame(winner14).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser14 = np.asarray(loser14_results[4:5][0])[0]\n",
    "median_loser14 = np.asarray(loser14_results[9:10][0])[0]\n",
    "upper_loser14 = np.asarray(loser14_results[14:15][0])[0]\n",
    "\n",
    "lower_winner14 = np.asarray(winner14_results[4:5][0])[0]\n",
    "median_winner14 = np.asarray(winner14_results[9:10][0])[0]\n",
    "upper_winner14 = np.asarray(winner14_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration24 :\n",
    "\n",
    "slice24 = 23\n",
    "\n",
    "loser24 = [train_regret_loser_1[slice24],\n",
    "       train_regret_loser_2[slice24],\n",
    "       train_regret_loser_3[slice24],\n",
    "       train_regret_loser_4[slice24],\n",
    "       train_regret_loser_5[slice24],\n",
    "       train_regret_loser_6[slice24],\n",
    "       train_regret_loser_7[slice24],\n",
    "       train_regret_loser_8[slice24],\n",
    "       train_regret_loser_9[slice24],\n",
    "       train_regret_loser_10[slice24],\n",
    "       train_regret_loser_11[slice24],\n",
    "       train_regret_loser_12[slice24],\n",
    "       train_regret_loser_13[slice24],\n",
    "       train_regret_loser_14[slice24],\n",
    "       train_regret_loser_15[slice24],\n",
    "       train_regret_loser_16[slice24],\n",
    "       train_regret_loser_17[slice24],\n",
    "       train_regret_loser_18[slice24],\n",
    "       train_regret_loser_19[slice24],\n",
    "       train_regret_loser_20[slice24]]\n",
    "\n",
    "winner24 = [train_regret_winner_1[slice24],\n",
    "       train_regret_winner_2[slice24],\n",
    "       train_regret_winner_3[slice24],\n",
    "       train_regret_winner_4[slice24],\n",
    "       train_regret_winner_5[slice24],\n",
    "       train_regret_winner_6[slice24],\n",
    "       train_regret_winner_7[slice24],\n",
    "       train_regret_winner_8[slice24],\n",
    "       train_regret_winner_9[slice24],\n",
    "       train_regret_winner_10[slice24],\n",
    "       train_regret_winner_11[slice24],\n",
    "       train_regret_winner_12[slice24],\n",
    "       train_regret_winner_13[slice24],\n",
    "       train_regret_winner_14[slice24],\n",
    "       train_regret_winner_15[slice24],\n",
    "       train_regret_winner_16[slice24],\n",
    "       train_regret_winner_17[slice24],\n",
    "       train_regret_winner_18[slice24],\n",
    "       train_regret_winner_19[slice24],\n",
    "       train_regret_winner_20[slice24]]\n",
    "\n",
    "loser24_results = pd.DataFrame(loser24).sort_values(by=[0], ascending=False)\n",
    "winner24_results = pd.DataFrame(winner24).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser24 = np.asarray(loser24_results[4:5][0])[0]\n",
    "median_loser24 = np.asarray(loser24_results[9:10][0])[0]\n",
    "upper_loser24 = np.asarray(loser24_results[14:15][0])[0]\n",
    "\n",
    "lower_winner24 = np.asarray(winner24_results[4:5][0])[0]\n",
    "median_winner24 = np.asarray(winner24_results[9:10][0])[0]\n",
    "upper_winner24 = np.asarray(winner24_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration34 :\n",
    "\n",
    "slice34 = 33\n",
    "\n",
    "loser34 = [train_regret_loser_1[slice34],\n",
    "       train_regret_loser_2[slice34],\n",
    "       train_regret_loser_3[slice34],\n",
    "       train_regret_loser_4[slice34],\n",
    "       train_regret_loser_5[slice34],\n",
    "       train_regret_loser_6[slice34],\n",
    "       train_regret_loser_7[slice34],\n",
    "       train_regret_loser_8[slice34],\n",
    "       train_regret_loser_9[slice34],\n",
    "       train_regret_loser_10[slice34],\n",
    "       train_regret_loser_11[slice34],\n",
    "       train_regret_loser_12[slice34],\n",
    "       train_regret_loser_13[slice34],\n",
    "       train_regret_loser_14[slice34],\n",
    "       train_regret_loser_15[slice34],\n",
    "       train_regret_loser_16[slice34],\n",
    "       train_regret_loser_17[slice34],\n",
    "       train_regret_loser_18[slice34],\n",
    "       train_regret_loser_19[slice34],\n",
    "       train_regret_loser_20[slice34]]\n",
    "\n",
    "winner34 = [train_regret_winner_1[slice34],\n",
    "       train_regret_winner_2[slice34],\n",
    "       train_regret_winner_3[slice34],\n",
    "       train_regret_winner_4[slice34],\n",
    "       train_regret_winner_5[slice34],\n",
    "       train_regret_winner_6[slice34],\n",
    "       train_regret_winner_7[slice34],\n",
    "       train_regret_winner_8[slice34],\n",
    "       train_regret_winner_9[slice34],\n",
    "       train_regret_winner_10[slice34],\n",
    "       train_regret_winner_11[slice34],\n",
    "       train_regret_winner_12[slice34],\n",
    "       train_regret_winner_13[slice34],\n",
    "       train_regret_winner_14[slice34],\n",
    "       train_regret_winner_15[slice34],\n",
    "       train_regret_winner_16[slice34],\n",
    "       train_regret_winner_17[slice34],\n",
    "       train_regret_winner_18[slice34],\n",
    "       train_regret_winner_19[slice34],\n",
    "       train_regret_winner_20[slice34]]\n",
    "\n",
    "loser34_results = pd.DataFrame(loser34).sort_values(by=[0], ascending=False)\n",
    "winner34_results = pd.DataFrame(winner34).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser34 = np.asarray(loser34_results[4:5][0])[0]\n",
    "median_loser34 = np.asarray(loser34_results[9:10][0])[0]\n",
    "upper_loser34 = np.asarray(loser34_results[14:15][0])[0]\n",
    "\n",
    "lower_winner34 = np.asarray(winner34_results[4:5][0])[0]\n",
    "median_winner34 = np.asarray(winner34_results[9:10][0])[0]\n",
    "upper_winner34 = np.asarray(winner34_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration44 :\n",
    "\n",
    "slice44 = 43\n",
    "\n",
    "loser44 = [train_regret_loser_1[slice44],\n",
    "       train_regret_loser_2[slice44],\n",
    "       train_regret_loser_3[slice44],\n",
    "       train_regret_loser_4[slice44],\n",
    "       train_regret_loser_5[slice44],\n",
    "       train_regret_loser_6[slice44],\n",
    "       train_regret_loser_7[slice44],\n",
    "       train_regret_loser_8[slice44],\n",
    "       train_regret_loser_9[slice44],\n",
    "       train_regret_loser_10[slice44],\n",
    "       train_regret_loser_11[slice44],\n",
    "       train_regret_loser_12[slice44],\n",
    "       train_regret_loser_13[slice44],\n",
    "       train_regret_loser_14[slice44],\n",
    "       train_regret_loser_15[slice44],\n",
    "       train_regret_loser_16[slice44],\n",
    "       train_regret_loser_17[slice44],\n",
    "       train_regret_loser_18[slice44],\n",
    "       train_regret_loser_19[slice44],\n",
    "       train_regret_loser_20[slice44]]\n",
    "\n",
    "winner44 = [train_regret_winner_1[slice44],\n",
    "       train_regret_winner_2[slice44],\n",
    "       train_regret_winner_3[slice44],\n",
    "       train_regret_winner_4[slice44],\n",
    "       train_regret_winner_5[slice44],\n",
    "       train_regret_winner_6[slice44],\n",
    "       train_regret_winner_7[slice44],\n",
    "       train_regret_winner_8[slice44],\n",
    "       train_regret_winner_9[slice44],\n",
    "       train_regret_winner_10[slice44],\n",
    "       train_regret_winner_11[slice44],\n",
    "       train_regret_winner_12[slice44],\n",
    "       train_regret_winner_13[slice44],\n",
    "       train_regret_winner_14[slice44],\n",
    "       train_regret_winner_15[slice44],\n",
    "       train_regret_winner_16[slice44],\n",
    "       train_regret_winner_17[slice44],\n",
    "       train_regret_winner_18[slice44],\n",
    "       train_regret_winner_19[slice44],\n",
    "       train_regret_winner_20[slice44]]\n",
    "\n",
    "loser44_results = pd.DataFrame(loser44).sort_values(by=[0], ascending=False)\n",
    "winner44_results = pd.DataFrame(winner44).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser44 = np.asarray(loser44_results[4:5][0])[0]\n",
    "median_loser44 = np.asarray(loser44_results[9:10][0])[0]\n",
    "upper_loser44 = np.asarray(loser44_results[14:15][0])[0]\n",
    "\n",
    "lower_winner44 = np.asarray(winner44_results[4:5][0])[0]\n",
    "median_winner44 = np.asarray(winner44_results[9:10][0])[0]\n",
    "upper_winner44 = np.asarray(winner44_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration54 :\n",
    "\n",
    "slice54 = 53\n",
    "\n",
    "loser54 = [train_regret_loser_1[slice54],\n",
    "       train_regret_loser_2[slice54],\n",
    "       train_regret_loser_3[slice54],\n",
    "       train_regret_loser_4[slice54],\n",
    "       train_regret_loser_5[slice54],\n",
    "       train_regret_loser_6[slice54],\n",
    "       train_regret_loser_7[slice54],\n",
    "       train_regret_loser_8[slice54],\n",
    "       train_regret_loser_9[slice54],\n",
    "       train_regret_loser_10[slice54],\n",
    "       train_regret_loser_11[slice54],\n",
    "       train_regret_loser_12[slice54],\n",
    "       train_regret_loser_13[slice54],\n",
    "       train_regret_loser_14[slice54],\n",
    "       train_regret_loser_15[slice54],\n",
    "       train_regret_loser_16[slice54],\n",
    "       train_regret_loser_17[slice54],\n",
    "       train_regret_loser_18[slice54],\n",
    "       train_regret_loser_19[slice54],\n",
    "       train_regret_loser_20[slice54]]\n",
    "\n",
    "winner54 = [train_regret_winner_1[slice54],\n",
    "       train_regret_winner_2[slice54],\n",
    "       train_regret_winner_3[slice54],\n",
    "       train_regret_winner_4[slice54],\n",
    "       train_regret_winner_5[slice54],\n",
    "       train_regret_winner_6[slice54],\n",
    "       train_regret_winner_7[slice54],\n",
    "       train_regret_winner_8[slice54],\n",
    "       train_regret_winner_9[slice54],\n",
    "       train_regret_winner_10[slice54],\n",
    "       train_regret_winner_11[slice54],\n",
    "       train_regret_winner_12[slice54],\n",
    "       train_regret_winner_13[slice54],\n",
    "       train_regret_winner_14[slice54],\n",
    "       train_regret_winner_15[slice54],\n",
    "       train_regret_winner_16[slice54],\n",
    "       train_regret_winner_17[slice54],\n",
    "       train_regret_winner_18[slice54],\n",
    "       train_regret_winner_19[slice54],\n",
    "       train_regret_winner_20[slice54]]\n",
    "\n",
    "loser54_results = pd.DataFrame(loser54).sort_values(by=[0], ascending=False)\n",
    "winner54_results = pd.DataFrame(winner54).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser54 = np.asarray(loser54_results[4:5][0])[0]\n",
    "median_loser54 = np.asarray(loser54_results[9:10][0])[0]\n",
    "upper_loser54 = np.asarray(loser54_results[14:15][0])[0]\n",
    "\n",
    "lower_winner54 = np.asarray(winner54_results[4:5][0])[0]\n",
    "median_winner54 = np.asarray(winner54_results[9:10][0])[0]\n",
    "upper_winner54 = np.asarray(winner54_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration64 :\n",
    "\n",
    "slice64 = 63\n",
    "\n",
    "loser64 = [train_regret_loser_1[slice64],\n",
    "       train_regret_loser_2[slice64],\n",
    "       train_regret_loser_3[slice64],\n",
    "       train_regret_loser_4[slice64],\n",
    "       train_regret_loser_5[slice64],\n",
    "       train_regret_loser_6[slice64],\n",
    "       train_regret_loser_7[slice64],\n",
    "       train_regret_loser_8[slice64],\n",
    "       train_regret_loser_9[slice64],\n",
    "       train_regret_loser_10[slice64],\n",
    "       train_regret_loser_11[slice64],\n",
    "       train_regret_loser_12[slice64],\n",
    "       train_regret_loser_13[slice64],\n",
    "       train_regret_loser_14[slice64],\n",
    "       train_regret_loser_15[slice64],\n",
    "       train_regret_loser_16[slice64],\n",
    "       train_regret_loser_17[slice64],\n",
    "       train_regret_loser_18[slice64],\n",
    "       train_regret_loser_19[slice64],\n",
    "       train_regret_loser_20[slice64]]\n",
    "\n",
    "winner64 = [train_regret_winner_1[slice64],\n",
    "       train_regret_winner_2[slice64],\n",
    "       train_regret_winner_3[slice64],\n",
    "       train_regret_winner_4[slice64],\n",
    "       train_regret_winner_5[slice64],\n",
    "       train_regret_winner_6[slice64],\n",
    "       train_regret_winner_7[slice64],\n",
    "       train_regret_winner_8[slice64],\n",
    "       train_regret_winner_9[slice64],\n",
    "       train_regret_winner_10[slice64],\n",
    "       train_regret_winner_11[slice64],\n",
    "       train_regret_winner_12[slice64],\n",
    "       train_regret_winner_13[slice64],\n",
    "       train_regret_winner_14[slice64],\n",
    "       train_regret_winner_15[slice64],\n",
    "       train_regret_winner_16[slice64],\n",
    "       train_regret_winner_17[slice64],\n",
    "       train_regret_winner_18[slice64],\n",
    "       train_regret_winner_19[slice64],\n",
    "       train_regret_winner_20[slice64]]\n",
    "\n",
    "loser64_results = pd.DataFrame(loser64).sort_values(by=[0], ascending=False)\n",
    "winner64_results = pd.DataFrame(winner64).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser64 = np.asarray(loser64_results[4:5][0])[0]\n",
    "median_loser64 = np.asarray(loser64_results[9:10][0])[0]\n",
    "upper_loser64 = np.asarray(loser64_results[14:15][0])[0]\n",
    "\n",
    "lower_winner64 = np.asarray(winner64_results[4:5][0])[0]\n",
    "median_winner64 = np.asarray(winner64_results[9:10][0])[0]\n",
    "upper_winner64 = np.asarray(winner64_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration74 :\n",
    "\n",
    "slice74 = 73\n",
    "\n",
    "loser74 = [train_regret_loser_1[slice74],\n",
    "       train_regret_loser_2[slice74],\n",
    "       train_regret_loser_3[slice74],\n",
    "       train_regret_loser_4[slice74],\n",
    "       train_regret_loser_5[slice74],\n",
    "       train_regret_loser_6[slice74],\n",
    "       train_regret_loser_7[slice74],\n",
    "       train_regret_loser_8[slice74],\n",
    "       train_regret_loser_9[slice74],\n",
    "       train_regret_loser_10[slice74],\n",
    "       train_regret_loser_11[slice74],\n",
    "       train_regret_loser_12[slice74],\n",
    "       train_regret_loser_13[slice74],\n",
    "       train_regret_loser_14[slice74],\n",
    "       train_regret_loser_15[slice74],\n",
    "       train_regret_loser_16[slice74],\n",
    "       train_regret_loser_17[slice74],\n",
    "       train_regret_loser_18[slice74],\n",
    "       train_regret_loser_19[slice74],\n",
    "       train_regret_loser_20[slice74]]\n",
    "\n",
    "winner74 = [train_regret_winner_1[slice74],\n",
    "       train_regret_winner_2[slice74],\n",
    "       train_regret_winner_3[slice74],\n",
    "       train_regret_winner_4[slice74],\n",
    "       train_regret_winner_5[slice74],\n",
    "       train_regret_winner_6[slice74],\n",
    "       train_regret_winner_7[slice74],\n",
    "       train_regret_winner_8[slice74],\n",
    "       train_regret_winner_9[slice74],\n",
    "       train_regret_winner_10[slice74],\n",
    "       train_regret_winner_11[slice74],\n",
    "       train_regret_winner_12[slice74],\n",
    "       train_regret_winner_13[slice74],\n",
    "       train_regret_winner_14[slice74],\n",
    "       train_regret_winner_15[slice74],\n",
    "       train_regret_winner_16[slice74],\n",
    "       train_regret_winner_17[slice74],\n",
    "       train_regret_winner_18[slice74],\n",
    "       train_regret_winner_19[slice74],\n",
    "       train_regret_winner_20[slice74]]\n",
    "\n",
    "loser74_results = pd.DataFrame(loser74).sort_values(by=[0], ascending=False)\n",
    "winner74_results = pd.DataFrame(winner74).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser74 = np.asarray(loser74_results[4:5][0])[0]\n",
    "median_loser74 = np.asarray(loser74_results[9:10][0])[0]\n",
    "upper_loser74 = np.asarray(loser74_results[14:15][0])[0]\n",
    "\n",
    "lower_winner74 = np.asarray(winner74_results[4:5][0])[0]\n",
    "median_winner74 = np.asarray(winner74_results[9:10][0])[0]\n",
    "upper_winner74 = np.asarray(winner74_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration84 :\n",
    "\n",
    "slice84 = 83\n",
    "\n",
    "loser84 = [train_regret_loser_1[slice84],\n",
    "       train_regret_loser_2[slice84],\n",
    "       train_regret_loser_3[slice84],\n",
    "       train_regret_loser_4[slice84],\n",
    "       train_regret_loser_5[slice84],\n",
    "       train_regret_loser_6[slice84],\n",
    "       train_regret_loser_7[slice84],\n",
    "       train_regret_loser_8[slice84],\n",
    "       train_regret_loser_9[slice84],\n",
    "       train_regret_loser_10[slice84],\n",
    "       train_regret_loser_11[slice84],\n",
    "       train_regret_loser_12[slice84],\n",
    "       train_regret_loser_13[slice84],\n",
    "       train_regret_loser_14[slice84],\n",
    "       train_regret_loser_15[slice84],\n",
    "       train_regret_loser_16[slice84],\n",
    "       train_regret_loser_17[slice84],\n",
    "       train_regret_loser_18[slice84],\n",
    "       train_regret_loser_19[slice84],\n",
    "       train_regret_loser_20[slice84]]\n",
    "\n",
    "winner84 = [train_regret_winner_1[slice84],\n",
    "       train_regret_winner_2[slice84],\n",
    "       train_regret_winner_3[slice84],\n",
    "       train_regret_winner_4[slice84],\n",
    "       train_regret_winner_5[slice84],\n",
    "       train_regret_winner_6[slice84],\n",
    "       train_regret_winner_7[slice84],\n",
    "       train_regret_winner_8[slice84],\n",
    "       train_regret_winner_9[slice84],\n",
    "       train_regret_winner_10[slice84],\n",
    "       train_regret_winner_11[slice84],\n",
    "       train_regret_winner_12[slice84],\n",
    "       train_regret_winner_13[slice84],\n",
    "       train_regret_winner_14[slice84],\n",
    "       train_regret_winner_15[slice84],\n",
    "       train_regret_winner_16[slice84],\n",
    "       train_regret_winner_17[slice84],\n",
    "       train_regret_winner_18[slice84],\n",
    "       train_regret_winner_19[slice84],\n",
    "       train_regret_winner_20[slice84]]\n",
    "\n",
    "loser84_results = pd.DataFrame(loser84).sort_values(by=[0], ascending=False)\n",
    "winner84_results = pd.DataFrame(winner84).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser84 = np.asarray(loser84_results[4:5][0])[0]\n",
    "median_loser84 = np.asarray(loser84_results[9:10][0])[0]\n",
    "upper_loser84 = np.asarray(loser84_results[14:15][0])[0]\n",
    "\n",
    "lower_winner84 = np.asarray(winner84_results[4:5][0])[0]\n",
    "median_winner84 = np.asarray(winner84_results[9:10][0])[0]\n",
    "upper_winner84 = np.asarray(winner84_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration94 :\n",
    "\n",
    "slice94 = 93\n",
    "\n",
    "loser94 = [train_regret_loser_1[slice94],\n",
    "       train_regret_loser_2[slice94],\n",
    "       train_regret_loser_3[slice94],\n",
    "       train_regret_loser_4[slice94],\n",
    "       train_regret_loser_5[slice94],\n",
    "       train_regret_loser_6[slice94],\n",
    "       train_regret_loser_7[slice94],\n",
    "       train_regret_loser_8[slice94],\n",
    "       train_regret_loser_9[slice94],\n",
    "       train_regret_loser_10[slice94],\n",
    "       train_regret_loser_11[slice94],\n",
    "       train_regret_loser_12[slice94],\n",
    "       train_regret_loser_13[slice94],\n",
    "       train_regret_loser_14[slice94],\n",
    "       train_regret_loser_15[slice94],\n",
    "       train_regret_loser_16[slice94],\n",
    "       train_regret_loser_17[slice94],\n",
    "       train_regret_loser_18[slice94],\n",
    "       train_regret_loser_19[slice94],\n",
    "       train_regret_loser_20[slice94]]\n",
    "\n",
    "winner94 = [train_regret_winner_1[slice94],\n",
    "       train_regret_winner_2[slice94],\n",
    "       train_regret_winner_3[slice94],\n",
    "       train_regret_winner_4[slice94],\n",
    "       train_regret_winner_5[slice94],\n",
    "       train_regret_winner_6[slice94],\n",
    "       train_regret_winner_7[slice94],\n",
    "       train_regret_winner_8[slice94],\n",
    "       train_regret_winner_9[slice94],\n",
    "       train_regret_winner_10[slice94],\n",
    "       train_regret_winner_11[slice94],\n",
    "       train_regret_winner_12[slice94],\n",
    "       train_regret_winner_13[slice94],\n",
    "       train_regret_winner_14[slice94],\n",
    "       train_regret_winner_15[slice94],\n",
    "       train_regret_winner_16[slice94],\n",
    "       train_regret_winner_17[slice94],\n",
    "       train_regret_winner_18[slice94],\n",
    "       train_regret_winner_19[slice94],\n",
    "       train_regret_winner_20[slice94]]\n",
    "\n",
    "loser94_results = pd.DataFrame(loser94).sort_values(by=[0], ascending=False)\n",
    "winner94_results = pd.DataFrame(winner94).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser94 = np.asarray(loser94_results[4:5][0])[0]\n",
    "median_loser94 = np.asarray(loser94_results[9:10][0])[0]\n",
    "upper_loser94 = np.asarray(loser94_results[14:15][0])[0]\n",
    "\n",
    "lower_winner94 = np.asarray(winner94_results[4:5][0])[0]\n",
    "median_winner94 = np.asarray(winner94_results[9:10][0])[0]\n",
    "upper_winner94 = np.asarray(winner94_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration5 :\n",
    "\n",
    "slice5 = 4\n",
    "\n",
    "loser5 = [train_regret_loser_1[slice5],\n",
    "       train_regret_loser_2[slice5],\n",
    "       train_regret_loser_3[slice5],\n",
    "       train_regret_loser_4[slice5],\n",
    "       train_regret_loser_5[slice5],\n",
    "       train_regret_loser_6[slice5],\n",
    "       train_regret_loser_7[slice5],\n",
    "       train_regret_loser_8[slice5],\n",
    "       train_regret_loser_9[slice5],\n",
    "       train_regret_loser_10[slice5],\n",
    "       train_regret_loser_11[slice5],\n",
    "       train_regret_loser_12[slice5],\n",
    "       train_regret_loser_13[slice5],\n",
    "       train_regret_loser_14[slice5],\n",
    "       train_regret_loser_15[slice5],\n",
    "       train_regret_loser_16[slice5],\n",
    "       train_regret_loser_17[slice5],\n",
    "       train_regret_loser_18[slice5],\n",
    "       train_regret_loser_19[slice5],\n",
    "       train_regret_loser_20[slice5]]\n",
    "\n",
    "winner5 = [train_regret_winner_1[slice5],\n",
    "       train_regret_winner_2[slice5],\n",
    "       train_regret_winner_3[slice5],\n",
    "       train_regret_winner_4[slice5],\n",
    "       train_regret_winner_5[slice5],\n",
    "       train_regret_winner_6[slice5],\n",
    "       train_regret_winner_7[slice5],\n",
    "       train_regret_winner_8[slice5],\n",
    "       train_regret_winner_9[slice5],\n",
    "       train_regret_winner_10[slice5],\n",
    "       train_regret_winner_11[slice5],\n",
    "       train_regret_winner_12[slice5],\n",
    "       train_regret_winner_13[slice5],\n",
    "       train_regret_winner_14[slice5],\n",
    "       train_regret_winner_15[slice5],\n",
    "       train_regret_winner_16[slice5],\n",
    "       train_regret_winner_17[slice5],\n",
    "       train_regret_winner_18[slice5],\n",
    "       train_regret_winner_19[slice5],\n",
    "       train_regret_winner_20[slice5]]\n",
    "\n",
    "loser5_results = pd.DataFrame(loser5).sort_values(by=[0], ascending=False)\n",
    "winner5_results = pd.DataFrame(winner5).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser5 = np.asarray(loser5_results[4:5][0])[0]\n",
    "median_loser5 = np.asarray(loser5_results[9:10][0])[0]\n",
    "upper_loser5 = np.asarray(loser5_results[14:15][0])[0]\n",
    "\n",
    "lower_winner5 = np.asarray(winner5_results[4:5][0])[0]\n",
    "median_winner5 = np.asarray(winner5_results[9:10][0])[0]\n",
    "upper_winner5 = np.asarray(winner5_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration15 :\n",
    "\n",
    "slice15 = 14\n",
    "\n",
    "loser15 = [train_regret_loser_1[slice15],\n",
    "       train_regret_loser_2[slice15],\n",
    "       train_regret_loser_3[slice15],\n",
    "       train_regret_loser_4[slice15],\n",
    "       train_regret_loser_5[slice15],\n",
    "       train_regret_loser_6[slice15],\n",
    "       train_regret_loser_7[slice15],\n",
    "       train_regret_loser_8[slice15],\n",
    "       train_regret_loser_9[slice15],\n",
    "       train_regret_loser_10[slice15],\n",
    "       train_regret_loser_11[slice15],\n",
    "       train_regret_loser_12[slice15],\n",
    "       train_regret_loser_13[slice15],\n",
    "       train_regret_loser_14[slice15],\n",
    "       train_regret_loser_15[slice15],\n",
    "       train_regret_loser_16[slice15],\n",
    "       train_regret_loser_17[slice15],\n",
    "       train_regret_loser_18[slice15],\n",
    "       train_regret_loser_19[slice15],\n",
    "       train_regret_loser_20[slice15]]\n",
    "\n",
    "winner15 = [train_regret_winner_1[slice15],\n",
    "       train_regret_winner_2[slice15],\n",
    "       train_regret_winner_3[slice15],\n",
    "       train_regret_winner_4[slice15],\n",
    "       train_regret_winner_5[slice15],\n",
    "       train_regret_winner_6[slice15],\n",
    "       train_regret_winner_7[slice15],\n",
    "       train_regret_winner_8[slice15],\n",
    "       train_regret_winner_9[slice15],\n",
    "       train_regret_winner_10[slice15],\n",
    "       train_regret_winner_11[slice15],\n",
    "       train_regret_winner_12[slice15],\n",
    "       train_regret_winner_13[slice15],\n",
    "       train_regret_winner_14[slice15],\n",
    "       train_regret_winner_15[slice15],\n",
    "       train_regret_winner_16[slice15],\n",
    "       train_regret_winner_17[slice15],\n",
    "       train_regret_winner_18[slice15],\n",
    "       train_regret_winner_19[slice15],\n",
    "       train_regret_winner_20[slice15]]\n",
    "\n",
    "loser15_results = pd.DataFrame(loser15).sort_values(by=[0], ascending=False)\n",
    "winner15_results = pd.DataFrame(winner15).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser15 = np.asarray(loser15_results[4:5][0])[0]\n",
    "median_loser15 = np.asarray(loser15_results[9:10][0])[0]\n",
    "upper_loser15 = np.asarray(loser15_results[14:15][0])[0]\n",
    "\n",
    "lower_winner15 = np.asarray(winner15_results[4:5][0])[0]\n",
    "median_winner15 = np.asarray(winner15_results[9:10][0])[0]\n",
    "upper_winner15 = np.asarray(winner15_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration25 :\n",
    "\n",
    "slice25 = 24\n",
    "\n",
    "loser25 = [train_regret_loser_1[slice25],\n",
    "       train_regret_loser_2[slice25],\n",
    "       train_regret_loser_3[slice25],\n",
    "       train_regret_loser_4[slice25],\n",
    "       train_regret_loser_5[slice25],\n",
    "       train_regret_loser_6[slice25],\n",
    "       train_regret_loser_7[slice25],\n",
    "       train_regret_loser_8[slice25],\n",
    "       train_regret_loser_9[slice25],\n",
    "       train_regret_loser_10[slice25],\n",
    "       train_regret_loser_11[slice25],\n",
    "       train_regret_loser_12[slice25],\n",
    "       train_regret_loser_13[slice25],\n",
    "       train_regret_loser_14[slice25],\n",
    "       train_regret_loser_15[slice25],\n",
    "       train_regret_loser_16[slice25],\n",
    "       train_regret_loser_17[slice25],\n",
    "       train_regret_loser_18[slice25],\n",
    "       train_regret_loser_19[slice25],\n",
    "       train_regret_loser_20[slice25]]\n",
    "\n",
    "winner25 = [train_regret_winner_1[slice25],\n",
    "       train_regret_winner_2[slice25],\n",
    "       train_regret_winner_3[slice25],\n",
    "       train_regret_winner_4[slice25],\n",
    "       train_regret_winner_5[slice25],\n",
    "       train_regret_winner_6[slice25],\n",
    "       train_regret_winner_7[slice25],\n",
    "       train_regret_winner_8[slice25],\n",
    "       train_regret_winner_9[slice25],\n",
    "       train_regret_winner_10[slice25],\n",
    "       train_regret_winner_11[slice25],\n",
    "       train_regret_winner_12[slice25],\n",
    "       train_regret_winner_13[slice25],\n",
    "       train_regret_winner_14[slice25],\n",
    "       train_regret_winner_15[slice25],\n",
    "       train_regret_winner_16[slice25],\n",
    "       train_regret_winner_17[slice25],\n",
    "       train_regret_winner_18[slice25],\n",
    "       train_regret_winner_19[slice25],\n",
    "       train_regret_winner_20[slice25]]\n",
    "\n",
    "loser25_results = pd.DataFrame(loser25).sort_values(by=[0], ascending=False)\n",
    "winner25_results = pd.DataFrame(winner25).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser25 = np.asarray(loser25_results[4:5][0])[0]\n",
    "median_loser25 = np.asarray(loser25_results[9:10][0])[0]\n",
    "upper_loser25 = np.asarray(loser25_results[14:15][0])[0]\n",
    "\n",
    "lower_winner25 = np.asarray(winner25_results[4:5][0])[0]\n",
    "median_winner25 = np.asarray(winner25_results[9:10][0])[0]\n",
    "upper_winner25= np.asarray(winner25_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration35 :\n",
    "\n",
    "slice35 = 34\n",
    "\n",
    "loser35 = [train_regret_loser_1[slice35],\n",
    "       train_regret_loser_2[slice35],\n",
    "       train_regret_loser_3[slice35],\n",
    "       train_regret_loser_4[slice35],\n",
    "       train_regret_loser_5[slice35],\n",
    "       train_regret_loser_6[slice35],\n",
    "       train_regret_loser_7[slice35],\n",
    "       train_regret_loser_8[slice35],\n",
    "       train_regret_loser_9[slice35],\n",
    "       train_regret_loser_10[slice35],\n",
    "       train_regret_loser_11[slice35],\n",
    "       train_regret_loser_12[slice35],\n",
    "       train_regret_loser_13[slice35],\n",
    "       train_regret_loser_14[slice35],\n",
    "       train_regret_loser_15[slice35],\n",
    "       train_regret_loser_16[slice35],\n",
    "       train_regret_loser_17[slice35],\n",
    "       train_regret_loser_18[slice35],\n",
    "       train_regret_loser_19[slice35],\n",
    "       train_regret_loser_20[slice35]]\n",
    "\n",
    "winner35 = [train_regret_winner_1[slice35],\n",
    "       train_regret_winner_2[slice35],\n",
    "       train_regret_winner_3[slice35],\n",
    "       train_regret_winner_4[slice35],\n",
    "       train_regret_winner_5[slice35],\n",
    "       train_regret_winner_6[slice35],\n",
    "       train_regret_winner_7[slice35],\n",
    "       train_regret_winner_8[slice35],\n",
    "       train_regret_winner_9[slice35],\n",
    "       train_regret_winner_10[slice35],\n",
    "       train_regret_winner_11[slice35],\n",
    "       train_regret_winner_12[slice35],\n",
    "       train_regret_winner_13[slice35],\n",
    "       train_regret_winner_14[slice35],\n",
    "       train_regret_winner_15[slice35],\n",
    "       train_regret_winner_16[slice35],\n",
    "       train_regret_winner_17[slice35],\n",
    "       train_regret_winner_18[slice35],\n",
    "       train_regret_winner_19[slice35],\n",
    "       train_regret_winner_20[slice35]]\n",
    "\n",
    "loser35_results = pd.DataFrame(loser35).sort_values(by=[0], ascending=False)\n",
    "winner35_results = pd.DataFrame(winner35).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser35 = np.asarray(loser35_results[4:5][0])[0]\n",
    "median_loser35 = np.asarray(loser35_results[9:10][0])[0]\n",
    "upper_loser35 = np.asarray(loser35_results[14:15][0])[0]\n",
    "\n",
    "lower_winner35 = np.asarray(winner35_results[4:5][0])[0]\n",
    "median_winner35 = np.asarray(winner35_results[9:10][0])[0]\n",
    "upper_winner35 = np.asarray(winner35_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration45 :\n",
    "\n",
    "slice45 = 44\n",
    "\n",
    "loser45 = [train_regret_loser_1[slice45],\n",
    "       train_regret_loser_2[slice45],\n",
    "       train_regret_loser_3[slice45],\n",
    "       train_regret_loser_4[slice45],\n",
    "       train_regret_loser_5[slice45],\n",
    "       train_regret_loser_6[slice45],\n",
    "       train_regret_loser_7[slice45],\n",
    "       train_regret_loser_8[slice45],\n",
    "       train_regret_loser_9[slice45],\n",
    "       train_regret_loser_10[slice45],\n",
    "       train_regret_loser_11[slice45],\n",
    "       train_regret_loser_12[slice45],\n",
    "       train_regret_loser_13[slice45],\n",
    "       train_regret_loser_14[slice45],\n",
    "       train_regret_loser_15[slice45],\n",
    "       train_regret_loser_16[slice45],\n",
    "       train_regret_loser_17[slice45],\n",
    "       train_regret_loser_18[slice45],\n",
    "       train_regret_loser_19[slice45],\n",
    "       train_regret_loser_20[slice45]]\n",
    "\n",
    "winner45 = [train_regret_winner_1[slice45],\n",
    "       train_regret_winner_2[slice45],\n",
    "       train_regret_winner_3[slice45],\n",
    "       train_regret_winner_4[slice45],\n",
    "       train_regret_winner_5[slice45],\n",
    "       train_regret_winner_6[slice45],\n",
    "       train_regret_winner_7[slice45],\n",
    "       train_regret_winner_8[slice45],\n",
    "       train_regret_winner_9[slice45],\n",
    "       train_regret_winner_10[slice45],\n",
    "       train_regret_winner_11[slice45],\n",
    "       train_regret_winner_12[slice45],\n",
    "       train_regret_winner_13[slice45],\n",
    "       train_regret_winner_14[slice45],\n",
    "       train_regret_winner_15[slice45],\n",
    "       train_regret_winner_16[slice45],\n",
    "       train_regret_winner_17[slice45],\n",
    "       train_regret_winner_18[slice45],\n",
    "       train_regret_winner_19[slice45],\n",
    "       train_regret_winner_20[slice45]]\n",
    "\n",
    "loser45_results = pd.DataFrame(loser45).sort_values(by=[0], ascending=False)\n",
    "winner45_results = pd.DataFrame(winner45).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser45 = np.asarray(loser45_results[4:5][0])[0]\n",
    "median_loser45 = np.asarray(loser45_results[9:10][0])[0]\n",
    "upper_loser45 = np.asarray(loser45_results[14:15][0])[0]\n",
    "\n",
    "lower_winner45 = np.asarray(winner45_results[4:5][0])[0]\n",
    "median_winner45 = np.asarray(winner45_results[9:10][0])[0]\n",
    "upper_winner45 = np.asarray(winner45_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration55 :\n",
    "\n",
    "slice55 = 54\n",
    "\n",
    "loser55 = [train_regret_loser_1[slice55],\n",
    "       train_regret_loser_2[slice55],\n",
    "       train_regret_loser_3[slice55],\n",
    "       train_regret_loser_4[slice55],\n",
    "       train_regret_loser_5[slice55],\n",
    "       train_regret_loser_6[slice55],\n",
    "       train_regret_loser_7[slice55],\n",
    "       train_regret_loser_8[slice55],\n",
    "       train_regret_loser_9[slice55],\n",
    "       train_regret_loser_10[slice55],\n",
    "       train_regret_loser_11[slice55],\n",
    "       train_regret_loser_12[slice55],\n",
    "       train_regret_loser_13[slice55],\n",
    "       train_regret_loser_14[slice55],\n",
    "       train_regret_loser_15[slice55],\n",
    "       train_regret_loser_16[slice55],\n",
    "       train_regret_loser_17[slice55],\n",
    "       train_regret_loser_18[slice55],\n",
    "       train_regret_loser_19[slice55],\n",
    "       train_regret_loser_20[slice55]]\n",
    "\n",
    "winner55 = [train_regret_winner_1[slice55],\n",
    "       train_regret_winner_2[slice55],\n",
    "       train_regret_winner_3[slice55],\n",
    "       train_regret_winner_4[slice55],\n",
    "       train_regret_winner_5[slice55],\n",
    "       train_regret_winner_6[slice55],\n",
    "       train_regret_winner_7[slice55],\n",
    "       train_regret_winner_8[slice55],\n",
    "       train_regret_winner_9[slice55],\n",
    "       train_regret_winner_10[slice55],\n",
    "       train_regret_winner_11[slice55],\n",
    "       train_regret_winner_12[slice55],\n",
    "       train_regret_winner_13[slice55],\n",
    "       train_regret_winner_14[slice55],\n",
    "       train_regret_winner_15[slice55],\n",
    "       train_regret_winner_16[slice55],\n",
    "       train_regret_winner_17[slice55],\n",
    "       train_regret_winner_18[slice55],\n",
    "       train_regret_winner_19[slice55],\n",
    "       train_regret_winner_20[slice55]]\n",
    "\n",
    "loser55_results = pd.DataFrame(loser55).sort_values(by=[0], ascending=False)\n",
    "winner55_results = pd.DataFrame(winner55).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser55 = np.asarray(loser55_results[4:5][0])[0]\n",
    "median_loser55 = np.asarray(loser55_results[9:10][0])[0]\n",
    "upper_loser55 = np.asarray(loser55_results[14:15][0])[0]\n",
    "\n",
    "lower_winner55 = np.asarray(winner55_results[4:5][0])[0]\n",
    "median_winner55 = np.asarray(winner55_results[9:10][0])[0]\n",
    "upper_winner55 = np.asarray(winner55_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration65 :\n",
    "\n",
    "slice65 = 64\n",
    "\n",
    "loser65 = [train_regret_loser_1[slice65],\n",
    "       train_regret_loser_2[slice65],\n",
    "       train_regret_loser_3[slice65],\n",
    "       train_regret_loser_4[slice65],\n",
    "       train_regret_loser_5[slice65],\n",
    "       train_regret_loser_6[slice65],\n",
    "       train_regret_loser_7[slice65],\n",
    "       train_regret_loser_8[slice65],\n",
    "       train_regret_loser_9[slice65],\n",
    "       train_regret_loser_10[slice65],\n",
    "       train_regret_loser_11[slice65],\n",
    "       train_regret_loser_12[slice65],\n",
    "       train_regret_loser_13[slice65],\n",
    "       train_regret_loser_14[slice65],\n",
    "       train_regret_loser_15[slice65],\n",
    "       train_regret_loser_16[slice65],\n",
    "       train_regret_loser_17[slice65],\n",
    "       train_regret_loser_18[slice65],\n",
    "       train_regret_loser_19[slice65],\n",
    "       train_regret_loser_20[slice65]]\n",
    "\n",
    "winner65 = [train_regret_winner_1[slice65],\n",
    "       train_regret_winner_2[slice65],\n",
    "       train_regret_winner_3[slice65],\n",
    "       train_regret_winner_4[slice65],\n",
    "       train_regret_winner_5[slice65],\n",
    "       train_regret_winner_6[slice65],\n",
    "       train_regret_winner_7[slice65],\n",
    "       train_regret_winner_8[slice65],\n",
    "       train_regret_winner_9[slice65],\n",
    "       train_regret_winner_10[slice65],\n",
    "       train_regret_winner_11[slice65],\n",
    "       train_regret_winner_12[slice65],\n",
    "       train_regret_winner_13[slice65],\n",
    "       train_regret_winner_14[slice65],\n",
    "       train_regret_winner_15[slice65],\n",
    "       train_regret_winner_16[slice65],\n",
    "       train_regret_winner_17[slice65],\n",
    "       train_regret_winner_18[slice65],\n",
    "       train_regret_winner_19[slice65],\n",
    "       train_regret_winner_20[slice65]]\n",
    "\n",
    "loser65_results = pd.DataFrame(loser65).sort_values(by=[0], ascending=False)\n",
    "winner65_results = pd.DataFrame(winner65).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser65 = np.asarray(loser65_results[4:5][0])[0]\n",
    "median_loser65 = np.asarray(loser65_results[9:10][0])[0]\n",
    "upper_loser65 = np.asarray(loser65_results[14:15][0])[0]\n",
    "\n",
    "lower_winner65 = np.asarray(winner65_results[4:5][0])[0]\n",
    "median_winner65 = np.asarray(winner65_results[9:10][0])[0]\n",
    "upper_winner65 = np.asarray(winner65_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration75 :\n",
    "\n",
    "slice75 = 74\n",
    "\n",
    "loser75 = [train_regret_loser_1[slice75],\n",
    "       train_regret_loser_2[slice75],\n",
    "       train_regret_loser_3[slice75],\n",
    "       train_regret_loser_4[slice75],\n",
    "       train_regret_loser_5[slice75],\n",
    "       train_regret_loser_6[slice75],\n",
    "       train_regret_loser_7[slice75],\n",
    "       train_regret_loser_8[slice75],\n",
    "       train_regret_loser_9[slice75],\n",
    "       train_regret_loser_10[slice75],\n",
    "       train_regret_loser_11[slice75],\n",
    "       train_regret_loser_12[slice75],\n",
    "       train_regret_loser_13[slice75],\n",
    "       train_regret_loser_14[slice75],\n",
    "       train_regret_loser_15[slice75],\n",
    "       train_regret_loser_16[slice75],\n",
    "       train_regret_loser_17[slice75],\n",
    "       train_regret_loser_18[slice75],\n",
    "       train_regret_loser_19[slice75],\n",
    "       train_regret_loser_20[slice75]]\n",
    "\n",
    "winner75 = [train_regret_winner_1[slice75],\n",
    "       train_regret_winner_2[slice75],\n",
    "       train_regret_winner_3[slice75],\n",
    "       train_regret_winner_4[slice75],\n",
    "       train_regret_winner_5[slice75],\n",
    "       train_regret_winner_6[slice75],\n",
    "       train_regret_winner_7[slice75],\n",
    "       train_regret_winner_8[slice75],\n",
    "       train_regret_winner_9[slice75],\n",
    "       train_regret_winner_10[slice75],\n",
    "       train_regret_winner_11[slice75],\n",
    "       train_regret_winner_12[slice75],\n",
    "       train_regret_winner_13[slice75],\n",
    "       train_regret_winner_14[slice75],\n",
    "       train_regret_winner_15[slice75],\n",
    "       train_regret_winner_16[slice75],\n",
    "       train_regret_winner_17[slice75],\n",
    "       train_regret_winner_18[slice75],\n",
    "       train_regret_winner_19[slice75],\n",
    "       train_regret_winner_20[slice75]]\n",
    "\n",
    "loser75_results = pd.DataFrame(loser75).sort_values(by=[0], ascending=False)\n",
    "winner75_results = pd.DataFrame(winner75).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser75 = np.asarray(loser75_results[4:5][0])[0]\n",
    "median_loser75 = np.asarray(loser75_results[9:10][0])[0]\n",
    "upper_loser75 = np.asarray(loser75_results[14:15][0])[0]\n",
    "\n",
    "lower_winner75 = np.asarray(winner75_results[4:5][0])[0]\n",
    "median_winner75 = np.asarray(winner75_results[9:10][0])[0]\n",
    "upper_winner75 = np.asarray(winner75_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration85 :\n",
    "\n",
    "slice85 = 84\n",
    "\n",
    "loser85 = [train_regret_loser_1[slice85],\n",
    "       train_regret_loser_2[slice85],\n",
    "       train_regret_loser_3[slice85],\n",
    "       train_regret_loser_4[slice85],\n",
    "       train_regret_loser_5[slice85],\n",
    "       train_regret_loser_6[slice85],\n",
    "       train_regret_loser_7[slice85],\n",
    "       train_regret_loser_8[slice85],\n",
    "       train_regret_loser_9[slice85],\n",
    "       train_regret_loser_10[slice85],\n",
    "       train_regret_loser_11[slice85],\n",
    "       train_regret_loser_12[slice85],\n",
    "       train_regret_loser_13[slice85],\n",
    "       train_regret_loser_14[slice85],\n",
    "       train_regret_loser_15[slice85],\n",
    "       train_regret_loser_16[slice85],\n",
    "       train_regret_loser_17[slice85],\n",
    "       train_regret_loser_18[slice85],\n",
    "       train_regret_loser_19[slice85],\n",
    "       train_regret_loser_20[slice85]]\n",
    "\n",
    "winner85 = [train_regret_winner_1[slice85],\n",
    "       train_regret_winner_2[slice85],\n",
    "       train_regret_winner_3[slice85],\n",
    "       train_regret_winner_4[slice85],\n",
    "       train_regret_winner_5[slice85],\n",
    "       train_regret_winner_6[slice85],\n",
    "       train_regret_winner_7[slice85],\n",
    "       train_regret_winner_8[slice85],\n",
    "       train_regret_winner_9[slice85],\n",
    "       train_regret_winner_10[slice85],\n",
    "       train_regret_winner_11[slice85],\n",
    "       train_regret_winner_12[slice85],\n",
    "       train_regret_winner_13[slice85],\n",
    "       train_regret_winner_14[slice85],\n",
    "       train_regret_winner_15[slice85],\n",
    "       train_regret_winner_16[slice85],\n",
    "       train_regret_winner_17[slice85],\n",
    "       train_regret_winner_18[slice85],\n",
    "       train_regret_winner_19[slice85],\n",
    "       train_regret_winner_20[slice85]]\n",
    "\n",
    "loser85_results = pd.DataFrame(loser85).sort_values(by=[0], ascending=False)\n",
    "winner85_results = pd.DataFrame(winner85).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser85 = np.asarray(loser85_results[4:5][0])[0]\n",
    "median_loser85 = np.asarray(loser85_results[9:10][0])[0]\n",
    "upper_loser85 = np.asarray(loser85_results[14:15][0])[0]\n",
    "\n",
    "lower_winner85 = np.asarray(winner85_results[4:5][0])[0]\n",
    "median_winner85 = np.asarray(winner85_results[9:10][0])[0]\n",
    "upper_winner85 = np.asarray(winner85_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration95 :\n",
    "\n",
    "slice95 = 94\n",
    "\n",
    "loser95 = [train_regret_loser_1[slice95],\n",
    "       train_regret_loser_2[slice95],\n",
    "       train_regret_loser_3[slice95],\n",
    "       train_regret_loser_4[slice95],\n",
    "       train_regret_loser_5[slice95],\n",
    "       train_regret_loser_6[slice95],\n",
    "       train_regret_loser_7[slice95],\n",
    "       train_regret_loser_8[slice95],\n",
    "       train_regret_loser_9[slice95],\n",
    "       train_regret_loser_10[slice95],\n",
    "       train_regret_loser_11[slice95],\n",
    "       train_regret_loser_12[slice95],\n",
    "       train_regret_loser_13[slice95],\n",
    "       train_regret_loser_14[slice95],\n",
    "       train_regret_loser_15[slice95],\n",
    "       train_regret_loser_16[slice95],\n",
    "       train_regret_loser_17[slice95],\n",
    "       train_regret_loser_18[slice95],\n",
    "       train_regret_loser_19[slice95],\n",
    "       train_regret_loser_20[slice95]]\n",
    "\n",
    "winner95 = [train_regret_winner_1[slice95],\n",
    "       train_regret_winner_2[slice95],\n",
    "       train_regret_winner_3[slice95],\n",
    "       train_regret_winner_4[slice95],\n",
    "       train_regret_winner_5[slice95],\n",
    "       train_regret_winner_6[slice95],\n",
    "       train_regret_winner_7[slice95],\n",
    "       train_regret_winner_8[slice95],\n",
    "       train_regret_winner_9[slice95],\n",
    "       train_regret_winner_10[slice95],\n",
    "       train_regret_winner_11[slice95],\n",
    "       train_regret_winner_12[slice95],\n",
    "       train_regret_winner_13[slice95],\n",
    "       train_regret_winner_14[slice95],\n",
    "       train_regret_winner_15[slice95],\n",
    "       train_regret_winner_16[slice95],\n",
    "       train_regret_winner_17[slice95],\n",
    "       train_regret_winner_18[slice95],\n",
    "       train_regret_winner_19[slice95],\n",
    "       train_regret_winner_20[slice95]]\n",
    "\n",
    "loser95_results = pd.DataFrame(loser95).sort_values(by=[0], ascending=False)\n",
    "winner95_results = pd.DataFrame(winner95).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser95 = np.asarray(loser95_results[4:5][0])[0]\n",
    "median_loser95 = np.asarray(loser95_results[9:10][0])[0]\n",
    "upper_loser95 = np.asarray(loser95_results[14:15][0])[0]\n",
    "\n",
    "lower_winner95 = np.asarray(winner95_results[4:5][0])[0]\n",
    "median_winner95 = np.asarray(winner95_results[9:10][0])[0]\n",
    "upper_winner95 = np.asarray(winner95_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration6 :\n",
    "\n",
    "slice6 = 5\n",
    "\n",
    "loser6 = [train_regret_loser_1[slice6],\n",
    "       train_regret_loser_2[slice6],\n",
    "       train_regret_loser_3[slice6],\n",
    "       train_regret_loser_4[slice6],\n",
    "       train_regret_loser_5[slice6],\n",
    "       train_regret_loser_6[slice6],\n",
    "       train_regret_loser_7[slice6],\n",
    "       train_regret_loser_8[slice6],\n",
    "       train_regret_loser_9[slice6],\n",
    "       train_regret_loser_10[slice6],\n",
    "       train_regret_loser_11[slice6],\n",
    "       train_regret_loser_12[slice6],\n",
    "       train_regret_loser_13[slice6],\n",
    "       train_regret_loser_14[slice6],\n",
    "       train_regret_loser_15[slice6],\n",
    "       train_regret_loser_16[slice6],\n",
    "       train_regret_loser_17[slice6],\n",
    "       train_regret_loser_18[slice6],\n",
    "       train_regret_loser_19[slice6],\n",
    "       train_regret_loser_20[slice6]]\n",
    "\n",
    "winner6 = [train_regret_winner_1[slice6],\n",
    "       train_regret_winner_2[slice6],\n",
    "       train_regret_winner_3[slice6],\n",
    "       train_regret_winner_4[slice6],\n",
    "       train_regret_winner_5[slice6],\n",
    "       train_regret_winner_6[slice6],\n",
    "       train_regret_winner_7[slice6],\n",
    "       train_regret_winner_8[slice6],\n",
    "       train_regret_winner_9[slice6],\n",
    "       train_regret_winner_10[slice6],\n",
    "       train_regret_winner_11[slice6],\n",
    "       train_regret_winner_12[slice6],\n",
    "       train_regret_winner_13[slice6],\n",
    "       train_regret_winner_14[slice6],\n",
    "       train_regret_winner_15[slice6],\n",
    "       train_regret_winner_16[slice6],\n",
    "       train_regret_winner_17[slice6],\n",
    "       train_regret_winner_18[slice6],\n",
    "       train_regret_winner_19[slice6],\n",
    "       train_regret_winner_20[slice6]]\n",
    "\n",
    "loser6_results = pd.DataFrame(loser6).sort_values(by=[0], ascending=False)\n",
    "winner6_results = pd.DataFrame(winner6).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser6 = np.asarray(loser6_results[4:5][0])[0]\n",
    "median_loser6 = np.asarray(loser6_results[9:10][0])[0]\n",
    "upper_loser6 = np.asarray(loser6_results[14:15][0])[0]\n",
    "\n",
    "lower_winner6 = np.asarray(winner6_results[4:5][0])[0]\n",
    "median_winner6 = np.asarray(winner6_results[9:10][0])[0]\n",
    "upper_winner6 = np.asarray(winner6_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration16 :\n",
    "\n",
    "slice16 = 15\n",
    "\n",
    "loser16 = [train_regret_loser_1[slice16],\n",
    "       train_regret_loser_2[slice16],\n",
    "       train_regret_loser_3[slice16],\n",
    "       train_regret_loser_4[slice16],\n",
    "       train_regret_loser_5[slice16],\n",
    "       train_regret_loser_6[slice16],\n",
    "       train_regret_loser_7[slice16],\n",
    "       train_regret_loser_8[slice16],\n",
    "       train_regret_loser_9[slice16],\n",
    "       train_regret_loser_10[slice16],\n",
    "       train_regret_loser_11[slice16],\n",
    "       train_regret_loser_12[slice16],\n",
    "       train_regret_loser_13[slice16],\n",
    "       train_regret_loser_14[slice16],\n",
    "       train_regret_loser_15[slice16],\n",
    "       train_regret_loser_16[slice16],\n",
    "       train_regret_loser_17[slice16],\n",
    "       train_regret_loser_18[slice16],\n",
    "       train_regret_loser_19[slice16],\n",
    "       train_regret_loser_20[slice16]]\n",
    "\n",
    "winner16 = [train_regret_winner_1[slice16],\n",
    "       train_regret_winner_2[slice16],\n",
    "       train_regret_winner_3[slice16],\n",
    "       train_regret_winner_4[slice16],\n",
    "       train_regret_winner_5[slice16],\n",
    "       train_regret_winner_6[slice16],\n",
    "       train_regret_winner_7[slice16],\n",
    "       train_regret_winner_8[slice16],\n",
    "       train_regret_winner_9[slice16],\n",
    "       train_regret_winner_10[slice16],\n",
    "       train_regret_winner_11[slice16],\n",
    "       train_regret_winner_12[slice16],\n",
    "       train_regret_winner_13[slice16],\n",
    "       train_regret_winner_14[slice16],\n",
    "       train_regret_winner_15[slice16],\n",
    "       train_regret_winner_16[slice16],\n",
    "       train_regret_winner_17[slice16],\n",
    "       train_regret_winner_18[slice16],\n",
    "       train_regret_winner_19[slice16],\n",
    "       train_regret_winner_20[slice16]]\n",
    "\n",
    "loser16_results = pd.DataFrame(loser16).sort_values(by=[0], ascending=False)\n",
    "winner16_results = pd.DataFrame(winner16).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser16 = np.asarray(loser16_results[4:5][0])[0]\n",
    "median_loser16 = np.asarray(loser16_results[9:10][0])[0]\n",
    "upper_loser16 = np.asarray(loser16_results[14:15][0])[0]\n",
    "\n",
    "lower_winner16 = np.asarray(winner16_results[4:5][0])[0]\n",
    "median_winner16 = np.asarray(winner16_results[9:10][0])[0]\n",
    "upper_winner16 = np.asarray(winner16_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration26 :\n",
    "\n",
    "slice26 = 25\n",
    "\n",
    "loser26 = [train_regret_loser_1[slice26],\n",
    "       train_regret_loser_2[slice26],\n",
    "       train_regret_loser_3[slice26],\n",
    "       train_regret_loser_4[slice26],\n",
    "       train_regret_loser_5[slice26],\n",
    "       train_regret_loser_6[slice26],\n",
    "       train_regret_loser_7[slice26],\n",
    "       train_regret_loser_8[slice26],\n",
    "       train_regret_loser_9[slice26],\n",
    "       train_regret_loser_10[slice26],\n",
    "       train_regret_loser_11[slice26],\n",
    "       train_regret_loser_12[slice26],\n",
    "       train_regret_loser_13[slice26],\n",
    "       train_regret_loser_14[slice26],\n",
    "       train_regret_loser_15[slice26],\n",
    "       train_regret_loser_16[slice26],\n",
    "       train_regret_loser_17[slice26],\n",
    "       train_regret_loser_18[slice26],\n",
    "       train_regret_loser_19[slice26],\n",
    "       train_regret_loser_20[slice26]]\n",
    "\n",
    "winner26 = [train_regret_winner_1[slice26],\n",
    "       train_regret_winner_2[slice26],\n",
    "       train_regret_winner_3[slice26],\n",
    "       train_regret_winner_4[slice26],\n",
    "       train_regret_winner_5[slice26],\n",
    "       train_regret_winner_6[slice26],\n",
    "       train_regret_winner_7[slice26],\n",
    "       train_regret_winner_8[slice26],\n",
    "       train_regret_winner_9[slice26],\n",
    "       train_regret_winner_10[slice26],\n",
    "       train_regret_winner_11[slice26],\n",
    "       train_regret_winner_12[slice26],\n",
    "       train_regret_winner_13[slice26],\n",
    "       train_regret_winner_14[slice26],\n",
    "       train_regret_winner_15[slice26],\n",
    "       train_regret_winner_16[slice26],\n",
    "       train_regret_winner_17[slice26],\n",
    "       train_regret_winner_18[slice26],\n",
    "       train_regret_winner_19[slice26],\n",
    "       train_regret_winner_20[slice26]]\n",
    "\n",
    "loser26_results = pd.DataFrame(loser26).sort_values(by=[0], ascending=False)\n",
    "winner26_results = pd.DataFrame(winner26).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser26 = np.asarray(loser26_results[4:5][0])[0]\n",
    "median_loser26 = np.asarray(loser26_results[9:10][0])[0]\n",
    "upper_loser26 = np.asarray(loser26_results[14:15][0])[0]\n",
    "\n",
    "lower_winner26 = np.asarray(winner26_results[4:5][0])[0]\n",
    "median_winner26 = np.asarray(winner26_results[9:10][0])[0]\n",
    "upper_winner26 = np.asarray(winner26_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration36 :\n",
    "\n",
    "slice36 = 35\n",
    "\n",
    "loser36 = [train_regret_loser_1[slice36],\n",
    "       train_regret_loser_2[slice36],\n",
    "       train_regret_loser_3[slice36],\n",
    "       train_regret_loser_4[slice36],\n",
    "       train_regret_loser_5[slice36],\n",
    "       train_regret_loser_6[slice36],\n",
    "       train_regret_loser_7[slice36],\n",
    "       train_regret_loser_8[slice36],\n",
    "       train_regret_loser_9[slice36],\n",
    "       train_regret_loser_10[slice36],\n",
    "       train_regret_loser_11[slice36],\n",
    "       train_regret_loser_12[slice36],\n",
    "       train_regret_loser_13[slice36],\n",
    "       train_regret_loser_14[slice36],\n",
    "       train_regret_loser_15[slice36],\n",
    "       train_regret_loser_16[slice36],\n",
    "       train_regret_loser_17[slice36],\n",
    "       train_regret_loser_18[slice36],\n",
    "       train_regret_loser_19[slice36],\n",
    "       train_regret_loser_20[slice36]]\n",
    "\n",
    "winner36 = [train_regret_winner_1[slice36],\n",
    "       train_regret_winner_2[slice36],\n",
    "       train_regret_winner_3[slice36],\n",
    "       train_regret_winner_4[slice36],\n",
    "       train_regret_winner_5[slice36],\n",
    "       train_regret_winner_6[slice36],\n",
    "       train_regret_winner_7[slice36],\n",
    "       train_regret_winner_8[slice36],\n",
    "       train_regret_winner_9[slice36],\n",
    "       train_regret_winner_10[slice36],\n",
    "       train_regret_winner_11[slice36],\n",
    "       train_regret_winner_12[slice36],\n",
    "       train_regret_winner_13[slice36],\n",
    "       train_regret_winner_14[slice36],\n",
    "       train_regret_winner_15[slice36],\n",
    "       train_regret_winner_16[slice36],\n",
    "       train_regret_winner_17[slice36],\n",
    "       train_regret_winner_18[slice36],\n",
    "       train_regret_winner_19[slice36],\n",
    "       train_regret_winner_20[slice36]]\n",
    "\n",
    "loser36_results = pd.DataFrame(loser36).sort_values(by=[0], ascending=False)\n",
    "winner36_results = pd.DataFrame(winner36).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser36 = np.asarray(loser36_results[4:5][0])[0]\n",
    "median_loser36 = np.asarray(loser36_results[9:10][0])[0]\n",
    "upper_loser36 = np.asarray(loser36_results[14:15][0])[0]\n",
    "\n",
    "lower_winner36 = np.asarray(winner36_results[4:5][0])[0]\n",
    "median_winner36 = np.asarray(winner36_results[9:10][0])[0]\n",
    "upper_winner36 = np.asarray(winner36_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration46 :\n",
    "\n",
    "slice46 = 45\n",
    "\n",
    "loser46 = [train_regret_loser_1[slice46],\n",
    "       train_regret_loser_2[slice46],\n",
    "       train_regret_loser_3[slice46],\n",
    "       train_regret_loser_4[slice46],\n",
    "       train_regret_loser_5[slice46],\n",
    "       train_regret_loser_6[slice46],\n",
    "       train_regret_loser_7[slice46],\n",
    "       train_regret_loser_8[slice46],\n",
    "       train_regret_loser_9[slice46],\n",
    "       train_regret_loser_10[slice46],\n",
    "       train_regret_loser_11[slice46],\n",
    "       train_regret_loser_12[slice46],\n",
    "       train_regret_loser_13[slice46],\n",
    "       train_regret_loser_14[slice46],\n",
    "       train_regret_loser_15[slice46],\n",
    "       train_regret_loser_16[slice46],\n",
    "       train_regret_loser_17[slice46],\n",
    "       train_regret_loser_18[slice46],\n",
    "       train_regret_loser_19[slice46],\n",
    "       train_regret_loser_20[slice46]]\n",
    "\n",
    "winner46 = [train_regret_winner_1[slice46],\n",
    "       train_regret_winner_2[slice46],\n",
    "       train_regret_winner_3[slice46],\n",
    "       train_regret_winner_4[slice46],\n",
    "       train_regret_winner_5[slice46],\n",
    "       train_regret_winner_6[slice46],\n",
    "       train_regret_winner_7[slice46],\n",
    "       train_regret_winner_8[slice46],\n",
    "       train_regret_winner_9[slice46],\n",
    "       train_regret_winner_10[slice46],\n",
    "       train_regret_winner_11[slice46],\n",
    "       train_regret_winner_12[slice46],\n",
    "       train_regret_winner_13[slice46],\n",
    "       train_regret_winner_14[slice46],\n",
    "       train_regret_winner_15[slice46],\n",
    "       train_regret_winner_16[slice46],\n",
    "       train_regret_winner_17[slice46],\n",
    "       train_regret_winner_18[slice46],\n",
    "       train_regret_winner_19[slice46],\n",
    "       train_regret_winner_20[slice46]]\n",
    "\n",
    "loser46_results = pd.DataFrame(loser46).sort_values(by=[0], ascending=False)\n",
    "winner46_results = pd.DataFrame(winner46).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser46 = np.asarray(loser46_results[4:5][0])[0]\n",
    "median_loser46 = np.asarray(loser46_results[9:10][0])[0]\n",
    "upper_loser46 = np.asarray(loser46_results[14:15][0])[0]\n",
    "\n",
    "lower_winner46 = np.asarray(winner46_results[4:5][0])[0]\n",
    "median_winner46 = np.asarray(winner46_results[9:10][0])[0]\n",
    "upper_winner46 = np.asarray(winner46_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration56 :\n",
    "\n",
    "slice56 = 55\n",
    "\n",
    "loser56 = [train_regret_loser_1[slice56],\n",
    "       train_regret_loser_2[slice56],\n",
    "       train_regret_loser_3[slice56],\n",
    "       train_regret_loser_4[slice56],\n",
    "       train_regret_loser_5[slice56],\n",
    "       train_regret_loser_6[slice56],\n",
    "       train_regret_loser_7[slice56],\n",
    "       train_regret_loser_8[slice56],\n",
    "       train_regret_loser_9[slice56],\n",
    "       train_regret_loser_10[slice56],\n",
    "       train_regret_loser_11[slice56],\n",
    "       train_regret_loser_12[slice56],\n",
    "       train_regret_loser_13[slice56],\n",
    "       train_regret_loser_14[slice56],\n",
    "       train_regret_loser_15[slice56],\n",
    "       train_regret_loser_16[slice56],\n",
    "       train_regret_loser_17[slice56],\n",
    "       train_regret_loser_18[slice56],\n",
    "       train_regret_loser_19[slice56],\n",
    "       train_regret_loser_20[slice56]]\n",
    "\n",
    "winner56 = [train_regret_winner_1[slice56],\n",
    "       train_regret_winner_2[slice56],\n",
    "       train_regret_winner_3[slice56],\n",
    "       train_regret_winner_4[slice56],\n",
    "       train_regret_winner_5[slice56],\n",
    "       train_regret_winner_6[slice56],\n",
    "       train_regret_winner_7[slice56],\n",
    "       train_regret_winner_8[slice56],\n",
    "       train_regret_winner_9[slice56],\n",
    "       train_regret_winner_10[slice56],\n",
    "       train_regret_winner_11[slice56],\n",
    "       train_regret_winner_12[slice56],\n",
    "       train_regret_winner_13[slice56],\n",
    "       train_regret_winner_14[slice56],\n",
    "       train_regret_winner_15[slice56],\n",
    "       train_regret_winner_16[slice56],\n",
    "       train_regret_winner_17[slice56],\n",
    "       train_regret_winner_18[slice56],\n",
    "       train_regret_winner_19[slice56],\n",
    "       train_regret_winner_20[slice56]]\n",
    "\n",
    "loser56_results = pd.DataFrame(loser56).sort_values(by=[0], ascending=False)\n",
    "winner56_results = pd.DataFrame(winner56).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser56 = np.asarray(loser56_results[4:5][0])[0]\n",
    "median_loser56 = np.asarray(loser56_results[9:10][0])[0]\n",
    "upper_loser56 = np.asarray(loser56_results[14:15][0])[0]\n",
    "\n",
    "lower_winner56 = np.asarray(winner56_results[4:5][0])[0]\n",
    "median_winner56 = np.asarray(winner56_results[9:10][0])[0]\n",
    "upper_winner56 = np.asarray(winner56_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration66 :\n",
    "\n",
    "slice66 = 65\n",
    "\n",
    "loser66 = [train_regret_loser_1[slice66],\n",
    "       train_regret_loser_2[slice66],\n",
    "       train_regret_loser_3[slice66],\n",
    "       train_regret_loser_4[slice66],\n",
    "       train_regret_loser_5[slice66],\n",
    "       train_regret_loser_6[slice66],\n",
    "       train_regret_loser_7[slice66],\n",
    "       train_regret_loser_8[slice66],\n",
    "       train_regret_loser_9[slice66],\n",
    "       train_regret_loser_10[slice66],\n",
    "       train_regret_loser_11[slice66],\n",
    "       train_regret_loser_12[slice66],\n",
    "       train_regret_loser_13[slice66],\n",
    "       train_regret_loser_14[slice66],\n",
    "       train_regret_loser_15[slice66],\n",
    "       train_regret_loser_16[slice66],\n",
    "       train_regret_loser_17[slice66],\n",
    "       train_regret_loser_18[slice66],\n",
    "       train_regret_loser_19[slice66],\n",
    "       train_regret_loser_20[slice66]]\n",
    "\n",
    "winner66 = [train_regret_winner_1[slice66],\n",
    "       train_regret_winner_2[slice66],\n",
    "       train_regret_winner_3[slice66],\n",
    "       train_regret_winner_4[slice66],\n",
    "       train_regret_winner_5[slice66],\n",
    "       train_regret_winner_6[slice66],\n",
    "       train_regret_winner_7[slice66],\n",
    "       train_regret_winner_8[slice66],\n",
    "       train_regret_winner_9[slice66],\n",
    "       train_regret_winner_10[slice66],\n",
    "       train_regret_winner_11[slice66],\n",
    "       train_regret_winner_12[slice66],\n",
    "       train_regret_winner_13[slice66],\n",
    "       train_regret_winner_14[slice66],\n",
    "       train_regret_winner_15[slice66],\n",
    "       train_regret_winner_16[slice66],\n",
    "       train_regret_winner_17[slice66],\n",
    "       train_regret_winner_18[slice66],\n",
    "       train_regret_winner_19[slice66],\n",
    "       train_regret_winner_20[slice66]]\n",
    "\n",
    "loser66_results = pd.DataFrame(loser66).sort_values(by=[0], ascending=False)\n",
    "winner66_results = pd.DataFrame(winner66).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser66 = np.asarray(loser66_results[4:5][0])[0]\n",
    "median_loser66 = np.asarray(loser66_results[9:10][0])[0]\n",
    "upper_loser66 = np.asarray(loser66_results[14:15][0])[0]\n",
    "\n",
    "lower_winner66 = np.asarray(winner66_results[4:5][0])[0]\n",
    "median_winner66 = np.asarray(winner66_results[9:10][0])[0]\n",
    "upper_winner66 = np.asarray(winner66_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration76 :\n",
    "\n",
    "slice76 = 75\n",
    "\n",
    "loser76 = [train_regret_loser_1[slice76],\n",
    "       train_regret_loser_2[slice76],\n",
    "       train_regret_loser_3[slice76],\n",
    "       train_regret_loser_4[slice76],\n",
    "       train_regret_loser_5[slice76],\n",
    "       train_regret_loser_6[slice76],\n",
    "       train_regret_loser_7[slice76],\n",
    "       train_regret_loser_8[slice76],\n",
    "       train_regret_loser_9[slice76],\n",
    "       train_regret_loser_10[slice76],\n",
    "       train_regret_loser_11[slice76],\n",
    "       train_regret_loser_12[slice76],\n",
    "       train_regret_loser_13[slice76],\n",
    "       train_regret_loser_14[slice76],\n",
    "       train_regret_loser_15[slice76],\n",
    "       train_regret_loser_16[slice76],\n",
    "       train_regret_loser_17[slice76],\n",
    "       train_regret_loser_18[slice76],\n",
    "       train_regret_loser_19[slice76],\n",
    "       train_regret_loser_20[slice76]]\n",
    "\n",
    "winner76 = [train_regret_winner_1[slice76],\n",
    "       train_regret_winner_2[slice76],\n",
    "       train_regret_winner_3[slice76],\n",
    "       train_regret_winner_4[slice76],\n",
    "       train_regret_winner_5[slice76],\n",
    "       train_regret_winner_6[slice76],\n",
    "       train_regret_winner_7[slice76],\n",
    "       train_regret_winner_8[slice76],\n",
    "       train_regret_winner_9[slice76],\n",
    "       train_regret_winner_10[slice76],\n",
    "       train_regret_winner_11[slice76],\n",
    "       train_regret_winner_12[slice76],\n",
    "       train_regret_winner_13[slice76],\n",
    "       train_regret_winner_14[slice76],\n",
    "       train_regret_winner_15[slice76],\n",
    "       train_regret_winner_16[slice76],\n",
    "       train_regret_winner_17[slice76],\n",
    "       train_regret_winner_18[slice76],\n",
    "       train_regret_winner_19[slice76],\n",
    "       train_regret_winner_20[slice76]]\n",
    "\n",
    "loser76_results = pd.DataFrame(loser76).sort_values(by=[0], ascending=False)\n",
    "winner76_results = pd.DataFrame(winner76).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser76 = np.asarray(loser76_results[4:5][0])[0]\n",
    "median_loser76 = np.asarray(loser76_results[9:10][0])[0]\n",
    "upper_loser76 = np.asarray(loser76_results[14:15][0])[0]\n",
    "\n",
    "lower_winner76 = np.asarray(winner76_results[4:5][0])[0]\n",
    "median_winner76 = np.asarray(winner76_results[9:10][0])[0]\n",
    "upper_winner76 = np.asarray(winner76_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration86 :\n",
    "\n",
    "slice86 = 85\n",
    "\n",
    "loser86 = [train_regret_loser_1[slice86],\n",
    "       train_regret_loser_2[slice86],\n",
    "       train_regret_loser_3[slice86],\n",
    "       train_regret_loser_4[slice86],\n",
    "       train_regret_loser_5[slice86],\n",
    "       train_regret_loser_6[slice86],\n",
    "       train_regret_loser_7[slice86],\n",
    "       train_regret_loser_8[slice86],\n",
    "       train_regret_loser_9[slice86],\n",
    "       train_regret_loser_10[slice86],\n",
    "       train_regret_loser_11[slice86],\n",
    "       train_regret_loser_12[slice86],\n",
    "       train_regret_loser_13[slice86],\n",
    "       train_regret_loser_14[slice86],\n",
    "       train_regret_loser_15[slice86],\n",
    "       train_regret_loser_16[slice86],\n",
    "       train_regret_loser_17[slice86],\n",
    "       train_regret_loser_18[slice86],\n",
    "       train_regret_loser_19[slice86],\n",
    "       train_regret_loser_20[slice86]]\n",
    "\n",
    "winner86 = [train_regret_winner_1[slice86],\n",
    "       train_regret_winner_2[slice86],\n",
    "       train_regret_winner_3[slice86],\n",
    "       train_regret_winner_4[slice86],\n",
    "       train_regret_winner_5[slice86],\n",
    "       train_regret_winner_6[slice86],\n",
    "       train_regret_winner_7[slice86],\n",
    "       train_regret_winner_8[slice86],\n",
    "       train_regret_winner_9[slice86],\n",
    "       train_regret_winner_10[slice86],\n",
    "       train_regret_winner_11[slice86],\n",
    "       train_regret_winner_12[slice86],\n",
    "       train_regret_winner_13[slice86],\n",
    "       train_regret_winner_14[slice86],\n",
    "       train_regret_winner_15[slice86],\n",
    "       train_regret_winner_16[slice86],\n",
    "       train_regret_winner_17[slice86],\n",
    "       train_regret_winner_18[slice86],\n",
    "       train_regret_winner_19[slice86],\n",
    "       train_regret_winner_20[slice86]]\n",
    "\n",
    "loser86_results = pd.DataFrame(loser86).sort_values(by=[0], ascending=False)\n",
    "winner86_results = pd.DataFrame(winner86).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser86 = np.asarray(loser86_results[4:5][0])[0]\n",
    "median_loser86 = np.asarray(loser86_results[9:10][0])[0]\n",
    "upper_loser86 = np.asarray(loser86_results[14:15][0])[0]\n",
    "\n",
    "lower_winner86 = np.asarray(winner86_results[4:5][0])[0]\n",
    "median_winner86 = np.asarray(winner86_results[9:10][0])[0]\n",
    "upper_winner86 = np.asarray(winner86_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration96 :\n",
    "\n",
    "slice96 = 95\n",
    "\n",
    "loser96 = [train_regret_loser_1[slice96],\n",
    "       train_regret_loser_2[slice96],\n",
    "       train_regret_loser_3[slice96],\n",
    "       train_regret_loser_4[slice96],\n",
    "       train_regret_loser_5[slice96],\n",
    "       train_regret_loser_6[slice96],\n",
    "       train_regret_loser_7[slice96],\n",
    "       train_regret_loser_8[slice96],\n",
    "       train_regret_loser_9[slice96],\n",
    "       train_regret_loser_10[slice96],\n",
    "       train_regret_loser_11[slice96],\n",
    "       train_regret_loser_12[slice96],\n",
    "       train_regret_loser_13[slice96],\n",
    "       train_regret_loser_14[slice96],\n",
    "       train_regret_loser_15[slice96],\n",
    "       train_regret_loser_16[slice96],\n",
    "       train_regret_loser_17[slice96],\n",
    "       train_regret_loser_18[slice96],\n",
    "       train_regret_loser_19[slice96],\n",
    "       train_regret_loser_20[slice96]]\n",
    "\n",
    "winner96 = [train_regret_winner_1[slice96],\n",
    "       train_regret_winner_2[slice96],\n",
    "       train_regret_winner_3[slice96],\n",
    "       train_regret_winner_4[slice96],\n",
    "       train_regret_winner_5[slice96],\n",
    "       train_regret_winner_6[slice96],\n",
    "       train_regret_winner_7[slice96],\n",
    "       train_regret_winner_8[slice96],\n",
    "       train_regret_winner_9[slice96],\n",
    "       train_regret_winner_10[slice96],\n",
    "       train_regret_winner_11[slice96],\n",
    "       train_regret_winner_12[slice96],\n",
    "       train_regret_winner_13[slice96],\n",
    "       train_regret_winner_14[slice96],\n",
    "       train_regret_winner_15[slice96],\n",
    "       train_regret_winner_16[slice96],\n",
    "       train_regret_winner_17[slice96],\n",
    "       train_regret_winner_18[slice96],\n",
    "       train_regret_winner_19[slice96],\n",
    "       train_regret_winner_20[slice96]]\n",
    "\n",
    "loser96_results = pd.DataFrame(loser96).sort_values(by=[0], ascending=False)\n",
    "winner96_results = pd.DataFrame(winner96).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser96 = np.asarray(loser96_results[4:5][0])[0]\n",
    "median_loser96 = np.asarray(loser96_results[9:10][0])[0]\n",
    "upper_loser96 = np.asarray(loser96_results[14:15][0])[0]\n",
    "\n",
    "lower_winner96 = np.asarray(winner96_results[4:5][0])[0]\n",
    "median_winner96 = np.asarray(winner96_results[9:10][0])[0]\n",
    "upper_winner96 = np.asarray(winner96_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration7 :\n",
    "\n",
    "slice7 = 6\n",
    "\n",
    "loser7 = [train_regret_loser_1[slice7],\n",
    "       train_regret_loser_2[slice7],\n",
    "       train_regret_loser_3[slice7],\n",
    "       train_regret_loser_4[slice7],\n",
    "       train_regret_loser_5[slice7],\n",
    "       train_regret_loser_6[slice7],\n",
    "       train_regret_loser_7[slice7],\n",
    "       train_regret_loser_8[slice7],\n",
    "       train_regret_loser_9[slice7],\n",
    "       train_regret_loser_10[slice7],\n",
    "       train_regret_loser_11[slice7],\n",
    "       train_regret_loser_12[slice7],\n",
    "       train_regret_loser_13[slice7],\n",
    "       train_regret_loser_14[slice7],\n",
    "       train_regret_loser_15[slice7],\n",
    "       train_regret_loser_16[slice7],\n",
    "       train_regret_loser_17[slice7],\n",
    "       train_regret_loser_18[slice7],\n",
    "       train_regret_loser_19[slice7],\n",
    "       train_regret_loser_20[slice7]]\n",
    "\n",
    "winner7 = [train_regret_winner_1[slice7],\n",
    "       train_regret_winner_2[slice7],\n",
    "       train_regret_winner_3[slice7],\n",
    "       train_regret_winner_4[slice7],\n",
    "       train_regret_winner_5[slice7],\n",
    "       train_regret_winner_6[slice7],\n",
    "       train_regret_winner_7[slice7],\n",
    "       train_regret_winner_8[slice7],\n",
    "       train_regret_winner_9[slice7],\n",
    "       train_regret_winner_10[slice7],\n",
    "       train_regret_winner_11[slice7],\n",
    "       train_regret_winner_12[slice7],\n",
    "       train_regret_winner_13[slice7],\n",
    "       train_regret_winner_14[slice7],\n",
    "       train_regret_winner_15[slice7],\n",
    "       train_regret_winner_16[slice7],\n",
    "       train_regret_winner_17[slice7],\n",
    "       train_regret_winner_18[slice7],\n",
    "       train_regret_winner_19[slice7],\n",
    "       train_regret_winner_20[slice7]]\n",
    "\n",
    "loser7_results = pd.DataFrame(loser7).sort_values(by=[0], ascending=False)\n",
    "winner7_results = pd.DataFrame(winner7).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser7 = np.asarray(loser7_results[4:5][0])[0]\n",
    "median_loser7 = np.asarray(loser7_results[9:10][0])[0]\n",
    "upper_loser7 = np.asarray(loser7_results[14:15][0])[0]\n",
    "\n",
    "lower_winner7 = np.asarray(winner7_results[4:5][0])[0]\n",
    "median_winner7 = np.asarray(winner7_results[9:10][0])[0]\n",
    "upper_winner7 = np.asarray(winner7_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration17 :\n",
    "\n",
    "slice17 = 16\n",
    "\n",
    "loser17 = [train_regret_loser_1[slice17],\n",
    "       train_regret_loser_2[slice17],\n",
    "       train_regret_loser_3[slice17],\n",
    "       train_regret_loser_4[slice17],\n",
    "       train_regret_loser_5[slice17],\n",
    "       train_regret_loser_6[slice17],\n",
    "       train_regret_loser_7[slice17],\n",
    "       train_regret_loser_8[slice17],\n",
    "       train_regret_loser_9[slice17],\n",
    "       train_regret_loser_10[slice17],\n",
    "       train_regret_loser_11[slice17],\n",
    "       train_regret_loser_12[slice17],\n",
    "       train_regret_loser_13[slice17],\n",
    "       train_regret_loser_14[slice17],\n",
    "       train_regret_loser_15[slice17],\n",
    "       train_regret_loser_16[slice17],\n",
    "       train_regret_loser_17[slice17],\n",
    "       train_regret_loser_18[slice17],\n",
    "       train_regret_loser_19[slice17],\n",
    "       train_regret_loser_20[slice17]]\n",
    "\n",
    "winner17 = [train_regret_winner_1[slice17],\n",
    "       train_regret_winner_2[slice17],\n",
    "       train_regret_winner_3[slice17],\n",
    "       train_regret_winner_4[slice17],\n",
    "       train_regret_winner_5[slice17],\n",
    "       train_regret_winner_6[slice17],\n",
    "       train_regret_winner_7[slice17],\n",
    "       train_regret_winner_8[slice17],\n",
    "       train_regret_winner_9[slice17],\n",
    "       train_regret_winner_10[slice17],\n",
    "       train_regret_winner_11[slice17],\n",
    "       train_regret_winner_12[slice17],\n",
    "       train_regret_winner_13[slice17],\n",
    "       train_regret_winner_14[slice17],\n",
    "       train_regret_winner_15[slice17],\n",
    "       train_regret_winner_16[slice17],\n",
    "       train_regret_winner_17[slice17],\n",
    "       train_regret_winner_18[slice17],\n",
    "       train_regret_winner_19[slice17],\n",
    "       train_regret_winner_20[slice17]]\n",
    "\n",
    "loser17_results = pd.DataFrame(loser17).sort_values(by=[0], ascending=False)\n",
    "winner17_results = pd.DataFrame(winner17).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser17 = np.asarray(loser17_results[4:5][0])[0]\n",
    "median_loser17 = np.asarray(loser17_results[9:10][0])[0]\n",
    "upper_loser17 = np.asarray(loser17_results[14:15][0])[0]\n",
    "\n",
    "lower_winner17 = np.asarray(winner17_results[4:5][0])[0]\n",
    "median_winner17 = np.asarray(winner17_results[9:10][0])[0]\n",
    "upper_winner17 = np.asarray(winner17_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration27 :\n",
    "\n",
    "slice27 = 26\n",
    "\n",
    "loser27 = [train_regret_loser_1[slice27],\n",
    "       train_regret_loser_2[slice27],\n",
    "       train_regret_loser_3[slice27],\n",
    "       train_regret_loser_4[slice27],\n",
    "       train_regret_loser_5[slice27],\n",
    "       train_regret_loser_6[slice27],\n",
    "       train_regret_loser_7[slice27],\n",
    "       train_regret_loser_8[slice27],\n",
    "       train_regret_loser_9[slice27],\n",
    "       train_regret_loser_10[slice27],\n",
    "       train_regret_loser_11[slice27],\n",
    "       train_regret_loser_12[slice27],\n",
    "       train_regret_loser_13[slice27],\n",
    "       train_regret_loser_14[slice27],\n",
    "       train_regret_loser_15[slice27],\n",
    "       train_regret_loser_16[slice27],\n",
    "       train_regret_loser_17[slice27],\n",
    "       train_regret_loser_18[slice27],\n",
    "       train_regret_loser_19[slice27],\n",
    "       train_regret_loser_20[slice27]]\n",
    "\n",
    "winner27 = [train_regret_winner_1[slice27],\n",
    "       train_regret_winner_2[slice27],\n",
    "       train_regret_winner_3[slice27],\n",
    "       train_regret_winner_4[slice27],\n",
    "       train_regret_winner_5[slice27],\n",
    "       train_regret_winner_6[slice27],\n",
    "       train_regret_winner_7[slice27],\n",
    "       train_regret_winner_8[slice27],\n",
    "       train_regret_winner_9[slice27],\n",
    "       train_regret_winner_10[slice27],\n",
    "       train_regret_winner_11[slice27],\n",
    "       train_regret_winner_12[slice27],\n",
    "       train_regret_winner_13[slice27],\n",
    "       train_regret_winner_14[slice27],\n",
    "       train_regret_winner_15[slice27],\n",
    "       train_regret_winner_16[slice27],\n",
    "       train_regret_winner_17[slice27],\n",
    "       train_regret_winner_18[slice27],\n",
    "       train_regret_winner_19[slice27],\n",
    "       train_regret_winner_20[slice27]]\n",
    "\n",
    "loser27_results = pd.DataFrame(loser27).sort_values(by=[0], ascending=False)\n",
    "winner27_results = pd.DataFrame(winner27).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser27 = np.asarray(loser27_results[4:5][0])[0]\n",
    "median_loser27 = np.asarray(loser27_results[9:10][0])[0]\n",
    "upper_loser27 = np.asarray(loser27_results[14:15][0])[0]\n",
    "\n",
    "lower_winner27 = np.asarray(winner27_results[4:5][0])[0]\n",
    "median_winner27 = np.asarray(winner27_results[9:10][0])[0]\n",
    "upper_winner27 = np.asarray(winner27_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration37 :\n",
    "\n",
    "slice37 = 36\n",
    "\n",
    "loser37 = [train_regret_loser_1[slice37],\n",
    "       train_regret_loser_2[slice37],\n",
    "       train_regret_loser_3[slice37],\n",
    "       train_regret_loser_4[slice37],\n",
    "       train_regret_loser_5[slice37],\n",
    "       train_regret_loser_6[slice37],\n",
    "       train_regret_loser_7[slice37],\n",
    "       train_regret_loser_8[slice37],\n",
    "       train_regret_loser_9[slice37],\n",
    "       train_regret_loser_10[slice37],\n",
    "       train_regret_loser_11[slice37],\n",
    "       train_regret_loser_12[slice37],\n",
    "       train_regret_loser_13[slice37],\n",
    "       train_regret_loser_14[slice37],\n",
    "       train_regret_loser_15[slice37],\n",
    "       train_regret_loser_16[slice37],\n",
    "       train_regret_loser_17[slice37],\n",
    "       train_regret_loser_18[slice37],\n",
    "       train_regret_loser_19[slice37],\n",
    "       train_regret_loser_20[slice37]]\n",
    "\n",
    "winner37 = [train_regret_winner_1[slice37],\n",
    "       train_regret_winner_2[slice37],\n",
    "       train_regret_winner_3[slice37],\n",
    "       train_regret_winner_4[slice37],\n",
    "       train_regret_winner_5[slice37],\n",
    "       train_regret_winner_6[slice37],\n",
    "       train_regret_winner_7[slice37],\n",
    "       train_regret_winner_8[slice37],\n",
    "       train_regret_winner_9[slice37],\n",
    "       train_regret_winner_10[slice37],\n",
    "       train_regret_winner_11[slice37],\n",
    "       train_regret_winner_12[slice37],\n",
    "       train_regret_winner_13[slice37],\n",
    "       train_regret_winner_14[slice37],\n",
    "       train_regret_winner_15[slice37],\n",
    "       train_regret_winner_16[slice37],\n",
    "       train_regret_winner_17[slice37],\n",
    "       train_regret_winner_18[slice37],\n",
    "       train_regret_winner_19[slice37],\n",
    "       train_regret_winner_20[slice37]]\n",
    "\n",
    "loser37_results = pd.DataFrame(loser37).sort_values(by=[0], ascending=False)\n",
    "winner37_results = pd.DataFrame(winner37).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser37 = np.asarray(loser37_results[4:5][0])[0]\n",
    "median_loser37 = np.asarray(loser37_results[9:10][0])[0]\n",
    "upper_loser37 = np.asarray(loser37_results[14:15][0])[0]\n",
    "\n",
    "lower_winner37 = np.asarray(winner37_results[4:5][0])[0]\n",
    "median_winner37 = np.asarray(winner37_results[9:10][0])[0]\n",
    "upper_winner37 = np.asarray(winner37_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration47 :\n",
    "\n",
    "slice47 = 46\n",
    "\n",
    "loser47 = [train_regret_loser_1[slice47],\n",
    "       train_regret_loser_2[slice47],\n",
    "       train_regret_loser_3[slice47],\n",
    "       train_regret_loser_4[slice47],\n",
    "       train_regret_loser_5[slice47],\n",
    "       train_regret_loser_6[slice47],\n",
    "       train_regret_loser_7[slice47],\n",
    "       train_regret_loser_8[slice47],\n",
    "       train_regret_loser_9[slice47],\n",
    "       train_regret_loser_10[slice47],\n",
    "       train_regret_loser_11[slice47],\n",
    "       train_regret_loser_12[slice47],\n",
    "       train_regret_loser_13[slice47],\n",
    "       train_regret_loser_14[slice47],\n",
    "       train_regret_loser_15[slice47],\n",
    "       train_regret_loser_16[slice47],\n",
    "       train_regret_loser_17[slice47],\n",
    "       train_regret_loser_18[slice47],\n",
    "       train_regret_loser_19[slice47],\n",
    "       train_regret_loser_20[slice47]]\n",
    "\n",
    "winner47 = [train_regret_winner_1[slice47],\n",
    "       train_regret_winner_2[slice47],\n",
    "       train_regret_winner_3[slice47],\n",
    "       train_regret_winner_4[slice47],\n",
    "       train_regret_winner_5[slice47],\n",
    "       train_regret_winner_6[slice47],\n",
    "       train_regret_winner_7[slice47],\n",
    "       train_regret_winner_8[slice47],\n",
    "       train_regret_winner_9[slice47],\n",
    "       train_regret_winner_10[slice47],\n",
    "       train_regret_winner_11[slice47],\n",
    "       train_regret_winner_12[slice47],\n",
    "       train_regret_winner_13[slice47],\n",
    "       train_regret_winner_14[slice47],\n",
    "       train_regret_winner_15[slice47],\n",
    "       train_regret_winner_16[slice47],\n",
    "       train_regret_winner_17[slice47],\n",
    "       train_regret_winner_18[slice47],\n",
    "       train_regret_winner_19[slice47],\n",
    "       train_regret_winner_20[slice47]]\n",
    "\n",
    "loser47_results = pd.DataFrame(loser47).sort_values(by=[0], ascending=False)\n",
    "winner47_results = pd.DataFrame(winner47).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser47 = np.asarray(loser47_results[4:5][0])[0]\n",
    "median_loser47 = np.asarray(loser47_results[9:10][0])[0]\n",
    "upper_loser47 = np.asarray(loser47_results[14:15][0])[0]\n",
    "\n",
    "lower_winner47 = np.asarray(winner47_results[4:5][0])[0]\n",
    "median_winner47 = np.asarray(winner47_results[9:10][0])[0]\n",
    "upper_winner47 = np.asarray(winner47_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration57 :\n",
    "\n",
    "slice57 = 56\n",
    "\n",
    "loser57 = [train_regret_loser_1[slice57],\n",
    "       train_regret_loser_2[slice57],\n",
    "       train_regret_loser_3[slice57],\n",
    "       train_regret_loser_4[slice57],\n",
    "       train_regret_loser_5[slice57],\n",
    "       train_regret_loser_6[slice57],\n",
    "       train_regret_loser_7[slice57],\n",
    "       train_regret_loser_8[slice57],\n",
    "       train_regret_loser_9[slice57],\n",
    "       train_regret_loser_10[slice57],\n",
    "       train_regret_loser_11[slice57],\n",
    "       train_regret_loser_12[slice57],\n",
    "       train_regret_loser_13[slice57],\n",
    "       train_regret_loser_14[slice57],\n",
    "       train_regret_loser_15[slice57],\n",
    "       train_regret_loser_16[slice57],\n",
    "       train_regret_loser_17[slice57],\n",
    "       train_regret_loser_18[slice57],\n",
    "       train_regret_loser_19[slice57],\n",
    "       train_regret_loser_20[slice57]]\n",
    "\n",
    "winner57 = [train_regret_winner_1[slice57],\n",
    "       train_regret_winner_2[slice57],\n",
    "       train_regret_winner_3[slice57],\n",
    "       train_regret_winner_4[slice57],\n",
    "       train_regret_winner_5[slice57],\n",
    "       train_regret_winner_6[slice57],\n",
    "       train_regret_winner_7[slice57],\n",
    "       train_regret_winner_8[slice57],\n",
    "       train_regret_winner_9[slice57],\n",
    "       train_regret_winner_10[slice57],\n",
    "       train_regret_winner_11[slice57],\n",
    "       train_regret_winner_12[slice57],\n",
    "       train_regret_winner_13[slice57],\n",
    "       train_regret_winner_14[slice57],\n",
    "       train_regret_winner_15[slice57],\n",
    "       train_regret_winner_16[slice57],\n",
    "       train_regret_winner_17[slice57],\n",
    "       train_regret_winner_18[slice57],\n",
    "       train_regret_winner_19[slice57],\n",
    "       train_regret_winner_20[slice57]]\n",
    "\n",
    "loser57_results = pd.DataFrame(loser57).sort_values(by=[0], ascending=False)\n",
    "winner57_results = pd.DataFrame(winner57).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser57 = np.asarray(loser57_results[4:5][0])[0]\n",
    "median_loser57 = np.asarray(loser57_results[9:10][0])[0]\n",
    "upper_loser57 = np.asarray(loser57_results[14:15][0])[0]\n",
    "\n",
    "lower_winner57 = np.asarray(winner57_results[4:5][0])[0]\n",
    "median_winner57 = np.asarray(winner57_results[9:10][0])[0]\n",
    "upper_winner57 = np.asarray(winner57_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration67 :\n",
    "\n",
    "slice67 = 66\n",
    "\n",
    "loser67 = [train_regret_loser_1[slice67],\n",
    "       train_regret_loser_2[slice67],\n",
    "       train_regret_loser_3[slice67],\n",
    "       train_regret_loser_4[slice67],\n",
    "       train_regret_loser_5[slice67],\n",
    "       train_regret_loser_6[slice67],\n",
    "       train_regret_loser_7[slice67],\n",
    "       train_regret_loser_8[slice67],\n",
    "       train_regret_loser_9[slice67],\n",
    "       train_regret_loser_10[slice67],\n",
    "       train_regret_loser_11[slice67],\n",
    "       train_regret_loser_12[slice67],\n",
    "       train_regret_loser_13[slice67],\n",
    "       train_regret_loser_14[slice67],\n",
    "       train_regret_loser_15[slice67],\n",
    "       train_regret_loser_16[slice67],\n",
    "       train_regret_loser_17[slice67],\n",
    "       train_regret_loser_18[slice67],\n",
    "       train_regret_loser_19[slice67],\n",
    "       train_regret_loser_20[slice67]]\n",
    "\n",
    "winner67 = [train_regret_winner_1[slice67],\n",
    "       train_regret_winner_2[slice67],\n",
    "       train_regret_winner_3[slice67],\n",
    "       train_regret_winner_4[slice67],\n",
    "       train_regret_winner_5[slice67],\n",
    "       train_regret_winner_6[slice67],\n",
    "       train_regret_winner_7[slice67],\n",
    "       train_regret_winner_8[slice67],\n",
    "       train_regret_winner_9[slice67],\n",
    "       train_regret_winner_10[slice67],\n",
    "       train_regret_winner_11[slice67],\n",
    "       train_regret_winner_12[slice67],\n",
    "       train_regret_winner_13[slice67],\n",
    "       train_regret_winner_14[slice67],\n",
    "       train_regret_winner_15[slice67],\n",
    "       train_regret_winner_16[slice67],\n",
    "       train_regret_winner_17[slice67],\n",
    "       train_regret_winner_18[slice67],\n",
    "       train_regret_winner_19[slice67],\n",
    "       train_regret_winner_20[slice67]]\n",
    "\n",
    "loser67_results = pd.DataFrame(loser67).sort_values(by=[0], ascending=False)\n",
    "winner67_results = pd.DataFrame(winner67).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser67 = np.asarray(loser67_results[4:5][0])[0]\n",
    "median_loser67 = np.asarray(loser67_results[9:10][0])[0]\n",
    "upper_loser67 = np.asarray(loser67_results[14:15][0])[0]\n",
    "\n",
    "lower_winner67 = np.asarray(winner67_results[4:5][0])[0]\n",
    "median_winner67 = np.asarray(winner67_results[9:10][0])[0]\n",
    "upper_winner67 = np.asarray(winner67_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration77 :\n",
    "\n",
    "slice77 = 76\n",
    "\n",
    "loser77 = [train_regret_loser_1[slice77],\n",
    "       train_regret_loser_2[slice77],\n",
    "       train_regret_loser_3[slice77],\n",
    "       train_regret_loser_4[slice77],\n",
    "       train_regret_loser_5[slice77],\n",
    "       train_regret_loser_6[slice77],\n",
    "       train_regret_loser_7[slice77],\n",
    "       train_regret_loser_8[slice77],\n",
    "       train_regret_loser_9[slice77],\n",
    "       train_regret_loser_10[slice77],\n",
    "       train_regret_loser_11[slice77],\n",
    "       train_regret_loser_12[slice77],\n",
    "       train_regret_loser_13[slice77],\n",
    "       train_regret_loser_14[slice77],\n",
    "       train_regret_loser_15[slice77],\n",
    "       train_regret_loser_16[slice77],\n",
    "       train_regret_loser_17[slice77],\n",
    "       train_regret_loser_18[slice77],\n",
    "       train_regret_loser_19[slice77],\n",
    "       train_regret_loser_20[slice77]]\n",
    "\n",
    "winner77 = [train_regret_winner_1[slice77],\n",
    "       train_regret_winner_2[slice77],\n",
    "       train_regret_winner_3[slice77],\n",
    "       train_regret_winner_4[slice77],\n",
    "       train_regret_winner_5[slice77],\n",
    "       train_regret_winner_6[slice77],\n",
    "       train_regret_winner_7[slice77],\n",
    "       train_regret_winner_8[slice77],\n",
    "       train_regret_winner_9[slice77],\n",
    "       train_regret_winner_10[slice77],\n",
    "       train_regret_winner_11[slice77],\n",
    "       train_regret_winner_12[slice77],\n",
    "       train_regret_winner_13[slice77],\n",
    "       train_regret_winner_14[slice77],\n",
    "       train_regret_winner_15[slice77],\n",
    "       train_regret_winner_16[slice77],\n",
    "       train_regret_winner_17[slice77],\n",
    "       train_regret_winner_18[slice77],\n",
    "       train_regret_winner_19[slice77],\n",
    "       train_regret_winner_20[slice77]]\n",
    "\n",
    "loser77_results = pd.DataFrame(loser77).sort_values(by=[0], ascending=False)\n",
    "winner77_results = pd.DataFrame(winner77).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser77 = np.asarray(loser77_results[4:5][0])[0]\n",
    "median_loser77 = np.asarray(loser77_results[9:10][0])[0]\n",
    "upper_loser77 = np.asarray(loser77_results[14:15][0])[0]\n",
    "\n",
    "lower_winner77 = np.asarray(winner77_results[4:5][0])[0]\n",
    "median_winner77 = np.asarray(winner77_results[9:10][0])[0]\n",
    "upper_winner77 = np.asarray(winner77_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration87 :\n",
    "\n",
    "slice87 = 86\n",
    "\n",
    "loser87 = [train_regret_loser_1[slice87],\n",
    "       train_regret_loser_2[slice87],\n",
    "       train_regret_loser_3[slice87],\n",
    "       train_regret_loser_4[slice87],\n",
    "       train_regret_loser_5[slice87],\n",
    "       train_regret_loser_6[slice87],\n",
    "       train_regret_loser_7[slice87],\n",
    "       train_regret_loser_8[slice87],\n",
    "       train_regret_loser_9[slice87],\n",
    "       train_regret_loser_10[slice87],\n",
    "       train_regret_loser_11[slice87],\n",
    "       train_regret_loser_12[slice87],\n",
    "       train_regret_loser_13[slice87],\n",
    "       train_regret_loser_14[slice87],\n",
    "       train_regret_loser_15[slice87],\n",
    "       train_regret_loser_16[slice87],\n",
    "       train_regret_loser_17[slice87],\n",
    "       train_regret_loser_18[slice87],\n",
    "       train_regret_loser_19[slice87],\n",
    "       train_regret_loser_20[slice87]]\n",
    "\n",
    "winner87 = [train_regret_winner_1[slice87],\n",
    "       train_regret_winner_2[slice87],\n",
    "       train_regret_winner_3[slice87],\n",
    "       train_regret_winner_4[slice87],\n",
    "       train_regret_winner_5[slice87],\n",
    "       train_regret_winner_6[slice87],\n",
    "       train_regret_winner_7[slice87],\n",
    "       train_regret_winner_8[slice87],\n",
    "       train_regret_winner_9[slice87],\n",
    "       train_regret_winner_10[slice87],\n",
    "       train_regret_winner_11[slice87],\n",
    "       train_regret_winner_12[slice87],\n",
    "       train_regret_winner_13[slice87],\n",
    "       train_regret_winner_14[slice87],\n",
    "       train_regret_winner_15[slice87],\n",
    "       train_regret_winner_16[slice87],\n",
    "       train_regret_winner_17[slice87],\n",
    "       train_regret_winner_18[slice87],\n",
    "       train_regret_winner_19[slice87],\n",
    "       train_regret_winner_20[slice87]]\n",
    "\n",
    "loser87_results = pd.DataFrame(loser87).sort_values(by=[0], ascending=False)\n",
    "winner87_results = pd.DataFrame(winner87).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser87 = np.asarray(loser87_results[4:5][0])[0]\n",
    "median_loser87 = np.asarray(loser87_results[9:10][0])[0]\n",
    "upper_loser87 = np.asarray(loser87_results[14:15][0])[0]\n",
    "\n",
    "lower_winner87 = np.asarray(winner87_results[4:5][0])[0]\n",
    "median_winner87 = np.asarray(winner87_results[9:10][0])[0]\n",
    "upper_winner87 = np.asarray(winner87_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration97 :\n",
    "\n",
    "slice97 = 96\n",
    "\n",
    "loser97 = [train_regret_loser_1[slice97],\n",
    "       train_regret_loser_2[slice97],\n",
    "       train_regret_loser_3[slice97],\n",
    "       train_regret_loser_4[slice97],\n",
    "       train_regret_loser_5[slice97],\n",
    "       train_regret_loser_6[slice97],\n",
    "       train_regret_loser_7[slice97],\n",
    "       train_regret_loser_8[slice97],\n",
    "       train_regret_loser_9[slice97],\n",
    "       train_regret_loser_10[slice97],\n",
    "       train_regret_loser_11[slice97],\n",
    "       train_regret_loser_12[slice97],\n",
    "       train_regret_loser_13[slice97],\n",
    "       train_regret_loser_14[slice97],\n",
    "       train_regret_loser_15[slice97],\n",
    "       train_regret_loser_16[slice97],\n",
    "       train_regret_loser_17[slice97],\n",
    "       train_regret_loser_18[slice97],\n",
    "       train_regret_loser_19[slice97],\n",
    "       train_regret_loser_20[slice97]]\n",
    "\n",
    "winner97 = [train_regret_winner_1[slice97],\n",
    "       train_regret_winner_2[slice97],\n",
    "       train_regret_winner_3[slice97],\n",
    "       train_regret_winner_4[slice97],\n",
    "       train_regret_winner_5[slice97],\n",
    "       train_regret_winner_6[slice97],\n",
    "       train_regret_winner_7[slice97],\n",
    "       train_regret_winner_8[slice97],\n",
    "       train_regret_winner_9[slice97],\n",
    "       train_regret_winner_10[slice97],\n",
    "       train_regret_winner_11[slice97],\n",
    "       train_regret_winner_12[slice97],\n",
    "       train_regret_winner_13[slice97],\n",
    "       train_regret_winner_14[slice97],\n",
    "       train_regret_winner_15[slice97],\n",
    "       train_regret_winner_16[slice97],\n",
    "       train_regret_winner_17[slice97],\n",
    "       train_regret_winner_18[slice97],\n",
    "       train_regret_winner_19[slice97],\n",
    "       train_regret_winner_20[slice97]]\n",
    "\n",
    "loser97_results = pd.DataFrame(loser97).sort_values(by=[0], ascending=False)\n",
    "winner97_results = pd.DataFrame(winner97).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser97 = np.asarray(loser97_results[4:5][0])[0]\n",
    "median_loser97 = np.asarray(loser97_results[9:10][0])[0]\n",
    "upper_loser97 = np.asarray(loser97_results[14:15][0])[0]\n",
    "\n",
    "lower_winner97 = np.asarray(winner97_results[4:5][0])[0]\n",
    "median_winner97 = np.asarray(winner97_results[9:10][0])[0]\n",
    "upper_winner97 = np.asarray(winner97_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration8 :\n",
    "\n",
    "slice8 = 7\n",
    "\n",
    "loser8 = [train_regret_loser_1[slice8],\n",
    "       train_regret_loser_2[slice8],\n",
    "       train_regret_loser_3[slice8],\n",
    "       train_regret_loser_4[slice8],\n",
    "       train_regret_loser_5[slice8],\n",
    "       train_regret_loser_6[slice8],\n",
    "       train_regret_loser_7[slice8],\n",
    "       train_regret_loser_8[slice8],\n",
    "       train_regret_loser_9[slice8],\n",
    "       train_regret_loser_10[slice8],\n",
    "       train_regret_loser_11[slice8],\n",
    "       train_regret_loser_12[slice8],\n",
    "       train_regret_loser_13[slice8],\n",
    "       train_regret_loser_14[slice8],\n",
    "       train_regret_loser_15[slice8],\n",
    "       train_regret_loser_16[slice8],\n",
    "       train_regret_loser_17[slice8],\n",
    "       train_regret_loser_18[slice8],\n",
    "       train_regret_loser_19[slice8],\n",
    "       train_regret_loser_20[slice8]]\n",
    "\n",
    "winner8 = [train_regret_winner_1[slice8],\n",
    "       train_regret_winner_2[slice8],\n",
    "       train_regret_winner_3[slice8],\n",
    "       train_regret_winner_4[slice8],\n",
    "       train_regret_winner_5[slice8],\n",
    "       train_regret_winner_6[slice8],\n",
    "       train_regret_winner_7[slice8],\n",
    "       train_regret_winner_8[slice8],\n",
    "       train_regret_winner_9[slice8],\n",
    "       train_regret_winner_10[slice8],\n",
    "       train_regret_winner_11[slice8],\n",
    "       train_regret_winner_12[slice8],\n",
    "       train_regret_winner_13[slice8],\n",
    "       train_regret_winner_14[slice8],\n",
    "       train_regret_winner_15[slice8],\n",
    "       train_regret_winner_16[slice8],\n",
    "       train_regret_winner_17[slice8],\n",
    "       train_regret_winner_18[slice8],\n",
    "       train_regret_winner_19[slice8],\n",
    "       train_regret_winner_20[slice8]]\n",
    "\n",
    "loser8_results = pd.DataFrame(loser8).sort_values(by=[0], ascending=False)\n",
    "winner8_results = pd.DataFrame(winner8).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser8 = np.asarray(loser8_results[4:5][0])[0]\n",
    "median_loser8 = np.asarray(loser8_results[9:10][0])[0]\n",
    "upper_loser8 = np.asarray(loser8_results[14:15][0])[0]\n",
    "\n",
    "lower_winner8 = np.asarray(winner8_results[4:5][0])[0]\n",
    "median_winner8 = np.asarray(winner8_results[9:10][0])[0]\n",
    "upper_winner8 = np.asarray(winner8_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration18 :\n",
    "\n",
    "slice18 = 17\n",
    "\n",
    "loser18 = [train_regret_loser_1[slice18],\n",
    "       train_regret_loser_2[slice18],\n",
    "       train_regret_loser_3[slice18],\n",
    "       train_regret_loser_4[slice18],\n",
    "       train_regret_loser_5[slice18],\n",
    "       train_regret_loser_6[slice18],\n",
    "       train_regret_loser_7[slice18],\n",
    "       train_regret_loser_8[slice18],\n",
    "       train_regret_loser_9[slice18],\n",
    "       train_regret_loser_10[slice18],\n",
    "       train_regret_loser_11[slice18],\n",
    "       train_regret_loser_12[slice18],\n",
    "       train_regret_loser_13[slice18],\n",
    "       train_regret_loser_14[slice18],\n",
    "       train_regret_loser_15[slice18],\n",
    "       train_regret_loser_16[slice18],\n",
    "       train_regret_loser_17[slice18],\n",
    "       train_regret_loser_18[slice18],\n",
    "       train_regret_loser_19[slice18],\n",
    "       train_regret_loser_20[slice18]]\n",
    "\n",
    "winner18 = [train_regret_winner_1[slice18],\n",
    "       train_regret_winner_2[slice18],\n",
    "       train_regret_winner_3[slice18],\n",
    "       train_regret_winner_4[slice18],\n",
    "       train_regret_winner_5[slice18],\n",
    "       train_regret_winner_6[slice18],\n",
    "       train_regret_winner_7[slice18],\n",
    "       train_regret_winner_8[slice18],\n",
    "       train_regret_winner_9[slice18],\n",
    "       train_regret_winner_10[slice18],\n",
    "       train_regret_winner_11[slice18],\n",
    "       train_regret_winner_12[slice18],\n",
    "       train_regret_winner_13[slice18],\n",
    "       train_regret_winner_14[slice18],\n",
    "       train_regret_winner_15[slice18],\n",
    "       train_regret_winner_16[slice18],\n",
    "       train_regret_winner_17[slice18],\n",
    "       train_regret_winner_18[slice18],\n",
    "       train_regret_winner_19[slice18],\n",
    "       train_regret_winner_20[slice18]]\n",
    "\n",
    "loser18_results = pd.DataFrame(loser18).sort_values(by=[0], ascending=False)\n",
    "winner18_results = pd.DataFrame(winner18).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser18 = np.asarray(loser18_results[4:5][0])[0]\n",
    "median_loser18 = np.asarray(loser18_results[9:10][0])[0]\n",
    "upper_loser18 = np.asarray(loser18_results[14:15][0])[0]\n",
    "\n",
    "lower_winner18 = np.asarray(winner18_results[4:5][0])[0]\n",
    "median_winner18 = np.asarray(winner18_results[9:10][0])[0]\n",
    "upper_winner18 = np.asarray(winner18_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration28 :\n",
    "\n",
    "slice28 = 27\n",
    "\n",
    "loser28 = [train_regret_loser_1[slice28],\n",
    "       train_regret_loser_2[slice28],\n",
    "       train_regret_loser_3[slice28],\n",
    "       train_regret_loser_4[slice28],\n",
    "       train_regret_loser_5[slice28],\n",
    "       train_regret_loser_6[slice28],\n",
    "       train_regret_loser_7[slice28],\n",
    "       train_regret_loser_8[slice28],\n",
    "       train_regret_loser_9[slice28],\n",
    "       train_regret_loser_10[slice28],\n",
    "       train_regret_loser_11[slice28],\n",
    "       train_regret_loser_12[slice28],\n",
    "       train_regret_loser_13[slice28],\n",
    "       train_regret_loser_14[slice28],\n",
    "       train_regret_loser_15[slice28],\n",
    "       train_regret_loser_16[slice28],\n",
    "       train_regret_loser_17[slice28],\n",
    "       train_regret_loser_18[slice28],\n",
    "       train_regret_loser_19[slice28],\n",
    "       train_regret_loser_20[slice28]]\n",
    "\n",
    "winner28 = [train_regret_winner_1[slice28],\n",
    "       train_regret_winner_2[slice28],\n",
    "       train_regret_winner_3[slice28],\n",
    "       train_regret_winner_4[slice28],\n",
    "       train_regret_winner_5[slice28],\n",
    "       train_regret_winner_6[slice28],\n",
    "       train_regret_winner_7[slice28],\n",
    "       train_regret_winner_8[slice28],\n",
    "       train_regret_winner_9[slice28],\n",
    "       train_regret_winner_10[slice28],\n",
    "       train_regret_winner_11[slice28],\n",
    "       train_regret_winner_12[slice28],\n",
    "       train_regret_winner_13[slice28],\n",
    "       train_regret_winner_14[slice28],\n",
    "       train_regret_winner_15[slice28],\n",
    "       train_regret_winner_16[slice28],\n",
    "       train_regret_winner_17[slice28],\n",
    "       train_regret_winner_18[slice28],\n",
    "       train_regret_winner_19[slice28],\n",
    "       train_regret_winner_20[slice28]]\n",
    "\n",
    "loser28_results = pd.DataFrame(loser28).sort_values(by=[0], ascending=False)\n",
    "winner28_results = pd.DataFrame(winner28).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser28 = np.asarray(loser28_results[4:5][0])[0]\n",
    "median_loser28 = np.asarray(loser28_results[9:10][0])[0]\n",
    "upper_loser28 = np.asarray(loser28_results[14:15][0])[0]\n",
    "\n",
    "lower_winner28 = np.asarray(winner28_results[4:5][0])[0]\n",
    "median_winner28 = np.asarray(winner28_results[9:10][0])[0]\n",
    "upper_winner28 = np.asarray(winner28_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration38 :\n",
    "\n",
    "slice38 = 37\n",
    "\n",
    "loser38 = [train_regret_loser_1[slice38],\n",
    "       train_regret_loser_2[slice38],\n",
    "       train_regret_loser_3[slice38],\n",
    "       train_regret_loser_4[slice38],\n",
    "       train_regret_loser_5[slice38],\n",
    "       train_regret_loser_6[slice38],\n",
    "       train_regret_loser_7[slice38],\n",
    "       train_regret_loser_8[slice38],\n",
    "       train_regret_loser_9[slice38],\n",
    "       train_regret_loser_10[slice38],\n",
    "       train_regret_loser_11[slice38],\n",
    "       train_regret_loser_12[slice38],\n",
    "       train_regret_loser_13[slice38],\n",
    "       train_regret_loser_14[slice38],\n",
    "       train_regret_loser_15[slice38],\n",
    "       train_regret_loser_16[slice38],\n",
    "       train_regret_loser_17[slice38],\n",
    "       train_regret_loser_18[slice38],\n",
    "       train_regret_loser_19[slice38],\n",
    "       train_regret_loser_20[slice38]]\n",
    "\n",
    "winner38 = [train_regret_winner_1[slice38],\n",
    "       train_regret_winner_2[slice38],\n",
    "       train_regret_winner_3[slice38],\n",
    "       train_regret_winner_4[slice38],\n",
    "       train_regret_winner_5[slice38],\n",
    "       train_regret_winner_6[slice38],\n",
    "       train_regret_winner_7[slice38],\n",
    "       train_regret_winner_8[slice38],\n",
    "       train_regret_winner_9[slice38],\n",
    "       train_regret_winner_10[slice38],\n",
    "       train_regret_winner_11[slice38],\n",
    "       train_regret_winner_12[slice38],\n",
    "       train_regret_winner_13[slice38],\n",
    "       train_regret_winner_14[slice38],\n",
    "       train_regret_winner_15[slice38],\n",
    "       train_regret_winner_16[slice38],\n",
    "       train_regret_winner_17[slice38],\n",
    "       train_regret_winner_18[slice38],\n",
    "       train_regret_winner_19[slice38],\n",
    "       train_regret_winner_20[slice38]]\n",
    "\n",
    "loser38_results = pd.DataFrame(loser38).sort_values(by=[0], ascending=False)\n",
    "winner38_results = pd.DataFrame(winner38).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser38 = np.asarray(loser38_results[4:5][0])[0]\n",
    "median_loser38 = np.asarray(loser38_results[9:10][0])[0]\n",
    "upper_loser38 = np.asarray(loser38_results[14:15][0])[0]\n",
    "\n",
    "lower_winner38 = np.asarray(winner38_results[4:5][0])[0]\n",
    "median_winner38 = np.asarray(winner38_results[9:10][0])[0]\n",
    "upper_winner38 = np.asarray(winner38_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration48 :\n",
    "\n",
    "slice48 = 47\n",
    "\n",
    "loser48 = [train_regret_loser_1[slice48],\n",
    "       train_regret_loser_2[slice48],\n",
    "       train_regret_loser_3[slice48],\n",
    "       train_regret_loser_4[slice48],\n",
    "       train_regret_loser_5[slice48],\n",
    "       train_regret_loser_6[slice48],\n",
    "       train_regret_loser_7[slice48],\n",
    "       train_regret_loser_8[slice48],\n",
    "       train_regret_loser_9[slice48],\n",
    "       train_regret_loser_10[slice48],\n",
    "       train_regret_loser_11[slice48],\n",
    "       train_regret_loser_12[slice48],\n",
    "       train_regret_loser_13[slice48],\n",
    "       train_regret_loser_14[slice48],\n",
    "       train_regret_loser_15[slice48],\n",
    "       train_regret_loser_16[slice48],\n",
    "       train_regret_loser_17[slice48],\n",
    "       train_regret_loser_18[slice48],\n",
    "       train_regret_loser_19[slice48],\n",
    "       train_regret_loser_20[slice48]]\n",
    "\n",
    "winner48 = [train_regret_winner_1[slice48],\n",
    "       train_regret_winner_2[slice48],\n",
    "       train_regret_winner_3[slice48],\n",
    "       train_regret_winner_4[slice48],\n",
    "       train_regret_winner_5[slice48],\n",
    "       train_regret_winner_6[slice48],\n",
    "       train_regret_winner_7[slice48],\n",
    "       train_regret_winner_8[slice48],\n",
    "       train_regret_winner_9[slice48],\n",
    "       train_regret_winner_10[slice48],\n",
    "       train_regret_winner_11[slice48],\n",
    "       train_regret_winner_12[slice48],\n",
    "       train_regret_winner_13[slice48],\n",
    "       train_regret_winner_14[slice48],\n",
    "       train_regret_winner_15[slice48],\n",
    "       train_regret_winner_16[slice48],\n",
    "       train_regret_winner_17[slice48],\n",
    "       train_regret_winner_18[slice48],\n",
    "       train_regret_winner_19[slice48],\n",
    "       train_regret_winner_20[slice48]]\n",
    "\n",
    "loser48_results = pd.DataFrame(loser48).sort_values(by=[0], ascending=False)\n",
    "winner48_results = pd.DataFrame(winner48).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser48 = np.asarray(loser48_results[4:5][0])[0]\n",
    "median_loser48 = np.asarray(loser48_results[9:10][0])[0]\n",
    "upper_loser48 = np.asarray(loser48_results[14:15][0])[0]\n",
    "\n",
    "lower_winner48 = np.asarray(winner48_results[4:5][0])[0]\n",
    "median_winner48 = np.asarray(winner48_results[9:10][0])[0]\n",
    "upper_winner48 = np.asarray(winner48_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration58 :\n",
    "\n",
    "slice58 = 57\n",
    "\n",
    "loser58 = [train_regret_loser_1[slice58],\n",
    "       train_regret_loser_2[slice58],\n",
    "       train_regret_loser_3[slice58],\n",
    "       train_regret_loser_4[slice58],\n",
    "       train_regret_loser_5[slice58],\n",
    "       train_regret_loser_6[slice58],\n",
    "       train_regret_loser_7[slice58],\n",
    "       train_regret_loser_8[slice58],\n",
    "       train_regret_loser_9[slice58],\n",
    "       train_regret_loser_10[slice58],\n",
    "       train_regret_loser_11[slice58],\n",
    "       train_regret_loser_12[slice58],\n",
    "       train_regret_loser_13[slice58],\n",
    "       train_regret_loser_14[slice58],\n",
    "       train_regret_loser_15[slice58],\n",
    "       train_regret_loser_16[slice58],\n",
    "       train_regret_loser_17[slice58],\n",
    "       train_regret_loser_18[slice58],\n",
    "       train_regret_loser_19[slice58],\n",
    "       train_regret_loser_20[slice58]]\n",
    "\n",
    "winner58 = [train_regret_winner_1[slice58],\n",
    "       train_regret_winner_2[slice58],\n",
    "       train_regret_winner_3[slice58],\n",
    "       train_regret_winner_4[slice58],\n",
    "       train_regret_winner_5[slice58],\n",
    "       train_regret_winner_6[slice58],\n",
    "       train_regret_winner_7[slice58],\n",
    "       train_regret_winner_8[slice58],\n",
    "       train_regret_winner_9[slice58],\n",
    "       train_regret_winner_10[slice58],\n",
    "       train_regret_winner_11[slice58],\n",
    "       train_regret_winner_12[slice58],\n",
    "       train_regret_winner_13[slice58],\n",
    "       train_regret_winner_14[slice58],\n",
    "       train_regret_winner_15[slice58],\n",
    "       train_regret_winner_16[slice58],\n",
    "       train_regret_winner_17[slice58],\n",
    "       train_regret_winner_18[slice58],\n",
    "       train_regret_winner_19[slice58],\n",
    "       train_regret_winner_20[slice58]]\n",
    "\n",
    "loser58_results = pd.DataFrame(loser58).sort_values(by=[0], ascending=False)\n",
    "winner58_results = pd.DataFrame(winner58).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser58 = np.asarray(loser58_results[4:5][0])[0]\n",
    "median_loser58 = np.asarray(loser58_results[9:10][0])[0]\n",
    "upper_loser58 = np.asarray(loser58_results[14:15][0])[0]\n",
    "\n",
    "lower_winner58 = np.asarray(winner58_results[4:5][0])[0]\n",
    "median_winner58 = np.asarray(winner58_results[9:10][0])[0]\n",
    "upper_winner58 = np.asarray(winner58_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration68 :\n",
    "\n",
    "slice68 = 67\n",
    "\n",
    "loser68 = [train_regret_loser_1[slice68],\n",
    "       train_regret_loser_2[slice68],\n",
    "       train_regret_loser_3[slice68],\n",
    "       train_regret_loser_4[slice68],\n",
    "       train_regret_loser_5[slice68],\n",
    "       train_regret_loser_6[slice68],\n",
    "       train_regret_loser_7[slice68],\n",
    "       train_regret_loser_8[slice68],\n",
    "       train_regret_loser_9[slice68],\n",
    "       train_regret_loser_10[slice68],\n",
    "       train_regret_loser_11[slice68],\n",
    "       train_regret_loser_12[slice68],\n",
    "       train_regret_loser_13[slice68],\n",
    "       train_regret_loser_14[slice68],\n",
    "       train_regret_loser_15[slice68],\n",
    "       train_regret_loser_16[slice68],\n",
    "       train_regret_loser_17[slice68],\n",
    "       train_regret_loser_18[slice68],\n",
    "       train_regret_loser_19[slice68],\n",
    "       train_regret_loser_20[slice68]]\n",
    "\n",
    "winner68 = [train_regret_winner_1[slice68],\n",
    "       train_regret_winner_2[slice68],\n",
    "       train_regret_winner_3[slice68],\n",
    "       train_regret_winner_4[slice68],\n",
    "       train_regret_winner_5[slice68],\n",
    "       train_regret_winner_6[slice68],\n",
    "       train_regret_winner_7[slice68],\n",
    "       train_regret_winner_8[slice68],\n",
    "       train_regret_winner_9[slice68],\n",
    "       train_regret_winner_10[slice68],\n",
    "       train_regret_winner_11[slice68],\n",
    "       train_regret_winner_12[slice68],\n",
    "       train_regret_winner_13[slice68],\n",
    "       train_regret_winner_14[slice68],\n",
    "       train_regret_winner_15[slice68],\n",
    "       train_regret_winner_16[slice68],\n",
    "       train_regret_winner_17[slice68],\n",
    "       train_regret_winner_18[slice68],\n",
    "       train_regret_winner_19[slice68],\n",
    "       train_regret_winner_20[slice68]]\n",
    "\n",
    "loser68_results = pd.DataFrame(loser68).sort_values(by=[0], ascending=False)\n",
    "winner68_results = pd.DataFrame(winner68).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser68 = np.asarray(loser68_results[4:5][0])[0]\n",
    "median_loser68 = np.asarray(loser68_results[9:10][0])[0]\n",
    "upper_loser68 = np.asarray(loser68_results[14:15][0])[0]\n",
    "\n",
    "lower_winner68 = np.asarray(winner68_results[4:5][0])[0]\n",
    "median_winner68 = np.asarray(winner68_results[9:10][0])[0]\n",
    "upper_winner68 = np.asarray(winner68_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration78 :\n",
    "\n",
    "slice78 = 77\n",
    "\n",
    "loser78 = [train_regret_loser_1[slice78],\n",
    "       train_regret_loser_2[slice78],\n",
    "       train_regret_loser_3[slice78],\n",
    "       train_regret_loser_4[slice78],\n",
    "       train_regret_loser_5[slice78],\n",
    "       train_regret_loser_6[slice78],\n",
    "       train_regret_loser_7[slice78],\n",
    "       train_regret_loser_8[slice78],\n",
    "       train_regret_loser_9[slice78],\n",
    "       train_regret_loser_10[slice78],\n",
    "       train_regret_loser_11[slice78],\n",
    "       train_regret_loser_12[slice78],\n",
    "       train_regret_loser_13[slice78],\n",
    "       train_regret_loser_14[slice78],\n",
    "       train_regret_loser_15[slice78],\n",
    "       train_regret_loser_16[slice78],\n",
    "       train_regret_loser_17[slice78],\n",
    "       train_regret_loser_18[slice78],\n",
    "       train_regret_loser_19[slice78],\n",
    "       train_regret_loser_20[slice78]]\n",
    "\n",
    "winner78 = [train_regret_winner_1[slice78],\n",
    "       train_regret_winner_2[slice78],\n",
    "       train_regret_winner_3[slice78],\n",
    "       train_regret_winner_4[slice78],\n",
    "       train_regret_winner_5[slice78],\n",
    "       train_regret_winner_6[slice78],\n",
    "       train_regret_winner_7[slice78],\n",
    "       train_regret_winner_8[slice78],\n",
    "       train_regret_winner_9[slice78],\n",
    "       train_regret_winner_10[slice78],\n",
    "       train_regret_winner_11[slice78],\n",
    "       train_regret_winner_12[slice78],\n",
    "       train_regret_winner_13[slice78],\n",
    "       train_regret_winner_14[slice78],\n",
    "       train_regret_winner_15[slice78],\n",
    "       train_regret_winner_16[slice78],\n",
    "       train_regret_winner_17[slice78],\n",
    "       train_regret_winner_18[slice78],\n",
    "       train_regret_winner_19[slice78],\n",
    "       train_regret_winner_20[slice78]]\n",
    "\n",
    "loser78_results = pd.DataFrame(loser78).sort_values(by=[0], ascending=False)\n",
    "winner78_results = pd.DataFrame(winner78).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser78 = np.asarray(loser78_results[4:5][0])[0]\n",
    "median_loser78 = np.asarray(loser78_results[9:10][0])[0]\n",
    "upper_loser78 = np.asarray(loser78_results[14:15][0])[0]\n",
    "\n",
    "lower_winner78 = np.asarray(winner78_results[4:5][0])[0]\n",
    "median_winner78 = np.asarray(winner78_results[9:10][0])[0]\n",
    "upper_winner78 = np.asarray(winner78_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration88 :\n",
    "\n",
    "slice88 = 87\n",
    "\n",
    "loser88 = [train_regret_loser_1[slice88],\n",
    "       train_regret_loser_2[slice88],\n",
    "       train_regret_loser_3[slice88],\n",
    "       train_regret_loser_4[slice88],\n",
    "       train_regret_loser_5[slice88],\n",
    "       train_regret_loser_6[slice88],\n",
    "       train_regret_loser_7[slice88],\n",
    "       train_regret_loser_8[slice88],\n",
    "       train_regret_loser_9[slice88],\n",
    "       train_regret_loser_10[slice88],\n",
    "       train_regret_loser_11[slice88],\n",
    "       train_regret_loser_12[slice88],\n",
    "       train_regret_loser_13[slice88],\n",
    "       train_regret_loser_14[slice88],\n",
    "       train_regret_loser_15[slice88],\n",
    "       train_regret_loser_16[slice88],\n",
    "       train_regret_loser_17[slice88],\n",
    "       train_regret_loser_18[slice88],\n",
    "       train_regret_loser_19[slice88],\n",
    "       train_regret_loser_20[slice88]]\n",
    "\n",
    "winner88 = [train_regret_winner_1[slice88],\n",
    "       train_regret_winner_2[slice88],\n",
    "       train_regret_winner_3[slice88],\n",
    "       train_regret_winner_4[slice88],\n",
    "       train_regret_winner_5[slice88],\n",
    "       train_regret_winner_6[slice88],\n",
    "       train_regret_winner_7[slice88],\n",
    "       train_regret_winner_8[slice88],\n",
    "       train_regret_winner_9[slice88],\n",
    "       train_regret_winner_10[slice88],\n",
    "       train_regret_winner_11[slice88],\n",
    "       train_regret_winner_12[slice88],\n",
    "       train_regret_winner_13[slice88],\n",
    "       train_regret_winner_14[slice88],\n",
    "       train_regret_winner_15[slice88],\n",
    "       train_regret_winner_16[slice88],\n",
    "       train_regret_winner_17[slice88],\n",
    "       train_regret_winner_18[slice88],\n",
    "       train_regret_winner_19[slice88],\n",
    "       train_regret_winner_20[slice88]]\n",
    "\n",
    "loser88_results = pd.DataFrame(loser88).sort_values(by=[0], ascending=False)\n",
    "winner88_results = pd.DataFrame(winner88).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser88 = np.asarray(loser88_results[4:5][0])[0]\n",
    "median_loser88 = np.asarray(loser88_results[9:10][0])[0]\n",
    "upper_loser88 = np.asarray(loser88_results[14:15][0])[0]\n",
    "\n",
    "lower_winner88 = np.asarray(winner88_results[4:5][0])[0]\n",
    "median_winner88 = np.asarray(winner88_results[9:10][0])[0]\n",
    "upper_winner88 = np.asarray(winner88_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration98 :\n",
    "\n",
    "slice98 = 97\n",
    "\n",
    "loser98 = [train_regret_loser_1[slice98],\n",
    "       train_regret_loser_2[slice98],\n",
    "       train_regret_loser_3[slice98],\n",
    "       train_regret_loser_4[slice98],\n",
    "       train_regret_loser_5[slice98],\n",
    "       train_regret_loser_6[slice98],\n",
    "       train_regret_loser_7[slice98],\n",
    "       train_regret_loser_8[slice98],\n",
    "       train_regret_loser_9[slice98],\n",
    "       train_regret_loser_10[slice98],\n",
    "       train_regret_loser_11[slice98],\n",
    "       train_regret_loser_12[slice98],\n",
    "       train_regret_loser_13[slice98],\n",
    "       train_regret_loser_14[slice98],\n",
    "       train_regret_loser_15[slice98],\n",
    "       train_regret_loser_16[slice98],\n",
    "       train_regret_loser_17[slice98],\n",
    "       train_regret_loser_18[slice98],\n",
    "       train_regret_loser_19[slice98],\n",
    "       train_regret_loser_20[slice98]]\n",
    "\n",
    "winner98 = [train_regret_winner_1[slice98],\n",
    "       train_regret_winner_2[slice98],\n",
    "       train_regret_winner_3[slice98],\n",
    "       train_regret_winner_4[slice98],\n",
    "       train_regret_winner_5[slice98],\n",
    "       train_regret_winner_6[slice98],\n",
    "       train_regret_winner_7[slice98],\n",
    "       train_regret_winner_8[slice98],\n",
    "       train_regret_winner_9[slice98],\n",
    "       train_regret_winner_10[slice98],\n",
    "       train_regret_winner_11[slice98],\n",
    "       train_regret_winner_12[slice98],\n",
    "       train_regret_winner_13[slice98],\n",
    "       train_regret_winner_14[slice98],\n",
    "       train_regret_winner_15[slice98],\n",
    "       train_regret_winner_16[slice98],\n",
    "       train_regret_winner_17[slice98],\n",
    "       train_regret_winner_18[slice98],\n",
    "       train_regret_winner_19[slice98],\n",
    "       train_regret_winner_20[slice98]]\n",
    "\n",
    "loser98_results = pd.DataFrame(loser98).sort_values(by=[0], ascending=False)\n",
    "winner98_results = pd.DataFrame(winner98).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser98 = np.asarray(loser98_results[4:5][0])[0]\n",
    "median_loser98 = np.asarray(loser98_results[9:10][0])[0]\n",
    "upper_loser98 = np.asarray(loser98_results[14:15][0])[0]\n",
    "\n",
    "lower_winner98 = np.asarray(winner98_results[4:5][0])[0]\n",
    "median_winner98 = np.asarray(winner98_results[9:10][0])[0]\n",
    "upper_winner98 = np.asarray(winner98_results[14:15][0])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration9 :\n",
    "\n",
    "slice9 = 8\n",
    "\n",
    "loser9 = [train_regret_loser_1[slice9],\n",
    "       train_regret_loser_2[slice9],\n",
    "       train_regret_loser_3[slice9],\n",
    "       train_regret_loser_4[slice9],\n",
    "       train_regret_loser_5[slice9],\n",
    "       train_regret_loser_6[slice9],\n",
    "       train_regret_loser_7[slice9],\n",
    "       train_regret_loser_8[slice9],\n",
    "       train_regret_loser_9[slice9],\n",
    "       train_regret_loser_10[slice9],\n",
    "       train_regret_loser_11[slice9],\n",
    "       train_regret_loser_12[slice9],\n",
    "       train_regret_loser_13[slice9],\n",
    "       train_regret_loser_14[slice9],\n",
    "       train_regret_loser_15[slice9],\n",
    "       train_regret_loser_16[slice9],\n",
    "       train_regret_loser_17[slice9],\n",
    "       train_regret_loser_18[slice9],\n",
    "       train_regret_loser_19[slice9],\n",
    "       train_regret_loser_20[slice9]]\n",
    "\n",
    "winner9 = [train_regret_winner_1[slice9],\n",
    "       train_regret_winner_2[slice9],\n",
    "       train_regret_winner_3[slice9],\n",
    "       train_regret_winner_4[slice9],\n",
    "       train_regret_winner_5[slice9],\n",
    "       train_regret_winner_6[slice9],\n",
    "       train_regret_winner_7[slice9],\n",
    "       train_regret_winner_8[slice9],\n",
    "       train_regret_winner_9[slice9],\n",
    "       train_regret_winner_10[slice9],\n",
    "       train_regret_winner_11[slice9],\n",
    "       train_regret_winner_12[slice9],\n",
    "       train_regret_winner_13[slice9],\n",
    "       train_regret_winner_14[slice9],\n",
    "       train_regret_winner_15[slice9],\n",
    "       train_regret_winner_16[slice9],\n",
    "       train_regret_winner_17[slice9],\n",
    "       train_regret_winner_18[slice9],\n",
    "       train_regret_winner_19[slice9],\n",
    "       train_regret_winner_20[slice9]]\n",
    "\n",
    "loser9_results = pd.DataFrame(loser9).sort_values(by=[0], ascending=False)\n",
    "winner9_results = pd.DataFrame(winner9).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser9 = np.asarray(loser9_results[4:5][0])[0]\n",
    "median_loser9 = np.asarray(loser9_results[9:10][0])[0]\n",
    "upper_loser9 = np.asarray(loser9_results[14:15][0])[0]\n",
    "\n",
    "lower_winner9 = np.asarray(winner9_results[4:5][0])[0]\n",
    "median_winner9 = np.asarray(winner9_results[9:10][0])[0]\n",
    "upper_winner9 = np.asarray(winner9_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration19 :\n",
    "\n",
    "slice19 = 18\n",
    "\n",
    "loser19 = [train_regret_loser_1[slice19],\n",
    "       train_regret_loser_2[slice19],\n",
    "       train_regret_loser_3[slice19],\n",
    "       train_regret_loser_4[slice19],\n",
    "       train_regret_loser_5[slice19],\n",
    "       train_regret_loser_6[slice19],\n",
    "       train_regret_loser_7[slice19],\n",
    "       train_regret_loser_8[slice19],\n",
    "       train_regret_loser_9[slice19],\n",
    "       train_regret_loser_10[slice19],\n",
    "       train_regret_loser_11[slice19],\n",
    "       train_regret_loser_12[slice19],\n",
    "       train_regret_loser_13[slice19],\n",
    "       train_regret_loser_14[slice19],\n",
    "       train_regret_loser_15[slice19],\n",
    "       train_regret_loser_16[slice19],\n",
    "       train_regret_loser_17[slice19],\n",
    "       train_regret_loser_18[slice19],\n",
    "       train_regret_loser_19[slice19],\n",
    "       train_regret_loser_20[slice19]]\n",
    "\n",
    "winner19 = [train_regret_winner_1[slice19],\n",
    "       train_regret_winner_2[slice19],\n",
    "       train_regret_winner_3[slice19],\n",
    "       train_regret_winner_4[slice19],\n",
    "       train_regret_winner_5[slice19],\n",
    "       train_regret_winner_6[slice19],\n",
    "       train_regret_winner_7[slice19],\n",
    "       train_regret_winner_8[slice19],\n",
    "       train_regret_winner_9[slice19],\n",
    "       train_regret_winner_10[slice19],\n",
    "       train_regret_winner_11[slice19],\n",
    "       train_regret_winner_12[slice19],\n",
    "       train_regret_winner_13[slice19],\n",
    "       train_regret_winner_14[slice19],\n",
    "       train_regret_winner_15[slice19],\n",
    "       train_regret_winner_16[slice19],\n",
    "       train_regret_winner_17[slice19],\n",
    "       train_regret_winner_18[slice19],\n",
    "       train_regret_winner_19[slice19],\n",
    "       train_regret_winner_20[slice19]]\n",
    "\n",
    "loser19_results = pd.DataFrame(loser19).sort_values(by=[0], ascending=False)\n",
    "winner19_results = pd.DataFrame(winner19).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser19 = np.asarray(loser19_results[4:5][0])[0]\n",
    "median_loser19 = np.asarray(loser19_results[9:10][0])[0]\n",
    "upper_loser19 = np.asarray(loser19_results[14:15][0])[0]\n",
    "\n",
    "lower_winner19 = np.asarray(winner19_results[4:5][0])[0]\n",
    "median_winner19 = np.asarray(winner19_results[9:10][0])[0]\n",
    "upper_winner19 = np.asarray(winner19_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration29 :\n",
    "\n",
    "slice29 = 28\n",
    "\n",
    "loser29 = [train_regret_loser_1[slice29],\n",
    "       train_regret_loser_2[slice29],\n",
    "       train_regret_loser_3[slice29],\n",
    "       train_regret_loser_4[slice29],\n",
    "       train_regret_loser_5[slice29],\n",
    "       train_regret_loser_6[slice29],\n",
    "       train_regret_loser_7[slice29],\n",
    "       train_regret_loser_8[slice29],\n",
    "       train_regret_loser_9[slice29],\n",
    "       train_regret_loser_10[slice29],\n",
    "       train_regret_loser_11[slice29],\n",
    "       train_regret_loser_12[slice29],\n",
    "       train_regret_loser_13[slice29],\n",
    "       train_regret_loser_14[slice29],\n",
    "       train_regret_loser_15[slice29],\n",
    "       train_regret_loser_16[slice29],\n",
    "       train_regret_loser_17[slice29],\n",
    "       train_regret_loser_18[slice29],\n",
    "       train_regret_loser_19[slice29],\n",
    "       train_regret_loser_20[slice29]]\n",
    "\n",
    "winner29 = [train_regret_winner_1[slice29],\n",
    "       train_regret_winner_2[slice29],\n",
    "       train_regret_winner_3[slice29],\n",
    "       train_regret_winner_4[slice29],\n",
    "       train_regret_winner_5[slice29],\n",
    "       train_regret_winner_6[slice29],\n",
    "       train_regret_winner_7[slice29],\n",
    "       train_regret_winner_8[slice29],\n",
    "       train_regret_winner_9[slice29],\n",
    "       train_regret_winner_10[slice29],\n",
    "       train_regret_winner_11[slice29],\n",
    "       train_regret_winner_12[slice29],\n",
    "       train_regret_winner_13[slice29],\n",
    "       train_regret_winner_14[slice29],\n",
    "       train_regret_winner_15[slice29],\n",
    "       train_regret_winner_16[slice29],\n",
    "       train_regret_winner_17[slice29],\n",
    "       train_regret_winner_18[slice29],\n",
    "       train_regret_winner_19[slice29],\n",
    "       train_regret_winner_20[slice29]]\n",
    "\n",
    "loser29_results = pd.DataFrame(loser29).sort_values(by=[0], ascending=False)\n",
    "winner29_results = pd.DataFrame(winner29).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser29 = np.asarray(loser29_results[4:5][0])[0]\n",
    "median_loser29 = np.asarray(loser29_results[9:10][0])[0]\n",
    "upper_loser29 = np.asarray(loser29_results[14:15][0])[0]\n",
    "\n",
    "lower_winner29 = np.asarray(winner29_results[4:5][0])[0]\n",
    "median_winner29 = np.asarray(winner29_results[9:10][0])[0]\n",
    "upper_winner29 = np.asarray(winner29_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration39 :\n",
    "\n",
    "slice39 = 38\n",
    "\n",
    "loser39 = [train_regret_loser_1[slice39],\n",
    "       train_regret_loser_2[slice39],\n",
    "       train_regret_loser_3[slice39],\n",
    "       train_regret_loser_4[slice39],\n",
    "       train_regret_loser_5[slice39],\n",
    "       train_regret_loser_6[slice39],\n",
    "       train_regret_loser_7[slice39],\n",
    "       train_regret_loser_8[slice39],\n",
    "       train_regret_loser_9[slice39],\n",
    "       train_regret_loser_10[slice39],\n",
    "       train_regret_loser_11[slice39],\n",
    "       train_regret_loser_12[slice39],\n",
    "       train_regret_loser_13[slice39],\n",
    "       train_regret_loser_14[slice39],\n",
    "       train_regret_loser_15[slice39],\n",
    "       train_regret_loser_16[slice39],\n",
    "       train_regret_loser_17[slice39],\n",
    "       train_regret_loser_18[slice39],\n",
    "       train_regret_loser_19[slice39],\n",
    "       train_regret_loser_20[slice39]]\n",
    "\n",
    "winner39 = [train_regret_winner_1[slice39],\n",
    "       train_regret_winner_2[slice39],\n",
    "       train_regret_winner_3[slice39],\n",
    "       train_regret_winner_4[slice39],\n",
    "       train_regret_winner_5[slice39],\n",
    "       train_regret_winner_6[slice39],\n",
    "       train_regret_winner_7[slice39],\n",
    "       train_regret_winner_8[slice39],\n",
    "       train_regret_winner_9[slice39],\n",
    "       train_regret_winner_10[slice39],\n",
    "       train_regret_winner_11[slice39],\n",
    "       train_regret_winner_12[slice39],\n",
    "       train_regret_winner_13[slice39],\n",
    "       train_regret_winner_14[slice39],\n",
    "       train_regret_winner_15[slice39],\n",
    "       train_regret_winner_16[slice39],\n",
    "       train_regret_winner_17[slice39],\n",
    "       train_regret_winner_18[slice39],\n",
    "       train_regret_winner_19[slice39],\n",
    "       train_regret_winner_20[slice39]]\n",
    "\n",
    "loser39_results = pd.DataFrame(loser39).sort_values(by=[0], ascending=False)\n",
    "winner39_results = pd.DataFrame(winner39).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser39 = np.asarray(loser39_results[4:5][0])[0]\n",
    "median_loser39 = np.asarray(loser39_results[9:10][0])[0]\n",
    "upper_loser39 = np.asarray(loser39_results[14:15][0])[0]\n",
    "\n",
    "lower_winner39 = np.asarray(winner39_results[4:5][0])[0]\n",
    "median_winner39 = np.asarray(winner39_results[9:10][0])[0]\n",
    "upper_winner39 = np.asarray(winner39_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration49 :\n",
    "\n",
    "slice49 = 48\n",
    "\n",
    "loser49 = [train_regret_loser_1[slice49],\n",
    "       train_regret_loser_2[slice49],\n",
    "       train_regret_loser_3[slice49],\n",
    "       train_regret_loser_4[slice49],\n",
    "       train_regret_loser_5[slice49],\n",
    "       train_regret_loser_6[slice49],\n",
    "       train_regret_loser_7[slice49],\n",
    "       train_regret_loser_8[slice49],\n",
    "       train_regret_loser_9[slice49],\n",
    "       train_regret_loser_10[slice49],\n",
    "       train_regret_loser_11[slice49],\n",
    "       train_regret_loser_12[slice49],\n",
    "       train_regret_loser_13[slice49],\n",
    "       train_regret_loser_14[slice49],\n",
    "       train_regret_loser_15[slice49],\n",
    "       train_regret_loser_16[slice49],\n",
    "       train_regret_loser_17[slice49],\n",
    "       train_regret_loser_18[slice49],\n",
    "       train_regret_loser_19[slice49],\n",
    "       train_regret_loser_20[slice49]]\n",
    "\n",
    "winner49 = [train_regret_winner_1[slice49],\n",
    "       train_regret_winner_2[slice49],\n",
    "       train_regret_winner_3[slice49],\n",
    "       train_regret_winner_4[slice49],\n",
    "       train_regret_winner_5[slice49],\n",
    "       train_regret_winner_6[slice49],\n",
    "       train_regret_winner_7[slice49],\n",
    "       train_regret_winner_8[slice49],\n",
    "       train_regret_winner_9[slice49],\n",
    "       train_regret_winner_10[slice49],\n",
    "       train_regret_winner_11[slice49],\n",
    "       train_regret_winner_12[slice49],\n",
    "       train_regret_winner_13[slice49],\n",
    "       train_regret_winner_14[slice49],\n",
    "       train_regret_winner_15[slice49],\n",
    "       train_regret_winner_16[slice49],\n",
    "       train_regret_winner_17[slice49],\n",
    "       train_regret_winner_18[slice49],\n",
    "       train_regret_winner_19[slice49],\n",
    "       train_regret_winner_20[slice49]]\n",
    "\n",
    "loser49_results = pd.DataFrame(loser49).sort_values(by=[0], ascending=False)\n",
    "winner49_results = pd.DataFrame(winner49).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser49 = np.asarray(loser49_results[4:5][0])[0]\n",
    "median_loser49 = np.asarray(loser49_results[9:10][0])[0]\n",
    "upper_loser49 = np.asarray(loser49_results[14:15][0])[0]\n",
    "\n",
    "lower_winner49 = np.asarray(winner49_results[4:5][0])[0]\n",
    "median_winner49 = np.asarray(winner49_results[9:10][0])[0]\n",
    "upper_winner49 = np.asarray(winner49_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration59 :\n",
    "\n",
    "slice59 = 58\n",
    "\n",
    "loser59 = [train_regret_loser_1[slice59],\n",
    "       train_regret_loser_2[slice59],\n",
    "       train_regret_loser_3[slice59],\n",
    "       train_regret_loser_4[slice59],\n",
    "       train_regret_loser_5[slice59],\n",
    "       train_regret_loser_6[slice59],\n",
    "       train_regret_loser_7[slice59],\n",
    "       train_regret_loser_8[slice59],\n",
    "       train_regret_loser_9[slice59],\n",
    "       train_regret_loser_10[slice59],\n",
    "       train_regret_loser_11[slice59],\n",
    "       train_regret_loser_12[slice59],\n",
    "       train_regret_loser_13[slice59],\n",
    "       train_regret_loser_14[slice59],\n",
    "       train_regret_loser_15[slice59],\n",
    "       train_regret_loser_16[slice59],\n",
    "       train_regret_loser_17[slice59],\n",
    "       train_regret_loser_18[slice59],\n",
    "       train_regret_loser_19[slice59],\n",
    "       train_regret_loser_20[slice59]]\n",
    "\n",
    "winner59 = [train_regret_winner_1[slice59],\n",
    "       train_regret_winner_2[slice59],\n",
    "       train_regret_winner_3[slice59],\n",
    "       train_regret_winner_4[slice59],\n",
    "       train_regret_winner_5[slice59],\n",
    "       train_regret_winner_6[slice59],\n",
    "       train_regret_winner_7[slice59],\n",
    "       train_regret_winner_8[slice59],\n",
    "       train_regret_winner_9[slice59],\n",
    "       train_regret_winner_10[slice59],\n",
    "       train_regret_winner_11[slice59],\n",
    "       train_regret_winner_12[slice59],\n",
    "       train_regret_winner_13[slice59],\n",
    "       train_regret_winner_14[slice59],\n",
    "       train_regret_winner_15[slice59],\n",
    "       train_regret_winner_16[slice59],\n",
    "       train_regret_winner_17[slice59],\n",
    "       train_regret_winner_18[slice59],\n",
    "       train_regret_winner_19[slice59],\n",
    "       train_regret_winner_20[slice59]]\n",
    "\n",
    "loser59_results = pd.DataFrame(loser59).sort_values(by=[0], ascending=False)\n",
    "winner59_results = pd.DataFrame(winner59).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser59 = np.asarray(loser59_results[4:5][0])[0]\n",
    "median_loser59 = np.asarray(loser59_results[9:10][0])[0]\n",
    "upper_loser59 = np.asarray(loser59_results[14:15][0])[0]\n",
    "\n",
    "lower_winner59 = np.asarray(winner59_results[4:5][0])[0]\n",
    "median_winner59 = np.asarray(winner59_results[9:10][0])[0]\n",
    "upper_winner59 = np.asarray(winner59_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration69 :\n",
    "\n",
    "slice69 = 68\n",
    "\n",
    "loser69 = [train_regret_loser_1[slice69],\n",
    "       train_regret_loser_2[slice69],\n",
    "       train_regret_loser_3[slice69],\n",
    "       train_regret_loser_4[slice69],\n",
    "       train_regret_loser_5[slice69],\n",
    "       train_regret_loser_6[slice69],\n",
    "       train_regret_loser_7[slice69],\n",
    "       train_regret_loser_8[slice69],\n",
    "       train_regret_loser_9[slice69],\n",
    "       train_regret_loser_10[slice69],\n",
    "       train_regret_loser_11[slice69],\n",
    "       train_regret_loser_12[slice69],\n",
    "       train_regret_loser_13[slice69],\n",
    "       train_regret_loser_14[slice69],\n",
    "       train_regret_loser_15[slice69],\n",
    "       train_regret_loser_16[slice69],\n",
    "       train_regret_loser_17[slice69],\n",
    "       train_regret_loser_18[slice69],\n",
    "       train_regret_loser_19[slice69],\n",
    "       train_regret_loser_20[slice69]]\n",
    "\n",
    "winner69 = [train_regret_winner_1[slice69],\n",
    "       train_regret_winner_2[slice69],\n",
    "       train_regret_winner_3[slice69],\n",
    "       train_regret_winner_4[slice69],\n",
    "       train_regret_winner_5[slice69],\n",
    "       train_regret_winner_6[slice69],\n",
    "       train_regret_winner_7[slice69],\n",
    "       train_regret_winner_8[slice69],\n",
    "       train_regret_winner_9[slice69],\n",
    "       train_regret_winner_10[slice69],\n",
    "       train_regret_winner_11[slice69],\n",
    "       train_regret_winner_12[slice69],\n",
    "       train_regret_winner_13[slice69],\n",
    "       train_regret_winner_14[slice69],\n",
    "       train_regret_winner_15[slice69],\n",
    "       train_regret_winner_16[slice69],\n",
    "       train_regret_winner_17[slice69],\n",
    "       train_regret_winner_18[slice69],\n",
    "       train_regret_winner_19[slice69],\n",
    "       train_regret_winner_20[slice69]]\n",
    "\n",
    "loser69_results = pd.DataFrame(loser69).sort_values(by=[0], ascending=False)\n",
    "winner69_results = pd.DataFrame(winner69).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser69 = np.asarray(loser69_results[4:5][0])[0]\n",
    "median_loser69 = np.asarray(loser69_results[9:10][0])[0]\n",
    "upper_loser69 = np.asarray(loser69_results[14:15][0])[0]\n",
    "\n",
    "lower_winner69 = np.asarray(winner69_results[4:5][0])[0]\n",
    "median_winner69 = np.asarray(winner69_results[9:10][0])[0]\n",
    "upper_winner69 = np.asarray(winner69_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration79 :\n",
    "\n",
    "slice79 = 78\n",
    "\n",
    "loser79 = [train_regret_loser_1[slice79],\n",
    "       train_regret_loser_2[slice79],\n",
    "       train_regret_loser_3[slice79],\n",
    "       train_regret_loser_4[slice79],\n",
    "       train_regret_loser_5[slice79],\n",
    "       train_regret_loser_6[slice79],\n",
    "       train_regret_loser_7[slice79],\n",
    "       train_regret_loser_8[slice79],\n",
    "       train_regret_loser_9[slice79],\n",
    "       train_regret_loser_10[slice79],\n",
    "       train_regret_loser_11[slice79],\n",
    "       train_regret_loser_12[slice79],\n",
    "       train_regret_loser_13[slice79],\n",
    "       train_regret_loser_14[slice79],\n",
    "       train_regret_loser_15[slice79],\n",
    "       train_regret_loser_16[slice79],\n",
    "       train_regret_loser_17[slice79],\n",
    "       train_regret_loser_18[slice79],\n",
    "       train_regret_loser_19[slice79],\n",
    "       train_regret_loser_20[slice79]]\n",
    "\n",
    "winner79 = [train_regret_winner_1[slice79],\n",
    "       train_regret_winner_2[slice79],\n",
    "       train_regret_winner_3[slice79],\n",
    "       train_regret_winner_4[slice79],\n",
    "       train_regret_winner_5[slice79],\n",
    "       train_regret_winner_6[slice79],\n",
    "       train_regret_winner_7[slice79],\n",
    "       train_regret_winner_8[slice79],\n",
    "       train_regret_winner_9[slice79],\n",
    "       train_regret_winner_10[slice79],\n",
    "       train_regret_winner_11[slice79],\n",
    "       train_regret_winner_12[slice79],\n",
    "       train_regret_winner_13[slice79],\n",
    "       train_regret_winner_14[slice79],\n",
    "       train_regret_winner_15[slice79],\n",
    "       train_regret_winner_16[slice79],\n",
    "       train_regret_winner_17[slice79],\n",
    "       train_regret_winner_18[slice79],\n",
    "       train_regret_winner_19[slice79],\n",
    "       train_regret_winner_20[slice79]]\n",
    "\n",
    "loser79_results = pd.DataFrame(loser79).sort_values(by=[0], ascending=False)\n",
    "winner79_results = pd.DataFrame(winner79).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser79 = np.asarray(loser79_results[4:5][0])[0]\n",
    "median_loser79 = np.asarray(loser79_results[9:10][0])[0]\n",
    "upper_loser79 = np.asarray(loser79_results[14:15][0])[0]\n",
    "\n",
    "lower_winner79 = np.asarray(winner79_results[4:5][0])[0]\n",
    "median_winner79 = np.asarray(winner79_results[9:10][0])[0]\n",
    "upper_winner79 = np.asarray(winner79_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration89 :\n",
    "\n",
    "slice89 = 88\n",
    "\n",
    "loser89 = [train_regret_loser_1[slice89],\n",
    "       train_regret_loser_2[slice89],\n",
    "       train_regret_loser_3[slice89],\n",
    "       train_regret_loser_4[slice89],\n",
    "       train_regret_loser_5[slice89],\n",
    "       train_regret_loser_6[slice89],\n",
    "       train_regret_loser_7[slice89],\n",
    "       train_regret_loser_8[slice89],\n",
    "       train_regret_loser_9[slice89],\n",
    "       train_regret_loser_10[slice89],\n",
    "       train_regret_loser_11[slice89],\n",
    "       train_regret_loser_12[slice89],\n",
    "       train_regret_loser_13[slice89],\n",
    "       train_regret_loser_14[slice89],\n",
    "       train_regret_loser_15[slice89],\n",
    "       train_regret_loser_16[slice89],\n",
    "       train_regret_loser_17[slice89],\n",
    "       train_regret_loser_18[slice89],\n",
    "       train_regret_loser_19[slice89],\n",
    "       train_regret_loser_20[slice89]]\n",
    "\n",
    "winner89 = [train_regret_winner_1[slice89],\n",
    "       train_regret_winner_2[slice89],\n",
    "       train_regret_winner_3[slice89],\n",
    "       train_regret_winner_4[slice89],\n",
    "       train_regret_winner_5[slice89],\n",
    "       train_regret_winner_6[slice89],\n",
    "       train_regret_winner_7[slice89],\n",
    "       train_regret_winner_8[slice89],\n",
    "       train_regret_winner_9[slice89],\n",
    "       train_regret_winner_10[slice89],\n",
    "       train_regret_winner_11[slice89],\n",
    "       train_regret_winner_12[slice89],\n",
    "       train_regret_winner_13[slice89],\n",
    "       train_regret_winner_14[slice89],\n",
    "       train_regret_winner_15[slice89],\n",
    "       train_regret_winner_16[slice89],\n",
    "       train_regret_winner_17[slice89],\n",
    "       train_regret_winner_18[slice89],\n",
    "       train_regret_winner_19[slice89],\n",
    "       train_regret_winner_20[slice89]]\n",
    "\n",
    "loser89_results = pd.DataFrame(loser89).sort_values(by=[0], ascending=False)\n",
    "winner89_results = pd.DataFrame(winner89).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser89 = np.asarray(loser89_results[4:5][0])[0]\n",
    "median_loser89 = np.asarray(loser89_results[9:10][0])[0]\n",
    "upper_loser89 = np.asarray(loser89_results[14:15][0])[0]\n",
    "\n",
    "lower_winner89 = np.asarray(winner89_results[4:5][0])[0]\n",
    "median_winner89 = np.asarray(winner89_results[9:10][0])[0]\n",
    "upper_winner89 = np.asarray(winner89_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.763750478984005, -7.102727891736948)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iteration99 :\n",
    "\n",
    "slice99 = 98\n",
    "\n",
    "loser99 = [train_regret_loser_1[slice99],\n",
    "       train_regret_loser_2[slice99],\n",
    "       train_regret_loser_3[slice99],\n",
    "       train_regret_loser_4[slice99],\n",
    "       train_regret_loser_5[slice99],\n",
    "       train_regret_loser_6[slice99],\n",
    "       train_regret_loser_7[slice99],\n",
    "       train_regret_loser_8[slice99],\n",
    "       train_regret_loser_9[slice99],\n",
    "       train_regret_loser_10[slice99],\n",
    "       train_regret_loser_11[slice99],\n",
    "       train_regret_loser_12[slice99],\n",
    "       train_regret_loser_13[slice99],\n",
    "       train_regret_loser_14[slice99],\n",
    "       train_regret_loser_15[slice99],\n",
    "       train_regret_loser_16[slice99],\n",
    "       train_regret_loser_17[slice99],\n",
    "       train_regret_loser_18[slice99],\n",
    "       train_regret_loser_19[slice99],\n",
    "       train_regret_loser_20[slice99]]\n",
    "\n",
    "winner99 = [train_regret_winner_1[slice99],\n",
    "       train_regret_winner_2[slice99],\n",
    "       train_regret_winner_3[slice99],\n",
    "       train_regret_winner_4[slice99],\n",
    "       train_regret_winner_5[slice99],\n",
    "       train_regret_winner_6[slice99],\n",
    "       train_regret_winner_7[slice99],\n",
    "       train_regret_winner_8[slice99],\n",
    "       train_regret_winner_9[slice99],\n",
    "       train_regret_winner_10[slice99],\n",
    "       train_regret_winner_11[slice99],\n",
    "       train_regret_winner_12[slice99],\n",
    "       train_regret_winner_13[slice99],\n",
    "       train_regret_winner_14[slice99],\n",
    "       train_regret_winner_15[slice99],\n",
    "       train_regret_winner_16[slice99],\n",
    "       train_regret_winner_17[slice99],\n",
    "       train_regret_winner_18[slice99],\n",
    "       train_regret_winner_19[slice99],\n",
    "       train_regret_winner_20[slice99]]\n",
    "\n",
    "loser99_results = pd.DataFrame(loser99).sort_values(by=[0], ascending=False)\n",
    "winner99_results = pd.DataFrame(winner99).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser99 = np.asarray(loser99_results[4:5][0])[0]\n",
    "median_loser99 = np.asarray(loser99_results[9:10][0])[0]\n",
    "upper_loser99 = np.asarray(loser99_results[14:15][0])[0]\n",
    "\n",
    "lower_winner99 = np.asarray(winner99_results[4:5][0])[0]\n",
    "median_winner99 = np.asarray(winner99_results[9:10][0])[0]\n",
    "upper_winner99 = np.asarray(winner99_results[14:15][0])[0]\n",
    "\n",
    "lower_loser99, lower_winner99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration10 :\n",
    "\n",
    "slice10 = 9\n",
    "\n",
    "loser10 = [train_regret_loser_1[slice10],\n",
    "       train_regret_loser_2[slice10],\n",
    "       train_regret_loser_3[slice10],\n",
    "       train_regret_loser_4[slice10],\n",
    "       train_regret_loser_5[slice10],\n",
    "       train_regret_loser_6[slice10],\n",
    "       train_regret_loser_7[slice10],\n",
    "       train_regret_loser_8[slice10],\n",
    "       train_regret_loser_9[slice10],\n",
    "       train_regret_loser_10[slice10],\n",
    "       train_regret_loser_11[slice10],\n",
    "       train_regret_loser_12[slice10],\n",
    "       train_regret_loser_13[slice10],\n",
    "       train_regret_loser_14[slice10],\n",
    "       train_regret_loser_15[slice10],\n",
    "       train_regret_loser_16[slice10],\n",
    "       train_regret_loser_17[slice10],\n",
    "       train_regret_loser_18[slice10],\n",
    "       train_regret_loser_19[slice10],\n",
    "       train_regret_loser_20[slice10]]\n",
    "\n",
    "winner10 = [train_regret_winner_1[slice10],\n",
    "       train_regret_winner_2[slice10],\n",
    "       train_regret_winner_3[slice10],\n",
    "       train_regret_winner_4[slice10],\n",
    "       train_regret_winner_5[slice10],\n",
    "       train_regret_winner_6[slice10],\n",
    "       train_regret_winner_7[slice10],\n",
    "       train_regret_winner_8[slice10],\n",
    "       train_regret_winner_9[slice10],\n",
    "       train_regret_winner_10[slice10],\n",
    "       train_regret_winner_11[slice10],\n",
    "       train_regret_winner_12[slice10],\n",
    "       train_regret_winner_13[slice10],\n",
    "       train_regret_winner_14[slice10],\n",
    "       train_regret_winner_15[slice10],\n",
    "       train_regret_winner_16[slice10],\n",
    "       train_regret_winner_17[slice10],\n",
    "       train_regret_winner_18[slice10],\n",
    "       train_regret_winner_19[slice10],\n",
    "       train_regret_winner_20[slice10]]\n",
    "\n",
    "loser10_results = pd.DataFrame(loser10).sort_values(by=[0], ascending=False)\n",
    "winner10_results = pd.DataFrame(winner10).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser10 = np.asarray(loser10_results[4:5][0])[0]\n",
    "median_loser10 = np.asarray(loser10_results[9:10][0])[0]\n",
    "upper_loser10 = np.asarray(loser10_results[14:15][0])[0]\n",
    "\n",
    "lower_winner10 = np.asarray(winner10_results[4:5][0])[0]\n",
    "median_winner10 = np.asarray(winner10_results[9:10][0])[0]\n",
    "upper_winner10 = np.asarray(winner10_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration20 :\n",
    "\n",
    "slice20 = 19\n",
    "\n",
    "loser20 = [train_regret_loser_1[slice20],\n",
    "       train_regret_loser_2[slice20],\n",
    "       train_regret_loser_3[slice20],\n",
    "       train_regret_loser_4[slice20],\n",
    "       train_regret_loser_5[slice20],\n",
    "       train_regret_loser_6[slice20],\n",
    "       train_regret_loser_7[slice20],\n",
    "       train_regret_loser_8[slice20],\n",
    "       train_regret_loser_9[slice20],\n",
    "       train_regret_loser_10[slice20],\n",
    "       train_regret_loser_11[slice20],\n",
    "       train_regret_loser_12[slice20],\n",
    "       train_regret_loser_13[slice20],\n",
    "       train_regret_loser_14[slice20],\n",
    "       train_regret_loser_15[slice20],\n",
    "       train_regret_loser_16[slice20],\n",
    "       train_regret_loser_17[slice20],\n",
    "       train_regret_loser_18[slice20],\n",
    "       train_regret_loser_19[slice20],\n",
    "       train_regret_loser_20[slice20]]\n",
    "\n",
    "winner20 = [train_regret_winner_1[slice20],\n",
    "       train_regret_winner_2[slice20],\n",
    "       train_regret_winner_3[slice20],\n",
    "       train_regret_winner_4[slice20],\n",
    "       train_regret_winner_5[slice20],\n",
    "       train_regret_winner_6[slice20],\n",
    "       train_regret_winner_7[slice20],\n",
    "       train_regret_winner_8[slice20],\n",
    "       train_regret_winner_9[slice20],\n",
    "       train_regret_winner_10[slice20],\n",
    "       train_regret_winner_11[slice20],\n",
    "       train_regret_winner_12[slice20],\n",
    "       train_regret_winner_13[slice20],\n",
    "       train_regret_winner_14[slice20],\n",
    "       train_regret_winner_15[slice20],\n",
    "       train_regret_winner_16[slice20],\n",
    "       train_regret_winner_17[slice20],\n",
    "       train_regret_winner_18[slice20],\n",
    "       train_regret_winner_19[slice20],\n",
    "       train_regret_winner_20[slice20]]\n",
    "\n",
    "loser20_results = pd.DataFrame(loser20).sort_values(by=[0], ascending=False)\n",
    "winner20_results = pd.DataFrame(winner20).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser20 = np.asarray(loser20_results[4:5][0])[0]\n",
    "median_loser20 = np.asarray(loser20_results[9:10][0])[0]\n",
    "upper_loser20 = np.asarray(loser20_results[14:15][0])[0]\n",
    "\n",
    "lower_winner20 = np.asarray(winner20_results[4:5][0])[0]\n",
    "median_winner20 = np.asarray(winner20_results[9:10][0])[0]\n",
    "upper_winner20 = np.asarray(winner20_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration30 :\n",
    "\n",
    "slice30 = 29\n",
    "\n",
    "loser30 = [train_regret_loser_1[slice30],\n",
    "       train_regret_loser_2[slice30],\n",
    "       train_regret_loser_3[slice30],\n",
    "       train_regret_loser_4[slice30],\n",
    "       train_regret_loser_5[slice30],\n",
    "       train_regret_loser_6[slice30],\n",
    "       train_regret_loser_7[slice30],\n",
    "       train_regret_loser_8[slice30],\n",
    "       train_regret_loser_9[slice30],\n",
    "       train_regret_loser_10[slice30],\n",
    "       train_regret_loser_11[slice30],\n",
    "       train_regret_loser_12[slice30],\n",
    "       train_regret_loser_13[slice30],\n",
    "       train_regret_loser_14[slice30],\n",
    "       train_regret_loser_15[slice30],\n",
    "       train_regret_loser_16[slice30],\n",
    "       train_regret_loser_17[slice30],\n",
    "       train_regret_loser_18[slice30],\n",
    "       train_regret_loser_19[slice30],\n",
    "       train_regret_loser_20[slice30]]\n",
    "\n",
    "winner30 = [train_regret_winner_1[slice30],\n",
    "       train_regret_winner_2[slice30],\n",
    "       train_regret_winner_3[slice30],\n",
    "       train_regret_winner_4[slice30],\n",
    "       train_regret_winner_5[slice30],\n",
    "       train_regret_winner_6[slice30],\n",
    "       train_regret_winner_7[slice30],\n",
    "       train_regret_winner_8[slice30],\n",
    "       train_regret_winner_9[slice30],\n",
    "       train_regret_winner_10[slice30],\n",
    "       train_regret_winner_11[slice30],\n",
    "       train_regret_winner_12[slice30],\n",
    "       train_regret_winner_13[slice30],\n",
    "       train_regret_winner_14[slice30],\n",
    "       train_regret_winner_15[slice30],\n",
    "       train_regret_winner_16[slice30],\n",
    "       train_regret_winner_17[slice30],\n",
    "       train_regret_winner_18[slice30],\n",
    "       train_regret_winner_19[slice30],\n",
    "       train_regret_winner_20[slice30]]\n",
    "\n",
    "loser30_results = pd.DataFrame(loser30).sort_values(by=[0], ascending=False)\n",
    "winner30_results = pd.DataFrame(winner30).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser30 = np.asarray(loser30_results[4:5][0])[0]\n",
    "median_loser30 = np.asarray(loser30_results[9:10][0])[0]\n",
    "upper_loser30 = np.asarray(loser30_results[14:15][0])[0]\n",
    "\n",
    "lower_winner30 = np.asarray(winner30_results[4:5][0])[0]\n",
    "median_winner30 = np.asarray(winner30_results[9:10][0])[0]\n",
    "upper_winner30 = np.asarray(winner30_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration40 :\n",
    "\n",
    "slice40 = 39\n",
    "\n",
    "loser40 = [train_regret_loser_1[slice40],\n",
    "       train_regret_loser_2[slice40],\n",
    "       train_regret_loser_3[slice40],\n",
    "       train_regret_loser_4[slice40],\n",
    "       train_regret_loser_5[slice40],\n",
    "       train_regret_loser_6[slice40],\n",
    "       train_regret_loser_7[slice40],\n",
    "       train_regret_loser_8[slice40],\n",
    "       train_regret_loser_9[slice40],\n",
    "       train_regret_loser_10[slice40],\n",
    "       train_regret_loser_11[slice40],\n",
    "       train_regret_loser_12[slice40],\n",
    "       train_regret_loser_13[slice40],\n",
    "       train_regret_loser_14[slice40],\n",
    "       train_regret_loser_15[slice40],\n",
    "       train_regret_loser_16[slice40],\n",
    "       train_regret_loser_17[slice40],\n",
    "       train_regret_loser_18[slice40],\n",
    "       train_regret_loser_19[slice40],\n",
    "       train_regret_loser_20[slice40]]\n",
    "\n",
    "winner40 = [train_regret_winner_1[slice40],\n",
    "       train_regret_winner_2[slice40],\n",
    "       train_regret_winner_3[slice40],\n",
    "       train_regret_winner_4[slice40],\n",
    "       train_regret_winner_5[slice40],\n",
    "       train_regret_winner_6[slice40],\n",
    "       train_regret_winner_7[slice40],\n",
    "       train_regret_winner_8[slice40],\n",
    "       train_regret_winner_9[slice40],\n",
    "       train_regret_winner_10[slice40],\n",
    "       train_regret_winner_11[slice40],\n",
    "       train_regret_winner_12[slice40],\n",
    "       train_regret_winner_13[slice40],\n",
    "       train_regret_winner_14[slice40],\n",
    "       train_regret_winner_15[slice40],\n",
    "       train_regret_winner_16[slice40],\n",
    "       train_regret_winner_17[slice40],\n",
    "       train_regret_winner_18[slice40],\n",
    "       train_regret_winner_19[slice40],\n",
    "       train_regret_winner_20[slice40]]\n",
    "\n",
    "loser40_results = pd.DataFrame(loser40).sort_values(by=[0], ascending=False)\n",
    "winner40_results = pd.DataFrame(winner40).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser40 = np.asarray(loser40_results[4:5][0])[0]\n",
    "median_loser40 = np.asarray(loser40_results[9:10][0])[0]\n",
    "upper_loser40 = np.asarray(loser40_results[14:15][0])[0]\n",
    "\n",
    "lower_winner40 = np.asarray(winner40_results[4:5][0])[0]\n",
    "median_winner40 = np.asarray(winner40_results[9:10][0])[0]\n",
    "upper_winner40 = np.asarray(winner40_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration50 :\n",
    "\n",
    "slice50 = 49\n",
    "\n",
    "loser50 = [train_regret_loser_1[slice50],\n",
    "       train_regret_loser_2[slice50],\n",
    "       train_regret_loser_3[slice50],\n",
    "       train_regret_loser_4[slice50],\n",
    "       train_regret_loser_5[slice50],\n",
    "       train_regret_loser_6[slice50],\n",
    "       train_regret_loser_7[slice50],\n",
    "       train_regret_loser_8[slice50],\n",
    "       train_regret_loser_9[slice50],\n",
    "       train_regret_loser_10[slice50],\n",
    "       train_regret_loser_11[slice50],\n",
    "       train_regret_loser_12[slice50],\n",
    "       train_regret_loser_13[slice50],\n",
    "       train_regret_loser_14[slice50],\n",
    "       train_regret_loser_15[slice50],\n",
    "       train_regret_loser_16[slice50],\n",
    "       train_regret_loser_17[slice50],\n",
    "       train_regret_loser_18[slice50],\n",
    "       train_regret_loser_19[slice50],\n",
    "       train_regret_loser_20[slice50]]\n",
    "\n",
    "winner50 = [train_regret_winner_1[slice50],\n",
    "       train_regret_winner_2[slice50],\n",
    "       train_regret_winner_3[slice50],\n",
    "       train_regret_winner_4[slice50],\n",
    "       train_regret_winner_5[slice50],\n",
    "       train_regret_winner_6[slice50],\n",
    "       train_regret_winner_7[slice50],\n",
    "       train_regret_winner_8[slice50],\n",
    "       train_regret_winner_9[slice50],\n",
    "       train_regret_winner_10[slice50],\n",
    "       train_regret_winner_11[slice50],\n",
    "       train_regret_winner_12[slice50],\n",
    "       train_regret_winner_13[slice50],\n",
    "       train_regret_winner_14[slice50],\n",
    "       train_regret_winner_15[slice50],\n",
    "       train_regret_winner_16[slice50],\n",
    "       train_regret_winner_17[slice50],\n",
    "       train_regret_winner_18[slice50],\n",
    "       train_regret_winner_19[slice50],\n",
    "       train_regret_winner_20[slice50]]\n",
    "\n",
    "loser50_results = pd.DataFrame(loser50).sort_values(by=[0], ascending=False)\n",
    "winner50_results = pd.DataFrame(winner50).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser50 = np.asarray(loser50_results[4:5][0])[0]\n",
    "median_loser50 = np.asarray(loser50_results[9:10][0])[0]\n",
    "upper_loser50 = np.asarray(loser50_results[14:15][0])[0]\n",
    "\n",
    "lower_winner50 = np.asarray(winner50_results[4:5][0])[0]\n",
    "median_winner50 = np.asarray(winner50_results[9:10][0])[0]\n",
    "upper_winner50 = np.asarray(winner50_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration60 :\n",
    "\n",
    "slice60 = 59\n",
    "\n",
    "loser60 = [train_regret_loser_1[slice60],\n",
    "       train_regret_loser_2[slice60],\n",
    "       train_regret_loser_3[slice60],\n",
    "       train_regret_loser_4[slice60],\n",
    "       train_regret_loser_5[slice60],\n",
    "       train_regret_loser_6[slice60],\n",
    "       train_regret_loser_7[slice60],\n",
    "       train_regret_loser_8[slice60],\n",
    "       train_regret_loser_9[slice60],\n",
    "       train_regret_loser_10[slice60],\n",
    "       train_regret_loser_11[slice60],\n",
    "       train_regret_loser_12[slice60],\n",
    "       train_regret_loser_13[slice60],\n",
    "       train_regret_loser_14[slice60],\n",
    "       train_regret_loser_15[slice60],\n",
    "       train_regret_loser_16[slice60],\n",
    "       train_regret_loser_17[slice60],\n",
    "       train_regret_loser_18[slice60],\n",
    "       train_regret_loser_19[slice60],\n",
    "       train_regret_loser_20[slice60]]\n",
    "\n",
    "winner60 = [train_regret_winner_1[slice60],\n",
    "       train_regret_winner_2[slice60],\n",
    "       train_regret_winner_3[slice60],\n",
    "       train_regret_winner_4[slice60],\n",
    "       train_regret_winner_5[slice60],\n",
    "       train_regret_winner_6[slice60],\n",
    "       train_regret_winner_7[slice60],\n",
    "       train_regret_winner_8[slice60],\n",
    "       train_regret_winner_9[slice60],\n",
    "       train_regret_winner_10[slice60],\n",
    "       train_regret_winner_11[slice60],\n",
    "       train_regret_winner_12[slice60],\n",
    "       train_regret_winner_13[slice60],\n",
    "       train_regret_winner_14[slice60],\n",
    "       train_regret_winner_15[slice60],\n",
    "       train_regret_winner_16[slice60],\n",
    "       train_regret_winner_17[slice60],\n",
    "       train_regret_winner_18[slice60],\n",
    "       train_regret_winner_19[slice60],\n",
    "       train_regret_winner_20[slice60]]\n",
    "\n",
    "loser60_results = pd.DataFrame(loser60).sort_values(by=[0], ascending=False)\n",
    "winner60_results = pd.DataFrame(winner60).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser60 = np.asarray(loser60_results[4:5][0])[0]\n",
    "median_loser60 = np.asarray(loser60_results[9:10][0])[0]\n",
    "upper_loser60 = np.asarray(loser60_results[14:15][0])[0]\n",
    "\n",
    "lower_winner60 = np.asarray(winner60_results[4:5][0])[0]\n",
    "median_winner60 = np.asarray(winner60_results[9:10][0])[0]\n",
    "upper_winner60 = np.asarray(winner60_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration70 :\n",
    "\n",
    "slice70 = 69\n",
    "\n",
    "loser70 = [train_regret_loser_1[slice70],\n",
    "       train_regret_loser_2[slice70],\n",
    "       train_regret_loser_3[slice70],\n",
    "       train_regret_loser_4[slice70],\n",
    "       train_regret_loser_5[slice70],\n",
    "       train_regret_loser_6[slice70],\n",
    "       train_regret_loser_7[slice70],\n",
    "       train_regret_loser_8[slice70],\n",
    "       train_regret_loser_9[slice70],\n",
    "       train_regret_loser_10[slice70],\n",
    "       train_regret_loser_11[slice70],\n",
    "       train_regret_loser_12[slice70],\n",
    "       train_regret_loser_13[slice70],\n",
    "       train_regret_loser_14[slice70],\n",
    "       train_regret_loser_15[slice70],\n",
    "       train_regret_loser_16[slice70],\n",
    "       train_regret_loser_17[slice70],\n",
    "       train_regret_loser_18[slice70],\n",
    "       train_regret_loser_19[slice70],\n",
    "       train_regret_loser_20[slice70]]\n",
    "\n",
    "winner70 = [train_regret_winner_1[slice70],\n",
    "       train_regret_winner_2[slice70],\n",
    "       train_regret_winner_3[slice70],\n",
    "       train_regret_winner_4[slice70],\n",
    "       train_regret_winner_5[slice70],\n",
    "       train_regret_winner_6[slice70],\n",
    "       train_regret_winner_7[slice70],\n",
    "       train_regret_winner_8[slice70],\n",
    "       train_regret_winner_9[slice70],\n",
    "       train_regret_winner_10[slice70],\n",
    "       train_regret_winner_11[slice70],\n",
    "       train_regret_winner_12[slice70],\n",
    "       train_regret_winner_13[slice70],\n",
    "       train_regret_winner_14[slice70],\n",
    "       train_regret_winner_15[slice70],\n",
    "       train_regret_winner_16[slice70],\n",
    "       train_regret_winner_17[slice70],\n",
    "       train_regret_winner_18[slice70],\n",
    "       train_regret_winner_19[slice70],\n",
    "       train_regret_winner_20[slice70]]\n",
    "\n",
    "loser70_results = pd.DataFrame(loser70).sort_values(by=[0], ascending=False)\n",
    "winner70_results = pd.DataFrame(winner70).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser70 = np.asarray(loser70_results[4:5][0])[0]\n",
    "median_loser70 = np.asarray(loser70_results[9:10][0])[0]\n",
    "upper_loser70 = np.asarray(loser70_results[14:15][0])[0]\n",
    "\n",
    "lower_winner70 = np.asarray(winner70_results[4:5][0])[0]\n",
    "median_winner70 = np.asarray(winner70_results[9:10][0])[0]\n",
    "upper_winner70 = np.asarray(winner70_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration80 :\n",
    "\n",
    "slice80 = 79\n",
    "\n",
    "loser80 = [train_regret_loser_1[slice80],\n",
    "       train_regret_loser_2[slice80],\n",
    "       train_regret_loser_3[slice80],\n",
    "       train_regret_loser_4[slice80],\n",
    "       train_regret_loser_5[slice80],\n",
    "       train_regret_loser_6[slice80],\n",
    "       train_regret_loser_7[slice80],\n",
    "       train_regret_loser_8[slice80],\n",
    "       train_regret_loser_9[slice80],\n",
    "       train_regret_loser_10[slice80],\n",
    "       train_regret_loser_11[slice80],\n",
    "       train_regret_loser_12[slice80],\n",
    "       train_regret_loser_13[slice80],\n",
    "       train_regret_loser_14[slice80],\n",
    "       train_regret_loser_15[slice80],\n",
    "       train_regret_loser_16[slice80],\n",
    "       train_regret_loser_17[slice80],\n",
    "       train_regret_loser_18[slice80],\n",
    "       train_regret_loser_19[slice80],\n",
    "       train_regret_loser_20[slice80]]\n",
    "\n",
    "winner80 = [train_regret_winner_1[slice80],\n",
    "       train_regret_winner_2[slice80],\n",
    "       train_regret_winner_3[slice80],\n",
    "       train_regret_winner_4[slice80],\n",
    "       train_regret_winner_5[slice80],\n",
    "       train_regret_winner_6[slice80],\n",
    "       train_regret_winner_7[slice80],\n",
    "       train_regret_winner_8[slice80],\n",
    "       train_regret_winner_9[slice80],\n",
    "       train_regret_winner_10[slice80],\n",
    "       train_regret_winner_11[slice80],\n",
    "       train_regret_winner_12[slice80],\n",
    "       train_regret_winner_13[slice80],\n",
    "       train_regret_winner_14[slice80],\n",
    "       train_regret_winner_15[slice80],\n",
    "       train_regret_winner_16[slice80],\n",
    "       train_regret_winner_17[slice80],\n",
    "       train_regret_winner_18[slice80],\n",
    "       train_regret_winner_19[slice80],\n",
    "       train_regret_winner_20[slice80]]\n",
    "\n",
    "loser80_results = pd.DataFrame(loser80).sort_values(by=[0], ascending=False)\n",
    "winner80_results = pd.DataFrame(winner80).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser80 = np.asarray(loser80_results[4:5][0])[0]\n",
    "median_loser80 = np.asarray(loser80_results[9:10][0])[0]\n",
    "upper_loser80 = np.asarray(loser80_results[14:15][0])[0]\n",
    "\n",
    "lower_winner80 = np.asarray(winner80_results[4:5][0])[0]\n",
    "median_winner80 = np.asarray(winner80_results[9:10][0])[0]\n",
    "upper_winner80 = np.asarray(winner80_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration90 :\n",
    "\n",
    "slice90 = 89\n",
    "\n",
    "loser90 = [train_regret_loser_1[slice90],\n",
    "       train_regret_loser_2[slice90],\n",
    "       train_regret_loser_3[slice90],\n",
    "       train_regret_loser_4[slice90],\n",
    "       train_regret_loser_5[slice90],\n",
    "       train_regret_loser_6[slice90],\n",
    "       train_regret_loser_7[slice90],\n",
    "       train_regret_loser_8[slice90],\n",
    "       train_regret_loser_9[slice90],\n",
    "       train_regret_loser_10[slice90],\n",
    "       train_regret_loser_11[slice90],\n",
    "       train_regret_loser_12[slice90],\n",
    "       train_regret_loser_13[slice90],\n",
    "       train_regret_loser_14[slice90],\n",
    "       train_regret_loser_15[slice90],\n",
    "       train_regret_loser_16[slice90],\n",
    "       train_regret_loser_17[slice90],\n",
    "       train_regret_loser_18[slice90],\n",
    "       train_regret_loser_19[slice90],\n",
    "       train_regret_loser_20[slice90]]\n",
    "\n",
    "winner90 = [train_regret_winner_1[slice90],\n",
    "       train_regret_winner_2[slice90],\n",
    "       train_regret_winner_3[slice90],\n",
    "       train_regret_winner_4[slice90],\n",
    "       train_regret_winner_5[slice90],\n",
    "       train_regret_winner_6[slice90],\n",
    "       train_regret_winner_7[slice90],\n",
    "       train_regret_winner_8[slice90],\n",
    "       train_regret_winner_9[slice90],\n",
    "       train_regret_winner_10[slice90],\n",
    "       train_regret_winner_11[slice90],\n",
    "       train_regret_winner_12[slice90],\n",
    "       train_regret_winner_13[slice90],\n",
    "       train_regret_winner_14[slice90],\n",
    "       train_regret_winner_15[slice90],\n",
    "       train_regret_winner_16[slice90],\n",
    "       train_regret_winner_17[slice90],\n",
    "       train_regret_winner_18[slice90],\n",
    "       train_regret_winner_19[slice90],\n",
    "       train_regret_winner_20[slice90]]\n",
    "\n",
    "loser90_results = pd.DataFrame(loser90).sort_values(by=[0], ascending=False)\n",
    "winner90_results = pd.DataFrame(winner90).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser90 = np.asarray(loser90_results[4:5][0])[0]\n",
    "median_loser90 = np.asarray(loser90_results[9:10][0])[0]\n",
    "upper_loser90 = np.asarray(loser90_results[14:15][0])[0]\n",
    "\n",
    "lower_winner90 = np.asarray(winner90_results[4:5][0])[0]\n",
    "median_winner90 = np.asarray(winner90_results[9:10][0])[0]\n",
    "upper_winner90 = np.asarray(winner90_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration100 :\n",
    "\n",
    "slice100 = 99\n",
    "\n",
    "loser100 = [train_regret_loser_1[slice100],\n",
    "       train_regret_loser_2[slice100],\n",
    "       train_regret_loser_3[slice100],\n",
    "       train_regret_loser_4[slice100],\n",
    "       train_regret_loser_5[slice100],\n",
    "       train_regret_loser_6[slice100],\n",
    "       train_regret_loser_7[slice100],\n",
    "       train_regret_loser_8[slice100],\n",
    "       train_regret_loser_9[slice100],\n",
    "       train_regret_loser_10[slice100],\n",
    "       train_regret_loser_11[slice100],\n",
    "       train_regret_loser_12[slice100],\n",
    "       train_regret_loser_13[slice100],\n",
    "       train_regret_loser_14[slice100],\n",
    "       train_regret_loser_15[slice100],\n",
    "       train_regret_loser_16[slice100],\n",
    "       train_regret_loser_17[slice100],\n",
    "       train_regret_loser_18[slice100],\n",
    "       train_regret_loser_19[slice100],\n",
    "       train_regret_loser_20[slice100]]\n",
    "\n",
    "winner100 = [train_regret_winner_1[slice100],\n",
    "       train_regret_winner_2[slice100],\n",
    "       train_regret_winner_3[slice100],\n",
    "       train_regret_winner_4[slice100],\n",
    "       train_regret_winner_5[slice100],\n",
    "       train_regret_winner_6[slice100],\n",
    "       train_regret_winner_7[slice100],\n",
    "       train_regret_winner_8[slice100],\n",
    "       train_regret_winner_9[slice100],\n",
    "       train_regret_winner_10[slice100],\n",
    "       train_regret_winner_11[slice100],\n",
    "       train_regret_winner_12[slice100],\n",
    "       train_regret_winner_13[slice100],\n",
    "       train_regret_winner_14[slice100],\n",
    "       train_regret_winner_15[slice100],\n",
    "       train_regret_winner_16[slice100],\n",
    "       train_regret_winner_17[slice100],\n",
    "       train_regret_winner_18[slice100],\n",
    "       train_regret_winner_19[slice100],\n",
    "       train_regret_winner_20[slice100]]\n",
    "\n",
    "loser100_results = pd.DataFrame(loser100).sort_values(by=[0], ascending=False)\n",
    "winner100_results = pd.DataFrame(winner100).sort_values(by=[0], ascending=False)\n",
    "\n",
    "### Best training regret minimization IQR - loser:\n",
    "lower_loser100 = np.asarray(loser100_results[4:5][0])[0]\n",
    "median_loser100 = np.asarray(loser100_results[9:10][0])[0]\n",
    "upper_loser100 = np.asarray(loser100_results[14:15][0])[0]\n",
    "\n",
    "lower_winner100 = np.asarray(winner100_results[4:5][0])[0]\n",
    "median_winner100 = np.asarray(winner100_results[9:10][0])[0]\n",
    "upper_winner100 = np.asarray(winner100_results[14:15][0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summarise arrays: 'Loser'\n",
    "\n",
    "lower_loser = [lower_loser1,\n",
    "            lower_loser2,\n",
    "            lower_loser3,\n",
    "            lower_loser4,\n",
    "            lower_loser5,\n",
    "            lower_loser6,\n",
    "            lower_loser7,\n",
    "            lower_loser8,\n",
    "            lower_loser9,\n",
    "            lower_loser10,\n",
    "            lower_loser11,\n",
    "            lower_loser12,\n",
    "            lower_loser13,\n",
    "            lower_loser14,\n",
    "            lower_loser15,\n",
    "            lower_loser16,\n",
    "            lower_loser17,\n",
    "            lower_loser18,\n",
    "            lower_loser19,\n",
    "            lower_loser20,\n",
    "            lower_loser21,\n",
    "            lower_loser22,\n",
    "            lower_loser23,\n",
    "            lower_loser24,\n",
    "            lower_loser25,\n",
    "            lower_loser26,\n",
    "            lower_loser27,\n",
    "            lower_loser28,\n",
    "            lower_loser29,\n",
    "            lower_loser30,\n",
    "            lower_loser31,\n",
    "            lower_loser32,\n",
    "            lower_loser33,\n",
    "            lower_loser34,\n",
    "            lower_loser35,\n",
    "            lower_loser36,\n",
    "            lower_loser37,\n",
    "            lower_loser38,\n",
    "            lower_loser39,\n",
    "            lower_loser40,\n",
    "            lower_loser41,\n",
    "            lower_loser42,\n",
    "            lower_loser43,\n",
    "            lower_loser44,\n",
    "            lower_loser45,\n",
    "            lower_loser46,\n",
    "            lower_loser47,\n",
    "            lower_loser48,\n",
    "            lower_loser49,\n",
    "            lower_loser50,\n",
    "            lower_loser51,\n",
    "            lower_loser52,\n",
    "            lower_loser53,\n",
    "            lower_loser54,\n",
    "            lower_loser55,\n",
    "            lower_loser56,\n",
    "            lower_loser57,\n",
    "            lower_loser58,\n",
    "            lower_loser59,\n",
    "            lower_loser60,\n",
    "            lower_loser61,\n",
    "            lower_loser62,\n",
    "            lower_loser63,\n",
    "            lower_loser64,\n",
    "            lower_loser65,\n",
    "            lower_loser66,\n",
    "            lower_loser67,\n",
    "            lower_loser68,\n",
    "            lower_loser69,\n",
    "            lower_loser70,\n",
    "            lower_loser71,\n",
    "            lower_loser72,\n",
    "            lower_loser73,\n",
    "            lower_loser74,\n",
    "            lower_loser75,\n",
    "            lower_loser76,\n",
    "            lower_loser77,\n",
    "            lower_loser78,\n",
    "            lower_loser79,\n",
    "            lower_loser80,\n",
    "            lower_loser81,\n",
    "            lower_loser82,\n",
    "            lower_loser83,\n",
    "            lower_loser84,\n",
    "            lower_loser85,\n",
    "            lower_loser86,\n",
    "            lower_loser87,\n",
    "            lower_loser88,\n",
    "            lower_loser89,\n",
    "            lower_loser90,\n",
    "            lower_loser91,\n",
    "            lower_loser92,\n",
    "            lower_loser93,\n",
    "            lower_loser94,\n",
    "            lower_loser95,\n",
    "            lower_loser96,\n",
    "            lower_loser97,\n",
    "            lower_loser98,\n",
    "            lower_loser99,\n",
    "            lower_loser100,\n",
    "            lower_loser101]\n",
    "\n",
    "median_loser = [median_loser1,\n",
    "            median_loser2,\n",
    "            median_loser3,\n",
    "            median_loser4,\n",
    "            median_loser5,\n",
    "            median_loser6,\n",
    "            median_loser7,\n",
    "            median_loser8,\n",
    "            median_loser9,\n",
    "            median_loser10,\n",
    "            median_loser11,\n",
    "            median_loser12,\n",
    "            median_loser13,\n",
    "            median_loser14,\n",
    "            median_loser15,\n",
    "            median_loser16,\n",
    "            median_loser17,\n",
    "            median_loser18,\n",
    "            median_loser19,\n",
    "            median_loser20,\n",
    "            median_loser21,\n",
    "            median_loser22,\n",
    "            median_loser23,\n",
    "            median_loser24,\n",
    "            median_loser25,\n",
    "            median_loser26,\n",
    "            median_loser27,\n",
    "            median_loser28,\n",
    "            median_loser29,\n",
    "            median_loser30,\n",
    "            median_loser31,\n",
    "            median_loser32,\n",
    "            median_loser33,\n",
    "            median_loser34,\n",
    "            median_loser35,\n",
    "            median_loser36,\n",
    "            median_loser37,\n",
    "            median_loser38,\n",
    "            median_loser39,\n",
    "            median_loser40,\n",
    "            median_loser41,\n",
    "            median_loser42,\n",
    "            median_loser43,\n",
    "            median_loser44,\n",
    "            median_loser45,\n",
    "            median_loser46,\n",
    "            median_loser47,\n",
    "            median_loser48,\n",
    "            median_loser49,\n",
    "            median_loser50,\n",
    "            median_loser51,\n",
    "            median_loser52,\n",
    "            median_loser53,\n",
    "            median_loser54,\n",
    "            median_loser55,\n",
    "            median_loser56,\n",
    "            median_loser57,\n",
    "            median_loser58,\n",
    "            median_loser59,\n",
    "            median_loser60,\n",
    "            median_loser61,\n",
    "            median_loser62,\n",
    "            median_loser63,\n",
    "            median_loser64,\n",
    "            median_loser65,\n",
    "            median_loser66,\n",
    "            median_loser67,\n",
    "            median_loser68,\n",
    "            median_loser69,\n",
    "            median_loser70,\n",
    "            median_loser71,\n",
    "            median_loser72,\n",
    "            median_loser73,\n",
    "            median_loser74,\n",
    "            median_loser75,\n",
    "            median_loser76,\n",
    "            median_loser77,\n",
    "            median_loser78,\n",
    "            median_loser79,\n",
    "            median_loser80,\n",
    "            median_loser81,\n",
    "            median_loser82,\n",
    "            median_loser83,\n",
    "            median_loser84,\n",
    "            median_loser85,\n",
    "            median_loser86,\n",
    "            median_loser87,\n",
    "            median_loser88,\n",
    "            median_loser89,\n",
    "            median_loser90,\n",
    "            median_loser91,\n",
    "            median_loser92,\n",
    "            median_loser93,\n",
    "            median_loser94,\n",
    "            median_loser95,\n",
    "            median_loser96,\n",
    "            median_loser97,\n",
    "            median_loser98,\n",
    "            median_loser99,\n",
    "            median_loser100,\n",
    "            median_loser101]\n",
    "\n",
    "upper_loser = [upper_loser1,\n",
    "            upper_loser2,\n",
    "            upper_loser3,\n",
    "            upper_loser4,\n",
    "            upper_loser5,\n",
    "            upper_loser6,\n",
    "            upper_loser7,\n",
    "            upper_loser8,\n",
    "            upper_loser9,\n",
    "            upper_loser10,\n",
    "            upper_loser11,\n",
    "            upper_loser12,\n",
    "            upper_loser13,\n",
    "            upper_loser14,\n",
    "            upper_loser15,\n",
    "            upper_loser16,\n",
    "            upper_loser17,\n",
    "            upper_loser18,\n",
    "            upper_loser19,\n",
    "            upper_loser20,\n",
    "            upper_loser21,\n",
    "            upper_loser22,\n",
    "            upper_loser23,\n",
    "            upper_loser24,\n",
    "            upper_loser25,\n",
    "            upper_loser26,\n",
    "            upper_loser27,\n",
    "            upper_loser28,\n",
    "            upper_loser29,\n",
    "            upper_loser30,\n",
    "            upper_loser31,\n",
    "            upper_loser32,\n",
    "            upper_loser33,\n",
    "            upper_loser34,\n",
    "            upper_loser35,\n",
    "            upper_loser36,\n",
    "            upper_loser37,\n",
    "            upper_loser38,\n",
    "            upper_loser39,\n",
    "            upper_loser40,\n",
    "            upper_loser41,\n",
    "            upper_loser42,\n",
    "            upper_loser43,\n",
    "            upper_loser44,\n",
    "            upper_loser45,\n",
    "            upper_loser46,\n",
    "            upper_loser47,\n",
    "            upper_loser48,\n",
    "            upper_loser49,\n",
    "            upper_loser50,\n",
    "            upper_loser51,\n",
    "            upper_loser52,\n",
    "            upper_loser53,\n",
    "            upper_loser54,\n",
    "            upper_loser55,\n",
    "            upper_loser56,\n",
    "            upper_loser57,\n",
    "            upper_loser58,\n",
    "            upper_loser59,\n",
    "            upper_loser60,\n",
    "            upper_loser61,\n",
    "            upper_loser62,\n",
    "            upper_loser63,\n",
    "            upper_loser64,\n",
    "            upper_loser65,\n",
    "            upper_loser66,\n",
    "            upper_loser67,\n",
    "            upper_loser68,\n",
    "            upper_loser69,\n",
    "            upper_loser70,\n",
    "            upper_loser71,\n",
    "            upper_loser72,\n",
    "            upper_loser73,\n",
    "            upper_loser74,\n",
    "            upper_loser75,\n",
    "            upper_loser76,\n",
    "            upper_loser77,\n",
    "            upper_loser78,\n",
    "            upper_loser79,\n",
    "            upper_loser80,\n",
    "            upper_loser81,\n",
    "            upper_loser82,\n",
    "            upper_loser83,\n",
    "            upper_loser84,\n",
    "            upper_loser85,\n",
    "            upper_loser86,\n",
    "            upper_loser87,\n",
    "            upper_loser88,\n",
    "            upper_loser89,\n",
    "            upper_loser90,\n",
    "            upper_loser91,\n",
    "            upper_loser92,\n",
    "            upper_loser93,\n",
    "            upper_loser94,\n",
    "            upper_loser95,\n",
    "            upper_loser96,\n",
    "            upper_loser97,\n",
    "            upper_loser98,\n",
    "            upper_loser99,\n",
    "            upper_loser100,\n",
    "            upper_loser101]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summarise arrays: 'Winner'\n",
    "\n",
    "lower_winner = [lower_winner1,\n",
    "            lower_winner2,\n",
    "            lower_winner3,\n",
    "            lower_winner4,\n",
    "            lower_winner5,\n",
    "            lower_winner6,\n",
    "            lower_winner7,\n",
    "            lower_winner8,\n",
    "            lower_winner9,\n",
    "            lower_winner10,\n",
    "            lower_winner11,\n",
    "            lower_winner12,\n",
    "            lower_winner13,\n",
    "            lower_winner14,\n",
    "            lower_winner15,\n",
    "            lower_winner16,\n",
    "            lower_winner17,\n",
    "            lower_winner18,\n",
    "            lower_winner19,\n",
    "            lower_winner20,\n",
    "            lower_winner21,\n",
    "            lower_winner22,\n",
    "            lower_winner23,\n",
    "            lower_winner24,\n",
    "            lower_winner25,\n",
    "            lower_winner26,\n",
    "            lower_winner27,\n",
    "            lower_winner28,\n",
    "            lower_winner29,\n",
    "            lower_winner30,\n",
    "            lower_winner31,\n",
    "            lower_winner32,\n",
    "            lower_winner33,\n",
    "            lower_winner34,\n",
    "            lower_winner35,\n",
    "            lower_winner36,\n",
    "            lower_winner37,\n",
    "            lower_winner38,\n",
    "            lower_winner39,\n",
    "            lower_winner40,\n",
    "            lower_winner41,\n",
    "            lower_winner42,\n",
    "            lower_winner43,\n",
    "            lower_winner44,\n",
    "            lower_winner45,\n",
    "            lower_winner46,\n",
    "            lower_winner47,\n",
    "            lower_winner48,\n",
    "            lower_winner49,\n",
    "            lower_winner50,\n",
    "            lower_winner51,\n",
    "            lower_winner52,\n",
    "            lower_winner53,\n",
    "            lower_winner54,\n",
    "            lower_winner55,\n",
    "            lower_winner56,\n",
    "            lower_winner57,\n",
    "            lower_winner58,\n",
    "            lower_winner59,\n",
    "            lower_winner60,\n",
    "            lower_winner61,\n",
    "            lower_winner62,\n",
    "            lower_winner63,\n",
    "            lower_winner64,\n",
    "            lower_winner65,\n",
    "            lower_winner66,\n",
    "            lower_winner67,\n",
    "            lower_winner68,\n",
    "            lower_winner69,\n",
    "            lower_winner70,\n",
    "            lower_winner71,\n",
    "            lower_winner72,\n",
    "            lower_winner73,\n",
    "            lower_winner74,\n",
    "            lower_winner75,\n",
    "            lower_winner76,\n",
    "            lower_winner77,\n",
    "            lower_winner78,\n",
    "            lower_winner79,\n",
    "            lower_winner80,\n",
    "            lower_winner81,\n",
    "            lower_winner82,\n",
    "            lower_winner83,\n",
    "            lower_winner84,\n",
    "            lower_winner85,\n",
    "            lower_winner86,\n",
    "            lower_winner87,\n",
    "            lower_winner88,\n",
    "            lower_winner89,\n",
    "            lower_winner90,\n",
    "            lower_winner91,\n",
    "            lower_winner92,\n",
    "            lower_winner93,\n",
    "            lower_winner94,\n",
    "            lower_winner95,\n",
    "            lower_winner96,\n",
    "            lower_winner97,\n",
    "            lower_winner98,\n",
    "            lower_winner99,\n",
    "            lower_winner100,\n",
    "            lower_winner101]\n",
    "\n",
    "median_winner = [median_winner1,\n",
    "            median_winner2,\n",
    "            median_winner3,\n",
    "            median_winner4,\n",
    "            median_winner5,\n",
    "            median_winner6,\n",
    "            median_winner7,\n",
    "            median_winner8,\n",
    "            median_winner9,\n",
    "            median_winner10,\n",
    "            median_winner11,\n",
    "            median_winner12,\n",
    "            median_winner13,\n",
    "            median_winner14,\n",
    "            median_winner15,\n",
    "            median_winner16,\n",
    "            median_winner17,\n",
    "            median_winner18,\n",
    "            median_winner19,\n",
    "            median_winner20,\n",
    "            median_winner21,\n",
    "            median_winner22,\n",
    "            median_winner23,\n",
    "            median_winner24,\n",
    "            median_winner25,\n",
    "            median_winner26,\n",
    "            median_winner27,\n",
    "            median_winner28,\n",
    "            median_winner29,\n",
    "            median_winner30,\n",
    "            median_winner31,\n",
    "            median_winner32,\n",
    "            median_winner33,\n",
    "            median_winner34,\n",
    "            median_winner35,\n",
    "            median_winner36,\n",
    "            median_winner37,\n",
    "            median_winner38,\n",
    "            median_winner39,\n",
    "            median_winner40,\n",
    "            median_winner41,\n",
    "            median_winner42,\n",
    "            median_winner43,\n",
    "            median_winner44,\n",
    "            median_winner45,\n",
    "            median_winner46,\n",
    "            median_winner47,\n",
    "            median_winner48,\n",
    "            median_winner49,\n",
    "            median_winner50,\n",
    "            median_winner51,\n",
    "            median_winner52,\n",
    "            median_winner53,\n",
    "            median_winner54,\n",
    "            median_winner55,\n",
    "            median_winner56,\n",
    "            median_winner57,\n",
    "            median_winner58,\n",
    "            median_winner59,\n",
    "            median_winner60,\n",
    "            median_winner61,\n",
    "            median_winner62,\n",
    "            median_winner63,\n",
    "            median_winner64,\n",
    "            median_winner65,\n",
    "            median_winner66,\n",
    "            median_winner67,\n",
    "            median_winner68,\n",
    "            median_winner69,\n",
    "            median_winner70,\n",
    "            median_winner71,\n",
    "            median_winner72,\n",
    "            median_winner73,\n",
    "            median_winner74,\n",
    "            median_winner75,\n",
    "            median_winner76,\n",
    "            median_winner77,\n",
    "            median_winner78,\n",
    "            median_winner79,\n",
    "            median_winner80,\n",
    "            median_winner81,\n",
    "            median_winner82,\n",
    "            median_winner83,\n",
    "            median_winner84,\n",
    "            median_winner85,\n",
    "            median_winner86,\n",
    "            median_winner87,\n",
    "            median_winner88,\n",
    "            median_winner89,\n",
    "            median_winner90,\n",
    "            median_winner91,\n",
    "            median_winner92,\n",
    "            median_winner93,\n",
    "            median_winner94,\n",
    "            median_winner95,\n",
    "            median_winner96,\n",
    "            median_winner97,\n",
    "            median_winner98,\n",
    "            median_winner99,\n",
    "            median_winner100,\n",
    "            median_winner101]\n",
    "\n",
    "upper_winner = [upper_winner1,\n",
    "            upper_winner2,\n",
    "            upper_winner3,\n",
    "            upper_winner4,\n",
    "            upper_winner5,\n",
    "            upper_winner6,\n",
    "            upper_winner7,\n",
    "            upper_winner8,\n",
    "            upper_winner9,\n",
    "            upper_winner10,\n",
    "            upper_winner11,\n",
    "            upper_winner12,\n",
    "            upper_winner13,\n",
    "            upper_winner14,\n",
    "            upper_winner15,\n",
    "            upper_winner16,\n",
    "            upper_winner17,\n",
    "            upper_winner18,\n",
    "            upper_winner19,\n",
    "            upper_winner20,\n",
    "            upper_winner21,\n",
    "            upper_winner22,\n",
    "            upper_winner23,\n",
    "            upper_winner24,\n",
    "            upper_winner25,\n",
    "            upper_winner26,\n",
    "            upper_winner27,\n",
    "            upper_winner28,\n",
    "            upper_winner29,\n",
    "            upper_winner30,\n",
    "            upper_winner31,\n",
    "            upper_winner32,\n",
    "            upper_winner33,\n",
    "            upper_winner34,\n",
    "            upper_winner35,\n",
    "            upper_winner36,\n",
    "            upper_winner37,\n",
    "            upper_winner38,\n",
    "            upper_winner39,\n",
    "            upper_winner40,\n",
    "            upper_winner41,\n",
    "            upper_winner42,\n",
    "            upper_winner43,\n",
    "            upper_winner44,\n",
    "            upper_winner45,\n",
    "            upper_winner46,\n",
    "            upper_winner47,\n",
    "            upper_winner48,\n",
    "            upper_winner49,\n",
    "            upper_winner50,\n",
    "            upper_winner51,\n",
    "            upper_winner52,\n",
    "            upper_winner53,\n",
    "            upper_winner54,\n",
    "            upper_winner55,\n",
    "            upper_winner56,\n",
    "            upper_winner57,\n",
    "            upper_winner58,\n",
    "            upper_winner59,\n",
    "            upper_winner60,\n",
    "            upper_winner61,\n",
    "            upper_winner62,\n",
    "            upper_winner63,\n",
    "            upper_winner64,\n",
    "            upper_winner65,\n",
    "            upper_winner66,\n",
    "            upper_winner67,\n",
    "            upper_winner68,\n",
    "            upper_winner69,\n",
    "            upper_winner70,\n",
    "            upper_winner71,\n",
    "            upper_winner72,\n",
    "            upper_winner73,\n",
    "            upper_winner74,\n",
    "            upper_winner75,\n",
    "            upper_winner76,\n",
    "            upper_winner77,\n",
    "            upper_winner78,\n",
    "            upper_winner79,\n",
    "            upper_winner80,\n",
    "            upper_winner81,\n",
    "            upper_winner82,\n",
    "            upper_winner83,\n",
    "            upper_winner84,\n",
    "            upper_winner85,\n",
    "            upper_winner86,\n",
    "            upper_winner87,\n",
    "            upper_winner88,\n",
    "            upper_winner89,\n",
    "            upper_winner90,\n",
    "            upper_winner91,\n",
    "            upper_winner92,\n",
    "            upper_winner93,\n",
    "            upper_winner94,\n",
    "            upper_winner95,\n",
    "            upper_winner96,\n",
    "            upper_winner97,\n",
    "            upper_winner98,\n",
    "            upper_winner99,\n",
    "            upper_winner100,\n",
    "            upper_winner101]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Visualise!\n",
    "\n",
    "title = obj_func\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(median_loser, color = 'Red')\n",
    "plt.plot(median_winner, color = 'Blue')\n",
    "\n",
    "xstar = np.arange(0, max_iter+1, step=1)\n",
    "plt.fill_between(xstar, lower_loser, upper_loser, facecolor = 'Red', alpha=0.4, label='GP ERM Regret: IQR')\n",
    "plt.fill_between(xstar, lower_winner, upper_winner, facecolor = 'Blue', alpha=0.4, label='STP ERM Regret: IQR ' r'($\\nu$' ' = {})'.format(df))\n",
    "\n",
    "plt.title(title, weight = 'bold', family = 'Arial')\n",
    "plt.xlabel('(Post-initialization) iteration $\\it{k}$', weight = 'bold', family = 'Arial') # x-axis label\n",
    "plt.ylabel('ln(Regret)', weight = 'bold', family = 'Arial') # y-axis label\n",
    "plt.legend(loc=0) # add plot legend\n",
    "\n",
    "plt.show() #visualise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
